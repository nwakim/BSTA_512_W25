[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSTA 512/612: Linear Models",
    "section": "",
    "text": "BSTA 512/612: Linear Models\n\nWinter 2025\n \nWelcome to BSTA 512/612! In this course, we will focus on linear models, and build our understanding of regression analysis. We will build some theoretical understanding in order to interpret and apply regression models appropriately. We will learn how to build a regression model, interpret the model, and diagnose potential issues with our model.  \n\n\n\n\n\n\n\n \n\n\n\n\n\n OneDrive Folder\n\n\n Class Zoom link\n\n\n\n\n\n\n\nInstructor\n Dr. Nicky Wakim\n Vanport 622A\n wakim@ohsu.edu\n Office Hours TBD\n\n\n\nTA Office Hours\n\n\nKatie\nLiv\nMiyuki\n\n TBD\n TBD\n TBD\n\n\n\n\n\nCourse details\n Mondays, Wednesdays\n Jan 6 - March 17\n 1:00 PM - 2:50 PM\n In-person, RPV 1215\n\n\n\nContacting me\nE-mail is the best way to get in contact with me. I will try to respond to all course-related e-mails within 24 hours Monday-Friday.\n\n\n\n\n\n\n\n\n\n View the source on GitHub"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Homework",
    "section": "",
    "text": "Homework\nAssignment\nAssignment due (@11pm)\nAnswers\nSolutions (.qmd)\nSolutions (.html)*\n\n\n\n\n0\n\n1/9\n\n\n\n\n\n1\n\n1/23\n\n\n\n\n\n2\n\n1/30\n\n\n\n\n\n3\n\n2/13\n\n\n\n\n\n4\n\n2/20\n\n\n\n\n\n5\n\n3/6\n*Please note that you need to download the .html file to see the LaTeX math properly."
  },
  {
    "objectID": "homework.html#file-naming",
    "href": "homework.html#file-naming",
    "title": "Homework",
    "section": "File Naming",
    "text": "File Naming\n\nFor HW Assignments, please use the following file naming: “Lastname_Firstinitial_HW0.qmd” and “Lastname_Firstinitial_HW0.html”"
  },
  {
    "objectID": "homework.html#rubrics",
    "href": "homework.html#rubrics",
    "title": "Homework",
    "section": "Rubrics",
    "text": "Rubrics\n\n\n\n\n\n\n\n\n\n1 point\n0 points\n\n\n\n\nCompletion\n75% of the question parts are thoroughly worked out and have an answer. “Question parts” means the sub-questions labeled “Part _”\nLess than 75% of question parts are thoroughly worked out. Attempts do not count as thoroughly worked out."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nDate\nLes-son\nTopic\nKey Info\nSlides QMD\nSlides PDF\nSlides Notes\nExit tix\nRecord-ing\nMuddy Points\n\n\n\n\n1\n1/6\n0\nWelcome\n\n\n\n\n\n\n\n\n\n\n\n1\nReview\n\n\n\n\n\n\n\n\n\n\n1/8\n2\nData Management\n\n\n\n\n\n\n\n\n\n\n1/9\n\nHW 0 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\n1/13\n3\nSimple Linear Regression\n\n\n\n\n\n\n\n\n\n\n1/15\n4\nSLR: Inference and Prediction\n\n\n\n\n\n\n\n\n\n\n1/16\n\nLab 1 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n1/20\n\nNo Class, MLKJ Day\n\n\n\n\n\n\n\n\n\n\n1/22\n5\nSLR: Evaluation\n\n\n\n\n\n\n\n\n\n\n1/23\n\nHW 1 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\n1/27\n6\nSLR: Diagnostics, Part 1\n\n\n\n\n\n\n\n\n\n\n1/29\n7\nSLR: Diagnostics, Part 2\n\n\n\n\n\n\n\n\n\n\n1/30\n\nHW 2 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5\n2/3\n8\nMultiple Linear Regression\n\n\n\n\n\n\n\n\n\n\n2/5\n9\nMLR: Inference\n\n\n\n\n\n\n\n\n\n\n2/6\n\nLab 2 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\n2/10\n10\nCategorical Covariates\n\n\n\n\n\n\n\n\n\n\n2/12\n\nCatch-up day\n\n\n\n\n\n\n\n\n\n\n2/13\n\nHW 3 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\n2/17\n\nNo Class, President’s Day\n\n\n\n\n\n\n\n\n\n\n2/19\n11\nInteractions, Part 1\n\n\n\n\n\n\n\n\n\n\n2/20\n\nHW 4 due 11pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\n2/24\n12\nInteractions, Part 2\n\n\n\n\n\n\n\n\n\n\n2/26\n13\nModel Selection\n\n\n\n\n\n\n\n\n\n\n2/27\n\nLab 3 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\n3/3\n14\nPurposeful Selection\n\n\n\n\n\n\n\n\n\n\n3/5\n15\nMLR: Diagnostics\n\n\n\n\n\n\n\n\n\n\n3/6\n\nHW 5 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10\n3/10\n\nCatch-up day\n\n\n\n\n\n\n\n\n\n\n3/12\n\nCatch-up day\n\n\n\n\n\n\n\n\n\n\n3/13\n\nLab 4 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\n3/17\n18\nProject Day!!\n\n\n\n\n\n\n\n\n\n\n3/17\n\nProject due 11pm"
  },
  {
    "objectID": "syllabus.html#description",
    "href": "syllabus.html#description",
    "title": "BSTA 512/612 Syllabus",
    "section": "Description",
    "text": "Description\nWelcome to BSTA 512/612! In this course, we will focus on linear models, and build our understanding of regression analysis. We will build some theoretical understanding in order to interpret and apply regression models appropriately. We will learn how to build a regression model, interpret the model, and diagnose potential issues with our model.\n\nCourse Learning Objectives\nAt the end of this course, students should be able to…\n\nAnalyze real-world data to answer questions about multivariable relationships for a continuous outcome\nBuild, fit, and evaluate linear regression models\nAssess whether a proposed model is appropriate and describe its limitations\nUse R and Quarto to write reproducible reports\nCommunicate results from statistical analyses to a general audience\n\nThese learning objectives were adapted from Maria Tackett’s Regression Analysis course."
  },
  {
    "objectID": "syllabus.html#instructors",
    "href": "syllabus.html#instructors",
    "title": "BSTA 512/612 Syllabus",
    "section": "Instructors",
    "text": "Instructors\nHere is the instructor page. This also has office hours!"
  },
  {
    "objectID": "syllabus.html#meeting-times",
    "href": "syllabus.html#meeting-times",
    "title": "BSTA 512/612 Syllabus",
    "section": "Meeting Times",
    "text": "Meeting Times\nMondays          1:00 PM – 2:50 PM PST in RPV 1205/1215\nWednesdays    1:00 PM – 2:50 PM PST in RPV 1205/1215\n\nKnown Exceptions\nWe will be in RPV 1217 on the following days:\n\nWednesday, Feb 5\nMonday, Feb 10\nWednesday, Feb 25\nWednesday, March 5\nMonday, March 10\nWednesday, March 12"
  },
  {
    "objectID": "syllabus.html#materials",
    "href": "syllabus.html#materials",
    "title": "BSTA 512/612 Syllabus",
    "section": "Materials",
    "text": "Materials\n\nTextbooks\nIn lieu of a formally published textbook, we will be referencing the following two online textbooks. Similar to our class, the books integrate R into their lessons.\n\nA Progressive Introduction to Linear Models by Joshua French\nIntroduction to Regression Methods for Public Health Using R by Ramzi W. Nahhas\n\n\nSupplemental Readings (Optional)\n\nAn Introduction to R (free pdf available)\n\n\n\n\nOnline Resources\n\nSakai\nWhile most course materials will be delivered online through this website, assignments will be turned in through Sakai, OHSU’s course management system. I will include a link on this website to the Sakai assignment page. \n\n\nWebex\nWebex software will be used for virtual office hours. To give everyone the best possible experience with Webex, I recommend the following best practices:\n\nPlease stay muted until you want to participate\nDuring office hours, please send a message in chat with your question or with a statement like “I have a question.” This makes sure I or the TA can address everyone’s questions in order. \nI encourage you to attend office hours with your video on. This helps me recognize you, and keep mental notes on what techniques/concepts I emphasize to facilitate your specific understanding. \n\n\n\nPoll Everywhere\nWe will use the Poll Everywhere tool as an interactive feature of the course. Poll Everywhere is a web-based application that allows students to participate by responding via text messages or by visiting a web page on an internet-enabled device (smartphone, tablet, laptop). Instructions will be displayed on-screen. The poll that is embedded within the presentation will update in real time. While there is no cost to use this software, standard text messaging rates will apply if you use your phone. Please make sure that you have a Poll Everywhere account before our first class. You are not required to use your OHSU/PSU email to make an account. \nDuring lectures I will pose questions to the class. These questions are designed to provide real-time feedback to both students and the instructor on how well students are grasping the material. This is meant to be an interactive, learning activity with NO contribution to your grade. Your identity will never be connected to your answers, so I encourage you to answer honestly.\n\n\nPennState STAT 501 Website\nPennState has a class offered to online MS students that has some overlap with our class. They have all their course notes posted on this page. This is a great source if you would like to see class notes with different phrasing.\nNot all of our topics are covered in their notes, but the most important ones are. If you are having trouble finding our course’s concepts on their page, please make ask me at Office Hours, after class, or in a private meeting. I do not explicitly state corresponding sections under our schedule because I believe it is important for you to develop skills involving resources and learning key words that can help you find answers. \n\n\nR: Statistical Computing Software\nStudents will use statistical software to complete homework assignments. Students are required to use R/RStudio for this course. R can be freely downloaded. Helpful documentation on installing R is available. I encourage you to install R prior to attending our first lecture. Please email me if you need help installing R or RStudio.\nYou will need to download the following three things:\n\nR https://www.r-project.org/\nRstudio https://posit.co/download/rstudio-desktop/\nQuarto https://quarto.org/docs/get-started/\n\n\nAdditional R Resources\nYour learning and practicing of R will hopefully not be limited to this course. One of the best aspects of programming in R is that many resources are freely available online. Here are just a few additional resources you may explore beyond this class to continue your training in R.\n\n\nUseful online R resources\n\nR for the rest of us\nStatistical tools for high-throughput data analysis. ggplot2 essentials\nR-bloggers\nStack Overflow for troubleshooting\nR Graphical Manual\nQuick-R. Accessing the power of R\nR for SAS, STATA, and SPSS Users\nggplot2\nLearn R 4 free\nJoin a local R user groups\nLearning Machines\n\n\n\n\nOnline R courses to complement or refresh material from class\n\nR for the rest of us\nCoursera: R programming\nedX: R basics\nData Carpentry: For Biologists\nData Carpentry: For Ecologists\nPsychiatric R\nR coder"
  },
  {
    "objectID": "syllabus.html#assessment",
    "href": "syllabus.html#assessment",
    "title": "BSTA 512/612 Syllabus",
    "section": "Assessment",
    "text": "Assessment\nThe course is structured around the following four components:\n\n\n\n\n\n\n\n\n\nComponent\nModality\nFrequency\nDescription\n\n\nLecture\nIn person\nTwice, Weekly\nCourse content is provided through in-person lectures. Lectures will consist of didactic lessons, interactive examples, and PollEverywhere questions. Sessions will be recorded through Explain Everything and posted to Sakai. Attending or viewing the lecture within 7 days of the original lecture date is mandatory. Class attendance will be taken through an Exit Ticket. If viewing the lecture asynchronously, you must take the Exit Ticket to verify your attendance.\n\n\nHomework\nOnline\nWeekly\nThe course includes 7 homework assignments. They are an opportunity for you to engage with important concepts, practice coding, and apply calculating skills. Homework assignments should be submitted online, and will be graded by me. Students are encouraged to work in groups for homework assignments, but each person should do their own summary and hand in their work. Homework assignments will be due on Thursday at 11 PM.\n\n\nProject (Labs and Poster)\nOnline\n4 labs, 1 final poster\nThe project will be a combination of submitted labs that will span the quarter and one final poster submitted at the end of the quarter. This is meant to translate the tools learned in the course to the work one may do in the workforce. This will help instill the procedure for shaping research goals, model selection, analyzing data, and interpreting meaningful results. Labs will guide you through the needed analysis and background for the project. The final poster will summarize your work over the labs by giving context and results to your research question. Students will work independently on each lab.\n\n\n\n\nTypes of assessments\nThis class will use a combination of formative and summative assessments to build and test our knowledge. Below I define each of these types of assessments:\n\nFormative assessment: Activity or work meant to help students learn and practice. Feedback on these assessments are meant to help the instructor and student identify gaps in knowledge and highlight accomplishments.\nSummative assessment: Work meant to test how well students have achieved learning objectives. Grading of these assessments are meant to gauge how well a student grasps the learning objectives and will be able to use their knowledge outside of the classroom."
  },
  {
    "objectID": "syllabus.html#assessment-breakdown",
    "href": "syllabus.html#assessment-breakdown",
    "title": "BSTA 512/612 Syllabus",
    "section": "Assessment Breakdown",
    "text": "Assessment Breakdown\n\nGrading & Requirements\nLetter grades will be assigned roughly according to the following scheme: A (&gt;=93%), A- (90-92%), B+ (88-89%), B(83-87%), B- (82-80%), C+(78-79%), C(73-77%), C- (70-72%), D (60 – 69%), F(&lt;60%).\nGrades will be based on homework assignments, midterm exam, class “attendance”, and final exam, as follows:\n\n\n\n\n\n\n\n\n\n\nCourse activity\nType of Assessment\nDue Date\nPercentage of final grade (BSTA 512)\nPercentage of final grade (BSTA 612)\n\n\nHomework\nFormative\nEvery 1-2 weeks\n48%\n43%\n\n\nProject Labs\nFormative/summative\nEvery 1-2 weeks\n35%\n35%\n\n\nProject Poster and Presentation\nSummative\n3/17\n10%\n10%\n\n\nExit tickets (Attendance)\nN/A\nTwice Weekly\n5%\n5%\n\n\nMid-Quarter Feedback\nN/A\nTBD\n2%\n2%\n\n\n612 Readings\nFormative\nApprox. every other week\n0%\n5%\n\n\n\n\n\nHomework grading\nNo student has the same amount of time available to dedicate to homework. This class may not be a priority to you, you may be taking several other courses, or you may need to dedicate time to other activities. Homeworks are formative assessments, meaning its purpose is to help you learn and practice. To reduce the pressure on you to have perfect or complete homework, I have a very simple grading policy: Your homework will be given a check mark if you turn in 75% of the questions parts completed (whether or not the 75% is correct). I highly encourage you to stay up-to-date with the homeworks and put in as much effort as you can. This will be the most helpful work in this class!\nIf you turn in the homework on time, the TAs will give you feedback (on one or more complete problems). There is no penalty for turning in the homework late, but you will not get feedback on your work. Please make sure to check the solutions or go to office hours to assess your work.\n\n\nViewing Grades in Sakai\nPoints you receive for graded activities will be posted to the Sakai Gradebook. Click on the Gradebook link on the left navigation to view your points."
  },
  {
    "objectID": "syllabus.html#course-instructor-evaluations",
    "href": "syllabus.html#course-instructor-evaluations",
    "title": "BSTA 512/612 Syllabus",
    "section": "Course & Instructor Evaluations",
    "text": "Course & Instructor Evaluations\n\nOngoing Course Feedback\nThroughout the duration of the course, you are also welcome to informally and anonymously submit your feedback through this Microsoft Form or Class Exit Tickets. This form will be available on Sakai. Students can submit feedback at any time and this form will be reviewed regularly by me. Your responses will be anonymous unless you elect to leave your email address. If I have done anything to make you feel uncomfortable, please give me feedback so I can change my behavior. Ultimately, this class is for you, and my individual social identity/behavior should not inhibit your learning. Thank you for your help making BSTA 512/612 a more successful class! Examples of ongoing feedback are:\n\nNicky talks a little fast during lecture time. May you speak slower?\nDuring Office Hours, Dr. Wakim made a face when I asked a question. This face made me feel self-conscious about my question.\nDr. W asked me a question about my experience that made me feel like a monolith. Please do not assume I can speak on behalf of my social identity groups.\nThe in-class examples do not make me more interested in the material.\n\n\n\nMid-quarter Feedback\nDuring the middle of the quarter, I will ask you to submit guided, anonymous feedback. Completion of feedback will be count towards your grade. To insure anonymity, I will ask you to sign a separate, written statement that you completed the feedback.\n\n\nFinal Course Feedback\nAt the conclusion of the course, you will be asked to complete a formal online review of the course and the instructor. Your feedback on this University evaluation is critical to improving future student learning in this course as well as providing metrics relevant to the instructor’s career advancement (or lack of). Since our class is on the smaller side, everyone’s participation is needed for feedback to be released."
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "BSTA 512/612 Syllabus",
    "section": "Schedule",
    "text": "Schedule\nPlease refer to the Schedule page. I will make changes to this schedule if we need more or less time on a concept. You do not need to read the corresponding chapters in the textbook for each class."
  },
  {
    "objectID": "syllabus.html#how-to-succeed-in-this-course",
    "href": "syllabus.html#how-to-succeed-in-this-course",
    "title": "BSTA 512/612 Syllabus",
    "section": "How to succeed in this course",
    "text": "How to succeed in this course\nEvery professor has different expectations when assigning certain work or providing certain resources. I want to walk through each class resource and assignment so that you know what you can do to succeed in this class. For resources, I want you to optimize the opportunities to learn. For assignments, I want you to know the strategies that students can use to learn the most and prepare for future exams.\n\nResources\n\n\n\n\n\n\n\n\nResource\nWhat is it?\nHow do I use it?\n\n\nOffice Hours\nBlocks of time a professor or TA dedicates for questions. The teaching staff will be located in a specific room. Several students may enter the space at a time and will ask specific or broad questions. If many students attend office hours, a queue will be created so that students can be served equally.\nThe main use of office hours is to ask questions about an assignment or lecture notes. You are welcome to sit and do homework in office hours. OH are also an informal way of meeting fellow students to collaborate with.\n\n\nLectures and lecture recordings\nTime shared between the professor and students where the professor conveys important class material. Material discussed in lectures include concepts, calculations, code, and examples. Lectures are a mix of presentation of information, working through examples together, interactive activities, and in-class polls.\nStudents should attend lectures in person if possible. You should attempt to understand new material presented by following the presentation slides, taking notes on additional details that may conveyed verbally, and working through examples with the professor. Students are encouraged to ask questions when you don’t understand the material at any point in the lecture.\n\n\nTextbooks\nWritten and published material that explains concepts, steps through calculations, provides examples, and provides practice problems. The listed textbooks is the basis for this course. While I am to cover all topics in class, the textbook provides alternative explanations and additional examples.\nWhile coming to class having read the accompanying textbook chapters helps understanding during class, I do not expect students to have read it. I see the textbook as a good resource if you are struggling with a specific topic after class, in need of an example while working on homework, or want additional practice when studying for the exam.\n\n\nWebsite\nThe course website is designed by me so that you have access to all the course materials in a more organized and flexible way. All resources delivered from me to you will be available on the website. Any assignments turned in will be through Sakai.\nYou can navigate through different course resources and information using the left-side tabs or top navigation bar. Course materials, like lecture notes, homework, data examples, and recordings, can be found under each week’s page under the schedule tab. You can also find the individual resources under the “Course Materials” tab on the left. Links to turn in assignments through Sakai will be given on the website. Please explore the tabs and get a sense of the organization.\n\n\nSakai\nSakai is a learning management system for higher ed. This is the university sanctioned LMS where we will submit assignments.\nYou will turn in assignments through Sakai under the “Submissions” tab. Generally, there will be a link to each assignment on the course website. You can also view your grades under “Gradebook” and links to Webex under “Webex.”\n\n\n\n\n\nAssignments\n\n\n\n\n\n\n\n\n\nAssignment\nType of assessment\nBefore you submit/take it\nAfter it is graded\n\n\nHomework\nFormative\n\nWork out each problem on your own as much as you can\nTalk through problems with a peer\nGo to Office Hours for help\nWrite down work that shows your thought process\nSearch your issue on Stack Exchange/Stack Overflow\n\n\nReview the solutions\nReview your mistakes\nFor solutions that involve writing sentences, check with me or a TA if your answer fits the solution\nGo to Office Hours to ask about your solutions\n\n\n\nProject Labs and Poster\nFormative and Summtive\n\nStart the lab as early as possible\nWork on R coding and check with classmates on work\nCome to Office Hours for help with specific R work\nFor the report, compile your work from the labs, and decide what is important in the analysis.\n\n\nThis will be graded at the end of the semester, so you will not have a chance to interact with my feedback as much\nIf you have questions about your grade, you may email me\nKeep the project paper for future reference\nYou can add this project to your resume!\n\n\n\nClass Exit Tickets\nN/A\n\nBring appropriate electronic device to participate in polls\nComplete the survey during the last 5 minutes of class or after class within 7 days\n\n\nReview muddiest and clearest points from the week\n\n\n\n\nIf you would like any other course resources explained in this format, please request it through the Ongoing Course Feedback."
  },
  {
    "objectID": "syllabus.html#course-policies-and-resources",
    "href": "syllabus.html#course-policies-and-resources",
    "title": "BSTA 512/612 Syllabus",
    "section": "Course Policies and Resources",
    "text": "Course Policies and Resources\n\nLate Work Policy\nI encourage you to make your best effort to submit all assignments on time, but I understand circumstances arise that are beyond our control. Please see this Swansea University’s page on extenuating circumstances for some examples. Not all circumstances are covered here, so please reach out if you have questions. \n\nThe class will end on March 21, 2025. All coursework is expected to be completed by then. If you have extenuating circumstances, and need additional time to complete class assignments, please contact me. Together, we will come up with a plan for completion and to sort out registrar logistics.\nIf you have extenuating circumstances that may jeopardize your ability to do work for several weeks, please contact me. We will come up with a plan to keep you on track in the course and prevent any delay in your education.\nFor homework, there is a due date posted, but you may turn in the assignment any time before the class ends. I will give you the check regardless of when you submit the assignment. However, if you would like feedback on the homework, you must turn it in on time OR email me and the TAs asking for feedback for your late homework.\nFor labs, you will have ONE no-questions-asked, 3-day extension. Please use this wisely! You just need to send me a quick email saying “I am using my no-questions-asked extension for Lab __.”\n\nBeyond the one no-questions-asked, 3-day extension, I ask you to email me directly. You can explain your circumstances and may ask me for an extension, but I won’t necessarily grant one.\n\nIf you have a emergency involving your self, family, pet, friend, classmate, or anything/one deemed important to you, please do not worry about immediately contacting me. We can work something out after your emergency. If I contact you during an emergency, it is only because I am worried, and you do NOT need to respond until you are able. \n\n\n\nRegrade Policy\nIf you think a question was incorrectly graded, first compare your answer to the answer key. If you believe a re-grade would be appropriate, write an email to me containing the question and a short explanation as to why the question(s) was/were incorrectly graded. Deadline: One week after assignments were returned to class (late requests will not be considered).\n\n\nAttendance Policy\nYou are expected to attend class, participate in-class polls, and complete the exit ticket. For students who miss class or need a review, I will make video and audio recordings of lectures available. There are no guarantees against technical or other challenges for the recording availability or quality.\nYou will need to attend at least 12 classes in person. There are 19 classes total, so you are welcome to watch the recordings for up to 7 classes in lieu of in-person attendance. While I want attendance to be a flexible thing, I need to set certain requirements around in-person attendance to align with the school’s policy.\nThis is meant to keep you on track within the course and prevent a pile up of material. Make sure to complete the exit ticket at the end of class to demonstrate attendance.\n\n\nPlagiarism and Attribution\nPlease note that this section has been motivated by Dr. Steven Bedrick’s Course Policies and Grading site for BMI 525. (Note that this is a good example of informal attribution of someone else’s work.)\nIn this class, it is easy to use ChatGPT or other AI tools to solve your homework for you. Many problems follow a basic structure that is especially easy for ChatGPT to solve. In this class, you may use ChatGPT to help with your homework. You may even ask for direct answers. However, there are a few things I do not want you to do:\n\nDo not copy ChatGPT’s answer directly into your homework. Your homework is graded for full credit if you turn it in, in any state, so turning in ChatGPT’s answers is unacceptable. I rather see half-written answers that show what you’re thinking than see a correct answer from ChatGPT.\nDo not stop once ChatGPT answered a question. If it gives an explanation, interact with it! Make sure you understand the thought process of ChatGPT. Try writing out the process to help cement it in your head. Check the answer with what we learn in class.\nDo not use ChatGPT on our quizzes! Hence, you need to really understand how to solve these problems even if you use ChatGPT on the homework.\n\nAt the end of the day, ChatGPT is a resource that will be available to you in a job and outside of school. Thus, we should use it as a tool in school as well! Let me know if ChatGPT helped you understand something! I would love to incorporate it into future classes!\n\n\n\n\n\n\nImportant\n\n\n\nYou can think of this class as assembling a toolbox. When a handyperson starts working for the first time, they need to buy their tools. For their first few jobs, they might need help finding their tools, or remembering which tool is best used for what action. Eventually, they get to know their tools well, and using them appropriately becomes second nature.\nFor now, ChatGPT can help us find and use our tools, but we need to work towards using them as second nature!"
  },
  {
    "objectID": "syllabus.html#course-expectations",
    "href": "syllabus.html#course-expectations",
    "title": "BSTA 512/612 Syllabus",
    "section": "Course Expectations",
    "text": "Course Expectations\n\nInstructor Expectations\nCommitment to your learning and your success\nI believe that everyone has the ability to be successful in this course and I have put a lot of effort into designing the course in a way that maximizes your learning to ensure your success. Please talk to me before or after class or stop by my office if there is anything you want to discuss or about which you are unclear. I want to be supportive of your learning and growth.\nInclusive & supportive learning community\nI believe that learning happens best when we all learn together, as a community. This means creating a space characterized by generous listening, civility, humility, patience, and hospitality. I will attempt to promote a safe climate where we examine content from multiple cultural perspectives, and I will strive to create and maintain a classroom atmosphere in which you feel free to both listen to others and express your views and ask questions to increase your learning.\nOpenness to feedback\nI appreciate straightforward feedback from you regarding how well the class is meeting your needs. Let me know if material is not clear or when its relevance to the student learning outcomes for the course is not apparent. In particular, let me know if you identify bias or stereotyping in my teaching materials as I will seek to continuously improve. Please also let me know if there’s an aspect of the class you find particularly interesting, helpful, or enjoyable!\nResponsiveness\nI will monitor email as well as the discussion board daily and try respond to all messages within 24 hours Monday-Friday.\nClear guidelines and prompt feedback on assignments\nI will provide clear instructions for all assignments, and a grading rubric when applicable. I will provide detailed feedback on your submissions and will update grades promptly in Sakai.\n\n\nStudent Expectations and Resources\nAttend class\nYou are expected to attend all scheduled class meetings synchronously or watch the recording within 7 days. Attendance is taken through exit tickets. If you have issues accessing the poll on a specific day, please let me know. \nParticipate\nI encourage you to participate actively in class and online discussions. I will expect all students, and all instructors, to be respectful of each other’s contributions, whether I agree with them or not. Professional interactions are expected.\nBuild rapport\nIf you find that you have any trouble keeping up with assignments or other aspects of the course, make sure you let me know as early as possible. As you will find, building rapport and effective relationships are key to becoming an effective professional. Make sure that you are proactive in informing me when difficulties arise during the quarter so that I can help you find a solution.\nComplete assignments\nAll assignments for this course will be submitted electronically through Sakai unless otherwise instructed.  I encourage you to make your best effort to submit all assignments on time, but I understand that sometimes circumstances arise that are beyond our control. If you need an extension, please contact me in congruence with the Late Policy.\nSeek help if you need it\nI believe it is important to support the physical and emotional well‐being of my students. If you are experiencing physical or mental health issues, I encourage you to use the resources on campus such as those listed below. If you have a health issue that is affecting your performance or participation in the course, and/or if you need help connecting with these resources, please contact me.\n\nStudent Health and Wellness Center (SHW), Website, 503-494-8665 (OHSU Students only)\nStudent Health and Counseling (SHAC), Website, 503-725-2800\n\nInform your instructor of any accommodations needed\nYou should speak with or email me before or during the first week of classes regarding any special needs. Students seeking academic accommodations should register with the appropriate service under the School policies below.\nSome religious holidays may occur on regularly scheduled class days. Because available class hours are so limited in number, we will have to hold class on all such days. Class video recordings will be available and you are encouraged to engage with the material outside of the regular class time. You are also encouraged to come to office hours with questions from the session.\nCommit to integrity\nAs a student in this course (and at PSU or OHSU) you are expected to maintain high degrees of professionalism, commitment to active learning and participation in this class and also integrity in your behavior in and out of the classroom.\nCheating and other forms of academic misconduct will not be tolerated in this course and will be dealt with firmly. Student academic misconduct refers to behavior that includes plagiarism, cheating on assignments, fabrication of data, falsification of records or official documents, intentional misuse of equipment or materials (including library materials), or aiding and abetting the perpetration of such acts. Preparation of exams, assigned on an individual basis, must represent each student’s own individual effort. When used, resource materials should be cited in conventional reference format."
  },
  {
    "objectID": "syllabus.html#course-communications",
    "href": "syllabus.html#course-communications",
    "title": "BSTA 512/612 Syllabus",
    "section": "Course Communications",
    "text": "Course Communications\nSakai announcements\nFor important/urgent matters, I will communicate with you using announcements via Sakai that will be delivered to your OHSU Email account as well as displayed in the Sakai course site Announcements section.\nE-mail\nE-mail should be used only for messages that are private in nature. Please send private messages to my OHSU email address (wakim@ohsu.edu). Messages sent through Sakai Inbox will not be answered."
  },
  {
    "objectID": "syllabus.html#further-student-resources",
    "href": "syllabus.html#further-student-resources",
    "title": "BSTA 512/612 Syllabus",
    "section": "Further Student Resources",
    "text": "Further Student Resources\n\nSPH Writing Lab\nThe School of Public Health Writing Support serves graduate students (master’s and PhD) in SPH, offering help on all professional writing tasks, including class papers, dissertations, job application documents, personal statements, and grant applications, to name a few. Leslie Bienen, MFA, DVM offers one-on-one writing support and other workshops. Appointments are virtual for the time being. You can make an appointment by contacting writingsupportsph@pdx.edu or making an appointment through Calendly.\n\n\nStudent Wellness\nI am committed to supporting the physical and emotional well-being of my students. Both PSU and OHSU have designated centers for student health. For OHSU, students can visit the Behavioral Health site, where you can find more information including the number to make an appointment. All student visits are free. OHSU students also have access to PSU’s Counseling Services through the school’s Student Health & Counseling. Information on additional student resources for OHSU students are available on the OHSU Health and Wellness Resource page. \n\n\nSupport for Food Insecurity\nStudents across the country experience food insecurity at alarming rates. OHSU and PSU both provide a list of resources to help combat food insecurity. Of note, the Committee to Improve Student Food Security (CISFS) at PSU provides a Free Food Market on the second Monday of each month. OHSU also provides SNAP Enrollment Assistance. The Supplemental Nutrition Assistance Program (SNAP) allocates money towards food for individuals below a certain income level. If you make less than $2,430 monthly, you may wish to enroll.\n\n\nSupport for Students with Children\nStudents who have children can use the PSU resource: Resource Center for Students with Children. Resources are mostly focused on students with younger children. There are several great resources available, including: family-friendly study spaces, new baby starter packs, free kids clothing, and further information on financial resources for childcare."
  },
  {
    "objectID": "syllabus.html#school-policies-and-resources",
    "href": "syllabus.html#school-policies-and-resources",
    "title": "BSTA 512/612 Syllabus",
    "section": "School Policies and Resources",
    "text": "School Policies and Resources\n\nSchool of Public Health Handbook\nAll students are responsible for following the policies and expectations outlined in the student handbook for their program of study. Students are responsible for their own academic work and are expected to have read and practice principles of academic honesty, as presented in the handbook.\n\n\nStudent Access & Accommodations\nThe School of Public Health values diversity and inclusion; we are committed to fostering mutual respect and full participation for all students. My goal is to create a learning environment that is equitable, usable, inclusive, and welcoming. If any aspects of instruction or course design result in barriers to your inclusion or learning, please notify me. \n\nIf you are already registered with disability services at either OHSU or PSU and you are taking a course at the opposite institution, you need to contact the office you’re registered with to transfer your accommodations.\nIf you are not already registered with a disability services office, and you have, or think you may have, a disability that may affect your work in this class, and feel you need accommodations, use the following table for guidance about which office to contact to initiate accommodations.\n\nResource Table\n\n\n\nEnrollment University and Standing\nWhere to Seek Accommodations\n\n\nUndergraduate School of Public Health major\nPSU’s Disability Resource Center\n\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\n\n\n\nAll PSU-registering Dual Degree (MSW/MPH and MURP/MPH) Graduate School of Public Health Majors and all PSU-registering PhD students admitted prior to fall 2016.\nPSU’s Disability Resource Center\n\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\nwww.pdx.edu/drc\n\n\n\nGraduate School of Public Health major (irrespective of institution at which you register)\nOHSU’s Office for Student Access\n(503) 494-0082\nStudentAccess@OHSU.edu\nOHSU Auditorium Building 330\n\n\nNon-SPH major, PSU-enrolled student\nPSU’s Disability Resource Center\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\nwww.pdx.edu/drc\n\n\nNon-SPH major, OHSU-enrolled student\nOHSU’s Office for Student Access\n(503) 494-0082\nStudentAccess@OHSU.edu\nOHSU Auditorium Building 330\n\n\n\n \nFor more information related accessibility and accommodations, please see the “Statement Regarding Students with Disabilities” within the Institutional Policies section of this syllabus.\n\n\nTitle IX\nThe School of Public Health is committed to providing an environment free of all forms of prohibited discrimination and discriminatory harassment. The School of Public Health students who have questions about an incident related to Title IX are welcome to contact either the OHSU or PSU’s Title IX Coordinator and they will direct you to the appropriate resource or office. Title IX pertains to any form of sex/gender discrimination, discriminatory harassment, sexual harassment or sexual violence.\n\nPSU’s Title IX Coordinator is Julie Caron, she may be reached at titleixccordinator@pdx.edu or 503-725-4410. Julie’s office is located at 1600 SW 4th Ave, In the Richard and Maureen Neuberger Center RMNC - Suite 830.\nThe OHSU Title IX Coordinator’s may be reachedat 503-494-0258 or titleix@ohsu.edu and is located at 2525 SW 3rd St.\n\nPlease note that faculty and the Title IX Coordinators will keep the information you disclose private but are not confidential. If you would like to speak with a confidential advocate, who will not disclose the information to a university official without your written consent, you may contact an advocate at PSU or OHSU.\n\nPSU’s confidential advocates are available in Women’s Resource Center (serving all genders) in Smith Student Memorial Union 479. You may schedule an appointment by (503-725-5672) or schedule on line at https://psuwrc.youcanbook.me. For more information about resources at PSU, please see PSU’s Response to Sexual Misconduct website.\nOHSU’s advocates are available through the Confidential Advocacy Program (CAP) at 833-495-CAPS (2277) or by email CAPsupport@ohsu.edu, but please note, email is not a secure form of communication. Also visit www.ohsu.edu/CAP.\n\nAt OHSU, if you encounter any harassment, or discrimination based on race, color, religion, age, national origin or ancestry, veteran or military status, sex, marital status, pregnancy or parenting status, sexual orientation, gender identity or expression, disability or any other protected status, please contact the Affirmative Action and Equal Opportunity (AAEO) Department at 503-494-5148 or aaeo@ohsu.edu.\nAt PSU, you may contact the Office of Equity and Compliance if you experience any form of discrimination or discriminatory harassment as listed above at equityandcompliance@pdx.edu or by calling 503-725-5919.\n\n\nTechnical Support\nThe OHSU ITG Help Desk is available to assist students with email account or network account access issues between 6 a.m. and 6 p.m., Monday through Friday at 503-494-2222. For technical support in using the Sakai Course Management System, please contact the Sakai Help Desk at 877-972-5249 or email us at sakai@ohsu.edu"
  },
  {
    "objectID": "syllabus.html#ohsu-competencies",
    "href": "syllabus.html#ohsu-competencies",
    "title": "BSTA 512/612 Syllabus",
    "section": "OHSU Competencies",
    "text": "OHSU Competencies\n\nList of OHSU Graduation Core Competencies\n\nProfessional Knowledge and Skills\nProfessionalism\nInformation Literacy\nCommunication\nTeamwork\nCommunity Engagement, Social Justice and Equity\nPatient Centered Care\n\nTo access a descriptive list of OHSU Graducation Core Competencies: OHSU Graduation Core Competencies"
  },
  {
    "objectID": "syllabus.html#institutional-policies-and-resources",
    "href": "syllabus.html#institutional-policies-and-resources",
    "title": "BSTA 512/612 Syllabus",
    "section": "Institutional Policies and Resources",
    "text": "Institutional Policies and Resources\n\nStatement Regarding Students with Disabilities\nOHSU is committed to inclusive and accessible learning environments in compliance with federal and state law. If you have a disability or think you may have a disability (mental health, attention-related, learning, vision, hearing, physical or health impacts) contact the Office for Student Access at (503) 494-0082 or OHSU Student Access to have a confidential conversation about academic accommodations. Information is also available at Student Access Website. Because accommodations may take time to implement and cannot be applied retroactively, it is important to have this discussion as soon as possible.\nPortland State students also have similar resources available via the PSU Disability Resource Center (website http://www.pdx.edu/drc ). Please contact the DRC at tel. (503) 725-4150 or email at drc@pdx.edu\n\n\nStudent Evaluation of Courses\nCourse evaluation results are extremely important and used to help improve courses and the learning experience of future students. Responses will always remain anonymous and will only be available to instructors after grades have been posted. The results of scaled questions and comments go to both the instructor and their unit head/supervisor. Refer to Student Evaluation of Courses and Instructional Effectiveness, *Policy No. 02-50-035.\n*To access the OHSU Student Evaluation of Courses and Instructional Effectiveness Policy, you must log into the OHSU O2 website.\n\n\nCopyright Information\nCopyright laws and fair use policies protect the rights of those who have produced the material. The copy in this course has been provided for private study, scholarship, or research. Other uses may require permission from the copyright holder. The user of this work is responsible for adhering to copyright law of the U.S. (Title 17, U.S. Code). To help you familiarize yourself with copyright and fair use policies, the University encourages you to visit its Copyright Web Page\nSakai course web sites contain material protected by copyrights held by the instructor, other individuals or institutions. Such material is used for educational purposes in accord with copyright law and/or with permission given by the owners of the original material. You may download one copy of the materials on any single computer for non-commercial, personal, or educational purposes only, provided that you (1) do not modify it, (2) use it only for the duration of this course, and (3) include both this notice and any copyright notice originally included with the material. Beyond this use, no material from the course web site may be copied, reproduced, re-published, uploaded, posted, transmitted, or distributed in any way without the permission of the original copyright holder. The instructor assumes no responsibility for individuals who improperly use copyrighted material placed on the web site.\n\n\nSyllabi Changes and Retention\nSyllabi are considered to be a learning agreement between students and the faculty of record. Information contained in syllabi, other than the minimum requirements, may be subject to change as deemed appropriate by the faculty of record in concurrence with the academic program and the Office of the Provost. Refer to the *Course Syllabi Policy, 02-50-050.\n*To access the OHSU Course Syllabus Policy, you must log into the OHSU O2 website.\n\n\nCommitment to Diversity & Inclusion\nOHSU is committed to creating and fostering a learning and working environment based on open communication and mutual respect. If you encounter sexual harassment, sexual misconduct, sexual assault, or discrimination based on race, color, religion, age, national origin, veteran’s status, ancestry, sex, marital status, pregnancy or parenting status, sexual orientation, gender identity, disability or any other protected status please contact the Affirmative Action and Equal Opportunity Department at 503-494-5148 or aaeo@ohsu.edu. Inquiries about Title IX compliance or sex/gender discrimination and harassment may be directed to the OHSU Title IX Coordinator at 503-494-0258 or titleix@ohsu.edu.\n\n\nModified Operations, Policy 01-40-010\nPortland Campus:  Marquam Hill and South Waterfront\nStudents should review O2 or call OHSU’s weather alert line at 503-494-9021 for the most up-to-date information on OHSU-wide modified operations which include but are not limited to delays or closures for inclement weather.\nIf your home institution is not on the Portland campus (Marquam Hill or South Waterfront, contact your home institution for more information.\n\n\nOHSU Resources Available to Students*:\nRemote Learning Resources\nThe Remote Learning webpage on O2 contains concise, practical resources, and strategies for students that need to quickly transition to a fully remote instructional format.\nRegistrar’s Office\nMackenzie Hall, Rm. 1120\n503-494-7800; Email the Registrar\nStudent Registration Information: \nTo Register for Classes\nOHSU ITG Help Desk\nRegular staff hours are 6 a.m. to 6 p.m., Monday through Friday, but phones are answered seven days a week, 24 hours a day. Call 503 494-2222.\nTeaching and Learning Center\nAcademic Support Counseling and Sakai Course Management System, please contact the TLC Help Desk at 877-972-5249 or email TLC Help Desk\nStudent Academic Support Services\nFor resources on improving student’s study strategies, time management, motivation, test-taking skills and more, Please access the Student Academic Support Services Sakai page. For one-on-one appointments or to arrange a workshop for students, please contact Emily Hillhouse.\nConfidential Advocacy Program\nSupport for OHSU employees, students, and volunteers who have experienced any form of sexual misconduct, including sexual harassment, sexual assault, intimate-partner violence, stalking, relationship/dating violence, and other forms — regardless of when or where it took place. Contact Us.\nConcourse Syllabus Management\nFor help with accessing your Concourse Syllabus:  Please contact the Sakai help Desk for all other Concourse inquiries please visit the Concourse Support - Sakai or please contact the Mark Rivera at rivermar@ohsu.edu or call 503-494-0934\nPublic Safety\nOHSU Public Safety-Portland Campus (Marquam Hill and South Waterfront)\n\nEmergency on Campus: 503-494-4444 (Portland)\nNon-emergency: 503-494-7744; Contact Public Safety\n\nStudent Health & Wellness Center \nBaird Hall, Rm. 18 (Primary Care) and Rm. 6 (Behavioral Health)\n503-494-8665; For urgent care after hours, 503-494-8311 and ask for the Nurse on call.\nWellness Center Information  \nWellness Center Website\nIf your home institution is not on the Portland campus, contact your home institution student support services for more information.\nOmbudsman Office\nGaines Hall, Rm. 117\n707 SW Gaines Street, Portland, OR 97239\n503-494-5397; Contact Ombudsman; Ombudsman Website\nLibrary: Biomedical Information Communication Center\nBICC Library Hours of Operation\n\n\nPrivacy While Learning\nStudents may be asked to take classes remotely through videoconferencing software like WebEx. Some of these remote classes will be recorded. Any recording will capture the presenter’s audio, video, and computer screen. Student video and audio will be recorded if and when you unmute your audio and share your video during the recorded sessions. These recordings will not be shared with or accessible to the public without prior written consent. \n\n\nStudent Central\nKey information for students across OHSU’s Schools of Dentistry, Medicine, Nursing, the OHSU-PSU School of Public Health and the College of Pharmacy. Student Central helps you find out more about student services, resources, policies and technology."
  },
  {
    "objectID": "instructors.html",
    "href": "instructors.html",
    "title": "Instructors",
    "section": "",
    "text": "Email: wakim@ohsu.edu\nOffice: VPT 622A\n\nPronouns: she/her/hers\nYou are welcome to address me as Nicky (pronounced “nik-EE”), Dr. Wakim (pronounced “wah-KEEM”), Dr. W, Dr. Nicky, Professor, Professor Wakim, or any combination of the prior.\nBest method to contact: Office hours or email for general course questions or E-mail/Calendly appointments for private communication.\n\n\n\n\n\n\n\n\n\n\nBrief professor statement: As a professor, my main goal is to instill a growth mindset into my students. Growth mindset means we are NOT stuck in our abilities or knowledge, and that we all can and will grow! This course aims to be as transparent as possible. I want you to understand my motivation for assessments, questions, and lessons. I also want those assessments to be clear, so please ask for clarification whenever needed.\n\n\nTBD"
  },
  {
    "objectID": "instructors.html#instructor-nicole-nicky-wakim-phd",
    "href": "instructors.html#instructor-nicole-nicky-wakim-phd",
    "title": "Instructors",
    "section": "",
    "text": "Email: wakim@ohsu.edu\nOffice: VPT 622A\n\nPronouns: she/her/hers\nYou are welcome to address me as Nicky (pronounced “nik-EE”), Dr. Wakim (pronounced “wah-KEEM”), Dr. W, Dr. Nicky, Professor, Professor Wakim, or any combination of the prior.\nBest method to contact: Office hours or email for general course questions or E-mail/Calendly appointments for private communication.\n\n\n\n\n\n\n\n\n\n\nBrief professor statement: As a professor, my main goal is to instill a growth mindset into my students. Growth mindset means we are NOT stuck in our abilities or knowledge, and that we all can and will grow! This course aims to be as transparent as possible. I want you to understand my motivation for assessments, questions, and lessons. I also want those assessments to be clear, so please ask for clarification whenever needed.\n\n\nTBD"
  },
  {
    "objectID": "instructors.html#teaching-assistants",
    "href": "instructors.html#teaching-assistants",
    "title": "Instructors",
    "section": "Teaching Assistants",
    "text": "Teaching Assistants\nLiv Ainsworth, Katie Hand, and Miyuki Sun will serve as our TAs for the quarter!! They will have the following office hours and will help answer questions on Slack.\n\nLiv Ainsworth\n\nOffice hours TBD\n\n\n\nKatie Hand\n\nOffice hours TBD\n\n\n\nMiyuki Sun\n\nOffice hours TBD"
  },
  {
    "objectID": "project.html#labs",
    "href": "project.html#labs",
    "title": "Project Central",
    "section": "Labs",
    "text": "Labs\n\n\n\nLab\nDue Date\nTopics\n\n\n\n\nLab 1\n1/16\nExploring the question\n\n\nLab 2\n2/6\nExploring the data\n\n\nLab 3\n2/27\nA little more data exploration + Fitting and interpreting a model\n\n\nLab 4\n3/13\nBuilding a model\n\n\n\n\nHelp with BMI variable\n\n\nLab rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work*\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nSome tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning**\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*While there is not a large emphasis on “correctness” in the labs, you must follow the correct procedure for certain tasks. The code/work grade will reflect whether or not you followed the procedure for analysis correctly.\n**Applies to questions with reasoning (like target population, choosing variables, revisiting research question)"
  },
  {
    "objectID": "project.html#report",
    "href": "project.html#report",
    "title": "Project Central",
    "section": "Report",
    "text": "Report\nReport Instructions\nDue 3/21/2024 at 11pm\n\nReading and listening sources\nIf you are interested in sources that discuss the social complexities of anti-fat bias, feel free to take a look at the following sources. Please be aware that these resources will discuss anti-fat bias and related histories, including racism and sexism.\n\nArticle: Implicit and explicit anti-fat bias: The role of weight-related attitudes and beliefs\nPodcast: Anti-Fat Bias by Maintenance Phase\nBook: Fearing the Black Body: The Racial Origins of Fat Phobia\n\nMultnomah County Library has unlimited loans for the audiobook\n\nBlog: Dances with Fat\n\nYou can subscribe to Ragen’s weekly newsletter for free\n\n\nIf you have additional sources that you would like to share, please send them to me!"
  },
  {
    "objectID": "data/NHANES_EDA.html",
    "href": "data/NHANES_EDA.html",
    "title": "NHANES",
    "section": "",
    "text": "NHANES\n\nlibrary(NHANES)\nlibrary(skimr)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\ndata(\"NHANES\")\n\n\nskim(NHANES)\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n# 16 Depressed             3327         0.667 FALSE          3 \"Non: 5246, Sev: 1009, Mos: 418\"            \n# 17 SleepTrouble          2228         0.777 FALSE          2 \"No: 5799, Yes: 1973\"                       \n# 18 PhysActive            1674         0.833 FALSE          2 \"Yes: 4649, No: 3677\"          \n\n\nNHANES18 &lt;- NHANES %&gt;% dplyr::filter(Age &gt;= 18)\nNHANES18 %&gt;% tabyl(Depressed, PhysActive)\n\n Depressed   No  Yes\n      None 2297 2949\n   Several  538  471\n      Most  275  143\n      &lt;NA&gt;  423  385\n\nNHANES18 %&gt;% drop_na(Depressed) %&gt;% tabyl(Depressed, PhysActive)\n\n Depressed   No  Yes\n      None 2297 2949\n   Several  538  471\n      Most  275  143\n\nNHANES18Dep &lt;- NHANES18 %&gt;% drop_na(Depressed)\nNHANES18Dep %&gt;% \n  tabyl(Depressed, PhysActive) %&gt;% \n  adorn_totals()\n\n Depressed   No  Yes\n      None 2297 2949\n   Several  538  471\n      Most  275  143\n     Total 3110 3563\n\nchisq_Dep_Phys&lt;- chisq.test(NHANES18Dep$Depressed, NHANES18Dep$PhysActive)\ntidy(chisq_Dep_Phys)\n\n# A tibble: 1 × 4\n  statistic  p.value parameter method                    \n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      96.9 9.26e-22         2 Pearson's Chi-squared test\n\nchisq_Dep_Phys$expected\n\n                     NHANES18Dep$PhysActive\nNHANES18Dep$Depressed        No       Yes\n              None    2444.9363 2801.0637\n              Several  470.2518  538.7482\n              Most     194.8119  223.1881\n\nchisq_Dep_Phys$observed\n\n                     NHANES18Dep$PhysActive\nNHANES18Dep$Depressed   No  Yes\n              None    2297 2949\n              Several  538  471\n              Most     275  143\n\nlibrary(moderndive)\nset.seed(5348)\n# 5347\nNHANES18Dep200 &lt;- NHANES18Dep %&gt;%\n  rep_sample_n(size = 200, reps = 1, replace = FALSE)\n\nNHANES18Dep200 %&gt;% \n  tabyl(Depressed, PhysActive) %&gt;% \n  adorn_totals()\n\n Depressed No Yes\n      None 63  79\n   Several 19  18\n      Most 10  11\n     Total 92 108\n\nchisq_Dep_Phys200&lt;- chisq.test(NHANES18Dep200$Depressed, NHANES18Dep200$PhysActive)\ntidy(chisq_Dep_Phys200)\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                    \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1     0.601   0.740         2 Pearson's Chi-squared test\n\nchisq_Dep_Phys200$expected\n\n                        NHANES18Dep200$PhysActive\nNHANES18Dep200$Depressed    No   Yes\n                 None    65.32 76.68\n                 Several 17.02 19.98\n                 Most     9.66 11.34\n\nchisq_Dep_Phys200$observed\n\n                        NHANES18Dep200$PhysActive\nNHANES18Dep200$Depressed No Yes\n                 None    63  79\n                 Several 19  18\n                 Most    10  11\n\n#------------\nset.seed(5349)\nNHANES18Dep400 &lt;- NHANES18Dep %&gt;%\n  rep_sample_n(size = 400, reps = 1, replace = FALSE)\n\nNHANES18Dep400 %&gt;% \n  tabyl(PhysActive, Depressed) %&gt;% \n  adorn_totals(where = c(\"row\", \"col\")) %&gt;% \n  adorn_title \n\n            Depressed                   \n PhysActive      None Several Most Total\n         No       115      32   27   174\n        Yes       199      26    1   226\n      Total       314      58   28   400\n\nchisq_Dep_Phys400&lt;- chisq.test(NHANES18Dep400$Depressed, NHANES18Dep400$PhysActive)\ntidy(chisq_Dep_Phys400)\n\n# A tibble: 1 × 4\n  statistic       p.value parameter method                    \n      &lt;dbl&gt;         &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      41.2 0.00000000115         2 Pearson's Chi-squared test\n\nchisq_Dep_Phys400$observed\n\n                        NHANES18Dep400$PhysActive\nNHANES18Dep400$Depressed  No Yes\n                 None    115 199\n                 Several  32  26\n                 Most     27   1\n\nchisq_Dep_Phys400$expected\n\n                        NHANES18Dep400$PhysActive\nNHANES18Dep400$Depressed     No    Yes\n                 None    136.59 177.41\n                 Several  25.23  32.77\n                 Most     12.18  15.82\n\nset.seed(5349)\nNHANES18Dep_PAy100 &lt;- NHANES18Dep %&gt;% filter(PhysActive == \"Yes\") %&gt;% \n  rep_sample_n(size = 100, reps = 1, replace = FALSE)\nNHANES18Dep_PAy100 %&gt;% tabyl(Depressed)\n\n Depressed  n percent\n      None 85    0.85\n   Several 12    0.12\n      Most  3    0.03\n\nNHANES18Dep_PAn100 &lt;- NHANES18Dep %&gt;% filter(PhysActive == \"No\") %&gt;% \n  rep_sample_n(size = 100, reps = 1, replace = FALSE)\nNHANES18Dep_PAn100 %&gt;% tabyl(Depressed)\n\n Depressed  n percent\n      None 78    0.78\n   Several 17    0.17\n      Most  5    0.05\n\n(DepPA200_table &lt;- matrix(c(83, 12, 5, 78, 16, 6), nrow = 2, ncol = 3, byrow = T))\n\n     [,1] [,2] [,3]\n[1,]   83   12    5\n[2,]   78   16    6\n\ndimnames(DepPA200_table) &lt;- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several\", \"Most\"))  # column names\nDepPA200_table\n\n     Depression\nPA    None Several Most\n  Yes   83      12    5\n  No    78      16    6\n\nchisq.test(DepPA200_table) \n\n\n    Pearson's Chi-squared test\n\ndata:  DepPA200_table\nX-squared = 0.81762, df = 2, p-value = 0.6644\n\nchisq.test(DepPA200_table)$expected\n\n     Depression\nPA    None Several Most\n  Yes 80.5      14  5.5\n  No  80.5      14  5.5\n\nset.seed(5349)\nNHANES18Dep_PAy50 &lt;- NHANES18Dep %&gt;% filter(PhysActive == \"Yes\") %&gt;% \n  rep_sample_n(size = 50, reps = 1, replace = FALSE)\nNHANES18Dep_PAy50 %&gt;% tabyl(Depressed)\n\n Depressed  n percent\n      None 43    0.86\n   Several  6    0.12\n      Most  1    0.02\n\nNHANES18Dep_PAn50 &lt;- NHANES18Dep %&gt;% filter(PhysActive == \"No\") %&gt;% \n  rep_sample_n(size = 50, reps = 1, replace = FALSE)\nNHANES18Dep_PAn50 %&gt;% tabyl(Depressed)\n\n Depressed  n percent\n      None 30    0.60\n   Several 14    0.28\n      Most  6    0.12"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02_key_info.html#key-dates",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "",
    "text": "Use visualizations and cut off points to flag potentially influential points using residuals, leverage, and Cook’s distance\nHandle influential points and assumption violations by checking data errors, reassessing the model, and making data transformations.\nImplement a model with data transformations and determine if it improves the model fit.\n\n\n\n\nWe have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\nThe residuals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun model1 through augment() (model1 is input)\n\nSo we assigned model1 as the output of the lm() function (model1 is output)\n\nWill give us values about each observation in the context of the fitted regression model\n\ncook’s distance (.cooksd), fitted value (.fitted, \\(\\widehat{Y}_i\\)), leverage (.hat), residual (.resid), standardized residuals (.std.resid)\n\n\n\naug1 &lt;- augment(model1) \nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…\n\n\nRDocumentation on the augment() function.\n\n\n\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nCheck if there is a linear relationship between the mean response (Y) and the explanatory variable (X)\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nCheck that the observations are independent\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\nCheck that the responses (at each level X) are normally distributed\n\nUsually measured through the residuals\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nCheck that the variance (or standard deviation) of the responses is equal for all levels of X\n\nUsually measured through the residuals"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "",
    "text": "We have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "",
    "text": "The residuals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#augment-getting-extra-information-on-the-fitted-model",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#augment-getting-extra-information-on-the-fitted-model",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "",
    "text": "Run model1 through augment() (model1 is input)\n\nSo we assigned model1 as the output of the lm() function (model1 is output)\n\nWill give us values about each observation in the context of the fitted regression model\n\ncook’s distance (.cooksd), fitted value (.fitted, \\(\\widehat{Y}_i\\)), leverage (.hat), residual (.resid), standardized residuals (.std.resid)\n\n\n\naug1 &lt;- augment(model1) \nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…\n\n\nRDocumentation on the augment() function."
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#revisiting-our-line-assumptions",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#revisiting-our-line-assumptions",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "",
    "text": "[L] Linearity of relationship between variables\n\n\nCheck if there is a linear relationship between the mean response (Y) and the explanatory variable (X)\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nCheck that the observations are independent\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\nCheck that the responses (at each level X) are normally distributed\n\nUsually measured through the residuals\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nCheck that the variance (or standard deviation) of the responses is equal for all levels of X\n\nUsually measured through the residuals"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#influential-points",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#influential-points",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Influential points",
    "text": "Influential points\n\n\n\n\nOutliers\n\n\n\nAn observation (\\(X_i, Y_i\\)) whose response \\(Y_i\\) does not follow the general trend of the rest of the data\n\n\n\n \n \n\n\n\n\n\n\n\n\nHigh leverage observations\n\n\n\nAn observation (\\(X_i, Y_i\\)) whose predictor \\(X_i\\) has an extreme value\n\\(X_i\\) can be an extremely high or low value compared to the rest of the observations"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#outliers",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#outliers",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Outliers",
    "text": "Outliers\n\nAn observation (\\(X_i, Y_i\\)) whose response \\(Y_i\\) does not follow the general trend of the rest of the data\nHow do we determine if a point is an outlier?\n\nScatterplot of \\(Y\\) vs. \\(X\\)\nFollowed by evaluation of its residual (and standardized residual)\n\n\n \n\nUse the internally standardized residual (aka studentized residual) to determine if an observation is an outlier"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#poll-everywhere-question-1",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#poll-everywhere-question-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#identifying-outliers",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#identifying-outliers",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Identifying outliers",
    "text": "Identifying outliers\n\n\n\n\nInternally standardized residual\n\n\n\\[\nr_i = \\frac{\\widehat\\epsilon_i}{\\sqrt{\\widehat\\sigma^2(1-h_{ii})}}\n\\]\n\n\n\n\nWe flag an observation if the standardized residual is “large”\n\nDifferent sources will define “large” differently\nPennState site uses \\(|r_i| &gt; 3\\)\nautoplot() shows the 3 observations with the highest standardized residuals\nOther sources use \\(|r_i| &gt; 2\\), which is a little more conservative\n\n\n\n\n\n\n \n\nggplot(data = aug1) + \n  geom_histogram(aes(x = .std.resid))"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#countries-that-are-outliers-r_i-2",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#countries-that-are-outliers-r_i-2",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Countries that are outliers (\\(|r_i| > 2\\))",
    "text": "Countries that are outliers (\\(|r_i| &gt; 2\\))\n\nWe can identify the countries that are outliers\n\n\naug1 %&gt;% \n  filter(abs(.std.resid) &gt; 2)\n\n# A tibble: 4 × 10\n  .rownames country     life_expectancy_year…¹ female_literacy_rate…² .std.resid\n  &lt;chr&gt;     &lt;chr&gt;                        &lt;dbl&gt;                  &lt;dbl&gt;      &lt;dbl&gt;\n1 33        Central Af…                   48                     44.2      -2.20\n2 152       South Afri…                   55.8                   92.2      -2.71\n3 161       Swaziland                     48.9                   87.3      -3.65\n4 187       Zimbabwe                      51.9                   80.1      -2.89\n# ℹ abbreviated names: ¹​life_expectancy_years_2011, ²​female_literacy_rate_2011\n# ℹ 5 more variables: .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;, .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;,\n#   .cooksd &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#high-leverage-observations",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#high-leverage-observations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "High leverage observations",
    "text": "High leverage observations\n\nAn observation (\\(X_i, Y_i\\)) whose response \\(X_i\\) is considered “extreme” compared to the other values of \\(X\\)\n\n \n\nHow do we determine if a point has high leverage?\n\nScatterplot of \\(Y\\) vs. \\(X\\)\nCalculating the leverage of each observation"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#leverage-h_i",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#leverage-h_i",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Leverage \\(h_i\\)",
    "text": "Leverage \\(h_i\\)\n\nValues of leverage are: \\(0 \\leq h_i \\leq 1\\)\nWe flag an observation if the leverage is “high”\n\nDifferent sources will define “high” differently\nSome textbooks use \\(h_i &gt; 4/n\\) where \\(n\\) = sample size\nSome people suggest \\(h_i &gt; 6/n\\)\nPennState site uses \\(h_i &gt; 3p/n\\) where \\(p\\) = number of regression coefficients\n\n\n\naug1 = aug1 %&gt;% relocate(.hat, .after = female_literacy_rate_2011)\naug1 %&gt;% arrange(desc(.hat))\n\n# A tibble: 80 × 10\n   .rownames country        life_expectancy_year…¹ female_literacy_rate…²   .hat\n   &lt;chr&gt;     &lt;chr&gt;                           &lt;dbl&gt;                  &lt;dbl&gt;  &lt;dbl&gt;\n 1 1         Afghanistan                      56.7                   13   0.136 \n 2 104       Mali                             60                     24.6 0.0980\n 3 34        Chad                             57                     25.4 0.0956\n 4 146       Sierra Leone                     55.7                   32.6 0.0757\n 5 62        Gambia                           66                     41.9 0.0540\n 6 70        Guinea-Bissau                    56.2                   42.1 0.0536\n 7 33        Central Afric…                   48                     44.2 0.0493\n 8 118       Nepal                            68.7                   46.7 0.0446\n 9 42        Cote d'Ivoire                    56.9                   47.6 0.0430\n10 169       Togo                             59.6                   48   0.0422\n# ℹ 70 more rows\n# ℹ abbreviated names: ¹​life_expectancy_years_2011, ²​female_literacy_rate_2011\n# ℹ 5 more variables: .std.resid &lt;dbl&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#   .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#countries-with-high-leverage-h_i-4n",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#countries-with-high-leverage-h_i-4n",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Countries with high leverage (\\(h_i > 4/n\\))",
    "text": "Countries with high leverage (\\(h_i &gt; 4/n\\))\n\nWe can look at the countries that have high leverage\n\n\naug1 %&gt;% \n  filter(.hat &gt; 4/80) %&gt;%\n  arrange(desc(.hat))\n\n# A tibble: 6 × 10\n  .rownames country       life_expectancy_years_…¹ female_literacy_rate…²   .hat\n  &lt;chr&gt;     &lt;chr&gt;                            &lt;dbl&gt;                  &lt;dbl&gt;  &lt;dbl&gt;\n1 1         Afghanistan                       56.7                   13   0.136 \n2 104       Mali                              60                     24.6 0.0980\n3 34        Chad                              57                     25.4 0.0956\n4 146       Sierra Leone                      55.7                   32.6 0.0757\n5 62        Gambia                            66                     41.9 0.0540\n6 70        Guinea-Bissau                     56.2                   42.1 0.0536\n# ℹ abbreviated names: ¹​life_expectancy_years_2011, ²​female_literacy_rate_2011\n# ℹ 5 more variables: .std.resid &lt;dbl&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#   .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#poll-everywhere-question-2",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#poll-everywhere-question-2",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#countries-with-high-leverage-h_i-4n-1",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#countries-with-high-leverage-h_i-4n-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Countries with high leverage (\\(h_i > 4/n\\))",
    "text": "Countries with high leverage (\\(h_i &gt; 4/n\\))\nLabel only countries with large leverage:\n\nggplot(aug1, aes(x = female_literacy_rate_2011, y = life_expectancy_years_2011,\n                 label = country)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  geom_text(aes(label = ifelse(.hat &gt; 0.05, as.character(country), ''))) +\n  geom_vline(xintercept = mean(aug1$female_literacy_rate_2011), color = \"grey\") +\n  geom_hline(yintercept = mean(aug1$life_expectancy_years_2011), color = \"grey\")"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#what-does-the-model-look-like-without-the-high-leverage-points",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#what-does-the-model-look-like-without-the-high-leverage-points",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "What does the model look like without the high leverage points?",
    "text": "What does the model look like without the high leverage points?\nSensitivity analysis removing countries with high leverage\n\nmodel1_lowlev &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                    data = aug1_lowlev)\ntidy(model1_lowlev) %&gt;% gt() %&gt;% # Without high-leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n49.563\n3.888\n12.746\n0.000\n    female_literacy_rate_2011\n0.247\n0.044\n5.562\n0.000\n  \n  \n  \n\n\n\ntidy(model1) %&gt;% gt() %&gt;% # With high leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n    female_literacy_rate_2011\n0.232\n0.031\n7.377\n0.000"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#cooks-distance",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#cooks-distance",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Cook’s distance",
    "text": "Cook’s distance\n\nMeasures the overall influence of an observation\n\n \n\nAttempts to measure how much influence a single observation has over the fitted model\n\nMeasures how all fitted values change when the \\(ith\\) observation is removed from the model\nCombines leverage and outlier information"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#identifying-points-with-high-cooks-distance",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#identifying-points-with-high-cooks-distance",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Identifying points with high Cook’s distance",
    "text": "Identifying points with high Cook’s distance\n\n\nThe Cook’s distance for the \\(i^{th}\\) observation is\n\\[d_i = \\frac{h_i}{2(1-h_i)} \\cdot r_i^2\\] where \\(h_i\\) is the leverage and \\(r_i\\) is the studentized residual\n\n\nAnother rule for Cook’s distance that is not strict:\n\nInvestigate observations that have \\(d_i &gt; 1\\)\n\nCook’s distance values are already in the augment tibble: .cooksd\n\n\n\n\naug1 = aug1 %&gt;% relocate(.cooksd, .after = female_literacy_rate_2011)\naug1 %&gt;% arrange(desc(.cooksd))\n\n# A tibble: 80 × 10\n   .rownames country       life_expectancy_year…¹ female_literacy_rate…² .cooksd\n   &lt;chr&gt;     &lt;chr&gt;                          &lt;dbl&gt;                  &lt;dbl&gt;   &lt;dbl&gt;\n 1 33        Central Afri…                   48                     44.2  0.126 \n 2 161       Swaziland                       48.9                   87.3  0.0903\n 3 152       South Africa                    55.8                   92.2  0.0577\n 4 187       Zimbabwe                        51.9                   80.1  0.0531\n 5 114       Morocco                         73.8                   57.6  0.0350\n 6 118       Nepal                           68.7                   46.7  0.0311\n 7 14        Bangladesh                      71                     53.4  0.0280\n 8 23        Botswana                        58.9                   85.6  0.0249\n 9 54        Equatorial G…                   61.4                   91.1  0.0231\n10 62        Gambia                          66                     41.9  0.0228\n# ℹ 70 more rows\n# ℹ abbreviated names: ¹​life_expectancy_years_2011, ²​female_literacy_rate_2011\n# ℹ 5 more variables: .hat &lt;dbl&gt;, .std.resid &lt;dbl&gt;, .fitted &lt;dbl&gt;,\n#   .resid &lt;dbl&gt;, .sigma &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#plotting-cooks-distance",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#plotting-cooks-distance",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Plotting Cook’s Distance",
    "text": "Plotting Cook’s Distance\n\n# plot(model) shows figures similar to autoplot()\n# adds on Cook's distance though\nplot(model1, which = 4)"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#model-without-those-4-points-qq-plot-residual-plot",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#model-without-those-4-points-qq-plot-residual-plot",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Model without those 4 points: QQ Plot, Residual plot",
    "text": "Model without those 4 points: QQ Plot, Residual plot\n\nmodel1_lowcd &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                    data = aug1_lowcd)\ntidy(model1_lowcd) %&gt;% gt() %&gt;% # Without high-leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n52.388\n2.078\n25.208\n0.000\n    female_literacy_rate_2011\n0.226\n0.024\n9.208\n0.000\n  \n  \n  \n\n\n\ntidy(model1) %&gt;% gt() %&gt;% # With high leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n    female_literacy_rate_2011\n0.232\n0.031\n7.377\n0.000"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#model-without-those-4-points-qq-plot-residual-plot-1",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#model-without-those-4-points-qq-plot-residual-plot-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Model without those 4 points: QQ Plot, Residual plot",
    "text": "Model without those 4 points: QQ Plot, Residual plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI am okay with this!\n\nAnd don’t forget: we may want more variables in our model!\nYou do not need to produce plots with the influential points taken out"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#summary-of-how-we-identify-influential-points",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#summary-of-how-we-identify-influential-points",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Summary of how we identify influential points",
    "text": "Summary of how we identify influential points\n\nUse scatterplot of \\(Y\\) vs. \\(X\\) to see if any points fall outside of range we expect\nUse standardized residuals, leverage, and Cook’s distance to further identify those points\nLook at the models run with and without the identified points to check for drastic changes\n\nLook at QQ plot and residuals to see if assumptions hold without those points\nLook at coefficient estimates to see if they change in sign and large magnitude\n\n\n \n\nNext: how to handle? It’s a little wishy washy"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#how-do-we-deal-with-influential-points",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#how-do-we-deal-with-influential-points",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "How do we deal with influential points?",
    "text": "How do we deal with influential points?\n\nIt’s always weird to be using numbers to help you diagnose an issue, but the issue kinda gets unresolved\nIf an observation is influential, we can check data errors:\n\nWas there a data entry or collection problem?\nIf you have reason to believe that the observation does not hold within the population (or gives you cause to redefine your population)\n\nIf an observation is influential, we can check our model:\n\nDid you leave out any important predictors?\nShould you consider adding some interaction terms?\nIs there any nonlinearity that needs to be modeled?\n\nBasically, deleting an observation should be justified outside of the numbers!\n\nIf it’s an honest data point, then it’s giving us important information!\n\nA really well thought out explanation from StackExchange"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#when-we-have-detected-problems-in-our-model",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#when-we-have-detected-problems-in-our-model",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "When we have detected problems in our model…",
    "text": "When we have detected problems in our model…\n\nWe have talked about influential points\nWe have talked about identifying issues with our LINE assumptions\n\nWhat are our options once we have identified issues in our linear regression model?\n\nSee if we need to add predictors to our model\n\nNicky’s thought for our life expectancy example\n\nTry a transformation if there is an issue with linearity or normality\nTry a transformation if there is unequal variance\nTry a weighted least squares approach if unequal variance (might be lesson at end of course)\nTry a robust estimation procedure if we have a lot of outlier issues (outside scope of class)"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#transformations",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#transformations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Transformations",
    "text": "Transformations\n\nWhen we have issues with our LINE (mostly linearity, normality, or equality of variance) assumptions\n\nWe can use transformations to improve the fit of the model\n\nTransformations can…\n\nMake the relationship more linear\nMake the residuals more normal\n“Stabilize” the variance so that it is more constant\nIt can also bring in or reduce outliers\n\nWe can transform the dependent (\\(Y\\)) variable of the independent (\\(X\\)) variable\n\nUsually we want to try transforming the \\(X\\) first\n\n\n \n\nRequires trial and error!!\nMajor drawback: interpreting the model becomes harder!"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#common-transformations",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#common-transformations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Common transformations",
    "text": "Common transformations\n\nTukey’s transformation (power) ladder\n\nUse R’s gladder() command from the describedata package\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower p\n-3\n-2\n-1\n-1/2\n0\n1/2\n1\n2\n3\n\n\n\n\n\n\\(\\frac{1}{x^3}\\)\n\\(\\frac{1}{x^2}\\)\n\\(\\frac{1}{x}\\)\n\\(\\frac{1}{\\sqrt{x}}\\)\n\\(\\log(x)\\)\n\\(\\sqrt{x}\\)\n\\(x\\)\n\\(x^2\\)\n\\(x^3\\)\n\n\n\n\n\n\nHow to use the power ladder for the general distribution shape\n\nIf data are skewed left, we need to compress smaller values towards the rest of the data\n\nGo “up” ladder to transformations with power &gt; 1\n\nIf data are skewed right, we need to compress larger values towards the rest of the data\n\nGo “down” ladder to transformations with power &lt; 1\n\n\n\n\n\nHow to use the power ladder for heteroscedasticity\n\nIf higher \\(X\\) values have more spread\n\nCompress larger values towards the rest of the data\nGo “down” ladder to transformations with power &lt; 1\n\nIf lower \\(X\\) values have more spread\n\nCompress smaller values towards the rest of the data\nGo “up” ladder to transformations with power &gt; 1"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#poll-everywhere-question-3",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#poll-everywhere-question-3",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#transform-dependent-variable",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#transform-dependent-variable",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Transform dependent variable?",
    "text": "Transform dependent variable?\n\nggplot(gapm, aes(x = life_expectancy_years_2011)) +\n  geom_histogram()"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#gladder-of-life-expectancy",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#gladder-of-life-expectancy",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "gladder() of life expectancy",
    "text": "gladder() of life expectancy\n\ngladder(gapm$life_expectancy_years_2011)"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#ladder-of-life-expectancy",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#ladder-of-life-expectancy",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "ladder() of life expectancy",
    "text": "ladder() of life expectancy\n\n\n\nladder() output tests various transformations of the data for normality\nShapiro-Wilkes test is used to assess for normality\n\n\\(H_0\\): data are from a normal population\n\\(H_A\\): data are NOT from a normal population\n\n\n\n\nladder(gapm$life_expectancy_years_2011) %&gt;% \n  gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      Transformation\n      statistic\n      p.value\n    \n  \n  \n    cubic\n0.963\n0.000\n    square\n0.956\n0.000\n    identity\n0.944\n0.000\n    sqrt\n0.935\n0.000\n    log\n0.924\n0.000\n    1/sqrt\n0.911\n0.000\n    inverse\n0.896\n0.000\n    1/square\n0.860\n0.000\n    1/cubic\n0.815\n0.000"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#transform-independent-variable",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#transform-independent-variable",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Transform independent variable?",
    "text": "Transform independent variable?\n\nggplot(gapm, aes(x = female_literacy_rate_2011)) +\n  geom_histogram()"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#gladder-of-female-literacy-rate",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#gladder-of-female-literacy-rate",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "gladder() of female literacy rate",
    "text": "gladder() of female literacy rate\n\ngladder(gapm$female_literacy_rate_2011)"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#ladder-of-female-literacy-rate",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#ladder-of-female-literacy-rate",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "ladder() of female literacy rate",
    "text": "ladder() of female literacy rate\n\n\n\nladder() output tests various transformations of the data for normality\nShapiro-Wilkes test is used to assess for normality\n\n\\(H_0\\): data are from a normal population\n\\(H_A\\): data are NOT from a normal population\n\n\n\n\nladder(gapm$female_literacy_rate_2011) %&gt;% \n  gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      Transformation\n      statistic\n      p.value\n    \n  \n  \n    cubic\n0.850\n0.000\n    square\n0.830\n0.000\n    identity\n0.792\n0.000\n    sqrt\n0.755\n0.000\n    log\n0.693\n0.000\n    1/sqrt\n0.599\n0.000\n    inverse\n0.479\n0.000\n    1/square\n0.264\n0.000\n    1/cubic\n0.159\n0.000"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#tips",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#tips",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Tips",
    "text": "Tips\n\nRecall, assessing our LINE assumptions are not on \\(Y\\) alone!!\n\nWe can use gladder() to get a sense of what our transformations will do to the data, but we need to check with our residuals again!!\n\nTransformations usually work better if all values are positive (or negative)\nIf observation has a 0, then we cannot perform certain transformations\nLog function only defined for positive values\n\nWe might take the \\(log(X+1)\\) if \\(X\\) includes a 0 value\n\nWhen we make cubic or sqaure transformations, we MUST include the original \\(X\\)\n\nWe do not do this for \\(Y\\) though"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#add-quadratic-and-cubic-transformations-to-dataset",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#add-quadratic-and-cubic-transformations-to-dataset",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Add quadratic and cubic transformations to dataset",
    "text": "Add quadratic and cubic transformations to dataset\n\nHelpful to make a new variable with the transformation in your dataset\n\n\ngapm &lt;- gapm %&gt;% \n  mutate(LE_2 = life_expectancy_years_2011^2,\n         LE_3 = life_expectancy_years_2011^3,\n         FLR_2 = female_literacy_rate_2011^2,\n         FLR_3 = female_literacy_rate_2011^3)\n\nglimpse(gapm)\n\nRows: 188\nColumns: 8\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andor…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9, 76.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.9, 99.5,…\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"…\n$ LE_2                       &lt;dbl&gt; 3214.89, 5882.89, 5882.89, 6822.76, 3708.81…\n$ LE_3                       &lt;dbl&gt; 182284.3, 451217.7, 451217.7, 563560.0, 225…\n$ FLR_2                      &lt;dbl&gt; 169.00, 9158.49, NA, NA, 3433.96, 9880.36, …\n$ FLR_3                      &lt;dbl&gt; 2197.0, 876467.5, NA, NA, 201230.1, 982107.…"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#we-are-going-to-compare-a-few-different-models-with-transformations",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#we-are-going-to-compare-a-few-different-models-with-transformations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "We are going to compare a few different models with transformations",
    "text": "We are going to compare a few different models with transformations\nWe are going to call life expectancy \\(LE\\) and female literacy rate \\(FLR\\)\n\nModel 1: \\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\nModel 2: \\(LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\nModel 3: \\(LE^3 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\nModel 4: \\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\epsilon\\)\nModel 5: \\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon\\)\nModel 6: \\(LE^3 = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon\\)"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#poll-everywhere-question-4",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#poll-everywhere-question-4",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#compare-scatterplots-does-linearity-improve",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#compare-scatterplots-does-linearity-improve",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Compare Scatterplots: does linearity improve?",
    "text": "Compare Scatterplots: does linearity improve?"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#run-models-with-transformations-examples",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#run-models-with-transformations-examples",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Run models with transformations: examples",
    "text": "Run models with transformations: examples\nModel 2: \\(LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\nmodel2 &lt;- lm(LE_2 ~ female_literacy_rate_2011,\n             data = gapm)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n2,401.272\n352.070\n6.820\n0.000\n    female_literacy_rate_2011\n31.174\n4.166\n7.484\n0.000\n  \n  \n  \n\n\n\n\nModel 6: \\(LE^3 = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon\\)\n\nmodel6 &lt;- lm(LE_3 ~ \n               female_literacy_rate_2011 + FLR_2 + FLR_3,\n             data = gapm)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n67,691.796\n149,056.945\n0.454\n0.651\n    female_literacy_rate_2011\n8,092.133\n8,473.154\n0.955\n0.343\n    FLR_2\n−128.596\n147.876\n−0.870\n0.387\n    FLR_3\n0.840\n0.794\n1.059\n0.293"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#normal-q-q-plots-comparison",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#normal-q-q-plots-comparison",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Normal Q-Q plots comparison",
    "text": "Normal Q-Q plots comparison"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#residual-plots-comparison",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#residual-plots-comparison",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Residual plots comparison",
    "text": "Residual plots comparison"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#summary-of-transformations",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#summary-of-transformations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Summary of transformations",
    "text": "Summary of transformations\n\nIf the model without the transformation is blatantly violating a LINE assumption\n\nThen a transformation is a good idea\n\nIf the model without a transformation is not following the LINE assumptions very well, but is mostly okay\n\nThen try to avoid a transformation\nThink about what predictors might need to be added\nEspecially if you keep seeing the same points as influential\n\nIf interpretability is important in your final work, then transformations are not a great solution"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#models-comparison",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#models-comparison",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Models comparison",
    "text": "Models comparison\n\n# library(gtsummary) for tbl_regression() and tbl_merge()\n\ntbl_model1 &lt;- tbl_regression(model1)\n\ntbl_model2 &lt;- tbl_regression(model2)\n\ntbl_model3 &lt;- tbl_regression(model3)\n\ntbl_model4 &lt;- tbl_regression(model4)\n\ntbl_model5 &lt;- tbl_regression(model5)\n\ntbl_model6 &lt;- tbl_regression(model6)\n\n# Compare models 1-3\ntbl_merge(\n  tbls = list(tbl_model1, tbl_model2, tbl_model3),\n  tab_spanner = c(\"Model 1: y=LE\", \"Model 2: y=LE^2\", \"Model 3: y=LE^3\")\n  )\n\n\n\n\n\n  \n    \n      Characteristic\n      \n        Model 1: y=LE\n      \n      \n        Model 2: y=LE^2\n      \n      \n        Model 3: y=LE^3\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    female_literacy_rate_2011\n0.23\n0.17, 0.29\n&lt;0.001\n31\n23, 39\n&lt;0.001\n3,166\n2,327, 4,006\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n# Compare models 4-6\ntbl_merge(\n  tbls = list(tbl_model4, tbl_model5, tbl_model6),\n  tab_spanner = c(\"Model 4: y=LE\", \"Model 5: y=LE\", \"Model 6: y=LE^3\")\n  )\n\n\n\n\n\n  \n    \n      Characteristic\n      \n        Model 4: y=LE\n      \n      \n        Model 5: y=LE\n      \n      \n        Model 6: y=LE^3\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    female_literacy_rate_2011\n0.02\n-0.38, 0.42\n&gt;0.9\n0.65\n-0.61, 1.9\n0.3\n8,092\n-8,784, 24,968\n0.3\n    FLR_2\n0.00\n0.00, 0.00\n0.3\n-0.01\n-0.03, 0.01\n0.4\n-129\n-423, 166\n0.4\n    FLR_3\n\n\n\n0.00\n0.00, 0.00\n0.3\n0.84\n-0.74, 2.4\n0.3\n  \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#other-fit-statistics-comparison",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#other-fit-statistics-comparison",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Other fit statistics comparison",
    "text": "Other fit statistics comparison\n\nglance(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4109366\n0.4033845\n6.142157\n54.4136\n1.501286e-10\n1\n-257.7164\n521.4329\n528.579\n2942.635\n78\n80\n  \n  \n  \n\n\n\nglance(model2) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4179234\n0.4104609\n812.8336\n56.00298\n9.352191e-11\n1\n-648.5445\n1303.089\n1310.235\n51534476\n78\n80\n  \n  \n  \n\n\n\nglance(model3) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4196986\n0.4122588\n82263.89\n56.41291\n8.285324e-11\n1\n-1017.917\n2041.835\n2048.981\n527853141587\n78\n80\n  \n  \n  \n\n\n\nglance(model4) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4195991\n0.4045238\n6.13629\n27.83346\n8.008115e-10\n2\n-257.1239\n522.2477\n531.7758\n2899.362\n77\n80\n  \n  \n  \n\n\n\nglance(model5) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4278828\n0.4052993\n6.132293\n18.94664\n2.844144e-09\n3\n-256.5488\n523.0977\n535.0078\n2857.981\n76\n80\n  \n  \n  \n\n\n\nglance(model6) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4414424\n0.419394\n81763.02\n20.02158\n1.160111e-09\n3\n-1016.39\n2042.78\n2054.69\n508074577758\n76\n80"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#example-chapter-5-problem-9",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#example-chapter-5-problem-9",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Example: Chapter 5 Problem 9",
    "text": "Example: Chapter 5 Problem 9\n\nIn an experiment designed to describe the dose–response curve for vitamin K, individual rats were depleted of their vitamin K reserves and then fed dried liver for 4 days at different dosage levels.\nThe response of each rat was measured as the concentration of a clotting agent needed to clot a sample of its blood in 3 minutes.\nThe results of the experiment on 12 rats are given in the following table; values are expressed in common logarithms for both dose and response.\n\nNote: by “common logarithm” the authors mean a base 10 logarithm\n\n\n\nQuestion: why did they choose a log-log transformation?\n\n\nrats &lt;- read_excel(\"data/CH05Q09.xls\")\nglimpse(rats)\n\nRows: 12\nColumns: 3\n$ RAT     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ LOGCONC &lt;dbl&gt; 2.65, 2.25, 2.26, 1.95, 1.72, 1.60, 1.55, 1.32, 1.13, 1.07, 0.…\n$ LOGDOSE &lt;dbl&gt; 0.18, 0.33, 0.42, 0.54, 0.65, 0.75, 0.83, 0.92, 1.01, 1.04, 1.…\n\nloglog_plot &lt;- ggplot(rats, aes(x = LOGDOSE, y = LOGCONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Transformed variables\")\nloglog_plot"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#reference-all-run-models",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#reference-all-run-models",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Reference: all run models",
    "text": "Reference: all run models\n\n\nModel 2: \\(LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\nmodel2 &lt;- lm(LE_2 ~ female_literacy_rate_2011,\n             data = gapm)\n\ntidy(model2) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n2401.27207\n352.069818\n6.820443\n1.726640e-09\n    female_literacy_rate_2011\n31.17351\n4.165624\n7.483514\n9.352191e-11\n  \n  \n  \n\n\n\n\nModel 3: \\(LE^3 \\sim FLR\\)\n\nmodel3 &lt;- lm(LE_3 ~ female_literacy_rate_2011,\n             data = gapm)\n\ntidy(model3) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n95453.189\n35631.6898\n2.678885\n9.005716e-03\n    female_literacy_rate_2011\n3166.481\n421.5875\n7.510853\n8.285324e-11\n  \n  \n  \n\n\n\n\nModel 4: \\(LE \\sim FLR + FLR^2\\)\n\nmodel4 &lt;- lm(life_expectancy_years_2011 ~ \n               female_literacy_rate_2011 + FLR_2,\n             data = gapm)\n\ntidy(model4) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n57.030875456\n6.282845592\n9.07723652\n8.512585e-14\n    female_literacy_rate_2011\n0.019348795\n0.201021963\n0.09625215\n9.235704e-01\n    FLR_2\n0.001578649\n0.001472592\n1.07202008\n2.870595e-01\n  \n  \n  \n\n\n\n\n\nModel 5: \\(LE \\sim FLR + FLR^2 + FLR^3\\)\n\nmodel5 &lt;- lm(life_expectancy_years_2011 ~ \n               female_literacy_rate_2011 + FLR_2 + FLR_3,\n             data = gapm)\n\ntidy(model5) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n4.732796e+01\n1.117939e+01\n4.2335001\n6.373341e-05\n    female_literacy_rate_2011\n6.517986e-01\n6.354934e-01\n1.0256576\n3.083065e-01\n    FLR_2\n-9.952763e-03\n1.109080e-02\n-0.8973895\n3.723451e-01\n    FLR_3\n6.245016e-05\n5.953283e-05\n1.0490038\n2.975008e-01\n  \n  \n  \n\n\n\n\nModel 6: \\(LE^3 \\sim FLR + FLR^2 + FLR^3\\)\n\nmodel6 &lt;- lm(LE_3 ~ \n               female_literacy_rate_2011 + FLR_2 + FLR_3,\n             data = gapm)\n\ntidy(model6) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n67691.7963283\n1.490569e+05\n0.4541338\n0.6510268\n    female_literacy_rate_2011\n8092.1325988\n8.473154e+03\n0.9550320\n0.3425895\n    FLR_2\n-128.5960879\n1.478757e+02\n-0.8696230\n0.3872447\n    FLR_3\n0.8404736\n7.937625e-01\n1.0588477\n0.2930229"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/01_Review_key_info.html#key-dates",
    "href": "lessons/14_Purposeful_selection/01_Review_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01_key_info.html#key-dates",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html",
    "title": "SLR: Inference and Prediction",
    "section": "",
    "text": "Estimate the variance of the residuals\nUsing a hypothesis test, determine if there is enough evidence that population slope \\(\\beta_1\\) is not 0 (applies to \\(\\beta_0\\) as well)\nCalculate and report the estimate and confidence interval for the population slope \\(\\beta_1\\) (applies to \\(\\beta_0\\) as well)\nCalculate and report the estimate and confidence interval for the expected/mean response given \\(X\\)\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)\n\n\n\n\n\n\n\n\n\nWe fit Gapminder data with female literacy rate as our independent variable and life expectancy as our dependent variable\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~\n               female_literacy_rate_2011,\n                 data = gapm)\n# Get regression table:\ntidy(model1) %&gt;% gt() %&gt;% \n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]\n\n\n\n\n\nRows: 188 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (2): life_expectancy_years_2011, female_literacy_rate_2011\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n \nThe (population) regression model is denoted by:\n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable with a…\n\nNormal distribution with mean 0 and constant variance \\(\\sigma^2\\)\ni.e. \\(\\epsilon \\sim N(0, \\sigma^2)\\)"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#lets-revisit-the-regression-analysis-process",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#lets-revisit-the-regression-analysis-process",
    "title": "SLR: Inference and Prediction",
    "section": "",
    "text": "Model Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#lets-remind-ourselves-of-the-model-that-we-fit-last-lesson",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#lets-remind-ourselves-of-the-model-that-we-fit-last-lesson",
    "title": "SLR: Inference and Prediction",
    "section": "",
    "text": "We fit Gapminder data with female literacy rate as our independent variable and life expectancy as our dependent variable\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~\n               female_literacy_rate_2011,\n                 data = gapm)\n# Get regression table:\ntidy(model1) %&gt;% gt() %&gt;% \n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]\n\n\n\n\n\nRows: 188 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (2): life_expectancy_years_2011, female_literacy_rate_2011\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#fitted-line-is-derived-from-the-population-slr-model",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#fitted-line-is-derived-from-the-population-slr-model",
    "title": "SLR: Inference and Prediction",
    "section": "",
    "text": "The (population) regression model is denoted by:\n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable with a…\n\nNormal distribution with mean 0 and constant variance \\(\\sigma^2\\)\ni.e. \\(\\epsilon \\sim N(0, \\sigma^2)\\)"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#widehatsigma2-needed-ingredient-for-inference",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#widehatsigma2-needed-ingredient-for-inference",
    "title": "SLR: Inference and Prediction",
    "section": "\\(\\widehat\\sigma^2\\): Needed ingredient for inference",
    "text": "\\(\\widehat\\sigma^2\\): Needed ingredient for inference\n\nRecall our population model residuals are distributed by \\(\\epsilon \\sim N(0, \\sigma^2)\\)\n\nAnd our estimated residuals are \\(\\widehat\\epsilon \\sim N(0, \\widehat\\sigma^2)\\)\n\nHence, the variance of the errors (residuals) is estimated by \\(\\widehat{\\sigma}^2\\)\n\n\n\\[\\widehat{\\sigma}^2 = S_{y|x}^2= \\frac{1}{n-2}\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 =\\frac{1}{n-2}SSE = MSE\\]"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#widehatsigma2-i-hope-r-can-calculate-that-for-me",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#widehatsigma2-i-hope-r-can-calculate-that-for-me",
    "title": "SLR: Inference and Prediction",
    "section": "\\(\\widehat\\sigma^2\\): I hope R can calculate that for me…",
    "text": "\\(\\widehat\\sigma^2\\): I hope R can calculate that for me…\n\nThe standard deviation \\(\\widehat{\\sigma}\\) is given in the R output as the Residual standard error\n\n\\(4^{th}\\) line from the bottom in the summary() output of the model:\n\n\n\n(m1_sum = summary(model1))\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\n  (108 observations deleted due to missingness)\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\nm1_sum$sigma\n\n[1] 6.142157\n\n# number of observations (pairs of data) used to run the model\nnobs(model1) \n\n[1] 80"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#widehatsigma2-to-sse",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#widehatsigma2-to-sse",
    "title": "SLR: Inference and Prediction",
    "section": "\\(\\widehat\\sigma^2\\) to SSE",
    "text": "\\(\\widehat\\sigma^2\\) to SSE\n\nRecall how we minimized the SSE to find our line of best fit\nSSE and \\(\\widehat\\sigma^2\\) are closely related:\n\n\\[\\begin{aligned}\n\\widehat{\\sigma}^2 & = \\frac{1}{n-2}SSE\\\\\n6.142^2 & = \\frac{1}{80-2}SSE\\\\\nSSE & = 78 \\cdot 6.142^2 = 2942.48\n\\end{aligned}\\]\n\n2942.48 is the smallest sums of squares of all possible regression lines through the data"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#do-we-trust-our-estimate-widehatbeta_1",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#do-we-trust-our-estimate-widehatbeta_1",
    "title": "SLR: Inference and Prediction",
    "section": "Do we trust our estimate \\(\\widehat\\beta_1\\)?",
    "text": "Do we trust our estimate \\(\\widehat\\beta_1\\)?\n\nSo far, we have shown that we think the estimate is 0.232  \n\\(\\widehat\\beta_1\\) uses our sample data to estimate the population parameter \\(\\beta_1\\)  \nInference helps us figure out mathematically how much we trust our best-fit line  \nAre we certain that the relationship between \\(X\\) and \\(Y\\) that we estimated reflects the true, underlying relationship?"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-2",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-2",
    "title": "SLR: Inference and Prediction",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#inference-for-the-population-slope-hypothesis-test-and-ci",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#inference-for-the-population-slope-hypothesis-test-and-ci",
    "title": "SLR: Inference and Prediction",
    "section": "Inference for the population slope: hypothesis test and CI",
    "text": "Inference for the population slope: hypothesis test and CI\n\n\n\n\nPopulation model\n\n\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma^2)\\)\n\\(\\sigma^2\\) is the variance of the residuals\n\n\n\n\nSample best-fit (least-squares) line\n\n\n\\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X \\]\nNote: Some sources use \\(b\\) instead of \\(\\widehat{\\beta}\\)\n\n\n\n\n\n \nWe have two options for inference:\n\nConduct the hypothesis test\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote: R reports p-values for 2-sided tests\n\nConstruct a 95% confidence interval for the population slope \\(\\beta_1\\)"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#steps-in-hypothesis-testing",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#steps-in-hypothesis-testing",
    "title": "SLR: Inference and Prediction",
    "section": "Steps in hypothesis testing",
    "text": "Steps in hypothesis testing"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#general-steps-for-hypothesis-test-for-population-slope-beta_1-t-test",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#general-steps-for-hypothesis-test-for-population-slope-beta_1-t-test",
    "title": "SLR: Inference and Prediction",
    "section": "General steps for hypothesis test for population slope \\(\\beta_1\\) (t-test)",
    "text": "General steps for hypothesis test for population slope \\(\\beta_1\\) (t-test)\n\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions (checked in our Model Evaluation step)\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nOften, we are curious if the coefficient is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(t\\), and follows a Student’s t-distribution.\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic for \\(\\widehat\\beta_1\\) is\n\\[t = \\frac{ \\widehat\\beta_1 - \\beta_1}{ \\text{SE}_{\\widehat\\beta_1}} = \\frac{ \\widehat\\beta_1}{ \\text{SE}_{\\widehat\\beta_1}}\\]\nwhen we assume \\(H_0: \\beta_1 = 0\\) is true.\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(2\\cdot P(T &gt; t)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at the \\(100\\alpha\\%\\) significiance level. There is (sufficient/insufficient) evidence that there is significant association between (\\(Y\\)) and (\\(X\\)) (p-value = \\(P(T &gt; t)\\))."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#standard-error-of-fitted-slope-widehatbeta_1",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#standard-error-of-fitted-slope-widehatbeta_1",
    "title": "SLR: Inference and Prediction",
    "section": "Standard error of fitted slope \\(\\widehat\\beta_1\\)",
    "text": "Standard error of fitted slope \\(\\widehat\\beta_1\\)\n\n   \n\n\n\\[\\text{SE}_{\\widehat\\beta_1} = \\frac{s_{\\textrm{residuals}}}{s_x\\sqrt{n-1}}\\]\n\n\\(\\text{SE}_{\\widehat\\beta_1}\\) is the variability of the statistic \\(\\widehat\\beta_1\\)\n\n\n   \n\n\n\n\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\n\n\n\n\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\n\n\n\\(n\\) is the sample size, or the number of (complete) pairs of points"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#calculating-standard-error-for-widehatbeta_1-12",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#calculating-standard-error-for-widehatbeta_1-12",
    "title": "SLR: Inference and Prediction",
    "section": "Calculating standard error for \\(\\widehat\\beta_1\\) (1/2)",
    "text": "Calculating standard error for \\(\\widehat\\beta_1\\) (1/2)\n\n\nOption 1: Calculate using the formula\n\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# standard deviation of the residuals (Residual standard error in summary() output)\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n# standard deviation of x's\n(s_x &lt;- sd(gapm$female_literacy_rate_2011, na.rm=T))\n\n[1] 21.95371\n\n# number of pairs of complete observations\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(se_b1 &lt;- s_resid/(s_x * sqrt(n-1))) # compare to SE in regression output\n\n[1] 0.03147744"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#calculating-standard-error-for-widehatbeta_1-22",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#calculating-standard-error-for-widehatbeta_1-22",
    "title": "SLR: Inference and Prediction",
    "section": "Calculating standard error for \\(\\widehat\\beta_1\\) (2/2)",
    "text": "Calculating standard error for \\(\\widehat\\beta_1\\) (2/2)\n\n\nOption 2: Use regression table\n\n\n# recall model1_b1 is regression table restricted to b1 row\nmodel1_b1 &lt;-tidy(model1) %&gt;% filter(term == \"female_literacy_rate_2011\")\nmodel1_b1 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 45) %&gt;% fmt_number(decimals = 4)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.2322\n0.0315\n7.3766\n0.0000"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#some-important-notes",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#some-important-notes",
    "title": "SLR: Inference and Prediction",
    "section": "Some important notes",
    "text": "Some important notes\n\nToday we are discussing the hypothesis test for a single coefficient\n\n \n\nThe test statistic for a single coefficient follows a Student’s t-distribution\n \n\nIt can also follow an F-distribution, but we will discuss this more with multiple linear regression and multi-level categorical covariates\n\n\n \n\nSingle coefficient testing can be done on any coefficient, but it is most useful for continuous covariates or binary covariates\n \n\nThis is because testing the single coefficient will still tell us something about the overall relationship between the covariate and the outcome\n\n \n\nWe will talk more about this with multiple linear regression and multi-level categorical covariates"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-3",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-3",
    "title": "SLR: Inference and Prediction",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-14",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-14",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (1/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (1/4)\n\nSteps 1-4 are setting up our hypothesis test: not much change from the general steps\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions (checked in our Model Evaluation step)\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nWe are testing if the slope is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(t\\), and follows a Student’s t-distribution."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-24",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-24",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/4)\n\n\n\nCompute the value of the test statistic\n\n\n\n\nOption 1: Calculate the test statistic using the values in the regression table\n\n\n# recall model1_b1 is regression table restricted to b1 row\nmodel1_b1 &lt;-tidy(model1) %&gt;% filter(term == \"female_literacy_rate_2011\")\nmodel1_b1 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n(TestStat_b1 &lt;- model1_b1$estimate / model1_b1$std.error)\n\n[1] 7.376557\n\n\n\nOption 2: Get the test statistic value (\\(t^*\\)) from R\n\n\nmodel1_b1 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-34",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-34",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (3/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (3/4)\n\n\n\nCalculate the p-value\n\n\n\n\nThe \\(p\\)-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true\nWe know the probability distribution of the test statistic (the null distribution) assuming \\(H_0\\) is true\nStatistical theory tells us that the test statistic \\(t\\) can be modeled by a \\(t\\)-distribution with \\(df = n-2\\).\n\nWe had 80 countries’ data, so \\(n=80\\)\n\nOption 1: Use pt() and our calculated test statistic\n\n\n(pv = 2*pt(TestStat_b1, df=80-2, lower.tail=F))\n\n[1] 1.501286e-10\n\n\n\nOption 2: Use the regression table output\n\n\nmodel1_b1 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-44",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-44",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (4/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (4/4)\n\n\n\nWrite conclusion for the hypothesis test\n\n\n\nWe reject the null hypothesis that the slope is 0 at the \\(5\\%\\) significance level. There is sufficient evidence that there is significant association between female life expectancy and female literacy rates (p-value &lt; 0.0001)."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#note-on-hypothesis-testing-using-r",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#note-on-hypothesis-testing-using-r",
    "title": "SLR: Inference and Prediction",
    "section": "Note on hypothesis testing using R",
    "text": "Note on hypothesis testing using R\n\nWe can basically skip Step 5 if we are using the “Option 2” route\n\n \n\nIn our assignments: if you use Option 2, Step 5 is optional\n\nUnless I specifically ask for the test statistic!!"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-14",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-14",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (1/4)",
    "text": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (1/4)\n\nSteps 1-4 are setting up our hypothesis test: not much change from the general steps\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions (checked in our Model Evaluation step)\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nWe are testing if the intercept is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_0 = 0\\\\\n\\text{vs. } H_A&: \\beta_0 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThis is the same as the slope. The test statistic is \\(t\\), and follows a Student’s t-distribution."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-24",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-24",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (2/4)",
    "text": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (2/4)\n\n\n\nCompute the value of the test statistic\n\n\n\n\nOption 1: Calculate the test statistic using the values in the regression table\n\n\n# recall model1_b1 is regression table restricted to b1 row\nmodel1_b0 &lt;-tidy(model1) %&gt;% filter(term == \"(Intercept)\")\nmodel1_b0 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n  \n  \n  \n\n\n\n(TestStat_b0 &lt;- model1_b0$estimate / model1_b0$std.error)\n\n[1] 19.1429\n\n\n\nOption 2: Get the test statistic value (\\(t^*\\)) from R\n\n\nmodel1_b0 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-34",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-34",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (3/4)",
    "text": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (3/4)\n\n\n\nCalculate the p-value\n\n\n\n \n\nOption 1: Use pt() and our calculated test statistic\n\n\n(pv = 2*pt(TestStat_b0, df=80-2, lower.tail=F))\n\n[1] 3.325312e-31\n\n\n \n\nOption 2: Use the regression table output\n\n\nmodel1_b0 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9279\n2.660407\n19.1429\n3.325312e-31"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-44",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-44",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (4/4)",
    "text": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (4/4)\n\n\n\nWrite conclusion for the hypothesis test\n\n\n\nWe reject the null hypothesis that the intercept is 0 at the \\(5\\%\\) significance level. There is sufficient evidence that the intercept for the association between average female life expectancy and female literacy rates is different from 0 (p-value &lt; 0.0001).\n   \n\nNote: if we fail to reject \\(H_0\\), then we could decide to remove the intercept from the model to force the regression line to go through the origin (0,0) if it makes sense to do so for the application."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#inference-for-the-population-slope-hypothesis-test-and-ci-1",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#inference-for-the-population-slope-hypothesis-test-and-ci-1",
    "title": "SLR: Inference and Prediction",
    "section": "Inference for the population slope: hypothesis test and CI",
    "text": "Inference for the population slope: hypothesis test and CI\n\n\n\n\nPopulation model\n\n\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma^2)\\)\n\\(\\sigma^2\\) is the variance of the residuals\n\n\n\n\nSample best-fit (least-squares) line\n\n\n\\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X \\]\nNote: Some sources use \\(b\\) instead of \\(\\widehat{\\beta}\\)\n\n\n\n\n\n \nWe have two options for inference:\n\nConduct the hypothesis test\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote: R reports p-values for 2-sided tests\n\nConstruct a 95% confidence interval for the population slope \\(\\beta_1\\)"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#confidence-interval-for-population-slope-beta_1",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#confidence-interval-for-population-slope-beta_1",
    "title": "SLR: Inference and Prediction",
    "section": "Confidence interval for population slope \\(\\beta_1\\)",
    "text": "Confidence interval for population slope \\(\\beta_1\\)\nRecall the general CI formula:\n\\[\\widehat{\\beta}_1 \\pm t_{\\alpha, n-2}^* \\cdot SE_{\\widehat{\\beta}_1}\\]\nTo construct the confidence interval, we need to:\n\nSet our \\(\\alpha\\)-level\nFind \\(\\widehat\\beta_1\\)\nCalculate the \\(t_{n-2}^*\\)\nCalculate \\(SE_{\\widehat{\\beta}_1}\\)"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#calculate-ci-for-population-slope-beta_1-12",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#calculate-ci-for-population-slope-beta_1-12",
    "title": "SLR: Inference and Prediction",
    "section": "Calculate CI for population slope \\(\\beta_1\\) (1/2)",
    "text": "Calculate CI for population slope \\(\\beta_1\\) (1/2)\n\n\n\\[\\widehat{\\beta}_1  \\pm t^*\\cdot SE_{\\beta_1}\\]\n\nwhere \\(t^*\\) is the \\(t\\)-distribution critical value with \\(df = n -2\\).\n\n\n\nOption 1: Calculate using each value\n\nSave values needed for CI:\n\nb1 &lt;- model1_b1$estimate\nSE_b1 &lt;- model1_b1$std.error\n\n\nnobs(model1) # sample size n\n\n[1] 80\n\n(tstar &lt;- qt(.975, df = 80-2))\n\n[1] 1.990847\n\n\nUse formula to calculate each bound\n\n(CI_LB &lt;- b1 - tstar*SE_b1)\n\n[1] 0.1695284\n\n(CI_UB &lt;- b1 + tstar*SE_b1)\n\n[1] 0.2948619"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#calculate-ci-for-population-slope-beta_1-22",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#calculate-ci-for-population-slope-beta_1-22",
    "title": "SLR: Inference and Prediction",
    "section": "Calculate CI for population slope \\(\\beta_1\\) (2/2)",
    "text": "Calculate CI for population slope \\(\\beta_1\\) (2/2)\n\n\n\\[\\widehat{\\beta}_1  \\pm t^*\\cdot SE_{\\beta_1}\\]\n\nwhere \\(t^*\\) is the \\(t\\)-distribution critical value with \\(df = n -2\\).\n\n\n\nOption 2: Use the regression table\n\n\ntidy(model1, conf.int = T) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n45.631\n56.224\n    female_literacy_rate_2011\n0.232\n0.031\n7.377\n0.000\n0.170\n0.295"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#reporting-the-coefficient-estimate-of-the-population-slope",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#reporting-the-coefficient-estimate-of-the-population-slope",
    "title": "SLR: Inference and Prediction",
    "section": "Reporting the coefficient estimate of the population slope",
    "text": "Reporting the coefficient estimate of the population slope\n\nWhen we report our results to someone else, we don’t usually show them our full hypothesis test\n\nIn an informal setting, someone may want to see it\n\nTypically, we report the estimate with the confidence interval\n\nFrom the confidence interval, your audience can also deduce the results of a hypothesis test\n\nOnce we found our CI, we often just write the interpretation of the coefficient estimate:\n\n\n\nGeneral statement for population slope inference\n\n\nFor every increase of 1 unit in the \\(X\\)-variable, there is an expected average increase of \\(\\widehat\\beta_1\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\nIn our example: For every 1% increase in female literacy rate, the average life expectancy is expected to increase, on average, 0.232 years (95% CI: 0.170, 0.295)."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-4",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-4",
    "title": "SLR: Inference and Prediction",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#for-reference-quick-ci-for-beta_0",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#for-reference-quick-ci-for-beta_0",
    "title": "SLR: Inference and Prediction",
    "section": "For reference: quick CI for \\(\\beta_0\\)",
    "text": "For reference: quick CI for \\(\\beta_0\\)\n\nCalculate CI for population intercept \\(\\beta_0\\): \\(\\widehat{\\beta}_0 \\pm t^*\\cdot SE_{\\beta_0}\\)\n\nwhere \\(t^*\\) is the \\(t\\)-distribution critical value with \\(df = n -2\\)\n\nUse the regression table\n\n\ntidy(model1, conf.int = T) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n45.631\n56.224\n    female_literacy_rate_2011\n0.232\n0.031\n7.377\n0.000\n0.170\n0.295\n  \n  \n  \n\n\n\n\n\n\nGeneral statement for population intercept inference\n\n\nThe expected outcome for the \\(Y\\)-variable is (\\(\\widehat\\beta_0\\)) when the \\(X\\)-variable is 0 (95% CI: LB, UB).\n\n\n\nFor example: The expected/average life expectancy is 50.9 years when the female literacy rate is 0 (95% CI: 45.63, 56.22)."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#finding-a-mean-response-given-a-value-of-our-independent-variable",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#finding-a-mean-response-given-a-value-of-our-independent-variable",
    "title": "SLR: Inference and Prediction",
    "section": "Finding a mean response given a value of our independent variable",
    "text": "Finding a mean response given a value of our independent variable\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n    female_literacy_rate_2011\n0.232\n0.031\n7.377\n0.000\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]\n\nWhat is the expected/predicted life expectancy for a country with female literacy rate 60%?\n\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot 60 = 64.82\\]\n\n(y_60 &lt;- 50.9 + 0.232*60)\n\n[1] 64.82\n\n\n\nHow do we interpret the expected value?\n\nWe sometimes call this “predicted” value, since we can technically use a literacy rate that is not in our sample\n\nHow variable is it?"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#mean-responseprediction-with-regression-line",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#mean-responseprediction-with-regression-line",
    "title": "SLR: Inference and Prediction",
    "section": "Mean response/prediction with regression line",
    "text": "Mean response/prediction with regression line\n\n\nRecall the population model:\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma^2)\\)\n\n\nWhen we take the expected value, at a given value \\(X^*\\), the average expected response at \\(X^*\\) is:\n\n\\[\\widehat{E}[Y|X^*] = \\widehat\\beta_0 + \\widehat\\beta_1 X^*\\]\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nThese are the points on the regression line\nThe mean responses have variability, and we can calculate a CI for it, for every value of \\(X^*\\)"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#ci-for-population-mean-response-eyx-or-mu_yx",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#ci-for-population-mean-response-eyx-or-mu_yx",
    "title": "SLR: Inference and Prediction",
    "section": "CI for population mean response (\\(E[Y|X^*]\\) or \\(\\mu_{Y|X^*})\\)",
    "text": "CI for population mean response (\\(E[Y|X^*]\\) or \\(\\mu_{Y|X^*})\\)\n\\[\\widehat{E}[Y|X^*] \\pm t_{n-2}^* \\cdot SE_{\\widehat{E}[Y|X^*]}\\]\n\\[SE_{\\widehat{E}[Y|X^*]} = s_{\\text{residuals}} \\sqrt{\\frac{1}{n} + \\frac{(X^* - \\overline{X})^2}{(n-1)s_X^2}}\\]\n\n\\(\\widehat{E}[Y|X^*]\\) is the predicted value at the specified point \\(X^*\\) of the explanatory variable\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\\(\\overline{X}\\) is the sample mean of the explanatory variable \\(x\\)\n\\(s_X\\) is the sample sd of the explanatory variable \\(X\\)\n\n\n\nRecall that \\(t_{n-2}^*\\) is calculated using qt() and depends on the confidence level (\\(1-\\alpha\\))"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#example-option-1-ci-for-mean-response-mu_yx",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#example-option-1-ci-for-mean-response-mu_yx",
    "title": "SLR: Inference and Prediction",
    "section": "Example Option 1: CI for mean response \\(\\mu_{Y|X^*}\\)",
    "text": "Example Option 1: CI for mean response \\(\\mu_{Y|X^*}\\)\nFind the 95% CI for the mean life expectancy when the female literacy rate is 60.\n\n\\[\\begin{align}\n\\widehat{E}[Y|X^*] &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E}[Y|X^*]}\\\\\n64.8596 &\\pm 1.990847 \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(X^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 0.9675541\\\\\n64.8596 &\\pm 1.926252\\\\\n(62.93335 &, 66.78586)\n\\end{align}\\]\n\n\n\n\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n\n\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011, na.rm=T))\n\n[1] 81.65375\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011, na.rm=T))\n\n[1] 21.95371\n\n\n\n\n\n(SE_Yx &lt;- s_resid *sqrt(1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] 0.9675541\n\n\n\n\n\n(MOE_Yx &lt;- SE_Yx*tstar)\n\n[1] 1.926252\n\n\n\n\n\n\nY60 - MOE_Yx\n\n[1] 62.93335\n\n\n\n\n\n\nY60 + MOE_Yx\n\n[1] 66.78586"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#example-option-2-ci-for-mean-response-mu_yx",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#example-option-2-ci-for-mean-response-mu_yx",
    "title": "SLR: Inference and Prediction",
    "section": "Example Option 2: CI for mean response \\(\\mu_{Y|X^*}\\)",
    "text": "Example Option 2: CI for mean response \\(\\mu_{Y|X^*}\\)\nFind the 95% CI’s for the mean life expectancy when the female literacy rate is 60 and 80.\n\nUse the base R predict() function\nRequires specification of a newdata “value”\n\nThe newdata value is \\(X^*\\)\nThis has to be in the format of a data frame though\nwith column name identical to the predictor variable in the model\n\n\n\nnewdata &lt;- data.frame(female_literacy_rate_2011 = c(60, 80)) \nnewdata\n\n  female_literacy_rate_2011\n1                        60\n2                        80\n\n\n\n\n\npredict(model1, \n        newdata=newdata, \n        interval=\"confidence\")\n\n       fit      lwr      upr\n1 64.85961 62.93335 66.78586\n2 69.50351 68.13244 70.87457\n\n\n\n\n\nInterpretation\n\n\nWe are 95% confident that the average life expectancy for a country with a 60% female literacy rate will be between 62.9 and 66.8 years."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-5",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-5",
    "title": "SLR: Inference and Prediction",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#confidence-bands-for-mean-response-mu_yx",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#confidence-bands-for-mean-response-mu_yx",
    "title": "SLR: Inference and Prediction",
    "section": "Confidence bands for mean response \\(\\mu_{Y|X^*}\\)",
    "text": "Confidence bands for mean response \\(\\mu_{Y|X^*}\\)\n\nOften we plot the CI for many values of X, creating confidence bands\nThe confidence bands are what ggplot creates when we set se = TRUE within geom_smooth\nThink about it: for what values of X are the confidence bands (intervals) narrowest?\n\n\nggplot(gapm,\n       aes(x=female_literacy_rate_2011, \n           y=life_expectancy_years_2011)) +\n  geom_point()+\n  geom_smooth(method = lm, se=TRUE)+\n  ggtitle(\"Life expectancy vs. female literacy rate\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "title": "SLR: Inference and Prediction",
    "section": "Width of confidence bands for mean response \\(\\mu_{Y|X^*}\\)",
    "text": "Width of confidence bands for mean response \\(\\mu_{Y|X^*}\\)\n\nFor what values of \\(X^*\\) are the confidence bands (intervals) narrowest? widest?\n\n\\[\\begin{align}\n\\widehat{E}[Y|X^*] &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E}[Y|X^*]}\\\\\n\\widehat{E}[Y|X^*] &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(X^* - \\bar{x})^2}{(n-1)s_x^2}}\n\\end{align}\\]\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro_key_info.html#key-dates",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html",
    "title": "Lesson 11: Interactions Continued",
    "section": "",
    "text": "Interpret the interaction component of a model with two continuous covariates, and how the main variable’s effect changes.\n\n\n\nWhen there are only two covariates in the model, test whether one is a confounder or effect modifier.\n\n\n\n\n\n\nWe can start by visualizing the relationship between life expectancy and female literacy rate by food supply\nQuestions of interest: Does the effect of female literacy rate on life expectancy differ depending on food supply?\n\nThis is the same as: Is food supply is an effect modifier for female literacy rate? Is food supply an effect modifier of the association between life expectancy and female literacy rate?\n\nLet’s run an interaction model to see!\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel we are fitting:\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\beta_3 FLR^c \\cdot FS^c + \\epsilon\\]\n\n\\(LE\\) as life expectancy\n\\(FLR^c\\) as the centered around the mean female literacy rate (continuous variable)\n\\(FS^c\\) as the centered around the mean food supply (continuous variable)\n\n\n\nCode to center FLR and FS\ngapm_sub = gapm_sub %&gt;% \n  mutate(FLR_c = FemaleLiteracyRate - mean(FemaleLiteracyRate), \n         FS_c = FoodSupplykcPPD - mean(FoodSupplykcPPD))\nmean_FS = mean(gapm_sub$FoodSupplykcPPD) %&gt;% round(digits = 0)\nmean_FLR = mean(gapm_sub$FemaleLiteracyRate) %&gt;% round(digits = 2)\n\n\nIn R:\n\nm_int_fs = lm(LifeExpectancyYrs ~ FLR_c + FS_c + FLR_c*FS_c, data = gapm_sub)\n\nOR\n\nm_int_fs = lm(LifeExpectancyYrs ~ FLR_c*FS_c, data = gapm_sub)\n\n\n\n\n\ntidy_m_fs = tidy(m_int_fs, conf.int=T) \ntidy_m_fs %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 5)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n70.32060\n0.72393\n97.13721\n0.00000\n68.87601\n71.76518\n    FLR_c\n0.15532\n0.03808\n4.07905\n0.00012\n0.07934\n0.23130\n    FS_c\n0.00849\n0.00182\n4.67908\n0.00001\n0.00487\n0.01212\n    FLR_c:FS_c\n−0.00001\n0.00008\n−0.06908\n0.94513\n−0.00016\n0.00015\n  \n  \n  \n\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & 70.32 + 0.16 \\cdot FLR^c + 0.01 \\cdot FS^c - 0.00001 \\cdot FLR^c \\cdot FS^c\n\\end{aligned}\\]\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & 70.32 + 0.16 \\cdot FLR^c + 0.01 \\cdot FS^c - 0.00001 \\cdot FLR^c \\cdot FS^c\n\\end{aligned}\\]\nTo identify different lines, we need to pick example values of Food Supply:\n\n\n\n\nFood Supply of 1812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot (-1000) + \\\\ & \\widehat\\beta_3 FLR^c \\cdot (-1000) \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 - 1000 \\widehat\\beta_2 \\big)+ \\\\ & \\big(\\widehat\\beta_1 - 1000 \\widehat\\beta_3  \\big) FLR^c\n\\end{aligned}\\]\n\n\n\n\n\nFood Supply of 2812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\\\ & \\widehat\\beta_3 FLR^c \\cdot 0 \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 \\big)+ \\\\ & \\big(\\widehat\\beta_1 \\big) FLR^c\n\\end{aligned}\\]\n\n\n\n\n\nFood Supply of 3812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot 1000 + \\\\ & \\widehat\\beta_3 FLR^c \\cdot 1000 \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 + 1000 \\widehat\\beta_2 \\big)+ \\\\ & \\big(\\widehat\\beta_1 + 1000 \\widehat\\beta_3  \\big) FLR^c\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_2 \\cdot FS^c \\bigg] + \\underbrace{\\bigg[\\widehat\\beta_1 + \\widehat\\beta_3 \\cdot FS^c \\bigg]}_\\text{FLR's effect} FLR \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in female literacy rate’s effect, for every one kcal PPD increase in food supply\n\nIn summary, the interaction term can be interpreted as “difference in adjusted female literacy rate effect for every 1 kcal PPD increase in food supply”\nIt will be helpful to test the interaction to round out this interpretation!!\n\n\n\n\n\nWe run an F-test for a single coefficients (\\(\\beta_3\\)) in the below model (see lesson 9)\n\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\beta_3 FLR^c \\cdot FS^c + \\epsilon\\]\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\[\\beta_3=0\\]\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\[\\beta_3\\neq0\\]\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\epsilon\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}\nLE = & \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\\\ & \\beta_3 FLR^c \\cdot FS^c + \\epsilon\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\nFit the reduced and full model\n\n\nm_int_fs_red = lm(LifeExpectancyYrs ~ FLR_c + FS_c, \n                   data = gapm_sub)\nm_int_fs_full = lm(LifeExpectancyYrs ~ FLR_c + FS_c +\n                  FLR_c*FS_c, data = gapm_sub)\n\n\n\nDisplay the ANOVA table with F-statistic and p-value\nanova(m_int_fs_red, m_int_fs_full) %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FLR_c + FS_c\n69.000\n2,005.556\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FLR_c + FS_c + FLR_c * FS_c\n68.000\n2,005.415\n1.000\n0.141\n0.005\n0.945\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and food supply (p = 0.945). Food supply is not an effect modifier of the association between female literacy rate and life expectancy."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#do-we-think-food-supply-is-an-effect-modifier-for-female-literacy-rate",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#do-we-think-food-supply-is-an-effect-modifier-for-female-literacy-rate",
    "title": "Lesson 11: Interactions Continued",
    "section": "",
    "text": "We can start by visualizing the relationship between life expectancy and female literacy rate by food supply\nQuestions of interest: Does the effect of female literacy rate on life expectancy differ depending on food supply?\n\nThis is the same as: Is food supply is an effect modifier for female literacy rate? Is food supply an effect modifier of the association between life expectancy and female literacy rate?\n\nLet’s run an interaction model to see!"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#model-with-interaction-between-two-continuous-variables",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#model-with-interaction-between-two-continuous-variables",
    "title": "Lesson 11: Interactions Continued",
    "section": "",
    "text": "Model we are fitting:\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\beta_3 FLR^c \\cdot FS^c + \\epsilon\\]\n\n\\(LE\\) as life expectancy\n\\(FLR^c\\) as the centered around the mean female literacy rate (continuous variable)\n\\(FS^c\\) as the centered around the mean food supply (continuous variable)\n\n\n\nCode to center FLR and FS\ngapm_sub = gapm_sub %&gt;% \n  mutate(FLR_c = FemaleLiteracyRate - mean(FemaleLiteracyRate), \n         FS_c = FoodSupplykcPPD - mean(FoodSupplykcPPD))\nmean_FS = mean(gapm_sub$FoodSupplykcPPD) %&gt;% round(digits = 0)\nmean_FLR = mean(gapm_sub$FemaleLiteracyRate) %&gt;% round(digits = 2)\n\n\nIn R:\n\nm_int_fs = lm(LifeExpectancyYrs ~ FLR_c + FS_c + FLR_c*FS_c, data = gapm_sub)\n\nOR\n\nm_int_fs = lm(LifeExpectancyYrs ~ FLR_c*FS_c, data = gapm_sub)"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#displaying-the-regression-table-and-writing-fitted-regression-equation",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#displaying-the-regression-table-and-writing-fitted-regression-equation",
    "title": "Lesson 11: Interactions Continued",
    "section": "",
    "text": "tidy_m_fs = tidy(m_int_fs, conf.int=T) \ntidy_m_fs %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 5)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n70.32060\n0.72393\n97.13721\n0.00000\n68.87601\n71.76518\n    FLR_c\n0.15532\n0.03808\n4.07905\n0.00012\n0.07934\n0.23130\n    FS_c\n0.00849\n0.00182\n4.67908\n0.00001\n0.00487\n0.01212\n    FLR_c:FS_c\n−0.00001\n0.00008\n−0.06908\n0.94513\n−0.00016\n0.00015\n  \n  \n  \n\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & 70.32 + 0.16 \\cdot FLR^c + 0.01 \\cdot FS^c - 0.00001 \\cdot FLR^c \\cdot FS^c\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#comparing-fitted-regression-lines-for-various-food-supply-values",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#comparing-fitted-regression-lines-for-various-food-supply-values",
    "title": "Lesson 11: Interactions Continued",
    "section": "",
    "text": "\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & 70.32 + 0.16 \\cdot FLR^c + 0.01 \\cdot FS^c - 0.00001 \\cdot FLR^c \\cdot FS^c\n\\end{aligned}\\]\nTo identify different lines, we need to pick example values of Food Supply:\n\n\n\n\nFood Supply of 1812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot (-1000) + \\\\ & \\widehat\\beta_3 FLR^c \\cdot (-1000) \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 - 1000 \\widehat\\beta_2 \\big)+ \\\\ & \\big(\\widehat\\beta_1 - 1000 \\widehat\\beta_3  \\big) FLR^c\n\\end{aligned}\\]\n\n\n\n\n\nFood Supply of 2812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\\\ & \\widehat\\beta_3 FLR^c \\cdot 0 \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 \\big)+ \\\\ & \\big(\\widehat\\beta_1 \\big) FLR^c\n\\end{aligned}\\]\n\n\n\n\n\nFood Supply of 3812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot 1000 + \\\\ & \\widehat\\beta_3 FLR^c \\cdot 1000 \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 + 1000 \\widehat\\beta_2 \\big)+ \\\\ & \\big(\\widehat\\beta_1 + 1000 \\widehat\\beta_3  \\big) FLR^c\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#interpretation-for-interaction-between-two-continuous-variables",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#interpretation-for-interaction-between-two-continuous-variables",
    "title": "Lesson 11: Interactions Continued",
    "section": "",
    "text": "\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_2 \\cdot FS^c \\bigg] + \\underbrace{\\bigg[\\widehat\\beta_1 + \\widehat\\beta_3 \\cdot FS^c \\bigg]}_\\text{FLR's effect} FLR \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in female literacy rate’s effect, for every one kcal PPD increase in food supply\n\nIn summary, the interaction term can be interpreted as “difference in adjusted female literacy rate effect for every 1 kcal PPD increase in food supply”\nIt will be helpful to test the interaction to round out this interpretation!!"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#test-interaction-between-two-continuous-variables",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#test-interaction-between-two-continuous-variables",
    "title": "Lesson 11: Interactions Continued",
    "section": "",
    "text": "We run an F-test for a single coefficients (\\(\\beta_3\\)) in the below model (see lesson 9)\n\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\beta_3 FLR^c \\cdot FS^c + \\epsilon\\]\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\[\\beta_3=0\\]\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\[\\beta_3\\neq0\\]\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\epsilon\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}\nLE = & \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\\\ & \\beta_3 FLR^c \\cdot FS^c + \\epsilon\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#test-interaction-between-two-continuous-variables-1",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#test-interaction-between-two-continuous-variables-1",
    "title": "Lesson 11: Interactions Continued",
    "section": "",
    "text": "Fit the reduced and full model\n\n\nm_int_fs_red = lm(LifeExpectancyYrs ~ FLR_c + FS_c, \n                   data = gapm_sub)\nm_int_fs_full = lm(LifeExpectancyYrs ~ FLR_c + FS_c +\n                  FLR_c*FS_c, data = gapm_sub)\n\n\n\nDisplay the ANOVA table with F-statistic and p-value\nanova(m_int_fs_red, m_int_fs_full) %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FLR_c + FS_c\n69.000\n2,005.556\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FLR_c + FS_c + FLR_c * FS_c\n68.000\n2,005.415\n1.000\n0.141\n0.005\n0.945\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and food supply (p = 0.945). Food supply is not an effect modifier of the association between female literacy rate and life expectancy."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#deciding-between-confounder-and-effect-modifier",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#deciding-between-confounder-and-effect-modifier",
    "title": "Lesson 11: Interactions Continued",
    "section": "Deciding between confounder and effect modifier",
    "text": "Deciding between confounder and effect modifier\n\nThis is more of a model selection question (in coming lectures)\nBut if we had a model with only TWO covariates, we could step through the following process:\n\nTest the interaction (of potential effect modifier): use a partial F-test to test if interaction term(s) explain enough variation compared to model without interaction\n\nRecall that for two continuous covariates, we will test a single coefficient\nFor a binary and continuous covariate, we will test a single coefficient\nFor two binary categorical covariates, we will test a single coefficient\nFor a multi-level categorical covariate (with any other type of covariate), we must test a group of coefficients!!\n\nThen look at the main effect (or potential confounder)\n\nIf interaction already included, then automatically included as main effect (and thus not checked for confounding)\nFor variables that are not included in any interactions:\n\nCheck to see if they are confounders by seeing whether exclusion of the variable changes any of the main effect of the primary explanatory variable by more than 10%"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#reminder-from-lesson-9-general-steps-for-f-test",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#reminder-from-lesson-9-general-steps-for-f-test",
    "title": "Lesson 11: Interactions Continued",
    "section": "Reminder from Lesson 9: General steps for F-test",
    "text": "Reminder from Lesson 9: General steps for F-test\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1=\\beta_2= \\ldots=\\beta_k=0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=1, 2, \\ldots, k\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}} = \\frac{MSR_{full}}{MSE_{full}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#step-1-testing-the-interaction",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#step-1-testing-the-interaction",
    "title": "Lesson 11: Interactions Continued",
    "section": "Step 1: Testing the interaction",
    "text": "Step 1: Testing the interaction\n\nWe test with \\(\\alpha = 0.10\\)\nFollow the F-test procedure in Lesson 9 (MLR: Inference/F-test)\n\nThis means we need to follow the 7 steps of the general F-test in previous slide (taken from Lesson 9)\n\nUse the hypothesis tests for the specific variable combo:\n\n\n\n\n\nBinary & continuous variable (Lesson 11, LOB 2)\n\n\nTesting a single coefficient for the interaction term using F-test comparing full model to reduced model\n\n\n\n\n\nMulti-level & continuous variables (Lesson 11, LOB 3)\n\n\nTesting group of coefficients for the interaction terms using F-test comparing full to reduced model\n\n\n\n\n\n\n\n\nBinary & multi-level variable (Lesson 11, LOB 4)\n\n\nTesting group of coefficients for the interaction terms using F-test comparing full to reduced model\n\n\n\n\n\nTwo continuous variables (Lesson 11, LOB 5)\n\n\nTesting a single coefficient for the interaction term using F-test comparing full to reduced model"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#poll-everywhere-questions-2-4",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#poll-everywhere-questions-2-4",
    "title": "Lesson 11: Interactions Continued",
    "section": "Poll Everywhere Questions 2-4",
    "text": "Poll Everywhere Questions 2-4"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-testing-a-confounder",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-testing-a-confounder",
    "title": "Lesson 11: Interactions Continued",
    "section": "Step 2: Testing a confounder",
    "text": "Step 2: Testing a confounder\n\nIf interaction already included:\n\nMeaning: F-test showed evidence for alternative/full model\nThen the variable is an effect modifier and we don’t need to consider it as a confounder\nThen automatically included as main effect (and thus not checked for confounding)\n\nFor variables that are not included in any interactions:\n\nCheck to see if they are confounders\nOne way to do this is by seeing whether exclusion of the variable changes any of the main effect of the primary explanatory variable by more than 10%\n\nIf the main effect of the primary explanatory variable changes by less than 10%, then the additional variable is neither an effect modifier nor a confounder\n\nWe leave the variable out of the model"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#testing-for-percent-change-delta-in-a-coefficient",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#testing-for-percent-change-delta-in-a-coefficient",
    "title": "Lesson 11: Interactions Continued",
    "section": "Testing for percent change ( \\(\\Delta\\%\\)) in a coefficient",
    "text": "Testing for percent change ( \\(\\Delta\\%\\)) in a coefficient\n\nLet’s say we have \\(X_1\\) and \\(X_2\\), and we specifically want to see if \\(X_2\\) is a confounder for \\(X_1\\) (the explanatory variable or variable of interest)\nIf we are only considering \\(X_1\\) and \\(X_2\\), then we need to run the following two models:\n\nFitted model 1 / reduced model (mod1): \\(\\widehat{Y} = \\widehat\\beta_0 + \\widehat\\beta_1X_1\\)\n\nWe call the above \\(\\widehat\\beta_1\\) the reduced model coefficient: \\(\\widehat\\beta_{1, \\text{mod1}}\\) or \\(\\widehat\\beta_{1, \\text{red}}\\)\n\nFitted model 2 / Full model (mod2): \\(\\widehat{Y} = \\widehat\\beta_0 + \\widehat\\beta_1X_1 +\\widehat\\beta_2X_2\\)\n\nWe call this \\(\\widehat\\beta_1\\) the full model coefficient: \\(\\widehat\\beta_{1, \\text{mod2}}\\) or \\(\\widehat\\beta_{1, \\text{full}}\\)\n\n\n\n\n\n\n\n\n\nCalculation for % change in coefficient\n\n\n\\[\n\\Delta\\% = 100\\% \\cdot\\frac{\\widehat\\beta_{1, \\text{mod1}} - \\widehat\\beta_{1, \\text{mod2}}}{\\widehat\\beta_{1, \\text{mod2}}} = 100\\% \\cdot \\frac{\\widehat\\beta_{1, \\text{red}} - \\widehat\\beta_{1, \\text{full}}}{\\widehat\\beta_{1, \\text{full}}}\n\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#is-food-supply-a-confounder-for-female-literacy-rate-13",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#is-food-supply-a-confounder-for-female-literacy-rate-13",
    "title": "Lesson 11: Interactions Continued",
    "section": "Is food supply a confounder for female literacy rate? (1/3)",
    "text": "Is food supply a confounder for female literacy rate? (1/3)\n\nRun models with and without food supply:\n \n\nModel 1 (reduced): \\(LE = \\beta_0 + \\beta_1 FLR^c + \\epsilon\\)\n\n\nmod1_red = lm(LifeExpectancyYrs ~ FLR_c, data = gapm_sub)\n\n \n\nModel 2 (full): \\(LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\epsilon\\)\n\n\nmod2_full = lm(LifeExpectancyYrs ~ FLR_c + FS_c, data = gapm_sub)\n\n\n \n\nNote that the full model when testing for confounding was the reduced model for testing an interaction\nFull and reduced are always relative qualifiers of the models that we are testing"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#is-food-supply-a-confounder-for-female-literacy-rate-23",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#is-food-supply-a-confounder-for-female-literacy-rate-23",
    "title": "Lesson 11: Interactions Continued",
    "section": "Is food supply a confounder for female literacy rate? (2/3)",
    "text": "Is food supply a confounder for female literacy rate? (2/3)\n\nRecord the coefficient estimate for centered female literacy rate in both models:\n\n\nModel 1 (reduced):\n\n\ntidy(mod1_red, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 5)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n70.29722\n0.72578\n96.85709\n0.00000\n68.84969\n71.74475\n    FLR_c\n0.22990\n0.03219\n7.14139\n0.00000\n0.16570\n0.29411\n  \n  \n  \n\n\n\n\n\nModel 2 (full):\n\n\ntidy(mod2_full, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 5)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n70.29722\n0.63537\n110.63985\n0.00000\n69.02969\n71.56475\n    FLR_c\n0.15670\n0.03216\n4.87271\n0.00001\n0.09254\n0.22085\n    FS_c\n0.00848\n0.00179\n4.72646\n0.00001\n0.00490\n0.01206\n  \n  \n  \n\n\n\n\n\nCalculate the percent change:\n\n\\[\n\\Delta\\% = 100\\% \\cdot \\frac{\\widehat\\beta_{1, \\text{mod1}} - \\widehat\\beta_{1, \\text{mod2}}}{\\widehat\\beta_{1, \\text{mod2}}} = 100\\% \\cdot \\frac{0.22990 - 0.15670}{0.15670} = 46.71\\%\n\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#is-food-supply-a-confounder-for-female-literacy-rate-33",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#is-food-supply-a-confounder-for-female-literacy-rate-33",
    "title": "Lesson 11: Interactions Continued",
    "section": "Is food supply a confounder for female literacy rate? (3/3)",
    "text": "Is food supply a confounder for female literacy rate? (3/3)\nThe percent change in female literacy rate’s coefficient estimate was 46.71%.\nThus, food supply is a confounder of female literacy rate in the association between life expectancy and female literacy rate."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#lets-try-this-out-on-one-of-our-potential-effect-modifiers-or-confounders",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#lets-try-this-out-on-one-of-our-potential-effect-modifiers-or-confounders",
    "title": "Lesson 11: Interactions Continued",
    "section": "Let’s try this out on one of our potential effect modifiers or confounders",
    "text": "Let’s try this out on one of our potential effect modifiers or confounders\n\n\n\nLook back at income level and world region: is income level an effect modifier, confounder, or has no effect on the association between life expectancy and world region?\nWe can start by visualizing the relationship between life expectancy and world region by income level\nSo we’ll need to revisit the work we did in previous slides on the interaction, then check fo condounding"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#determining-if-income-level-is-an-effect-modifier-confounder-or-neither",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#determining-if-income-level-is-an-effect-modifier-confounder-or-neither",
    "title": "Lesson 11: Interactions Continued",
    "section": "Determining if income level is an effect modifier, confounder, or neither",
    "text": "Determining if income level is an effect modifier, confounder, or neither\n\nStep 1: Testing the interaction/effect modifier\n\nCompare model with and without interaction using F-test to see if interaction is significant\nModels\n\nModel 1 (red): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\beta_4 I(\\text{high income})+ \\epsilon \\end{aligned}\\)\n\n \n\nModel 2 (full): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\beta_4 I(\\text{high income})+ \\\\ & \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\)\n\n\nStep 2: Testing a confounder (only if not an effect modifier)\n\nCompare model with and without main effect for additional variable (income level) using F-test to see if additional variable (income level) is a confounder\nModels\n\nModel 1 (reduced): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\epsilon \\end{aligned}\\)\nModel 2 (full): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\beta_4 I(\\text{high income})+ \\epsilon \\end{aligned}\\)"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#step-1-results-from-lesson-11-lob-4",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#step-1-results-from-lesson-11-lob-4",
    "title": "Lesson 11: Interactions Continued",
    "section": "Step 1: Results from Lesson 11 LOB 4",
    "text": "Step 1: Results from Lesson 11 LOB 4\n\nFit the reduced and full model\n\n\nm_int_wr_inc_red = lm(LifeExpectancyYrs ~ income_levels2 + four_regions, \n                   data = gapm_sub)\nm_int_wr_inc_full = lm(LifeExpectancyYrs ~ income_levels2 + four_regions +\n                          income_levels2*four_regions, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\nanova(m_int_wr_inc_red, m_int_wr_inc_full) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ income_levels2 + four_regions\n67.000\n1,693.242\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ income_levels2 + four_regions + income_levels2 * four_regions\n64.000\n1,681.304\n3.000\n11.938\n0.151\n0.928\n  \n  \n  \n\n\n\n# newdata &lt;- data.frame(four_regions = c(\"Africa\", \"Americas\", \"Asia\", \"Europe\"), \n#                       FLR_c = c()) \n# (pred = predict(m_int_wr_full, \n#                 newdata=newdata, \n#                 interval=\"confidence\"))\n\n\nConclusion: There is not a significant interaction between world region and income level (p = 0.928).\nThus, income level is not an effect modifier of world region. However, we can continue to test if income level is a confounder."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-see-if-income-is-a-confounder",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-see-if-income-is-a-confounder",
    "title": "Lesson 11: Interactions Continued",
    "section": "Step 2: See if income is a confounder",
    "text": "Step 2: See if income is a confounder\n\nFit the reduced and full model for testing the confounder\n\n \n\nModel 1 (reduced): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\epsilon \\end{aligned}\\)\n\n\nmod1_wr_inc_red = lm(LifeExpectancyYrs ~ four_regions, \n                   data = gapm_sub)\n\n \n\nModel 2 (full): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\beta_4 I(\\text{high income})+ \\epsilon \\end{aligned}\\)\n\n\nmod1_wr_inc_full = lm(LifeExpectancyYrs ~ four_regions + income_levels2, \n                   data = gapm_sub)"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-see-if-income-is-a-confounder-1",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-see-if-income-is-a-confounder-1",
    "title": "Lesson 11: Interactions Continued",
    "section": "Step 2: See if income is a confounder",
    "text": "Step 2: See if income is a confounder\n\nRecord the coefficient estimate for centered female literacy rate in both models:\nModel 1 (reduced):\\(\\begin{aligned}\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{Americas}) + \\widehat\\beta_2 I(\\text{Asia}) + \\widehat\\beta_3 I(\\text{Europe}) \\end{aligned}\\)\n\n\ntidy(mod1_wr_inc_red, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 5)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n61.27000\n1.16508\n52.58870\n0.00000\n58.94512\n63.59488\n    four_regionsAmericas\n14.33000\n1.90257\n7.53193\n0.00000\n10.53349\n18.12651\n    four_regionsAsia\n8.11824\n1.71883\n4.72313\n0.00001\n4.68837\n11.54810\n    four_regionsEurope\n14.78217\n1.59304\n9.27924\n0.00000\n11.60332\n17.96103\n  \n  \n  \n\n\n\n\n\nModel 2 (full): \\(\\begin{aligned}\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{Americas}) + \\widehat\\beta_2 I(\\text{Asia}) + \\widehat\\beta_3 I(\\text{Europe}) + \\widehat\\beta_4 I(\\text{high income})\\end{aligned}\\)\n\n\ntidy(mod1_wr_inc_full, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 5)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n60.54716\n1.16190\n52.11048\n0.00000\n58.22800\n62.86632\n    four_regionsAmericas\n12.04102\n2.05816\n5.85038\n0.00000\n7.93292\n16.14912\n    four_regionsAsia\n7.77808\n1.66414\n4.67394\n0.00001\n4.45645\n11.09971\n    four_regionsEurope\n12.51938\n1.79139\n6.98864\n0.00000\n8.94375\n16.09501\n    income_levels2Higher income\n3.61419\n1.46967\n2.45917\n0.01651\n0.68070\n6.54767"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-see-if-income-is-a-confounder-2",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-see-if-income-is-a-confounder-2",
    "title": "Lesson 11: Interactions Continued",
    "section": "Step 2: See if income is a confounder",
    "text": "Step 2: See if income is a confounder\n\nCalculate the percent change for \\(\\widehat\\beta_1\\): \\[\n\\Delta\\%  = 100\\% \\cdot \\frac{\\widehat\\beta_{1, \\text{mod1}} - \\widehat\\beta_{1, \\text{mod2}}}{\\widehat\\beta_{1, \\text{mod2}}} = 100\\% \\cdot \\frac{14.33000 - 12.04102}{12.04102} = 19.01\n\\]\nCalculate the percent change for \\(\\widehat\\beta_2\\): \\[\n\\Delta\\%  = 100\\% \\cdot \\frac{\\widehat\\beta_{2, \\text{mod1}} - \\widehat\\beta_{2, \\text{mod2}}}{\\widehat\\beta_{2, \\text{mod2}}} = 100\\% \\cdot \\frac{8.11824 - 7.77808}{7.77808} = 4.37\n\\]\nCalculate the percent change for \\(\\widehat\\beta_3\\): \\[\n\\Delta\\%  = 100\\% \\cdot \\frac{\\widehat\\beta_{3, \\text{mod1}} - \\widehat\\beta_{3, \\text{mod2}}}{\\widehat\\beta_{3, \\text{mod2}}} = 100\\% \\cdot \\frac{14.78217 - 12.51938}{12.51938} = 18.07\n\\]\nNote that two of these % changes are greater than 10%, and one is less than 10%…"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-see-if-income-is-a-confounder-3",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-see-if-income-is-a-confounder-3",
    "title": "Lesson 11: Interactions Continued",
    "section": "Step 2: See if income is a confounder",
    "text": "Step 2: See if income is a confounder\n\n\n\nThere is no set rule when we have more than one estimated coefficient that we examine for confoundeing\nIn this, I would consider\n\nThe majority of coefficients (2/3 coefficients) changes more than 10%\nThe change in coefficients for all three are in the same direction\nThe plot of life expectancy vs world region by income level have a shift in mean life expectancy from lower to higher income level\n\nThus, I would conclude that income level is a confounder, so we would leave income level’s main effect in the model"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#if-you-want-extra-practice",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#if-you-want-extra-practice",
    "title": "Lesson 11: Interactions Continued",
    "section": "If you want extra practice",
    "text": "If you want extra practice\n\nTry out this procedure to determine if a variable is an effect modifier or confounder or nothing on the other interactions we tested out in Lesson 11"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#general-interpretation-of-the-interaction-term-reference",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#general-interpretation-of-the-interaction-term-reference",
    "title": "Lesson 11: Interactions Continued",
    "section": "General interpretation of the interaction term (reference)",
    "text": "General interpretation of the interaction term (reference)\n\\(E[Y\\mid X_{1},X_{2} ]=\\beta_0 + \\underbrace{(\\beta_1+\\beta_3X_{2}) }_\\text{$X_{1}$'s effect} X_{1}+ \\underbrace{\\beta_2X_{2}}_\\text{$X_{2}$ held constant}\\)\n\\({\\color{white}{E[Y\\mid X_{1},X_{2} ]}}=\\beta_0 + \\underbrace{(\\beta_2+\\beta_3X_{1}) }_\\text{$X_{2}$'s effect}X_{2} + \\underbrace{\\beta_1X_{1}}_\\text{$X_{1}$ held constant}\\)\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in \\(X_{1}\\)’s effect, per unit increase in \\(X_{2}\\);\n\n\\(\\beta_3\\) = mean change in \\(X_{2}\\)’s effect, per unit increase in \\(X_{1}\\);\nwhere the “\\(X_{1}\\) effect” equals the change in \\(E[Y]\\) per unit increase in \\(X_{1}\\) with \\(X_{2}\\) held constant, i.e. “adjusted \\(X_{1}\\) effect”\n\nIn summary, the interaction term can be interpreted as “difference in adjusted \\(X_1\\) (or \\(X_2\\)) effect per unit increase in \\(X_2\\) (or \\(X_1\\))”"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#a-glimpse-at-how-interactions-might-be-incorporated-into-model-selection",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#a-glimpse-at-how-interactions-might-be-incorporated-into-model-selection",
    "title": "Lesson 11: Interactions Continued",
    "section": "A glimpse at how interactions might be incorporated into model selection",
    "text": "A glimpse at how interactions might be incorporated into model selection\n\nIdentify outcome (Y) and primary explanatory (X) variables\nDecide which other variables might be important and could be potential confounders. Add these to the model.\n\nThis is often done by indentifying variables that previous research deemed important, or researchers believe could be important\nFrom a statistical perspective, we often include variables that are significantly associated with the outcome (in their respective SLR)\n\n(Optional step) Test 3 way interactions\n\nThis makes our model incredibly hard to interpret. Our class will not cover this!!\nWe will skip to testing 2 way interactions\n\nTest 2 way interactions\n\nWhen testing a 2 way interaction, make sure the full and reduced models contain the main effects\nFirst test all the 2 way interactions together using a partial F-test (with \\(alpha = 0.10\\))\n\nIf this test not significant, do not test 2-way interactions individually\nIf partial F-test is significant, then test each of the 2-way interactions\n\n\nRemaining main effects - to include of not to include?\n\nFor variables that are included in any interactions, they will be automatically included as main effects and thus not checked for confounding\nFor variables that are not included in any interactions:\n\nCheck to see if they are confounders by seeing whether exclusion of the variable(s) changes any of the coefficient of the primary explanatory variable (including interactions) X by more than 10%\n\nIf any of X’s coefficients change when removing the potential confounder, then keep it in the model"
  },
  {
    "objectID": "lessons/11_Interactions_01/01_Review_key_info.html#key-dates",
    "href": "lessons/11_Interactions_01/01_Review_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/13_Model_selection/01_Review_key_info.html#key-dates",
    "href": "lessons/13_Model_selection/01_Review_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html",
    "href": "lessons/13_Model_selection/13_Model_selection.html",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Understand the motivation for model selection, including bias-variance trade off and alignment of research goals (association vs. prediction)\nExplain the general process or idea behind different model selection techniques\nRecognize common model fit statistics and understand what they measure"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#why-cant-i-just-throw-in-all-the-variables-into-my-model",
    "href": "lessons/13_Model_selection/13_Model_selection.html#why-cant-i-just-throw-in-all-the-variables-into-my-model",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Why can’t I just throw in all the variables into my model?",
    "text": "Why can’t I just throw in all the variables into my model?\n\nFirst, let’s think about the number of observations in our dataset\nFor example: In the Gapminder dataset, I can use an indicator for each country\n\nRemember that each country is an observation\nSo we have a perfectly fit model - a covariate for each observation\nBut we cannot generalize this to any other countries\nAnd we haven’t identified any meaningful relationships between life expectancy and other measured characteristics\n\nMore covariates in the model is not always better\n\nOverfitting the data limits our generalizability and prevents us from answering research questions"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#model-complexity-vs.-parsimony",
    "href": "lessons/13_Model_selection/13_Model_selection.html#model-complexity-vs.-parsimony",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Model Complexity vs. Parsimony",
    "text": "Model Complexity vs. Parsimony\n\n\nSuppose we have \\(p = 30\\) covariates (in the true model) and n = 50 observations. We could consider the following two alternatives:\n\nWe could fit a model using all of the covariates.\n\nIn this case, \\(\\widehat\\beta\\) is unbiased for \\(\\beta\\) (in a linear model fit using OLS). But \\(\\widehat\\beta\\) has very high variance.\n\nWe could fit a model using only the five strongest covariates.\n\nIn this case, \\(\\widehat\\beta\\) will be biased for \\(\\beta\\), but it will have lower variance (compared to the estimate including all covariates)\n\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#bias-variance-trade-off",
    "href": "lessons/13_Model_selection/13_Model_selection.html#bias-variance-trade-off",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Bias-variance trade off",
    "text": "Bias-variance trade off\n\n\n\nRecall mean square error is a function of SSE (sum of squared residuals)\n\\[\nMSE = \\dfrac{1}{n} \\sum_{i=1}^{n} \\big(Y_i - \\widehat{Y}_i \\big)^2\n\\]\nMSE can also be written as a function of the bias and variance\n\\[\nMSE = \\text{bias}\\big(\\widehat\\beta\\big)^2 + \\text{variance}\\big(\\widehat\\beta\\big)\n\\]\nFor the same data:\n\nMore covariates in model: less bias, more variance\nLess covariates in model: more bias, less variance\n\nOut goal: find a model with just the right amount of covariates to balance bias and vairance\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#some-important-definitions",
    "href": "lessons/13_Model_selection/13_Model_selection.html#some-important-definitions",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Some important definitions",
    "text": "Some important definitions\n\nModel selection: picking the “best” model from a set of possible models\n\nModels will have the same outcome, but typically differ by the covariates that are included, their transformations, and their interactions\n\n\n \n\nModel selection strategies: a process or framework that helps us pick our “best” model\n\nThese strategies often differ by the approach and criteria used to the determine the “best” model\n\n\n \n\nOverfitting: result of fitting a model so closely to our particular sample data that it cannot be generalized to other samples (or the population)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#model-selection-basics-slide-adjusted-from-jodi-lapidus",
    "href": "lessons/13_Model_selection/13_Model_selection.html#model-selection-basics-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Model Selection basics (slide adjusted from Jodi Lapidus)",
    "text": "Model Selection basics (slide adjusted from Jodi Lapidus)\n\n“Because models always fall far short of the complex reality under study, there are no best or optimal strategies for modeling.”\n\nFrom: Statistical Foundations for Model-Based Adjustments\n\nNot all statistical texts provide practical advice on model development\n\nA lot of resources include methods/code to compare models, but does not include much advice re: selecting which model to ultimately use.\nOther texts are sparse on details or incorporate simplistic approaches\n\nModel development strategy should align with research goals\n\nPrediction vs. Estimating Association\nStrategy may depend on study design and data set size"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#the-goals-of-association-vs.-prediction",
    "href": "lessons/13_Model_selection/13_Model_selection.html#the-goals-of-association-vs.-prediction",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "The goals of association vs. prediction",
    "text": "The goals of association vs. prediction\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nGoal: Understand one variable’s (or a group of variable’s) effect on the response after adjusting for other factors\nMainly interpret the coefficient of the variable that is the focus of the study\n\nInterpreting the coefficients of the other variables is not important, but can help bring context\n\nAny variables not selected for the final model have still been adjusted for, since they had a chance to be in the model\nExample: How is body mass of a penguin associated with flipper length?\n\n\n\n\n\n\nPrediction\n\n\n\nGoal: to calculate the most precise prediction of the response variable\nInterpreting coefficients is not important\nChoose only the variables that are strong predictors of the response variable\n\nExcluding irrelevant variables can help reduce widths of the prediction intervals\n\nExample: What is the flipper length of a penguin with body mass of 3000 g (and all its other characteristics)?"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#model-building-for-association-vs.-prediction",
    "href": "lessons/13_Model_selection/13_Model_selection.html#model-building-for-association-vs.-prediction",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Model building for association vs. prediction",
    "text": "Model building for association vs. prediction\nMore information on the two analysis goals:\n\n\n\n\n\nIf you ever get the chance, check out Dr. Kristin Sainani’s series on Statistics"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#poll-everywhere-question-1",
    "href": "lessons/13_Model_selection/13_Model_selection.html#poll-everywhere-question-1",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#model-selection-strategies-for-continuous-outcomes",
    "href": "lessons/13_Model_selection/13_Model_selection.html#model-selection-strategies-for-continuous-outcomes",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Model selection strategies for continuous outcomes",
    "text": "Model selection strategies for continuous outcomes\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nSelection of potential models is tied more with the research context with some incorporation of prediction scores\nPre-specification of multivariable model\nPurposeful model selection\n\n“Risk factor modeling”\n\nChange in Estimate (CIE) approaches\n\nWill learn in Survival Analysis (BSTA 514)\n\n\n\n\n\n\n\nPrediction\n\n\n\nSelection of potential models is fully dependent on prediction scores\nAutomated strategies\n\nStepwise selection (forward/backward)\n\nYou’ll see these a lot, but they’re not really good methods\n\nBest subset\nRegularization techniques (LASSO, Ridge, Elastic net)\n\n\n\n\n\n\n\nFor categorical outcomes, there are more prediction model selection strategies (will learn more in BSTA 513)\n\nExamples: Decision trees, Random forest, Neural networks, K-means"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#pre-specification-of-multivariable-model-slide-adjusted-from-jodi-lapidus",
    "href": "lessons/13_Model_selection/13_Model_selection.html#pre-specification-of-multivariable-model-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Pre-specification of multivariable model (slide adjusted from Jodi Lapidus)",
    "text": "Pre-specification of multivariable model (slide adjusted from Jodi Lapidus)\n\nIn a clinical trial, we often have to write and finalize a statistical analysis plan (SAP) before the trial starts\nIf we wish to compare treatment effects adjusted for covariates, all covariates typically specified in advance\n\nExample: Comparing effectiveness of 3-drug vs. 2-drug regimen for delaying AIDS onset or death. Covariates such as severity of HIV infection at baseline would have been specified in advance.\nVariables such as study site, as well as any randomization stratification variables are common covariates.\n\nIn these cases, only a limited number of multivariable models are fit and reported\n\nDo not perform all the model building steps outlined in Hosmer and Lemeshow texts"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#purposeful-model-selection-slide-adjusted-from-jodi-lapidus",
    "href": "lessons/13_Model_selection/13_Model_selection.html#purposeful-model-selection-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Purposeful model selection (slide adjusted from Jodi Lapidus)",
    "text": "Purposeful model selection (slide adjusted from Jodi Lapidus)\n\nCan use this type of model selection for any type of regression\nCareful, well-thought out variable selection process\n\nConsiders both confounding and interaction, as well as checking model assumptions, fit, etc.\n\nOften a reasonable strategy, especially in epidemiology and more exploratory clinical studies\n\nHowever, not always appropriate!\nE.g. clinical trials with model specified in advance. (pre-specified model)\n\n\n \n\nThis is the selection process that we will focus on in this class!"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#change-in-estimate-cie-approach-slide-adjusted-from-jodi-lapidus",
    "href": "lessons/13_Model_selection/13_Model_selection.html#change-in-estimate-cie-approach-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Change in estimate (CIE) approach (slide adjusted from Jodi Lapidus)",
    "text": "Change in estimate (CIE) approach (slide adjusted from Jodi Lapidus)\n\nCIE strategies select covariates on the basis of how much their control changes exposure effect estimates\n\nObserved change is presumed to measure confounding by the covariate.\n\nWhat estimate?\n\nH/L text suggest using coefficients from the model\nWe typically use the coefficient estimate from the explanatory variable that we are most interested in\n\nWhat magnitude change is ”important”?\n\nH/L text suggest 10%\n\nOne must choose an effect measure to judge change importance, where “importance” needs to be evaluated along a contextually meaningful scale\nAccurate assessment of confounding may require examining changes from removing entire sets of covariates\n\nAdd in or eliminate candidate confounders one at time?\nAdd in or eliminate candidate confounders in sets?"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#stepwise-selection-slide-adjusted-from-adrianna-westbrook",
    "href": "lessons/13_Model_selection/13_Model_selection.html#stepwise-selection-slide-adjusted-from-adrianna-westbrook",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Stepwise selection (slide adjusted from Adrianna Westbrook)",
    "text": "Stepwise selection (slide adjusted from Adrianna Westbrook)\n\nThis is an incredibly common approach that statisticians use, often because it is an older and more recognized method\n\nBUT IT IS ALSO ONE OF THE WORST MODEL SELECTION STRATEGIES!!\n\nMajor disadvantages to stepwise selection:\n\nProne to overfitting\nBiased estimates\nCements the wrong idea that we are looking for our “most significant” covariates\n\nPredictors/covariates are added or removed one at time if they are below a certain threshold (usually p-value below 0.10 to 0.20)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#stepwise-selection-two-common-approaches",
    "href": "lessons/13_Model_selection/13_Model_selection.html#stepwise-selection-two-common-approaches",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Stepwise selection: two common approaches",
    "text": "Stepwise selection: two common approaches\n\nI will introduce two of the approaches so that you understand the general process if a collaborator ever mentions stepwise selection\nForward selection:\n\nFor \\(p\\) cvariates potential covariates, run all simple linear regressions:\n\n\\(Y= \\beta_0 + \\beta_1 X_1 + \\epsilon\\) through \\(Y= \\beta_0 + \\beta_1 X_{p} + \\epsilon\\)\nInclude the \\(X_i\\) with the lowest p-value (assuming it is below the threshold)\n\nNow run \\(Y= \\beta_0 + \\beta_1 X_i + \\beta_2 X_1 + \\epsilon\\) through \\(Y= \\beta_0 + \\beta_1 X_i + \\beta_2 X_{p} + \\epsilon\\) and enter the next \\(X_j\\) with the lowest p-value\nContinue process until no more predictors come back with a p-value below the threshold\n\nBackward selection:\n\nStart with a full model (\\(Y= \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p + \\epsilon\\)) and remove predictor with the highest p-value (assuming it is above the threshold)\nRepeatedly remove the variable with the highest p-value until all remaining variables meet the stopping criteria (are below the threshold)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#best-subset-slide-adjusted-from-adrianna-westbrook",
    "href": "lessons/13_Model_selection/13_Model_selection.html#best-subset-slide-adjusted-from-adrianna-westbrook",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Best subset (slide adjusted from Adrianna Westbrook)",
    "text": "Best subset (slide adjusted from Adrianna Westbrook)\n\nI don’t see this approach very often\nQuite literally making subsets of the data and using the “best” one\nGeneral steps:\n\nRun every possible model fitting 1 to all possible \\(p\\) predictors/covariates\nYou can limit number of potential predictors\n\\(2^p\\) = total number of models where \\(p\\) = number of predictors\nYou will get the best fitting model within each category (i.e., 1 predictor model, 2 predictor model,…, \\(p\\) predictor model)\nThen have to find the best fitting model between the best models from each category\n\nMajor disadvantages to best subset:\n\nDoes not account for interactions\nNeeds to run a lot of models (takes A LOT of time)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#regularization-techniques",
    "href": "lessons/13_Model_selection/13_Model_selection.html#regularization-techniques",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Regularization techniques",
    "text": "Regularization techniques\n\nRegularization techniques (LASSO, ridge, elastic net) adds a penalization that shrinks (or regularizes) coefficients down to reduce overfitting\n\n\n\n\n\n\n\n\n\n\n\nLASSO (Least About Shrinkage and Selection Operator)\nRidge\nElastic Net\n\n\nPenalization\nL-1 Norm, uses absolute value\nL-2 Norm, uses squared value\nBest of both worlds, L-1 and L-2 used\n\n\nPro’s\nReduces overfitting, will shrink coefficient to zero\nReduces overfitting, handles collinearity, can handle k&gt;n\nReduces overfitting, handles collinearity, handles k&gt;n, shrinks coefficients to zero\n\n\nCon’s\nCannot handle k&gt;n, doesn’t handle multicollinearity well\nDoes not shrink coefficients to zero, difficult to interpret\nMore difficult for R to do than the other two (but not really that bad)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#poll-everywhere-question-2",
    "href": "lessons/13_Model_selection/13_Model_selection.html#poll-everywhere-question-2",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#introduction-to-model-fit-statistics",
    "href": "lessons/13_Model_selection/13_Model_selection.html#introduction-to-model-fit-statistics",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Introduction to model fit statistics",
    "text": "Introduction to model fit statistics\n\nSo far we have compared models using the F-test\nThe F-test is a great way to compare models that are nested\n\nBasically, this means that the “full” model contains all the covariates that the “reduced” model contains\nThe full model will have additional covariates, but the covariates in the reduced is a subset of the covariates in the full\n\nWhat if we want to compare models that are not nested?\n\nThere is a special group of fit statistics that can help us compare models\nNote: these are sometimes used in the model building process (within one strategy)\n\nHelpful if we want to compare selected models across strategies\nHelpful if we have a few “final” models with different covariates that we want to compare"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#common-model-fit-statistics",
    "href": "lessons/13_Model_selection/13_Model_selection.html#common-model-fit-statistics",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Common model fit statistics",
    "text": "Common model fit statistics\n\nThe following model fit statistics combine information about the SSE, the number of parameters in the model, and the sample size\nFor Mallow’s Cp, AIC, and BIC: smaller values indicate better model fit!\nFor Adjusted R-squared: larger values indicate better model fit!\n\n\n\n\nFit statistic\nEquation\nR code\n\n\n\n\nR-squared / Adjusted R-squared\n\\(Adj. R^2 = 1 - \\frac{SSE/(n-p-1)}{SSY/(n-1)}\\)\nWithin summary(model_name)\n\n\nMallow’s \\(C_p\\)\n\\(C_p = \\Bigg[ \\dfrac{\\widehat\\sigma^2_p}{\\widehat\\sigma^2_{max}} - 1 \\Bigg](n-p) + p\\)\nols_mallows_cp()\n\n\nAkaike information criterion (AIC)\n\\(AIC = n\\log(SSE) - n \\log(n) + 2(p+1)\\)\nAIC(model_name)\n\n\nBayesian information criterion (BIC)\n\\(BIC = n\\log(SSE) - n\\log(n) + log(n)\\cdot(p+1)\\)\nBIC(model_name)\n\n\n\n \n\nWe don’t need to know the exact formulas for them!"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#common-model-fit-statistics-1",
    "href": "lessons/13_Model_selection/13_Model_selection.html#common-model-fit-statistics-1",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Common model fit statistics",
    "text": "Common model fit statistics\n\nThere is no hypothesis testing for these fit statistics\n\nOnly helpful if you are comparing models\nWorks for nested and non-nested models\n\nCommon to report all or some of them\nAll of the fit statistics will not necessarily reach a consensus about the best fitting model\n\nEach weigh SSE, number of parameters, and number of observations differently\n\n\n\n\n\nhttps://www.researchgate.net/figure/Model-Fit-Statistics_tbl1_308844501"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/01_Review_key_info.html#key-dates",
    "href": "lessons/15_MLR_Diagnostics/01_Review_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html",
    "href": "lessons/01_Review/01_Review.html",
    "title": "Lesson 1: Review",
    "section": "",
    "text": "In 511, we talked about categorical and continuous outcomes (dependent variables)\n \nWe also talked about their relationship with 1-2 continuous or categorical exposure (independent variables or predictor)\n \nWe had many good ways to assess the relationship between an outcome and exposure:\n \n\n\n\n\n\n\n\n\n\n\nContinuous Outcome\nCategorical Outcome\n\n\nContinuous Exposure\nCorrelation, simple linear regression\n??\n\n\nCategorical Exposure\nt-tests, paired t-tests, 2 sample t-tests, ANOVA\nproportion t-test, Chi-squared goodness of fit test, Fisher’s Exact test, Chi-squared test of independence, etc."
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#what-did-we-learn-in-511",
    "href": "lessons/01_Review/01_Review.html#what-did-we-learn-in-511",
    "title": "Lesson 1: Review",
    "section": "What did we learn in 511?",
    "text": "What did we learn in 511?\n\nIn 511, we talked about categorical and continuous outcomes (dependent variables)\n \nWe also talked about their relationship with 1-2 continuous or categorical exposure (independent variables or predictor)\n \nWe had many good ways to assess the relationship between an outcome and exposure:\n \n\n\n\n\n\n\n\n\n\n\nContinuous Outcome\nCategorical Outcome\n\n\nContinuous Exposure\nCorrelation, simple linear regression\n??\n\n\nCategorical Exposure\nt-tests, paired t-tests, 2 sample t-tests, ANOVA\nproportion t-test, Chi-squared goodness of fit test, Fisher’s Exact test, Chi-squared test of independence, etc."
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#what-did-we-learn-in-511-1",
    "href": "lessons/01_Review/01_Review.html#what-did-we-learn-in-511-1",
    "title": "Lesson 1: Review",
    "section": "What did we learn in 511?",
    "text": "What did we learn in 511?\n\nYou set up a really important foundation\n\nIncluding distributions, mathematical definitions, hypothesis testing, and more!\n\n \nTests and statistical approaches learned are incredibly helpful!\n \nWhile you had to learn a lot of different tests and approaches for each combination of categorical/continuous exposure with categorical/continuous outcome\n\nThose tests cannot handle more complicated data\n\n \nWhat happens when other variables influence the relationship between your exposure and outcome?\n\nDo we just ignore them?"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#what-will-we-learn-in-this-class",
    "href": "lessons/01_Review/01_Review.html#what-will-we-learn-in-this-class",
    "title": "Lesson 1: Review",
    "section": "What will we learn in this class?",
    "text": "What will we learn in this class?\n\nWe will be building towards models that can handle many variables!\n \n\nRegression is the building block for modeling multivariable relationships\n\n \nIn Linear Models we will build, interpret, and evaluate linear regression models"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#process-of-regression-data-analysis",
    "href": "lessons/01_Review/01_Review.html#process-of-regression-data-analysis",
    "title": "Lesson 1: Review",
    "section": "Process of regression data analysis",
    "text": "Process of regression data analysis"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#main-sections-of-the-course",
    "href": "lessons/01_Review/01_Review.html#main-sections-of-the-course",
    "title": "Lesson 1: Review",
    "section": "Main sections of the course",
    "text": "Main sections of the course\n\nReview\nIntro to SLR: estimation and testing\n\nModel fitting\n\nIntro to MLR: estimation and testing\n\nModel fitting\n\nDiving into our predictors: categorical variables, interactions between variable\n\nModel fitting\n\nKey ingredients: model evaluation, diagnostics, selection, and building\n\nModel evaluation and Model selection"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#main-sections-of-the-course-1",
    "href": "lessons/01_Review/01_Review.html#main-sections-of-the-course-1",
    "title": "Lesson 1: Review",
    "section": "Main sections of the course",
    "text": "Main sections of the course\n\n\nReview\n\n\n\nIntro to SLR: estimation and testing\n\nModel fitting\n\nIntro to MLR: estimation and testing\n\nModel fitting\n\nDiving into our predictors: categorical variables, interactions between variable\n\nModel fitting\n\nKey ingredients: model evaluation, diagnostics, selection, and building\n\nModel evaluation and Model selection"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#before-we-begin",
    "href": "lessons/01_Review/01_Review.html#before-we-begin",
    "title": "Lesson 1: Review",
    "section": "Before we begin",
    "text": "Before we begin\n\nMeike has some really good online notes, code, and work on her BSTA 511 page"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#learning-objectives",
    "href": "lessons/01_Review/01_Review.html#learning-objectives",
    "title": "Lesson 1: Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify important descriptive statistics and visualize data from a continuous variable\nIdentify important distributions that will be used in 512/612\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\nUse our previous tools in 511 to conduct a hypothesis test\nDefine error rates and power"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#learning-objectives-1",
    "href": "lessons/01_Review/01_Review.html#learning-objectives-1",
    "title": "Lesson 1: Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\n\nIdentify important descriptive statistics and visualize data from a continuous variable\n\n\n\nIdentify important distributions that will be used in 512/612\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\nUse our previous tools in 511 to conduct a hypothesis test\nDefine error rates and power"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#some-basic-statistics-talk",
    "href": "lessons/01_Review/01_Review.html#some-basic-statistics-talk",
    "title": "Lesson 1: Review",
    "section": "Some Basic Statistics “Talk”",
    "text": "Some Basic Statistics “Talk”\n\n\n\nRandom variable \\(Y\\)\n\nSample \\(Y_i, i=1,\\dots, n\\)\n\nSummation:\n\\(\\sum_{i=1}^n Y_i =Y_1 + Y_2 + \\ldots + Y_n\\)\nProduct:\n\\(\\prod_{i=1}^n Y_i = Y_1 \\times Y_2 \\times \\ldots \\times Y_n\\)"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#descriptive-statistics-continuous-variables",
    "href": "lessons/01_Review/01_Review.html#descriptive-statistics-continuous-variables",
    "title": "Lesson 1: Review",
    "section": "Descriptive Statistics: continuous variables",
    "text": "Descriptive Statistics: continuous variables\n\n\nMeasures of central tendency\n\nSample mean\n\\[\n\\bar{x} = \\dfrac{x_1+x_2+...+x_n}{n}=\\dfrac{\\sum_{i=1}^nx_i}{n}\n\\]\nMedian\n\n\nMeasures of variability (or dispersion)\n\nSample variance\n\nAverage of the squared deviations from the sample mean\n\nSample standard deviation\n\\[\ns = \\sqrt{\\dfrac{(x_1-\\bar{x})^2+(x_2-\\bar{x})^2+...+(x_n-\\bar{x})^2}{n-1}}=\\sqrt{\\dfrac{\\sum_{i=1}^n(x_i-\\bar{x})^2}{n-1}}\n\\]\nIQR\n\nRange from 1st to 3rd quartile"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#descriptive-statistics-continuous-variables-r-code",
    "href": "lessons/01_Review/01_Review.html#descriptive-statistics-continuous-variables-r-code",
    "title": "Lesson 1: Review",
    "section": "Descriptive Statistics: continuous variables (R code)",
    "text": "Descriptive Statistics: continuous variables (R code)\n\n\nMeasures of central tendency\n\nSample mean\n\nmean( sample )\n\nMedian\n\nmedian( sample )\n\n\n\nMeasures of variability (or dispersion)\n\nSample variance\n\nvar( sample )\n\nSample standard deviation\n\nsd( sample )\n\nIQR\n\nIQR( sample )"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#data-visualization",
    "href": "lessons/01_Review/01_Review.html#data-visualization",
    "title": "Lesson 1: Review",
    "section": "Data visualization",
    "text": "Data visualization\n\nUsing the library ggplot2 to visualize data\nWe will load the package:\n\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#histogram-using-ggplot2",
    "href": "lessons/01_Review/01_Review.html#histogram-using-ggplot2",
    "title": "Lesson 1: Review",
    "section": "Histogram using ggplot2",
    "text": "Histogram using ggplot2\nWe can make a basic graph for a continuous variable:\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_histogram(data = dds.discr, \n       aes(x = age))\n\n\n\n\n\n\n\n\n\n\nSome more information on histograms using ggplot2"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#spruced-up-histogram-using-ggplot2",
    "href": "lessons/01_Review/01_Review.html#spruced-up-histogram-using-ggplot2",
    "title": "Lesson 1: Review",
    "section": "Spruced up histogram using ggplot2",
    "text": "Spruced up histogram using ggplot2\nWe can make a more formal, presentable graph:\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram() +\n  theme(text = element_text(size=20)) +\n  labs(x = \"Age\", \n       y = \"Count\", \n       title = \"Distribution of Age in Sample\")\n\n\nI would like you to turn in homework, labs, and project reports with graphs like these."
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#other-basic-plots-from-ggplot2",
    "href": "lessons/01_Review/01_Review.html#other-basic-plots-from-ggplot2",
    "title": "Lesson 1: Review",
    "section": "Other basic plots from ggplot2",
    "text": "Other basic plots from ggplot2\nWe can also make a density and boxplot for the continuous variable with ggplot2\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_density()\n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_boxplot()"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#learning-objectives-2",
    "href": "lessons/01_Review/01_Review.html#learning-objectives-2",
    "title": "Lesson 1: Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify important descriptive statistics and visualize data from a continuous variable\n\n\n\nIdentify important distributions that will be used in 512/612\n\n\n\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\nUse our previous tools in 511 to conduct a hypothesis test\nDefine error rates and power"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#distributions-that-will-be-used-in-this-class",
    "href": "lessons/01_Review/01_Review.html#distributions-that-will-be-used-in-this-class",
    "title": "Lesson 1: Review",
    "section": "Distributions that will be used in this class",
    "text": "Distributions that will be used in this class\n\nNormal distribution\nChi-square distribution\nt distribution\nF distribution"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#normal-distribution",
    "href": "lessons/01_Review/01_Review.html#normal-distribution",
    "title": "Lesson 1: Review",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\n\n\nNotation: \\(Y\\sim N(\\mu,\\sigma^2)\\)\nArguably, the most important distribution in statistics\nIf we know \\(E(Y)=\\mu\\), \\(Var(Y)=\\sigma^2\\) then\n\n2/3 of \\(Y\\)’s distribution lies within 1 \\(\\sigma\\) of \\(\\mu\\)\n95% \\(\\ldots\\) \\(\\ldots\\) is within \\(\\mu\\pm 2\\sigma\\)\n\\(&gt;99\\)% \\(\\ldots\\) \\(\\ldots\\) lies within \\(\\mu\\pm 3\\sigma\\)\n\nLinear combinations of Normal’s are Normal\ne.g., \\((aY+b)\\sim \\mbox{N}(a\\mu+b,\\;a^2\\sigma^2)\\)\nStandard normal: \\(Z=\\frac{Y-\\mu}{\\sigma} \\sim \\mbox{N}(0,1)\\)"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#chi-squared-distribution-models-sampling-variance",
    "href": "lessons/01_Review/01_Review.html#chi-squared-distribution-models-sampling-variance",
    "title": "Lesson 1: Review",
    "section": "Chi-squared distribution: models sampling variance",
    "text": "Chi-squared distribution: models sampling variance\n\n\n\nNotation: \\(X \\sim \\chi^2_{df}\\) OR \\(X \\sim \\chi^2_{\\nu}\\)\n\nDegrees of freedom (df): \\(df=n-1\\)\n\\(X\\) takes on only positive values\n\nIf \\(Z_i\\sim \\mbox{N}(0,1)\\), then \\(Z_i^2\\sim \\chi^2_1\\)\n\nA standard normal distribution squared is the Chi squared distribution with df of 1.\n\n\n\n\nUsed in hypothesis testing and CI’s for variance or standard deviation\n\nSample variance (and SD) is random and thus can be modeled by a probability distribution: Chi-sqaured\n\nChi-squared distribution used to model the ratio of the sample variance \\(s^2\\) to population variance \\(\\sigma^2\\):\n\n\\(\\dfrac{(n-1)s^2}{\\sigma^2}\\sim \\chi^2_{n-1}\\)"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#students-t-distribution",
    "href": "lessons/01_Review/01_Review.html#students-t-distribution",
    "title": "Lesson 1: Review",
    "section": "Student’s t Distribution",
    "text": "Student’s t Distribution\n\n\n\nNotation: \\(T \\sim t_{df}\\) OR \\(T \\sim t_{n-1}\\)\n\nDegrees of freedom (df): \\(df=n-1\\)\n\\(T = \\dfrac{\\bar{x} - \\mu_x}{\\dfrac{s}{\\sqrt{n}}}\\sim t_{n-1}\\)\n\nIn linear modeling, used for inference on individual regression parameters\n\nThink: our estimated coefficients (\\(\\hat{\\beta}\\))"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#f-distribution",
    "href": "lessons/01_Review/01_Review.html#f-distribution",
    "title": "Lesson 1: Review",
    "section": "F-Distribution",
    "text": "F-Distribution\n\n\n\nModel ratio of sample variances\n\nRatio of variances is important for hypothesis testing of regression models\n\nIf \\(X_1^2\\sim \\chi^2_{df1}\\) and \\(X_2^2\\sim \\chi^2_{df2}\\), where \\(X_1^2\\perp X_2^2\\), then:\n\n\\[\\dfrac{X_1^2/df1}{X_2^2/df2} \\sim F_{df1,df2}\\] - only takes on positive values\n\nImportant relationship with \\(t\\) distribution: \\(T^2 \\sim F_{1,\\nu}\\)\n\nThe square of a t-distribution with \\(df=\\nu\\)\nis an F-distribution with numerator df (\\(df_1 = 1\\)) and denominator df (\\(df_2 = \\nu\\))"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#f-distribution-1",
    "href": "lessons/01_Review/01_Review.html#f-distribution-1",
    "title": "Lesson 1: Review",
    "section": "F-Distribution",
    "text": "F-Distribution\n\n\n\nModel ratio of sample variances\n\nRatio of variances is important for hypothesis testing of regression models\n\nIf \\(X_1^2\\sim \\chi^2_{df1}\\) and \\(X_2^2\\sim \\chi^2_{df2}\\), where \\(X_1^2\\perp X_2^2\\), then:\n\n\\[\\dfrac{X_1^2/df1}{X_2^2/df2} \\sim F_{df1,df2}\\] - only takes on positive values\n\nImportant relationship with \\(t\\) distribution: \\(T^2 \\sim F_{1,\\nu}\\)\n\nThe square of a t-distribution with \\(df=\\nu\\)\nis an F-distribution with numerator df (\\(df_1 = 1\\)) and denominator df (\\(df_2 = \\nu\\))\n\n\n\n\n\n\nIs there a relationship between our chi-squared and F-distribution?\n\n\nRecall, \\(\\dfrac{(n-1)s^2}{\\sigma^2}\\sim \\chi^2_{n-1}\\).\nThe F-distribution for a ratio of variances between two models is: \\(F = \\dfrac{s_1^2\\sigma^2_2}{s_2^2\\sigma^2_1} \\sim F_{n_1-1, n_2-1}\\)"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#r-code-for-probability-distributions",
    "href": "lessons/01_Review/01_Review.html#r-code-for-probability-distributions",
    "title": "Lesson 1: Review",
    "section": "R code for probability distributions",
    "text": "R code for probability distributions\n\n\nHere is a site with the various probability distributions and their R code.\n\nIt also includes practice with R code to see what each function outputs"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#learning-objectives-3",
    "href": "lessons/01_Review/01_Review.html#learning-objectives-3",
    "title": "Lesson 1: Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify important descriptive statistics and visualize data from a continuous variable\nIdentify important distributions that will be used in 512/612\n\n\n\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\n\n\n\nUse our previous tools in 511 to conduct a hypothesis test\nDefine error rates and power"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#confidence-interval-for-one-mean",
    "href": "lessons/01_Review/01_Review.html#confidence-interval-for-one-mean",
    "title": "Lesson 1: Review",
    "section": "Confidence interval for one mean",
    "text": "Confidence interval for one mean\n\n\nThe confidence interval for population mean \\(\\mu\\):\n\\[\n\\bar{x} \\pm t^{*}\\dfrac{s}{\\sqrt{n}}\n\\]\n\nwhere \\(t^*\\) is the critical value for the 95% (or other percent) corresponding to the t-distribution and dependent on \\(df=n-1\\)\n\n\n\nWe can use R to find the critical t-value, \\(t^*\\)\n\n\nFor example the critical value for the 95% CI with \\(n=10\\) subjects is…\n\nqt(0.975, df=9)\n\n[1] 2.262157\n\n\n\nRecall, that as the \\(df\\) increases, the t-distribution converges towards the Normal distribution"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#confidence-interval-for-one-mean-1",
    "href": "lessons/01_Review/01_Review.html#confidence-interval-for-one-mean-1",
    "title": "Lesson 1: Review",
    "section": "Confidence interval for one mean",
    "text": "Confidence interval for one mean\n\n\nThe confidence interval for population mean \\(\\mu\\):\n\\[\n\\bar{x} \\pm t^{*}\\dfrac{s}{\\sqrt{n}}\n\\]\n\nwhere \\(t^*\\) is the critical value for the 95% (or other percent) corresponding to the t-distribution and dependent on \\(df=n-1\\)\n\n\n\nWe can use R to find the critical t-value, \\(t^*\\)\n\n\nFor example the critical value for the 95% CI with \\(n=10\\) subjects is…\n\nqt(0.975, df=9)\n\n[1] 2.262157\n\n\n\nRecall, that as the \\(df\\) increases, the t-distribution converges towards the Normal distribution\n\n\n\n\nWe can also use t.test in R to calculate the confidence interval if we have a dataset.\n\nt.test(dds.discr$age)\n\n\n    One Sample t-test\n\ndata:  dds.discr$age\nt = 39.053, df = 999, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.65434 23.94566\nsample estimates:\nmean of x \n     22.8"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#confidence-interval-for-two-independent-means",
    "href": "lessons/01_Review/01_Review.html#confidence-interval-for-two-independent-means",
    "title": "Lesson 1: Review",
    "section": "Confidence interval for two independent means",
    "text": "Confidence interval for two independent means\n\n\nThe confidence interval for difference in independent population means, \\(\\mu_1\\) and \\(\\mu_2\\):\n\\[\n\\bar{x}_1 - \\bar{x}_2 \\pm t^{*}\\sqrt{\\dfrac{s_1^2}{n_1} + \\dfrac{s_2^2}{n_2}}\n\\]\n\nwhere \\(t^*\\) is the critical value for the 95% (or other percent) corresponding to the t-distribution and dependent on \\(df=n_1 + n_2 -2\\)"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#heres-a-decent-source-for-other-r-code-for-tests-in-511",
    "href": "lessons/01_Review/01_Review.html#heres-a-decent-source-for-other-r-code-for-tests-in-511",
    "title": "Lesson 1: Review",
    "section": "Here’s a decent source for other R code for tests in 511",
    "text": "Here’s a decent source for other R code for tests in 511\nWebsite from UCLA"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#learning-objectives-4",
    "href": "lessons/01_Review/01_Review.html#learning-objectives-4",
    "title": "Lesson 1: Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify important descriptive statistics and visualize data from a continuous variable\nIdentify important distributions that will be used in 512/612\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\n\n\n\nUse our previous tools in 511 to conduct a hypothesis test\n\n\n\nDefine error rates and power"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#steps-in-hypothesis-testing",
    "href": "lessons/01_Review/01_Review.html#steps-in-hypothesis-testing",
    "title": "Lesson 1: Review",
    "section": "Steps in hypothesis testing",
    "text": "Steps in hypothesis testing"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#example-one-sample-t-test",
    "href": "lessons/01_Review/01_Review.html#example-one-sample-t-test",
    "title": "Lesson 1: Review",
    "section": "Example: one sample t-test",
    "text": "Example: one sample t-test\n\nBodyTemps = read.csv(\"data/BodyTemperatures.csv\")\n\nggplot(data = BodyTemps, \n       aes(x = Temperature)) +\n  geom_histogram() +\n  theme(text = element_text(size=20)) +\n  labs(x = \"Temperature\", y = \"Count\", \n       title = \"Distribution of Body Temperature in Sample\") +\n  geom_vline(aes(xintercept = mean(BodyTemps$Temperature, na.rm = T)), \n             color = \"red\", linewidth = 2)"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#example-one-sample-t-test-using-p-value-approach",
    "href": "lessons/01_Review/01_Review.html#example-one-sample-t-test-using-p-value-approach",
    "title": "Lesson 1: Review",
    "section": "Example: one sample t-test using p-value approach",
    "text": "Example: one sample t-test using p-value approach\nWe want to see what the mean population body temperature is.\n\nState the null and alternative hypotheses:\n\n\n\n\n\n\n\n\\(H_0: \\mu = 98.6\\)\n\\(H_0\\): The population mean body temperature is 98.6 degrees F\n\n\n\\(H_A: \\mu \\neq 98.6\\)\n\\(H_A\\): The population mean body temperature is not 98.6 degrees F\n\n\n\nThe significance level is \\(\\alpha = 0.05\\)\nThe test statistic, \\(t_{\\bar{x}}\\) follows a student’s t-distribution with \\(df = n-1 = 129\\)\nThe test statistic is: \\(t_{\\bar{x}} = \\dfrac{\\bar{x}-\\mu_0}{\\dfrac{s}{\\sqrt{n}}}\\) and with the data: \\(t_{\\bar{x}} = \\dfrac{98.25-98.6}{\\dfrac{0.73}{\\sqrt{130}}} = -5.45\\)\nCalculate the p-value: \\(p-value = P(t \\leq -5.45) + P(t \\geq 5.45)\\)\n\n2*pt(-5.4548, df = 130-1, lower.tail=T)\n\n[1] 2.410889e-07\n\n\nConclusion: We reject the null hypothesis. There is sufficient evidence that the (population) mean body temperature after is different from 98.6 degree ( \\(p-value &lt; 0.001\\))."
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#example-one-sample-t-test-using-critical-values-approach",
    "href": "lessons/01_Review/01_Review.html#example-one-sample-t-test-using-critical-values-approach",
    "title": "Lesson 1: Review",
    "section": "Example: one sample t-test using critical values approach",
    "text": "Example: one sample t-test using critical values approach\nWe want to see what the mean population body temperature is.\n\nState the null and alternative hypotheses:\n\n\n\n\n\n\n\n\\(H_0: \\mu = 98.6\\)\n\\(H_0\\): The population mean body temperature is 98.6 degrees F\n\n\n\\(H_A: \\mu \\neq 98.6\\)\n\\(H_A\\): The population mean body temperature is not 98.6 degrees F\n\n\n\nThe significance level is \\(\\alpha = 0.05\\)\nThe test statistic, \\(t_{\\bar{x}}\\) follows a student’s t-distribution with \\(df = n-1 = 129\\)\nDecision rule (critical value): For \\(\\alpha=0.05\\) , \\(2*P(t \\geq t^*) = 0.05\\)\n\nqt(0.05/2, df = 130-1, lower.tail=F)\n\n[1] 1.978524\n\n\nThe test statistic is: \\(t_{\\bar{x}} = \\dfrac{\\bar{x}-\\mu_0}{\\dfrac{s}{\\sqrt{n}}}\\) and with the data: \\(t_{\\bar{x}} = \\dfrac{98.25-98.6}{\\dfrac{0.73}{\\sqrt{130}}} = -5.45\\)\nConclusion: We reject the null hypothesis. There is sufficient evidence that the (population) mean body temperature after is different from 98.6 degree ( 95% CI: \\(98.12, 98.38\\))."
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#how-did-we-get-the-95-ci",
    "href": "lessons/01_Review/01_Review.html#how-did-we-get-the-95-ci",
    "title": "Lesson 1: Review",
    "section": "How did we get the 95% CI?",
    "text": "How did we get the 95% CI?\n\nThe t.test function can help us answer this, and give us the needed information for both approaches.\n\n\nBodyTemps = read.csv(\"data/BodyTemperatures.csv\")\n\nt.test(x = BodyTemps$Temperature, \n       # alternative = \"two-sided\", \n       mu = 98.6)\n\n\n    One Sample t-test\n\ndata:  BodyTemps$Temperature\nt = -5.4548, df = 129, p-value = 2.411e-07\nalternative hypothesis: true mean is not equal to 98.6\n95 percent confidence interval:\n 98.12200 98.37646\nsample estimates:\nmean of x \n 98.24923"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#learning-objectives-5",
    "href": "lessons/01_Review/01_Review.html#learning-objectives-5",
    "title": "Lesson 1: Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify important descriptive statistics and visualize data from a continuous variable\nIdentify important distributions that will be used in 512/612\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\nUse our previous tools in 511 to conduct a hypothesis test\n\n\n\nDefine error rates and power"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#outcomes-of-our-hypothesis-test",
    "href": "lessons/01_Review/01_Review.html#outcomes-of-our-hypothesis-test",
    "title": "Lesson 1: Review",
    "section": "Outcomes of our hypothesis test",
    "text": "Outcomes of our hypothesis test"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#prabilities-of-outcomes",
    "href": "lessons/01_Review/01_Review.html#prabilities-of-outcomes",
    "title": "Lesson 1: Review",
    "section": "Prabilities of outcomes",
    "text": "Prabilities of outcomes\n\nType 1 error is \\(\\alpha\\)\n\nThe probability that we falsly reject the null hypothesis (but the null is true!!)\n\nPower is \\(1-\\beta\\)\n\nThe probability of correctly rejecting the null hypothesis"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#what-i-think-is-the-most-intuitive-way-to-look-at-it",
    "href": "lessons/01_Review/01_Review.html#what-i-think-is-the-most-intuitive-way-to-look-at-it",
    "title": "Lesson 1: Review",
    "section": "What I think is the most intuitive way to look at it",
    "text": "What I think is the most intuitive way to look at it\n\n\n\nLesson 1: Review"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html",
    "href": "lessons/00_Welcome/00_Welcome.html",
    "title": "Welcome to BSTA 512/612!",
    "section": "",
    "text": "Call me “Nicky,” “Dr. W,” “Professor Wakim,” or any combo!\n\nIf you are comfortable with it, I prefer Nicky\n\nAssistant Professor of Biostatistics\n \nOriginally from DC area (Virginia side!)\nTwo kitties\nVolleyball, biking, pickleball\nBut also sleeping, TV, and reading\nJust started taking a couple classes at PCC (French, ceramics, yoga)\nSlowly regrowing my plant collection after moving from Michigan\n\n\n\n\n Video"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#nicky-wakim-sheher",
    "href": "lessons/00_Welcome/00_Welcome.html#nicky-wakim-sheher",
    "title": "Welcome to BSTA 512/612!",
    "section": "Nicky Wakim (she/her)",
    "text": "Nicky Wakim (she/her)\n\n\n\nCall me “Nicky,” “Dr. W,” “Professor Wakim,” or any combo!\n\nIf you are comfortable with it, I prefer Nicky\n\nAssistant Professor of Biostatistics\n \nOriginally from DC area (Virginia side!)\nTwo kitties\nVolleyball, biking, pickleball\nBut also sleeping, TV, and reading\nJust started taking a couple classes at PCC (French, ceramics, yoga)\nSlowly regrowing my plant collection after moving from Michigan\n\n\n\n\n Video"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#some-important-tasks",
    "href": "lessons/00_Welcome/00_Welcome.html#some-important-tasks",
    "title": "Welcome to BSTA 512/612!",
    "section": "Some important tasks",
    "text": "Some important tasks\n\nJoin the Slack page!\nStar the class website: https://nwakim.github.io/W2024_BSTA_512/\nComplete the WhenIsGood for office hours\nComplete Homework 0 by this Thursday at 11pm!\nHighly suggest that you make an appointment with a learning specialist through Student Academic Support Services!"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website",
    "href": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website",
    "title": "Welcome to BSTA 512/612!",
    "section": "Let’s visit the website",
    "text": "Let’s visit the website\n\nHomepage\n\nGitHub\n\nSyllabus\nSchedule\n\nWeeks, class info, quizzes, homeworks, projects\n\nSearch\n\n\n\nImportant Note\n\n\nThis is my first time teaching the course. I will work hard to answer your questions in class, but I will often need some time outside of class to make sure I give you the best answer possible! Also, many of the examples are not my own. I will work to improve examples, but if you have feedback or suggestions, I am happy to hear them!"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#lets-go-through-the-syllabus",
    "href": "lessons/00_Welcome/00_Welcome.html#lets-go-through-the-syllabus",
    "title": "Welcome to BSTA 512/612!",
    "section": "Let’s go through the syllabus!",
    "text": "Let’s go through the syllabus!\nSyllabus page\n\n\nWelcome"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR_key_info.html#key-dates",
    "href": "lessons/03_SLR/03_SLR_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR_shiny_app.html",
    "href": "lessons/03_SLR/03_SLR_shiny_app.html",
    "title": "Untitled",
    "section": "",
    "text": "Loading required package: ggplot2\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks plotly::filter(), stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nLife Expectancy Simple Linear Regression\nLet’s try to find a model fit with the lowest score!\nClick two distinct points of the plot to draw a line. Try to minimize the score (at the top of the plot) using different lines. To create a new line just click on the plot more than twice. In this activity, the number we get is our “score.” In statistics, this score is actually called the sum of squared errors (SSE). Were you able to get the best-fit line presented in class?"
  },
  {
    "objectID": "lessons/18_In_class_activities/12_In_class_activities.html#example-1-palmer-penguins",
    "href": "lessons/18_In_class_activities/12_In_class_activities.html#example-1-palmer-penguins",
    "title": "Lesson 12: In-class exercise!!",
    "section": "Example 1: Palmer Penguins",
    "text": "Example 1: Palmer Penguins\n\nRevisit the Palmer Penguins dataset that we say in HW 4"
  },
  {
    "objectID": "lessons/18_In_class_activities/12_In_class_activities.html#lets-take-a-look-at-the-variables",
    "href": "lessons/18_In_class_activities/12_In_class_activities.html#lets-take-a-look-at-the-variables",
    "title": "Lesson 12: In-class exercise!!",
    "section": "Let’s take a look at the variables",
    "text": "Let’s take a look at the variables\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…"
  },
  {
    "objectID": "lessons/18_In_class_activities/12_In_class_activities.html#questions-we-can-ask-and-answer-so-far",
    "href": "lessons/18_In_class_activities/12_In_class_activities.html#questions-we-can-ask-and-answer-so-far",
    "title": "Lesson 12: In-class exercise!!",
    "section": "Questions we can ask and answer so far…",
    "text": "Questions we can ask and answer so far…\n\nUsing SLR, does each variable predict flipper length significantly?"
  },
  {
    "objectID": "lessons/18_In_class_activities/12_In_class_activities.html#example",
    "href": "lessons/18_In_class_activities/12_In_class_activities.html#example",
    "title": "Lesson 12: In-class exercise!!",
    "section": "Example",
    "text": "Example\n\nThe following example comes from this textbook\n\n\nSuppose that we are conducting an observational study of adults to assess whether physical activity level (PAL) is associated with systolic blood pressure (SBP), accounting (i.e., controlling) for AGE and SAB (sex assigned at birth). The extraneous variable here is AGE and SAB, while the explanatory variable (variable of interest) is PAL. We need to determine whether we can ignore AGE and/or SAB in our analysis and still correctly assess the PAL–SBP association."
  },
  {
    "objectID": "lessons/18_In_class_activities/12_In_class_activities.html#explore-the-data-first-thing-to-do",
    "href": "lessons/18_In_class_activities/12_In_class_activities.html#explore-the-data-first-thing-to-do",
    "title": "Lesson 12: In-class exercise!!",
    "section": "Explore the data: First thing to do",
    "text": "Explore the data: First thing to do\n\n\nCode\nggplot(SBP) + geom_histogram(aes(x = SBP))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCode\nggplot(SBP) + geom_histogram(aes(x = PAL))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCode\nggplot(SBP) + geom_histogram(aes(x = AGE))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html",
    "title": "MLR: Inference / F-test",
    "section": "",
    "text": "Interpret MLR (population) coefficient estimates with additional variable in model\nUnderstand the use of the general F-test and interpret what it measures.\nUnderstand the context of the Overall F-test, conduct the needed hypothesis test, and interpret the results.\nUnderstand the context of the single covariate F-test, conduct the needed hypothesis test, and interpret the results.\nUnderstand the context of the group of covariates F-test, conduct the needed hypothesis test, and interpret the results.\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#lets-map-that-to-our-regression-analysis-process",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#lets-map-that-to-our-regression-analysis-process",
    "title": "MLR: Inference / F-test",
    "section": "",
    "text": "Model Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#interpreting-the-estimated-population-coefficients",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#interpreting-the-estimated-population-coefficients",
    "title": "MLR: Inference / F-test",
    "section": "Interpreting the estimated population coefficients",
    "text": "Interpreting the estimated population coefficients\n\nFor a population model: \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\epsilon\\]\n\nWhere \\(X_1\\) and \\(X_2\\) are continuous variables\nNo need to specify \\(Y\\) because it required to be continuous in linear regression\n\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB)."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#getting-these-interpretations-from-our-regression-table",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#getting-these-interpretations-from-our-regression-table",
    "title": "MLR: Inference / F-test",
    "section": "Getting these interpretations from our regression table",
    "text": "Getting these interpretations from our regression table\nWe fit the regression model in R and printed the regression table:\n\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model: \\(\\widehat{\\text{LE}} = 33.595 + 0.157 \\text{ FLR} + 0.008 \\text{ FS}\\)\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 0 kcal PPD (95% CI: 24.674, 41.517).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012)."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#lets-just-examine-the-general-interpretation-vs.-the-example",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#lets-just-examine-the-general-interpretation-vs.-the-example",
    "title": "MLR: Inference / F-test",
    "section": "Let’s just examine the general interpretation vs. the example",
    "text": "Let’s just examine the general interpretation vs. the example\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 0 kcal PPD (95% CI: 24.674, 41.517).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012)."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#what-we-need-in-our-interpretations-of-coefficients-reference",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#what-we-need-in-our-interpretations-of-coefficients-reference",
    "title": "MLR: Inference / F-test",
    "section": "What we need in our interpretations of coefficients (reference)",
    "text": "What we need in our interpretations of coefficients (reference)\n\n\n\nUnits of Y\nUnits of X\nDiscussing intercept: Mean or average or expected before Y\nDiscussing coefficient for continuous covariate: Mean or average or expected before difference, increase, or decrease\n\nOR: Mean or average or expected before Y\nOnly need before difference or Y!!\n\nConfidence interval\n\n\n\nIf other covariates in the model\n\nDiscussing intercept: Must state that variables are equal to 0\n\nor at their centered value if centered!\n\nDiscussing coefficient for covariate: Must state “adjusting for all other variables”, “Controlling for all other variables”, or “Holding all other variables constant”\n\nIf only one other variable in the model, then replace “all other variables” with the single variable name"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#we-must-revisit-our-dear-friend-the-f-test",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#we-must-revisit-our-dear-friend-the-f-test",
    "title": "MLR: Inference / F-test",
    "section": "We must revisit our dear friend, the F-test!",
    "text": "We must revisit our dear friend, the F-test!\n\n\n\nhttps://www.writerswrite.co.za/foreshadowing/"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#remember-from-lesson-5-f-test-vs.-t-test-for-the-population-slope",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#remember-from-lesson-5-f-test-vs.-t-test-for-the-population-slope",
    "title": "MLR: Inference / F-test",
    "section": "Remember from Lesson 5: F-test vs. t-test for the population slope",
    "text": "Remember from Lesson 5: F-test vs. t-test for the population slope\nThe square of a \\(t\\)-distribution with \\(df = \\nu\\) is an \\(F\\)-distribution with \\(df = 1, \\nu\\)\n\\[T_{\\nu}^2 \\sim F_{1,\\nu}\\]\n\nWe can use either F-test or t-test to run the following hypothesis test:\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote that the F-test does not support one-sided alternative tests, but the t-test does!"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#remember-from-lesson-5-planting-a-seed-about-the-f-test",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#remember-from-lesson-5-planting-a-seed-about-the-f-test",
    "title": "MLR: Inference / F-test",
    "section": "Remember from Lesson 5: Planting a seed about the F-test",
    "text": "Remember from Lesson 5: Planting a seed about the F-test\nWe can think about the hypothesis test for the slope…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_1\\neq0\\)\n\n\n\n\nin a slightly different way…\n\n\n\n\n\n\nNull model (\\(\\beta_1=0\\))\n\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\nSmaller (reduced) model\n\n\n\n\n\n\nAlternative model (\\(\\beta_1\\neq0\\))\n\n\n\n\\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)\nLarger (full) model\n\n\n\n\n\n\nIn multiple linear regression, we can start using this framework to test multiple coefficient parameters at once\n\nDecide whether or not to reject the smaller reduced model in favor of the larger full model\nCannot do this with the t-test!"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#we-can-extend-this",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#we-can-extend-this",
    "title": "MLR: Inference / F-test",
    "section": "We can extend this!!",
    "text": "We can extend this!!\nWe can create a hypothesis test for more than one coefficient at a time…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=\\beta_2=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_1\\neq0\\) and/or \\(\\beta_2\\neq0\\)\n\n\n\n\nin a slightly different way…\n\n\n\n\n\n\nNull model\n\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\nSmaller (reduced) model\n\n\n\n\n\n\nAlternative* model\n\n\n\n\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\)\nLarger (full) model\n\n\n\n\n\n*This is not quite the alternative, but if we reject the null, then this is the model we move forward with"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#poll-everywhere-question-1",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#poll-everywhere-question-1",
    "title": "MLR: Inference / F-test",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#building-a-very-important-toolkit-three-types-of-tests",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#building-a-very-important-toolkit-three-types-of-tests",
    "title": "MLR: Inference / F-test",
    "section": "Building a very important toolkit: three types of tests",
    "text": "Building a very important toolkit: three types of tests\n\n\nOverall test\n\n\nDoes at least one of the covariates/predictors contribute significantly to the prediction of Y?\n\n\n\n\nTest for addition of a single variable (covariate subset test)\n\n\nDoes the addition of one particular covariate add significantly to the prediction of Y achieved by other covariates already present in the model?\n\n\n\n\nTest for addition of group of variables (covariate subset test)\n\n\nDoes the addition of some group of covariates add significantly to the prediction of Y achieved by other covariates already present in the model?"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#variation-explained-vs.-unexplained",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#variation-explained-vs.-unexplained",
    "title": "MLR: Inference / F-test",
    "section": "Variation: Explained vs. Unexplained",
    "text": "Variation: Explained vs. Unexplained\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 &= \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY &= SSR + SSE\n\\end{aligned}\\]\n\n\\(Y_i - \\overline{Y}\\) = the deviation of \\(Y_i\\) around the mean \\(\\overline{Y}\\)\n\n(the total amount deviation unexplained at \\(X_{i1},\\ldots,X_{ik}\\) ).\n\n\\(\\widehat{Y}_i- \\overline{Y}\\) = the deviation of the fitted value \\(\\widehat{Y}_i\\) around the mean \\(\\overline{Y}\\)\n\n(the amount deviation explained by the regression at \\(X_{i1},\\ldots,X_{ik}\\) ).\n\n\\(Y_i - \\widehat{Y}_i\\) = the deviation of the observation \\(Y\\) around the fitted regression line\n\n(the amount deviation unexplained by the regression at \\(X_{i1},\\ldots,X_{ik}\\) )"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#another-way-to-think-of-ssy-ssr-and-sse",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#another-way-to-think-of-ssy-ssr-and-sse",
    "title": "MLR: Inference / F-test",
    "section": "Another way to think of SSY, SSR, and SSE",
    "text": "Another way to think of SSY, SSR, and SSE\n\nLet’s create a data frame of each component within the SS’s\n\nDifference in SSY: \\(Y_i - \\overline{Y}\\)\nDifference in SSR: \\(\\widehat{Y}_i- \\overline{Y}\\)\nDifference in SSE: \\(Y_i - \\widehat{Y}_i\\)\n\nUsing our simple linear regression model as an example:\n\n\nslr1 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate, data = gapm_sub)\naug_slr1 = augment(slr1)\nSS_df = gapm_sub %&gt;% select(LifeExpectancyYrs) %&gt;%\n  mutate(SSY_diff = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         y_fit = aug_slr1$.fitted, \n         SSR_diff = y_fit - mean(LifeExpectancyYrs), \n         SSE_diff = aug_slr1$.resid)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#plot-the-components-of-each-sum-of-squares",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#plot-the-components-of-each-sum-of-squares",
    "title": "MLR: Inference / F-test",
    "section": "Plot the components of each sum of squares",
    "text": "Plot the components of each sum of squares\n\nSSY_plot = ggplot(SS_df, aes(SSY_diff)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) \nSSR_plot = ggplot(SS_df, aes(SSR_diff)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) \nSSE_plot = ggplot(SS_df, aes(SSE_diff)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) \ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\\[SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = 64.64\\]\n\\[SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = 27.24\\]\n\\[SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = 37.39\\]"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#when-running-a-f-test-for-linear-models",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#when-running-a-f-test-for-linear-models",
    "title": "MLR: Inference / F-test",
    "section": "When running a F-test for linear models…",
    "text": "When running a F-test for linear models…\n\nWe need to define a larger, full model (more parameters)\nWe need to define a smaller, reduced model (fewer parameters)\nUse the F-statistic to decide whether or not we reject the smaller model\n\nThe F-statistic compares the SSE of each model to determine if the full model explains a significant amount of additional variance\n\n\n\n\n \n\\[\nF = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\n\\]\n\n\n\\(SSE(R) \\geq SSE(F)\\)\nNumerator measures difference in unexplained variation between the models\n\nBig difference = added parameters greatly reduce the unexplained variation (increase explained variation)\nSmaller difference = added parameters don’t reduce the unexplained variation\n\nTake ratio of difference to the unexplained variation in the full model"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#poll-everywhere-question-2",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#poll-everywhere-question-2",
    "title": "MLR: Inference / F-test",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#we-will-keep-working-with-the-mlr-model-from-last-class",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#we-will-keep-working-with-the-mlr-model-from-last-class",
    "title": "MLR: Inference / F-test",
    "section": "We will keep working with the MLR model from last class",
    "text": "We will keep working with the MLR model from last class\nNew population model for example:\n\\[\\text{Life expectancy} = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_2 \\text{Food supply} + \\epsilon\\]\n\n# Fit regression model:\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\ntidy(mr1, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{Life expectancy}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{Female literacy rate} + \\widehat{\\beta}_2 \\text{Food supply} \\\\\n\\widehat{\\text{Life expectancy}} &= 33.595 + 0.157\\ \\text{Female literacy rate}\n+ 0.008\\ \\text{Food supply}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#overall-f-test",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#overall-f-test",
    "title": "MLR: Inference / F-test",
    "section": "Overall F-test",
    "text": "Overall F-test\nDoes at least one of the covariates/predictors contribute significantly to the prediction of Y?\n\nFor a general population MLR model, \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\nWe can create a hypothesis test for all the covariate coefficients…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=\\beta_2= \\ldots=\\beta_k=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\nAt least one \\(\\beta_j\\neq0\\) (for \\(j=1, 2, \\ldots, k\\))\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k + \\epsilon\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#overall-f-test-general-steps-for-hypothesis-test",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#overall-f-test-general-steps-for-hypothesis-test",
    "title": "MLR: Inference / F-test",
    "section": "Overall F-test: general steps for hypothesis test",
    "text": "Overall F-test: general steps for hypothesis test\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1=\\beta_2= \\ldots=\\beta_k=0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=1, 2, \\ldots, k\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}} = \\frac{MSR_{full}}{MSE_{full}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject if: \\(P(F_{k, n-k-1} &gt; F) &lt; \\alpha\\)\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that at least one predictor’s coefficient is not 0 (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#overall-f-test-a-word-on-the-conclusion",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#overall-f-test-a-word-on-the-conclusion",
    "title": "MLR: Inference / F-test",
    "section": "Overall F-test: a word on the conclusion",
    "text": "Overall F-test: a word on the conclusion\n\nIf \\(H_0\\) is rejected, we conclude there is sufficient evidence that at least one predictor’s coefficient is different from zero.\nSame as: at least one independent variable contributes significantly to the prediction of \\(Y\\)\n\n \n\nIf \\(H_0\\) is not rejected, we conclude there is insufficient evidence that at least one predictor’s coefficient is different from zero.\nSame as: Not enough evidence that at least one independent variable contributes significantly to the prediction of \\(Y\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#lets-think-about-our-mlr-example-for-life-expectancy",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#lets-think-about-our-mlr-example-for-life-expectancy",
    "title": "MLR: Inference / F-test",
    "section": "Let’s think about our MLR example for life expectancy",
    "text": "Let’s think about our MLR example for life expectancy\nOur proposed population model\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\ \\text{FS}\n\\end{aligned}\\]\nOur main question for the Overall F-test: Is the regression model containing female literacy rate and food supply useful in estimating countries’ life expectancy?\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(LE = \\beta_0 + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model",
    "title": "MLR: Inference / F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\nmod_red1 = lm(LifeExpectancyYrs ~ 1, data = gapm_sub)\naug_red1  = augment(mod_red1)\n\nmod_full1 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD,\n               data = gapm_sub)\naug_full1  = augment(mod_full1)\n\nSS_df2 = gapm_sub %&gt;% select(LifeExpectancyYrs) %&gt;%\n  mutate(SSY_diff_r1 = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         SSR_diff_r1 = aug_red1$.fitted - mean(LifeExpectancyYrs), \n         SSE_diff_r1 = aug_red1$.resid, \n         SSY_diff_f1 = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         SSR_diff_f1 = aug_full1$.fitted - mean(LifeExpectancyYrs), \n         SSE_diff_f1 = aug_full1$.resid)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-1",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-1",
    "title": "MLR: Inference / F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\n\nReduced / null model \\[LE = \\beta_0 + \\epsilon\\]\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 0\\]\n \n\\[SSE = 64.64\\]\n\n\n\nFull / Alternative model \\[LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\]\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 36.39\\]\n \n\\[SSE = 28.25\\]"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#poll-everywhere-question-3",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#poll-everywhere-question-3",
    "title": "MLR: Inference / F-test",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (1/3)",
    "text": "So let’s step through our hypothesis test (1/3)\n\n\n\nMet underlying LINE assumptions\n\n\n\n \n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1=\\beta_2=0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_1\\neq0 \\text{ or } \\beta_2\\neq0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k =2\\) and denominator \\(df=n-k-1 = 72 - 2-1=69\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (2/3)",
    "text": "So let’s step through our hypothesis test (2/3)\n\n\n\nCompute the value of the test statistic / 6. Calculate the p-value\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}=44.443\\] OR use ANOVA table:\n\nanova(mod_red1, mod_full1) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ 1\n71.000\n4,589.119\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD\n69.000\n2,005.556\n2.000\n2,583.563\n44.443\n0.000"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (3/3)",
    "text": "So let’s step through our hypothesis test (3/3)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n \nWe reject the null hypothesis at the 5% significance level. There is sufficient evidence that either countries’ female literacy rate or the food supply (or both) contributes significantly to the prediction of life expectancy (p-value &lt; 0.001)."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#covariate-subset-test-single-variable",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#covariate-subset-test-single-variable",
    "title": "MLR: Inference / F-test",
    "section": "Covariate subset test: Single variable",
    "text": "Covariate subset test: Single variable\nDoes the addition of one particular covariate of interest add significantly to the prediction of Y achieved by other covariates already present in the model?\n\nFor a general population MLR model, \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\beta_j X_j +\\ldots + \\beta_k X_k + \\epsilon\\]\n\nWe can create a hypothesis test for a single \\(j\\) covariate coefficient (where \\(j\\) can be any value \\(1, 2, \\ldots, k\\))…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_j=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_j\\neq0\\)\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(\\begin{aligned}Y = &\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_j X_j +\\\\ &\\ldots + \\beta_k X_k + \\epsilon \\end{aligned}\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#single-covariate-f-test-general-steps-for-hypothesis-test-reference",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#single-covariate-f-test-general-steps-for-hypothesis-test-reference",
    "title": "MLR: Inference / F-test",
    "section": "Single covariate F-test: general steps for hypothesis test (reference)",
    "text": "Single covariate F-test: general steps for hypothesis test (reference)\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_j=0\\\\\n\\text{vs. } H_A&: \\beta_j\\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that predictor/covariate \\(j\\) significantly improves the prediction of Y, given all the other covariates are in the model (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#lets-think-about-our-mlr-example-for-life-expectancy-1",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#lets-think-about-our-mlr-example-for-life-expectancy-1",
    "title": "MLR: Inference / F-test",
    "section": "Let’s think about our MLR example for life expectancy",
    "text": "Let’s think about our MLR example for life expectancy\nOur proposed population model\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\ \\text{FS}\n\\end{aligned}\\]\nOur main question for the single covariate subset F-test: Is the regression model containing food supply improve the estimation of countries’ life expectancy, given female literacy rate is already in the model?\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-2",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-2",
    "title": "MLR: Inference / F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\n\nReduced / null model \\[LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\]\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 27.24\\]\n \n\\[SSE = 37.39\\]\n\n\n\nFull / Alternative model \\[LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\]\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 36.39\\]\n \n\\[SSE = 28.25\\]"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#poll-everywhere-question-4",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#poll-everywhere-question-4",
    "title": "MLR: Inference / F-test",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13-1",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13-1",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (1/3)",
    "text": "So let’s step through our hypothesis test (1/3)\n\n\n\nMet underlying LINE assumptions\n\n\n\n \n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_2=0\\\\\n\\text{vs. } H_A&: \\beta_2\\neq0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k =2\\) and denominator \\(df=n-k-1 = 72 - 2-1=69\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23-1",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23-1",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (2/3)",
    "text": "So let’s step through our hypothesis test (2/3)\n\n\n\nCompute the value of the test statistic / 6. Calculate the p-value\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\] ANOVA table:\n\nanova(mod_red2, mod_full2) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate\n70.000\n2,654.875\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD\n69.000\n2,005.556\n1.000\n649.319\n22.339\n0.000"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33-1",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33-1",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (3/3)",
    "text": "So let’s step through our hypothesis test (3/3)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n \nWe reject the null hypothesis at the 5% significance level. There is sufficient evidence that countries’ food supply contributes significantly to the prediction of life expectancy, given that female literacy rate is already in the model (p-value &lt; 0.001)."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#covariate-subset-test-group-of-variables",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#covariate-subset-test-group-of-variables",
    "title": "MLR: Inference / F-test",
    "section": "Covariate subset test: group of variables",
    "text": "Covariate subset test: group of variables\nDoes the addition of some group of covariates of interest add significantly to the prediction of Y obtained through other independent variables already present in the model?\n\nFor a general population MLR model, \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\nWe can create a hypothesis test for a group of covariate coefficients (subset of many)… For example…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=\\beta_3 =0\\) (this can be any coefficients)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\nAt least one \\(\\beta_j\\neq0\\) (for \\(j=2,3\\))\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(Y = \\beta_0 + \\beta_2 X_2 + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X + \\beta_3 X_3+\\epsilon\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#covariate-subset-f-test-general-steps-for-hypothesis-test-reference",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#covariate-subset-f-test-general-steps-for-hypothesis-test-reference",
    "title": "MLR: Inference / F-test",
    "section": "Covariate subset F-test: general steps for hypothesis test (reference)",
    "text": "Covariate subset F-test: general steps for hypothesis test (reference)\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\nFor example:\n\\[\\begin{align}\nH_0 &: \\beta_1 = \\beta_3 = 0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=1,3\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that predictors/covariates \\(2,3\\) significantly improve the prediction of Y, given all the other covariates are in the model (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#we-need-to-slightly-alter-our-mlr-example-for-life-expectancy",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#we-need-to-slightly-alter-our-mlr-example-for-life-expectancy",
    "title": "MLR: Inference / F-test",
    "section": "We need to slightly alter our MLR example for life expectancy",
    "text": "We need to slightly alter our MLR example for life expectancy\nOur proposed population model to include water source percent (WS):\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\beta_3 WS + \\epsilon\\]\n\nWe don’t have a fitted multiple regression model for this yet!\n\nOur main question for the group covariate subset F-test: Is the regression model containing food supply and water source percent improve the estimation of countries’ life expectancy, given percent female literacy rate is already in the model?\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\beta_3 WS + \\epsilon\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-3",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-3",
    "title": "MLR: Inference / F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\n\nReduced / null model \\[LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\]\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 27.24\\]\n \n\\[SSE = 37.39\\]\n\n\n\nFull / Alternative model \\[LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\beta_3 WS + \\epsilon\\]\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 43.26\\]\n \n\\[SSE = 21.38\\]"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13-2",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13-2",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (1/3)",
    "text": "So let’s step through our hypothesis test (1/3)\n\n\n\nMet underlying LINE assumptions\n\n\n\n \n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_2=\\beta_3=0\\\\\n\\text{vs. } H_A&: \\beta_2\\neq0 \\text{ and/or } \\beta_3\\neq0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k =2\\) and denominator \\(df=n-k-1 = 72 - 2-1=69\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23-2",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23-2",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (2/3)",
    "text": "So let’s step through our hypothesis test (2/3)\n\n\n\nCompute the value of the test statistic / 6. Calculate the p-value\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\] ANOVA table:\n\nanova(mod_red3, mod_full3) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate\n70.000\n2,654.875\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD + WaterSourcePrct\n68.000\n1,517.916\n2.000\n1,136.959\n25.467\n0.000"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33-2",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33-2",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (3/3)",
    "text": "So let’s step through our hypothesis test (3/3)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n \nWe reject the null hypothesis at the 5% significance level. There is sufficient evidence that countries’ food supply or water source (or both) contribute significantly to the prediction of life expectancy, given that female literacy rate is already in the model (p-value &lt; 0.001)."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#other-ways-to-word-the-hypothesis-tests-reference",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#other-ways-to-word-the-hypothesis-tests-reference",
    "title": "MLR: Inference / F-test",
    "section": "Other ways to word the hypothesis tests (reference)",
    "text": "Other ways to word the hypothesis tests (reference)\n\nSingle covariate subset F-test\n\n\\(H_0:\\) \\(X^*\\) does not significantly improve the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_p\\) are already in the model\n\\(H_A:\\) \\(X^*\\) significantly improves the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_p\\) are already in the model\n\nGroup covariate subset F-test\n\n\\(H_0:\\) The addition of the \\(s\\) variables \\(X_1^*, X_2^*, \\ldots, X_s^*\\) does not significantly improve the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_q\\) are already in the model\n\\(H_A:\\) The addition of the \\(s\\) variables \\(X_1^*, X_2^*, \\ldots, X_s^*\\) significantly improves the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_q\\) are already in the model"
  },
  {
    "objectID": "homework/HW_05.html",
    "href": "homework/HW_05.html",
    "title": "Homework 5",
    "section": "",
    "text": "Caution\n\n\n\nThis homework is not ready to be worked on!! Nicky needs to update it."
  },
  {
    "objectID": "homework/HW_05.html#directions",
    "href": "homework/HW_05.html#directions",
    "title": "Homework 5",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW5.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: \"BSTA 512/612\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nIf you are computing something by hand, you may take a picture of your work and insert the image in this file. You may also use LaTeX to write it inline.\nWrite all answers in complete sentences as if communicating the results to a collaborator. This means including a sentence summarizing results in the context of the research study.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_05.html#question-1",
    "href": "homework/HW_05.html#question-1",
    "title": "Homework 5",
    "section": "Question 1",
    "text": "Question 1\nWe are going to revisit the Palmer Penguins dataset from Homework 4. Choosing what to test, interpretations of coefficients, F-test conclusions, and interactions\nFor this problem we will be using the penguins dataset from the palmerpenguins R package. We will look at the association between flipper length of penguins (measured in mm) and specific species of penguins.\nDescription from help file:\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\n\nMore info about the data.\n\n# first install the palmerpenguins package\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\ndata(penguins)\n\n# run the command below to learn more about the variables in the penguins dataset\n# ?penguins\n\n\nPart a\nMake a plot of flipper length (outcome) and body mass (explanatory variable). Discuss what you see in the plot.\n\n\nPart b\nWrite the simple linear regression model that we will fit for the association between body mass and flipper length. If you use any short hand, please write it out. For example: Let \\(BD\\) represent bill depth.\n\n\nPart c\nRun the simple linear regression model for the association between body mass and flipper length. Display the regression table output.\n\n\nPart d\nInterpret the coefficient for body mass. Note that as we move forward with a multivariate model, we will refer to this is estimate at the the crude or unadjusted coefficient estimate.\n\n\nPart e\nDiscuss how centering body mass might help with interpretability. Then, center body mass around the mean, run the model again, and display the regression table. Does the intercept and/or slope change from Part c?\n\n\nPart f\nMake a plot of flipper length (outcome) and body mass (explanatory variable) by bill depth. Discuss what you see in the plot. (Hint: bill depth will be the color in the plot.)\n\n\nPart g\nMake a plot of flipper length (outcome) and body mass (explanatory variable) by penguin species. Discuss what you see in the plot and relate it back to the plot in Part f.\n\n\nPart h\nUsing only body mass and bill depth as covariates, write out the model that we would fit including the main effects of body mass and bill depth and their interaction. How many coefficients are tested when we test for a significant interaction?\n\n\n\n\n\n\nNote\n\n\n\nBoth covariates should be centered. For the rest of the homework, we will use the centered body mass and bill depth.\n\n\n\n\nPart i\nCenter bill depth.\n\n\nPart j\nUsing only body mass and bill depth as covariates, test if bill depth is an effect modifier or confounder of body mass, or if it is not in the model at all.\n\n\nPart k\nUsing only body mass and species as covariates, write out the model that we would fit including the main effects of body mass and species and their interaction. How many coefficients are tested when we test for a significant interaction?\nHint: Homework 4 can help guide us with the species’ categories.\n\n\nPart l\nUsing only body mass and species as covariates, test if species is an effect modifier or confounder of body mass, or if it is not in the model at all. Note that\n\n\nPart m\nUsing the results in the above parts, we will move forward with the following model:\n\\[\\begin{aligned}\nFL = & \\beta_0 + \\beta_1 BM^c + \\beta_2 BD^c +  \\beta_3 I(\\textrm{Chinstrap}) + \\beta_4 I(\\textrm{Gentoo}) +  \\\\ & \\beta_5 BM^c \\cdot I(\\textrm{Chinstrap}) + \\beta_6 BM^c \\cdot I(\\textrm{Gentoo}) + \\epsilon\n\\end{aligned}\\]\nRun the above model and display the regression table output.\nPlease note that this is not exactly the best method for selecting a model. I just wanted to step us through a similar thought process.\n\n\nPart n\nInterpret each coefficient in the model above. There should be 7 total interpretations."
  },
  {
    "objectID": "homework/HW_00.html",
    "href": "homework/HW_00.html",
    "title": "Homework 0",
    "section": "",
    "text": "Caution\n\n\n\nThis homework is not ready to be worked on!! Nicky needs to update it."
  },
  {
    "objectID": "homework/HW_00.html#directions",
    "href": "homework/HW_00.html#directions",
    "title": "Homework 0",
    "section": "Directions",
    "text": "Directions\nThis homework must be turned into Sakai. I want to make sure we are all familiar with the process of downloading a .qmd file, editing it, and resubmitting an .html and .qmd file.\nHere are the instructions for downloading and submitting your homework:\n\nGo to this github site to download the homework’s .qmd file.\nWhen you reach the site, it should look like this (from my 2023 class):\nClick the “Download raw file” icon () to download the .qmd file. This will likely download the file into your “Downloads” folder. It is up to you to move the file into the appropriate folder.\nPlease rename you homework as Lastname_Firstinitial_HW0.qmd . This will help organize the homeworks when the TAs grade them.\nPlease also update the following line under subtitle: \"BSTA 512/612\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\nEdit the document with your explanations and code. If you are writing out an answer or calculating by hand, you can take a picture of your work and embed it within the .qmd file. The pictures will be viewable on the .html file.\nPlease upload your homework to this Sakai assignment. Upload both your .qmd code file and the rendered .html file.\n\nThese instructions will not appear on every homework. You may come back to this page for reference.\n\nPurpose\nThis homework is meant to introduce yourself to me and your peers. In the first class, we will all briefly introduce ourselves, but we don’t have enough time in-depth introductions. Thus, I’d like you to share some information with the class over Slack.\n\n\nGrading\nGrading will be done as a check/no check for turning in your work. If you are stressed about time, please turn in whatever you have completed."
  },
  {
    "objectID": "homework/HW_00.html#questions",
    "href": "homework/HW_00.html#questions",
    "title": "Homework 0",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nPlease upload a picture of yourself to Sakai. Make sure your face is visible in this picture. It will help me and the TAs identify you.\nPlease follow the below steps to add a picture of yourself in Sakai:\n\nClick on the top right circle on your Sakai page. If you do not have a picture already, your initials will be showing.\nClick “Profile.”\nOnce in your profile, hover over the square then click “Change picture.”\nUpload a picture file of yourself.\n\nPlease include the picture you used below. This will make sure we are all able to insert a picture within Quarto. It will also make sure I can see the photo within the .html file.\n\n\nQuestion 2\nProvide a pronunciation of your name: Please follow the below steps to add an audio and written pronunciation of your name in Sakai:\n\nClick on the top right circle on your Sakai page. If you do not have a picture already, your initials will be showing.\nClick “Profile.”\nHover over the section “Name Pronunciation and Pronouns.” Click the edit button that should appear in the top right corner of the section.\nAdd a recording of your name pronunciation.\nOPTIONAL You may also edit the phonetic pronunciation of your name. I realize that doing this is may require a lot of time researching phonetics.\nOPTIONAL If you are comfortable sharing your pronouns, please change these as well.\n\n\n\nQuestion 3\nPlease complete the following whenisgood poll so that we can schedule office hours. Please use a unique identifier (does not have to be your name), so that I can make sure each student can attend at least one office hour.\n\n\nQuestion 4\nCompletion of this question is only necessary if you have accommodations. If you have any learning accommodations, please email me about your needs. I should receive a direct email from the Office of Student Access, but it is important that we discuss how accommodations will translate to our class.\n\n\nQuestion 5\nGo to the OneDrive folder and request access.\n\n\nQuestion 5\nIf you are submitting a late homework or informing me about an extension, who should be included in the email?\n(See Syllabus for answer)"
  },
  {
    "objectID": "homework/HW_03.html",
    "href": "homework/HW_03.html",
    "title": "Homework 3",
    "section": "",
    "text": "Caution\n\n\n\nThis homework is not ready to be worked on!! Nicky needs to update it."
  },
  {
    "objectID": "homework/HW_03.html#directions",
    "href": "homework/HW_03.html#directions",
    "title": "Homework 3",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW0.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: \"BSTA 512/612\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nIf you are computing something by hand, you may take a picture of your work and insert the image in this file. You may also use LaTeX to write it inline.\nWrite all answers in complete sentences as if communicating the results to a collaborator. This means including a sentence summarizing results in the context of the research study.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_03.html#questions",
    "href": "homework/HW_03.html#questions",
    "title": "Homework 3",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nA high respiratory rate is a potential diagnostic indicator of respiratory infection in children. To judge whether a respiratory rate is “high” however, a physician must have a clear picture of the distribution of normal rates. To this end, Italian researchers measured the respiratory rates (in breaths/minute) of 618 children between the ages of 15 days and 3 years (measured in months).\nThe data and problem framing came from the Sleuth3 package. Please make sure to run the following code to load the data. You can directly access the dataset ex0824 from the package. I have included a new assignment of the data to q1_data if you would like to use that.\n\nif(!require(Sleuth3)) { install.packages(\"Sleuth3\"); library(Sleuth3) }\nq1_data = ex0824 \n\n\nPart a\nCreate a scatterplot of the dependent and independent variables with both the best-fit line and a smoothed curve through the points. Describe the relationship between the dependent and independent variables, and also comment on whether you think it is reasonable to use a linear regression to model the relationship.\n\n\nPart b\nWrite out the population regression model for the simple linear regression model. Please leave the variables untransformed for now.\n\n\nPart c\nFit the regression model, display the regression table, and write out the fitted regression line.\n\n\nPart d\nAssess the normality of the model’s fitted residuals by creating a histogram, density plot, and boxplot of the residuals to visually inspect the distribution of the residuals, and describe any deviations from normality.\n\n\nPart e\nAssess the normality of the model’s fitted residuals by creating QQ plot of the residuals.\nBonus work, but not required: Compare the QQ plot to 4 such plots simulated from normal data, and discuss why or why not the residuals could have come from a normal distribution.\n\n\nPart f\nTest the normality of the model’s fitted residuals and comment on whether the test’s conclusion is consistent with your visual inspection or not. Make sure to include the hypotheses, needed R code, and a conclusion to the test based on the p-value (as shown in these slides).\n\n\nPart g\nCreate a residual plot using ggplot and the residuals. Discuss what this means in the context of our model assumptions.\n\n\nPart h\nDetermine whether there are any observations with high leverage. Please use the cutoff, \\(h_i &gt; 6/n\\). If there are observations with high leverage, print the observations and state how many high leverage points there are.\n\n\nPart i\nPrint the 10 observations with highest Cook’s distance. If there are observations with high Cook’s distance (\\(d_i &gt;1\\)), state how many observations have high Cook’s distance.\n\n\nPart j\nCreate a histogram for rate. Describe its distribution shapes.\n\n\nPart k\nUsing the above histogram, and Tukey’s ladder of transformations, discuss the range of transformations that will be appropriate for Rate. Explain your reasoning.\nThen use gladder() to decide on two possible transformations. Explain your reasoning.\nNote: questions below will ask about model fit with the transformations. For now, just explain why you chose the two that you did.\n\n\nPart l\nAdd the two rate transformations you chose above to the dataset. You do not need to print any output, just make sure the code is visible.\n\n\nPart m\nCreate scatterplots using two transformed rates and age. Discuss if either transformation potentially improves the model fit. Explain why or why not. Note: including lines will help!\n\n\nPart n\nUsing one of the transoformed outcomes, fit the regression model, display the regression table, and write out the fitted regression line.\n\n\nPart o\nAssess the normality of the model’s fitted residuals by creating QQ plot of the residuals. Does the transformation improve the QQ plot?\n\n\nPart p\nCreate a residual plot using ggplot and the residuals. Discuss what this means in the context of our model assumptions. Does the transformation improve our model assumptions?\n\n\nPart q\nBetween the model with the untransformed outcome and the transformed outcome, which would you recommend using for analysis? (Hint: there are pros and cons to both models)\n\n\n\nQuestion 2\nThis question uses the same dataset as HW 2, question 1.\nThis question is based on data collected as part of an observational study of patients who suffered from stroke.\nDataset: The main goal was to study various psychological factors: optimism, fatalism, depression, spirituality, and their relationship with stroke severity and other health outcomes among the study participants. Data were collected using questionnaires during a baseline interview and also medical chart review. More information about this study can be found in the article Fatalism, optimism, spirituality, depressive symptoms and stroke outcome: a population based analysis.\nThe dataset that you will work with is called completedata.sas7bdat. The two variables we are interested in are:\n\nCovariate 1: Fatalism (larger values indicate that the individual feels less control of their life)\n\nPotential scores range from 8 to 40\n\nCovariate 2: Optimism (larger values indicate that the individual feels higher levels of optimism)\n\nPotential scores range from 6 to 24\n\nCovariate 3: Spirituality (larger values indicate that the individual has more belief in a higher power)\n\nPotential scores range from 2 to 8\n\nOutcome: Depression (larger values imply increased depression)\n\nPotential scores range from 0 to 27\n\n\nFor our homework purposes we will assume each variable is continuous.\n\ndep_df = read_sas(here(\"./data/completedata.sas7bdat\"))\n\n\nPart a\nFit the regression model with all the covariates (Fatalism, Optimism, Spirituality), display the regression table, and write out the fitted regression line.\n\n\nPart b\nInterpret each coefficient (\\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\)).\nDoes the intercept make sense for the range of values that each covariate can take? Explain.\n\n\nPart c\nRecall in Homework 2, we ran a simple linear regression model for Depression vs. Fatalism with the following interpretation for the coefficient: For every 1 point higher fatalism score, there is an expected difference of 0.25 points higher depression score (95%CI: 0.17, 0.32).\nDoes the addition of Optimism and Spirituality change our coefficient estimate for Fatalism? (No need for an official hypothesis test here. I just want us to note some differences.)\n\n\nPart d\nFrom the fitted regression model, calculate the regression line when Optimism score is 10 and Spirituality score is 6.\n\n\nPart e\nDoes at least one of the covariates contribute significantly to the prediction of Depression? (Note: this is an overall test. Please follow the hypothesis test steps. To complete step 4-6, simply output your ANOVA table.)\n\n\nPart f\nDoes the addition of Spirituality add significantly to the prediction of Depression achieved by Fatalism and Optimism?\n\n\nPart g\nDoes the addition of Spirituality and Optimism add significantly to the prediction of Depression achieved by Fatalism?"
  },
  {
    "objectID": "labs/Lab_03_work.html",
    "href": "labs/Lab_03_work.html",
    "title": "Lab 3 Instructions",
    "section": "",
    "text": "IMPORTANT TO READ\n\n\n\n\nPlease do not delete the rubric from your .qmd file. I will use it to circle the grades!\nThere is an intructions file and a file for you to edit and turn in. Please only work in the latter file!!"
  },
  {
    "objectID": "labs/Lab_03_work.html#directions",
    "href": "labs/Lab_03_work.html#directions",
    "title": "Lab 3 Instructions",
    "section": "1 Directions",
    "text": "1 Directions\nPlease turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here.\n\n\n\n\n\n\nCaution\n\n\n\nThis is the instructions file. The link above will take you to the editing file where you can add your work and turn it in!! Please do not remove anything from the editing file!!\n\n\n\n1.1 Purpose\nThe main purpose of this lab is to perform some quality control on our data, recode some of the multi-selection categorical variables, continue data exploration, and start analyzing the main relationship of our research question.\n\n\n1.2 Grading\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\n1.2.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning (like Section 2.4 and explanations in Section 2.5)"
  },
  {
    "objectID": "labs/Lab_03_work.html#lab-activities",
    "href": "labs/Lab_03_work.html#lab-activities",
    "title": "Lab 3 Instructions",
    "section": "2 Lab activities",
    "text": "2 Lab activities\nBefore starting this lab, you should go back to Lab 2, save a new .rda file that contains all the new variables from that Lab. Then you can load it here!\n\n2.1 Restate your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\nHow is implicit anti-fat bias, as measured by the IAT score, associated with “insert main independent variable here”?\n\n\n2.2 Quality Control\nThere are a few more issues with the data that we need to look into. First, there is another coding for NA values in the race variable: -999. We will need to filter out these observations.\nWe will also need to look at individuals who have potentially answered the survey questions untruthfully. We cannot catch everything, but a good place to start is by looking at individuals who have done more than one of the following:\n\nselected the earliest or latest possible birth year\nselected the lowest or highest possible education\nselected all gender identities (for those using gender identity)\nselected all races (for those using multiple selection race)\nselected the lowest or highest weight (for those looking at BMI)\nselected the lowest or highest height (for those looking at BMI)\n\nI want to take a second to mention that any of the above selections, and combinations of the above selections, are valid. However, we should start to flag the possibility that someone has not gone through the survey properly if we notice that most or all of the respondent’s answers are the first answer choice, last answer choice, or selected all options. Additionally, not all of these carry the same importance in discerning validity. For example, a recorded age of 111 years old is the most striking to me. When paired with other selections that are the maximum or minimum (or first or last) option, then I will record it for future investigation. If this observation looks to be an outlier or high leverage point in our analysis, that is when I’ll decide to remove it.\n\n\n\n\n\n\nTasks\n\n\n\n\nFilter out observations with a value of -999 in the race variable.\nGlimpse at the observations that may indicate a respondent who has not properly completed the survey portion. This will require filtering for specific answer choices. Please see examples of filter() on it’s documentation page.\n\n\n\n\n\n2.3 Working with multi-selection variables\nIn the list of variables that we may choose to work with (in Lab 2), there are two that allowed respondents to select multiple categories. The two variables are genderIdentitiy and raceombmulti. If you did not choose these variables to work with, you may skip this section.\nIf you chose one or both of these variables, then we need to make new variables that correspond to indicators for each possible selection in the respective variable.\nLet’s start with the grepl function. For this function, we can input one of our column names and a value, then it will output, for each row, if the value is in the column. For example, in genderIdentity an individual may identify as a “Trans female/Trans woman” and “Gender queer/Gender nonconforming.” In our dataset in R, this would show as [4,5] in genderIdentity. If we want to create two separate indicators for anyone who identifies as “Trans female/Trans woman” then I need to look for the value 4 in the column genderIdentity. I will run a separate indicator to find individuals who identify as “Gender queer/Gender nonconforming.” Here is an example code of how I would use grepl to do this:\n\niat_prep_new = iat_prep_old %&gt;%\n  mutate(ind_tf_tw = grepl(4, genderIdentity), \n         ind_gq_gnc = grepl(5, genderIdentity))\n\nYou will need to extend this to all other gender identities.\nFor race, raceombmulti is also the follow up question to raceomb_002. So our indicators need to reflect both variables. In this case, we need to use grepl on both columns at once. For example, if I want to create an indicator for individuals who identify as American Indian/Alaskan Native then I need to find individuals who identify as American Indian/Alaskan Native only and individuals who identify as American Indian/Alaskan Native in addition to another race. For example, my code might look like:\n\niat_prep_new = iat_prep_old %&gt;%\n  mutate(ind_AIAN = grepl(1, raceomb_002) | grepl(1, raceombmulti))\n\nI suggest only searching for 1-7 in both raceomb_002 and raceombmulti. Note that if raceomb_002 = 8 , then individuals identified as “multiracial” and will select values in raceombmulti.\n\n\n\n\n\n\nTask\n\n\n\nIf you are using genderIdentity or raceombmulti, create indicator variables for each possible selection.\n\n\n\n\n2.4 Thinking about potential confounders and effect modifiers\nBefore we explore more of the data, I want us to take a second to think through potential confounders and effect modifiers from the covariates that we selected in Lab 2. For some of the covariates, we were asked to explain why we chose them. Now I want you to consider how each could alter the relationship between IAT score and your variable of interest (from your research question). For each covariate, explain how it might or might not change the relationship. For example, if our variable of interest is fat group identity, then we may consider that self-perception of size is a confounder since it could be linked with fat group identity and potentially be associated with IAT score.\nFor multi-level, unordered categorical covariates, you might consider if a specific category has an impact. For example, we might consider creating an indicator for white, non-Hispanic/Lantinx respondents since the history of fatphobia is tied with white-centered colonization and white supremacy (Redpath, 2023). Thus IAT scores might look different for White respondents vs. minority respondents (those who answered American Indian/Alaskan Natives; East Asian; South Asian; Native Hawaiian or other Pacific Islander; Black or African American; or Hispanic or Latino). Alternatively, we may not want to center our analysis on whiteness. The same fatphobic history involving white supremacy was particularly targetting Black people. So perhaps we want make an indicator for Black or African American respondents. Another option is leaving race as is - we may have enough data to handle the inclusion of all groups!\nThe purpose of this section is to make sure we are thinking about the relationships between variables in our analysis. I do not want us to make any decisions based solely on the data. I want any changes or manipulations in our variables to be motivated by research-backed evidence.\nFinally, for this project, we are most interested in the relationship we identified in our research question! Other variables are supporting this question, and improving that model fit so that we get as close to the true relationship in our research question as possible!\n\n\n\n\n\n\nTask\n\n\n\nFor each variable, consider how each could alter the relationship between IAT score and your variable of interest (from your research question). For each covariate, explain how it might or might not change the relationship.\n\n\n\n\n2.5 Continuing data exploration\nIn this section, we are going to further explore the variables that we might be adjusting for in our model (potential covariates outside the variable or interest in our research question).\n\n2.5.1 Bivariate exploration\nWe want to look at all other relationships between IAT score and each covariate (outside of the research question variable). Some of you have already made these plots in Lab 2, so you can simply refine them and display them here. There are a few questions that I want you to consider:\n\nFor categorical variables, is there an inherent order? Does the ordered values follow an approximately linear relationship? Are the categories “evenly spaced”? For example, education categories are not necessarily evenly spaced.\nAgain for categorical variables, is there a natural place to divide the categories up? For example, in education, it might be helpful to control for the fact that students in college might be asked to complete this test as an assignment. Thus, we might make an indicator for individuals in college vs. not. This decision can be informed by our plot, but it should not be driven by our plot!!\n\n\n\n\n\n\n\nTask\n\n\n\nFor each variable outside of your research question, create the appropriate plot to visualize the relationship between IAT score and the variable. Comment if there is an obvious trend or not.\n\n\n\n\n2.5.2 Multivariate exploration\nNow we want to extend our plots for Lab 2 where we looked at the outcome (IAT score) and our main variable of interest (as identified by our research question). Here, we will run the same plot, but include another variable. This will help us visualize potential confounders or effect modifiers. Note that if you made indicator variables (for race, gender identity or any other variable), then you should have a plot for each indicator variable.\nYou will need to really think about what kind of plot will best displays these relationships! IAT score is continuous, and many of your variable of interest is categorical. You may consider side-by-side boxplots where the color is the additional variable. You might also consider a jitter plot or only plotting the means. Remember you’re goal for plotting is to get a sense of the relationship only from the plot! Your audience should not have to work hard to understand what the plot is communicating. For example, I wanted to look at IAT score, internalization of societal standards, and race. I might make my plot like such:\n\n\nCode to contruct multivariate plot\nggplot(iat_prep2, aes(x = important_001, y=IAT_score, color = as.factor(raceomb_002_f))) +\n  # geom_jitter(size = 2, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 3, shape = 18) +\n  stat_summary(fun = mean, geom=\"line\") +\n  scale_x_discrete(limits = levels(iat_prep2$important_001_f)) +\n  labs(x = \"Importance of weight to sense of self \\n (Internalization of societal standards)\", \n       y = \"IAT score\",\n       title = \"Mean IAT scores for importance of weight to sense of self by race\", \n       color = \"Race\") +\n  theme(axis.text.x = element_text(angle = 45, size = 8, hjust = 1))\n\n\n\n\n\n\n\nCode to contruct multivariate plot\nggplot(iat_prep2, aes(x = important_001, y=IAT_score, color = as.factor(raceomb_002_f))) +\n  # geom_jitter(size = 2, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 3, shape = 18) +\n  stat_summary(fun = mean, geom=\"line\") +\n  scale_x_discrete(limits = levels(iat_prep2$important_001_f), labels = function(x) str_wrap(x, width=10)) +\n  labs(x = \"Importance of weight to sense of self \\n (Internalization of societal standards)\", \n       y = \"IAT score\",\n       title = \"Mean IAT scores for importance of weight to sense of self by race\", \n       color = \"Race\")\n\n\n\n\n\nNote that the above plot is specific for these variables!! Other variables may require a different type of visua\nlization!! Also note that I originally had geom_jitter() in my plot, which would make the plot really hard to understand!! Try uncommenting it to see what I mean by “hard to understand.” Also, think about why I connected the mean IAT scores across the different levels of internalization. I had a hard time connecting specific race’s points to identify a trend. Again, try commenting out stat_summary(fun = mean, geom=\"line\") to see what I mean.\n\n\n\n\n\n\nNote\n\n\n\nAn aside: You may see that collapsing groups might wash out differences. If we make an indicator for Black of African American respondents, as we mentioned above, then including White respondents with other minority groups may wash out their association with IAT score and wrongly lead us to a model that says identifying as Black or African American has no association with IAT, where we clearly see that Black or African American respondents have a unique trajectory for IAT scores.\n\n\nPlease make sure that you have made the needed changes to your plot in Lab 2. I noticed many unordered groups in plots where there should be an inherent order and unreadable axes because the text was not tilted. Please see discussion on Slack for what some students did to achieve these plots.\nHere are a few sources that might help you get started with the visualizations:\n\nIntro to R\nModern Data Visualization with R\n\n\n\n\n\n\n\nTask\n\n\n\nFor at least 3 variables outside of your research question, create the appropriate plot to visualize the relationship between IAT score, your main variable (in research question), and the variable outside your research question. Comment whether you can determine anything from the plot or not. If you can, is there any indication that the variable is a confounder or effect modifier?\n\n\n\n\n\n2.6 Fit a simple linear regression\nAs a starting point, it is good to fit a simple linear regression for our primary research question. This is often called the “crude” association. It just means that we are not adjusting for any other variables, and establishing the “starting point” for our analysis. It is likely that the results of the regression will change as we add other variables in the model.\n\n\n\n\n\n\nTask\n\n\n\nRun a simple linear regression model for the relationship in your primary research question. Print the regression table. Interpret the results and comment on the initial trend you see.\n\n\n\n\n\n\n\n\nBonus Task\n\n\n\nThis is not required in Lab 3. However, if you want to run a multiple linear regression model with one other variable that you plotted in Multivariate Exploration, then you should try it! Do the results align with your ideas in Section 2.4 and/or the visualization you saw in Section 2.5?"
  },
  {
    "objectID": "labs/Lab_03_work.html#bibliography",
    "href": "labs/Lab_03_work.html#bibliography",
    "title": "Lab 3 Instructions",
    "section": "3 Bibliography",
    "text": "3 Bibliography\nRedpath, F. (2023). Abolish the Body Mass Index: A Historical and Current Analysis of the Traumatizing Nature of the BMI. Tapestries:  Interwoven Voices of Local and Global Identities, 12(1). https://digitalcommons.macalester.edu/tapestries/vol12/iss1/12"
  },
  {
    "objectID": "labs/Lab_03.html",
    "href": "labs/Lab_03.html",
    "title": "Lab 3",
    "section": "",
    "text": "# PLEASE DO NOT REMOVE THIS CODE CHUNK!!!\n### ADD YOUR LIBRARIES HERE!!! ####\n\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(here)"
  },
  {
    "objectID": "labs/Lab_03.html#directions",
    "href": "labs/Lab_03.html#directions",
    "title": "Lab 3",
    "section": "1 Directions",
    "text": "1 Directions\nYou can download the .qmd file for this lab here.\nThis is your editing file. Please do not remove anything from this editing file!! You will only add your code and work to this file.\n\n1.1 Grading\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\n1.1.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning (like Section 2.4 and explanations in Section 2.5)"
  },
  {
    "objectID": "labs/Lab_03.html#lab-activities",
    "href": "labs/Lab_03.html#lab-activities",
    "title": "Lab 3",
    "section": "2 Lab activities",
    "text": "2 Lab activities\nBefore starting this lab, you should go back to Lab 2, save a new .rda file that contains all the new variables from that Lab. Then you can load it here!\n\n2.1 Restate your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\n\n\n2.2 Quality Control\n\n\n\n\n\n\nTasks\n\n\n\n\nFilter out observations with a value of -999 in the race variable.\nGlimpse at the observations that may indicate a respondent who has not properly completed the survey portion. This will require filtering for specific answer choices. Please see examples of filter() on it’s documentation page.\n\n\n\n\n\n2.3 Working with multi-selection variables\n\n\n\n\n\n\nTask\n\n\n\nIf you are using genderIdentity or raceombmulti, create indicator variables for each possible selection.\n\n\n\n\n2.4 Thinking about potential confounders and effect modifiers\n\n\n\n\n\n\nTask\n\n\n\nFor each variable, consider how each could alter the relationship between IAT score and your variable of interest (from your research question). For each covariate, explain how it might or might not change the relationship.\n\n\n\n\n2.5 Continuing data exploration\n\n2.5.1 Bivariate exploration\n\n\n\n\n\n\nTask\n\n\n\nFor each variable outside of your research question, create the appropriate plot to visualize the relationship between IAT score and the variable. Comment if there is an obvious trend or not.\n\n\n\n\n2.5.2 Multivariate exploration\n\n\n\n\n\n\nTask\n\n\n\nFor at least 3 variables outside of your research question, create the appropriate plot to visualize the relationship between IAT score, your main variable (in research question), and the variable outside your research question. Comment whether you can determine anything from the plot or not. If you can, is there any indication that the variable is a confounder or effect modifier?\n\n\n\n\n\n2.6 Fit a simple linear regression\n\n\n\n\n\n\nTask\n\n\n\nRun a simple linear regression model for the relationship in your primary research question. Print the regression table. Interpret the results and comment on the initial trend you see.\n\n\n\n\n\n\n\n\nBonus Task\n\n\n\nThis is not required in Lab 3. However, if you want to run a multiple linear regression model with one other variable that you plotted in Multivariate Exploration, then you should try it! Do the results align with your ideas in Section 2.4 and/or the visualization you saw in Section 2.5?"
  },
  {
    "objectID": "labs/Lab_02.html",
    "href": "labs/Lab_02.html",
    "title": "Lab 2",
    "section": "",
    "text": "IMPORTANT TO READ\n\n\n\n\nPlease do not delete the rubric from your .qmd file. I will use it to circle the grades!\nPlease delete everything that I wrote below except the headers and the task bubbles from the file you turn in! And include your answers and code. This just helps keep the lab clean when I’m grading it."
  },
  {
    "objectID": "labs/Lab_02.html#directions",
    "href": "labs/Lab_02.html#directions",
    "title": "Lab 2",
    "section": "Directions",
    "text": "Directions\nPlease turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here.\nThe rest of this lab’s instructions are embedded into the lab activities.\n\nPurpose\nThe main purpose of this lab is to introduce our dataset, codebook, and variables. We will continue to think about the context of our research question, but our main focus is to become familiar with the data.\n\n\nGrading\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\nRubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nSome tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning (like target population, choosing variables, revisiting research question)"
  },
  {
    "objectID": "labs/Lab_02.html#lab-activities",
    "href": "labs/Lab_02.html#lab-activities",
    "title": "Lab 2",
    "section": "Lab activities",
    "text": "Lab activities\n\n1. Access and download the data\nThis serves as good practice for accessing data that is online or needs to be downloaded from a collaborator.\nData can be accessed here. Under “Weight IAT 2004-2022” there are several drop down menus:\n\nI opened the first “Datasets & Codebooks,” then selected “OSF Storage (United States).” Once selected, the “Download as zip” option pops up in the top right part of the Files section.\n\nWe will be working with the Weight_IAT.public.2021.csv dataset. Please locate the zip file called Weight IAT.public.2021-CSV.zip . T0 download, you need to click the row of the zip file, but you can’t click the name of the zip file. If a link opens, then you clicked the name. If the row is highlighted blue and clickable “Download” and “View” buttons appear on the top right, then you selected it correctly! (See below image for what it should look like.)\n\nThen click the “Download” button to download! Note that the name does not have an underscore between “Weight” and “IAT.” I like to have my datasets named without spaces, so I will replace the space with an underscore.\nFor the codebook, perform the same process for the file named: Weight_IAT_public_2021_codebook.xlsx\nYou will need to unzip the actual data.\nMove the data to a folder that you can easily access as you work from this document. I like to have a folder named data to house my data.\n\n\n\n\n\n\nTask Summary\n\n\n\nDownload the 2021 data and codebook from the archives and store in accessible folder.\n\n\n\n\n2. Load data and needed packages\nFirst, load the packages that you will need in the remainder of this lab. You can add to this as you need to. At the top of your R code chunk, you can add the following option to repress the messages from the loading packages:\n\n\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(here)\nif(!require(lubridate)) { install.packages(\"lubridate\"); library(lubridate) }\n\nUsing R, load the data (csv file) into this document. Note that this is a csv file that we can load with basic R packages. Name your dataset something that feels intuitive to you and will distinguish it from other datasets that you work with.\nLoading the csv file every time you render will take a long time. One way to speed this up is by saving the data as an rda file (R data file). Change the following R code to save the rda file. You will also need to remove the #| eval: false at the top of the code chunk once you have corrected the code. If you are confused on the syntax, don’t forget that you can use ?save for more information.\n\nsave(&lt;whatever you called the read csv file&gt;, file = \"Where you would like to save the file with its name\")\n\nCheck that you have an rda file where you saved it. Now use load() with the file path to load the rda data here.\n\nload(file = \"Where you would like to save the file with its name\")\n\nAt this point, if you think you loaded the file correctly, add #| eval: false to the code chunk where you loaded the csv file and back to the chunk where you saved the rda file.\nTake a glimpse at the data to make sure you loaded it correctly.\nHow many rows and columns are in the dataset? Do you think we will need all these variables for our analysis?\n\n\n\n\n\n\nTask Summary\n\n\n\nRead csv, save as rda, load rda, glimpse at data.\nHow many rows and columns are in the dataset? Do you think we will need all these variables for our analysis?\n\n\n\n\n3. Data wrangling\nAs you go through this process, it is important that you look at the codebook for more information on each variable.\n\n3.1 What’s our target population?\nAs many of you mentioned in Lab 1, individuals taking the IAT test are not necessarily representative of the world population. I want you to articulate the target population that you think our analysis can give information about. To what population can we generalize our analysis results? We can get very specific with this population, but try to restrict your population to 3-5 characteristics.\nAfter you articulate the population, I want to add one more restriction to our population: US residency. The sample includes individuals residing in many different countries. Since we are discussing attitudes and beliefs that is inherently connected to society and culture, I think it is important that we restrict our analysis and discussion to a country that we have some social experience in. Thus, let’s restrict our data to the US only by filtering the variable countryres to category 1 (corresponding to the US).\n\n\n\n\n\n\nTask\n\n\n\nDescribe our target population. Keep your description to 3-5 characteristics, not including our restriction on the US population.\n\n\n\n\n3.2 Restrict your analysis to 1 outcome and 9 possible covariates/predictors\nWe are going to restrict our analysis to the single outcome, IAT score, which is named D_biep.Thin_Good_all. You can rename this variable.\nWe will also restrict our analysis to the following 9 potential variables so our work is a little more manageable.\n\n\n\n\n\n\nTask\n\n\n\nFrom the following 8 attitudes and beliefs, please select 3 that you think will be the most important variables related to your research question. In 1-2 lines, briefly explain why you chose each variable. This can be informal and bulleted.\n\n\n(Make sure you chose the variable that is part of your research question!)\n\nExplicit anti-fat bias (att7)\nSelf-perception of weight (iam_001)\nFat group identity (identfat_001 )\nThin group identity (identthen_001 )\nControllability of weight of others (controlother_001)\nControllability of weight of yourself (controlyou_001)\nAwareness of societal standards (mostpref_001 )\nInternalization of societal standards (important_001)\n\nWe will start our data exploration with the following 4 demographic variables:\n\nAge (we need to construct)\nRace (raceomb_002 or raceombmulti)\nEthnicity (ethnicityomb)\nSex assigned at birth (birthSex)\n\nPlease pick 2 additional variables to include in your analysis:\n\nEducation (edu)\nGender (genderIdentity)\nSelf-reported BMI (through self-reported height and weight)\nPolitical identity\nReligion\n\nI have chosen these variables for a mixture of reasons. For example, I have left out variables about residence and occupation because those variables have hundreds of categories that would be overwhelming in linear regression. For the 4 required demographic variables, I chose age because I really want us to get practice with a continuous variable. I chose race and ethnicity because of the intertwined history of racism and anti-fat bias in Western countries (including the U.S. where most participants reside).\n\n\n\n\n\n\nA note of the available variables on race\n\n\n\nThe dataset has two separate race variables. One has mutually exclusive categories (raceomb_002) and the other allows participants to make multiple selections (raceombmulti). The former (raceomb_002) allows one participant to identify with only one race category.\nImportant lesson from We All Count about using a multiple selection race question. We can try out all these options!\n\n\nFinally, I chose sex assigned at birth because adults in 2021 in the US were likely raised in a society where your sex assigned at birth impacted the gender stereotypes that you were raised in, which could impact exposure to diet culture. This in addition to the many medical conditions associated with one’s sex assigned at birth that may affect weight. The reason why I am leaving gender as an optional variable is because the question on gender allows participants to chose multiple options. The binary sex assigned at birth will make our analysis a little easier from a statistics stand point. Unfortunately, we need to balance achievable learning objectives and the most appropriate variable. Since I have required race as a variable and has a multi-level option, I do not want to overload our analysis with another multi-level variable. Sex assigned at birth will not create more work for you (that is outside of the course objectives) while capturing medical conditions and some of the societal impact of diet culture. This is certainly a limitation in our analysis that we should address in our discussion. I do encourage you to look into gender if the binary sex assigned at birth does not feel right for you. I am happy to help!\n\n\n\n\n\n\nA word on self-reported BMI\n\n\n\nThis variable is rooted in racism and anti-fat bias. The American Medical Association made a few press releases on policies using BMI as a measure, with alternative measures (frankly, just other measures of fatness to use as a diagnostic tool instead of checking true indicators of health). However, I can think of a couple examples where BMI might help us understand some context in this research, so I have left it as an option. Although still self-reported, it might be interesting to see how BMI (which is the closest measurement available in this dataset to an “objective” measure of fatness) is related to individuals’ attitudes and beliefs. I am not saying there is anything to the relationship, but it might be worth checking out if you are interested.\nI will also say, in this dataset, there are MANY issues constructing the variable for BMI from height and weight. If you do not feel strongly about including it, I would suggest you avoid the variable self-reported BMI. It is not worth bringing in a racist and anti-fat variable into the dataset if you do not have a specific use for it. If you do plan to use it, please come to me for help as early as possible!\n\n\nIf you would like to investigate a variable outside the list, please let me know by emailing or chatting with me.\n\n\n\n\n\n\nTask\n\n\n\nUsing R, select your identified variables from your dataset. Your new dataset should have 10 columns for the 10 variables.\n\n\n\n\n3.3 Manipulating variables that are coded as numeric variables\nMany variables in this dataset are coded as numeric values, but have specific categories linking up to the numbers. Using mutate() and cases() similar to our Data Management lesson, please create a new categorical variable with the specified categories from the codebook. Make sure that you create a variable with a new name! Since some of these variables are ordered categories, we will investigate if it’s appropriate to use the numeric or categorical version of the variable.\n\n\n\n\n\n\nExample of how I would create new variable for self-perception of weight (iam_001):\n\n\n\nBy looking at the codebook, I see that respondents answer the following question: “Currently, I am:”\n\n“Very underweight”\n“Moderately underweight”\n“Slightly underweight”\n“Neither underweight nor underweight”\n“Slightly overweight”\n“Moderately overweight”\n“Very overweight”\n\nIf I look at the data as is, I see that the variable is numeric.\n\niat_2021 %&gt;%\n  dplyr::select(iam_001) %&gt;%\n  tbl_summary()\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 465,8861\n    \n  \n  \n    iam_001\n\n        1\n2,023 (0.6%)\n        2\n7,902 (2.4%)\n        3\n24,399 (7.3%)\n        4\n148,081 (44%)\n        5\n88,566 (27%)\n        6\n43,090 (13%)\n        7\n18,978 (5.7%)\n        Unknown\n132,847\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\nAgain, I want to create a varaible with the answers instead of numbers, so I will change transform the variable to include the text:\n\niat_2021 = iat_2021 %&gt;%\n  mutate(iam_001_f = case_match(iam_001,\n                             7 ~ \"Very overweight\",\n                             6 ~ \"Moderately overweight\",\n                             5 ~ \"Slightly overweight\",\n                             4 ~ \"Neither underweight nor underweight\",\n                             3 ~ \"Slightly underweight\",\n                             2 ~ \"Moderately underweight\",\n                             1 ~ \"Very underweight\",\n                             .default = NA # to add NA if unknown\n                             ) %&gt;% factor())\niat_2021 %&gt;%\n  dplyr::select(iam_001_f) %&gt;%\n  tbl_summary()\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 465,8861\n    \n  \n  \n    iam_001_f\n\n        Moderately overweight\n43,090 (13%)\n        Moderately underweight\n7,902 (2.4%)\n        Neither underweight nor underweight\n148,081 (44%)\n        Slightly overweight\n88,566 (27%)\n        Slightly underweight\n24,399 (7.3%)\n        Very overweight\n18,978 (5.7%)\n        Very underweight\n2,023 (0.6%)\n        Unknown\n132,847\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\nggplot(data=iat_2021) +\n  geom_boxplot(aes(x = iam_001_f, y = IAT_score))\n\nWarning: Removed 129215 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nI have called the new variable iam_001_f to indicate that the variable is not in factor form. You can also call it something like iam_001_cat to indicate the categorical form.\n\n\n\n\n\n\n\n\nTask\n\n\n\nIdentify and list the variables that are coded numerically and correspond to categories. Create a new variable for the categorical/factor version of the variable. It is up to you to check that your code ran properly!! If you are using multi-choice categorical variables (might include race, gender), then do not convert the variable yet!\n\n\n\n\n3.4 Creating age from birth date and test date\nThis dataset does not have an available “age” variable. However, we have enough information to determine each individual’s age from the test date and their self-reported birth date. We can use the lubridate package to configure the age. First, we need to use make_date() to construct the birth date and test date. Below, I have implemented make_date() to make the birth date.\n\n\n\n\n\n\nTask\n\n\n\nFrom the codebook, find the variables that we can use to construct the test date. Then use make_date() to create the test date.\n\n\n\niat_2021 = iat_2021 %&gt;%\n  mutate(birthdate = make_date(month = birthmonth, year = birthyear), \n         testdate = make_date(month = month, year = year))\n\nOnce the two dates are created, we can use further use lubridate to calculate the age in years. This code is a little complicated, so here is an example of how I have created age:\n\niat_2021 = iat_2021 %&gt;%\n  mutate(age = interval(start = birthdate, end = testdate) %&gt;%\n          as.period() %&gt;% year()) %&gt;%\n  select(-birthmonth, -birthyear, -year, -month, \n         -testdate, -birthdate)\n\nNote that the name of my dataset is iat_2021 and I feed it into mutate(). Within mutate(), I assigned age to the interval between the name of my birth date (birthdate) and the name of my test date (testdate). I need to convert the interval to a period of time (as.period()), then to a measurement of years (year()).\n\n\n\n\n\n\nTask\n\n\n\nFollowing the above example, create an age variable that measures the years between individuals’ birth and test date. Then remove the variables used to make age.\n\n\n\n\n3.5 If you chose BMI, create the variable\nRaw data from weight and height are categorical. This is according to the codebook associated with this dataset. Please find your codebook file named Weight_IAT_public_2021_codebook.csv . You can find the value names for myweight_002 and myheight_002.\n\nFor example, in the weight variable,\n\nmost categories identify a lower limit to the weight in the group. One example group is weight is greater than or equal to 200 pounds and less than 205 pounds (labelled as “200 lb :: 91 kg”).\nthe first category for weight is “below 50lb:: 23kg” with 258 observations\nthe last category for weight is “above 440lb:: above 200kg” with 295 observations\n\nWhile the 5 groups of weight leading up the last category have 33, 28, 34, 20, and 89 observations, respectively.\n\n\n\nI will post an extra resource outlining some of my work on the BMI variable.\n\n\n3.6 Make a new dataset with only complete cases\nHandling missing data is outside the scope of our class. There are many techniques to handling missing data, but we will use complete case analysis. This means we will only use observations that have information for every variable we chose. The function drop_na() will give you the complete cases. You can feed your dataset into the function and assign it as a new dataframe.\nFor example:\n\nnew_df = old_df %&gt;% drop_na()\n\n\n\n\n4. Some exploratory data analysis\n\n4.1 Peek at your outcome\nThis serves as a check to make sure we are all looking at the correct outcome: IAT score.\n\n\n\n\n\n\nTask\n\n\n\nPlease plot a histogram of the IAT scores. What do you notice about the outcome?\n\n\n\n\n4.2 Univariate exploratory data analysis\n\n\n\n\n\n\nTask\n\n\n\nUsing ggplot or tables, visualize your variables. Get a sense of each variable’s distribution. Do you notice anything out of the ordinary?\n\n\n\n\n4.3 Bivariate exploratory data analysis\n\n\n\n\n\n\nTask\n\n\n\nTake a look at the scatterplot, violin, or box plot of IAT score and your variable of interest. Use R and ggplot to make this plot. If your variable of interest is categorical, then make sure to use a violin or boxplot.\n\n\n\n\n\n5. Revisit your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate the research question that you proposed in Lab 1. Please make sure it is only one question, one sentence long. What are your thoughts on the research question now that we looked at the data? Feel free to change it now that we’ve looked at the data. If you change your question, make sure 4.2 reflects the new research question.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn research, we typically do NOT change our research question after looking at the data! Researchers typically form their questions from other research and their expertise. We may not have expertise in this field and we have not been studying implicit bias, so I want to be a little more flexible with our analysis."
  },
  {
    "objectID": "labs/Lab_01.html",
    "href": "labs/Lab_01.html",
    "title": "Lab 1",
    "section": "",
    "text": "Caution\n\n\n\nThis lab is not ready to be worked on!! Nicky needs to update it."
  },
  {
    "objectID": "labs/Lab_01.html#directions",
    "href": "labs/Lab_01.html#directions",
    "title": "Lab 1",
    "section": "Directions",
    "text": "Directions\nPlease turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy. We can work out another way for you to turn in the labs.\nYou can download the .qmd file for this lab here.\n\nPurpose\nThis lab will serve as an introduction to our quarter long project.\nThere will be no analysis in this lab. Instead, we are building our knowledge around the research question.\n\n\nGrading\nEach lab will have a slightly different grading rubric. Since this lab does not include coding nor analysis, this portion of the rubric is excluded."
  },
  {
    "objectID": "labs/Lab_01.html#lab-activities",
    "href": "labs/Lab_01.html#lab-activities",
    "title": "Lab 1",
    "section": "Lab activities",
    "text": "Lab activities\n\n1. Reading and listening activities\n\n1.1 Article: Implicit and explicit anti-fat bias: The role of weight-related attitudes and beliefs\nThis article will serve as a reference point for our project. The article is meant to introduce social scientists’ approaches to research and analyses. However, the article is not meant to be a basis for which we perform our analysis.\n\n\n\n\n\n\nWarning\n\n\n\nThis article discusses anti-fat bias. It uses words that may be triggering to larger-bodied people.\n\n\nPlease read sections 1 - 2, through 2.2 (“Procedures and measures”). Answer the following questions:\n\nIn your own words, what is anti-fat bias?\nWhat were the three social theoretical models that the paper discusses? Which do you personally think is the biggest contributor to anti-fat bias and why?\nFrom the following measures in section 2.2, select two and discuss why the named measure may or may not accurately represent the italicized statement taken from the IAT questionnaire. Feel free to answer this question after taking the IAT yourself.\n\nSelf-perception of weight\nThin/fat group identity\nControllability of weight\nAwareness of societal standards\nInternalization of societal standards\n\nFor example, for Self-perception of weight, the italicized statement is the following statement outlined in red:\n\n\n\n1.2 Podcast: Anti-Fat Bias by Maintenance Phase\n\n\n\n\n\n\nWarning\n\n\n\nThis podcast shares the experience of one of its hosts that involves anti-fat bias. This may be triggering if you have experienced this type of bias.\n\n\nThis is an optional listening for this lab, but I highly encourage you listen at some point this quarter. This is a really good way to see how research can be integrated into conversation and experience.\nIf you decide to listen, feel free to share a quote that most impacted you.\n\n\n\n2. Familiarizing ourselves with the Implicit Association Test (IAT)\n\n2.1 Learn more about the test\nVisit the Project Implicit site, and read about the test. What is your initial reaction to the test? What questions about the test do you have? Do you have any questions about the test’s validity? The point here is not to attempt to discredit the test itself, but see what specific questions the test can help us answer and what is outside the scope of our analysis. For example, are there any potential issues with the fact that people are self-selected to take the test? Does that mean our sample is representative of our population? Is it an issue that someone can take the test more than once?\nThis exercise will serve as a good starting point for the discussion section of our project report. The more effort you put in here and now, the more prepared you will be for the report.\n\n\n2.2 Take the test\nYou will spend 15 minutes taking the IAT. You can go to the Project Implicit website, register, and select a specific test to take. Once registered, you can click “Take a Test,” read the Preliminary Information, and then click “I wish to proceed” at the bottom. Then you can click the button “Weight IAT” to take this particular test.\nI will not check that you have completed this test, but it will help you understand the data you are analyzing.\n\n\n\n3. Get a sense of how you would like to analyze the data\nFor our project, we will examine the association betwen the IAT score and one other variable. From the above article, and the introduced variables in section 2.2, which association are you most interested in analyzing? Please write this in the form of a research question.\nWe will have a chance to adjust our research question once we have explored the data in Lab 2.\n\n\n4. Compile above work into an introduction\nAt this point, you have done a lot of the work needed to write an introduction for your report. Write a brief description of anti-fat bias, IAT, your research question, and the context for the question. This description should be in complete sentences and written as a single paragraph.\nIn the next lab, we will work on a summary of the dataset (e.g. where are the data from, when were they collected, how many subjects, what are the variables, what are the exposure and outcomes variables of interest, etc.)."
  },
  {
    "objectID": "labs/Lab_04_instructions.html",
    "href": "labs/Lab_04_instructions.html",
    "title": "Lab 4 Instructions",
    "section": "",
    "text": "Caution\n\n\n\nRead to go ! (3/8/2024)"
  },
  {
    "objectID": "labs/Lab_04_instructions.html#directions",
    "href": "labs/Lab_04_instructions.html#directions",
    "title": "Lab 4 Instructions",
    "section": "1 Directions",
    "text": "1 Directions\nPlease turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here.\n\n\n\n\n\n\nCaution\n\n\n\nThis is the instructions file. The link above will take you to the editing file where you can add your work and turn it in!! Please do not remove anything from the editing file!!\n\n\n\n1.1 Purpose\nThe main purpose of this lab is to perform model selection, identify one or more potential final models, and start our interpretation of our main relationship.\n\n\n1.2 Grading\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades."
  },
  {
    "objectID": "labs/Lab_04_instructions.html#lab-activities",
    "href": "labs/Lab_04_instructions.html#lab-activities",
    "title": "Lab 4 Instructions",
    "section": "2 Lab activities",
    "text": "2 Lab activities\nBefore starting this lab, you should go back to Lab 2, save a new .rda file that contains all the new variables from that Lab. Then you can load it here!\n\n2.1 Restate your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\nHow is implicit anti-fat bias, as measured by the IAT score, associated with “insert main independent variable here”?\n\n\n2.2 Step 1: Simple linear regressions / analysis\nWe have done most of this step through visualizations in Lab 2 and 3. Now, we will quickly run a simple linear regression model for each covariate against the IAT score (outcome). Remember, the goal of this is to see if each covariate explains enough variation of the outcome, IAT score. You should have at least 9 simple linear regression models and their results. Results include the F-statistic and p-value from the test if each covariate explains enough variation of the outcome. Please revisit the slides from Lesson 5 (SLR: More inference + Evaluation) for more help with this test.\n\n\n\n\n\n\nVERY IMPORTANT FOR VARIABLES WE ORDERED USING FACTOR!!\n\n\n\nI asked that you order variables to make plots more interpretable. However, for the lm(), R reads the ordered variables in an unexpected way. For these variables to run correctly in R, we need to unorder the variables. We can also set a reference level that makes sense.\nFor example, I may want to unorder my variable iam_001 and set the reference to Neither underweight nor overweight. I can do this with:\n\niat_2021_new = iat_2021_old %&gt;% \n  mutate(iam_unordered = factor( iam_ordered, ordered = FALSE ) %&gt;% \n           relevel( ref = \"Neither underweight nor overweight\"))\n\n\n\nRecall, we mentioned 3 options to running and outputting the results of\n\nWe can run lm() for each covariate in separate lines of code, and use something like summary() or anova() to look at the results of each. (More time consuming to write, but less complicated coding)\nWe can use lapply() to run lm() and display the anova() on each covariate in one line of code. (Less time consuming to write, but more complicated coding, and more prone to errors that may not be apparent from output)\nWe can use sapply() to run lm(), anova(), and display the p-value for each covariate in one line of code. (Less time consuming to write, but more complicated coding, more prone to errors that may not be apparent from output, and no sense of what’s going on in the regression)\n\nPlease take a note for yourself if your dataset contains the original numeric versions of variables that we created factors for. I am not saying that you should take them out. They might be useful if our sample is not big enough to handle all the categorical covariates that we’ve included, but I think our sample is large enough.\n\n\n\n\n\n\nTasks\n\n\n\n\nRun a simple linear regression model for each covariate against the IAT score (outcome).\nDisplay results from the test if each covariate explains enough variation of the outcome. This may be from three options in the instructions: summary()/anova() only, lapply(), or sapply()\n\nInterpretation of the results will be in the next step.\n\n\n\n\n2.3 Step 2: Preliminary variable selection\nUsing the previous p-values from the F-test on each covariate’s SLR, decide which covariates will be included in the initial model. Recall the decision rule: we keep covariates that explain enough variation using p-value &lt; 0.25. Note that because our sample size is so large, the p-values might be really small. For now, that’s okay, but this means we may want to alter our Step 3 a little bit.\nOnce you have decided on the covariates, run the model and display the regression table.\n\n\n\n\n\n\nTasks\n\n\n\n\nDecide which covariates will be included in the initial model and list them.\nRun the initial model and display the regression table.\n\nNo need to write out the model, but you may in addition to the list.\n\n\n\n\n2.4 Step 3: Assess change in coefficient\nNow that all the selected variables are in one initial model, we can start considering the effect of each variable (outside of our main research question).\nRemember our general rule: We can remove a variable if (1) p-value &gt; 0.05 for the F-test to include or exclude the variable and (2) change in coefficient (\\(\\Delta\\%\\)) of our explanatory variable is &lt; 10%. Please remember that the p-values for the F-test for a multi-level categorical variable must be calculated by creating a reduced and full model.\nIt might be helpful to copy your list of covariates here and make note of the ones that you are removing. It was hard for me to keep track of all the variables when our dataset contains sooo many categorical covariates, and the regression table is so long.\nSince our sample size is quite large, most (if not all) of the F-tests will conclude that the variable should be kept in the model. At this point, I advise that you turn to some common sense and the change in coefficients.\n\nFor common sense, you may notice that some of your covariates are essentially measuring the same thing. If there is clinical relevance to having both in the model, then keep them in, but if not, you will have to decide which is more interpretable/relevant/aligned with your research question. For example, if you chose variables involving attitudes and beliefs that are measuring similar things, then you might exclude one. There are measurements like “I am …” with relative weight groups and “Compared to most…” with relative weight groups. These two might capture a lot of the same information, so we may chose one. (Additionally, this might create issues with multicollinearity, which we will discuss on the last day, so just keep that in mind!) Another example is if you used gender identity, this might be a good time to throw out sex assigned at birth. Remember, my reasoning for using SAB was that (1) lab work has been extensive and I wanted to give you an option to avoid multi-selection variables, and (2) it might capture some of the differences around fat attitudes tied with gender. If you included gender identity in your work, then sex assigned at birth could be superfluous.\nFor change in coefficients, focus on the variable of your research question. Does the removal of variables change the coefficients for your explanatory variable? Remember what we discussed with change in coefficients when our explanatory variable is a multi-level categorical variable (Lesson 11.2 Interactions continued slides 26-28). You may find these changes small, which tracks with a lot of our plots in Lab 3. Nothing seemed to have such a big effect on IAT score, and as a consequence it’s hard to see big changes for a potential confounder.\n\nNote that I put common sense first. The change in coefficients may not be very large, and may lead you to think we don’t need a lot of the variables in our model. However, I would let common sense override the change in coefficients if your reasoning is well justified.\npsst… There might be some code in Step 4 that might help you get started in this step.\n\n\n\n\n\n\nTasks\n\n\n\nRemove variables from the initial model based on your common sense, change in coefficient, and/or p-values of the F-tests.\nYou do NOT need to show all your work here. You just need to include:\n\nA brief explanation of what variables were dropped and why (a sentence per variable), and\nAn example of your process with one variable is enough (including code that you ran)\n\n\n\n\n\n2.5 Step 4: Assess scale for continuous variables\nThere is one variable in our model (unless you removed it) that is continuous: age. We need to assess the scale for age. In this step we will have ZERO delivarables. To save you time, I will walk you through my thought process, and why I determined age is fine as is. If you still want to try something else out with age, then you can!\nFirst, we can start with a scatterplot of IAT score and age. Your plot may look a little different than mine.\n\nggplot(data = iat, aes(x = age, y = IAT_score)) +\n  geom_point(size = 0.8) + geom_smooth() + xlim(0, 111)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nIn the above scatterplot, it looks like the relationship is mostly linear (and increasing) until we get to approximately 90 years old. At that point IAT score decreases with age. Let’s say we 100% believe there is suddenly more responders around 110 years old than 90-100 year olds. I’m already skeptical of this since we did a quality control in Lab 3. We’ll play it out because it’s not worth making judgement calls on what we consider “admissable” data.\nWe could do quantiles, splines, or polynomials, but those approaches will either make more categorical variables or make the relationship between age and IAT score harder to interpret. We have a pretty linear relationship up until the higher ages!\nI wanted to investigate the linearity a little more so I created an indicator for individuals who are 100 years or older:\n\niat1 = iat %&gt;% mutate(ind_age_100 = ifelse(age &gt; 100, \"TRUE\", \"FALSE\"))\n\nNow I can see if the linearity differs between the two groups of ages:\n\nggplot(iat1, aes(x = age, y = IAT_score, color = ind_age_100)) +\n  geom_point(size = 0.8) + geom_smooth(method = \"lm\")\n\n\n\n\nI am happy to see that both groups’ IAT score are increasing with age. It actually looks like my indicator might be a confounder… In that case, we only need to include the indicator in the model so that the relationship between age and IAT is adjusted for the indicator. I can test to see if the indicator is a big enough confoudner using the change in coefficient of age and my explanatory variable.\nHere’s the model without the indicator:\n\nprelim_model = lm(IAT_score ~ iam_unordered + identfat + comptomost + \n                  ind_m + ind_f + ind_tmm + ind_twf + ind_gqnc + \n                    ind_other +\n                  race + \n                  ethn +\n                  edu_14_f +\n                  age, data = iat1)\n\nAnd we’ll take a look at the coefficients for the model:\n\nprelim_model$coefficients[c(2:6, 46)] # by using c(2:6, 46) I am telling R to \n\n      iam_unorderedVery underweight iam_unorderedModerately underweight \n                       -0.061151601                        -0.015513320 \n  iam_unorderedSlightly underweight    iam_unorderedSlightly overweight \n                        0.006954580                        -0.023369363 \n iam_unorderedModerately overweight                                 age \n                       -0.056237038                         0.003857134 \n\n                                      # only print certain variables' coefficients\n\nThen we can run the model with the indicator, then look at the coefficients:\n\nprelim_model2 = lm(IAT_score ~ iam_unordered + identfat + comptomost + \n                  ind_m + ind_f + ind_tmm + ind_twf + ind_gqnc + ind_other +\n                  race + \n                  ethn +\n                  edu_14_f +\n                  age + ind_age_100, \n                 data = iat1)\nprelim_model2$coefficients[c(2:6, 46)] # by using c(2:6, 46) I am telling R to \n\n      iam_unorderedVery underweight iam_unorderedModerately underweight \n                       -0.060385515                        -0.015352043 \n  iam_unorderedSlightly underweight    iam_unorderedSlightly overweight \n                        0.006817642                        -0.023677757 \n iam_unorderedModerately overweight                                 age \n                       -0.056762301                         0.003918822 \n\n                                       # only print certain variables' coefficients\n\nWe can check the % change in the coefficients between the models.\nRecall, \\[\n\\Delta\\% = 100\\% \\cdot \\frac{\\widehat\\beta_{FLR, full} - \\widehat\\beta_{FLR, red}}{\\widehat\\beta_{FLR, full}}\n\\] Here’s how I quickly do it with the coefficients:\n\n100 * ( prelim_model2$coefficients[c(2:6, 46)] - prelim_model$coefficients[c(2:6, 46)] ) /\n  prelim_model2$coefficients[c(2:6, 46)]\n\n      iam_unorderedVery underweight iam_unorderedModerately underweight \n                         -1.2686585                          -1.0505274 \n  iam_unorderedSlightly underweight    iam_unorderedSlightly overweight \n                         -2.0085770                           1.3024639 \n iam_unorderedModerately overweight                                 age \n                          0.9253735                           1.5741387 \n\n\nBased on %’s above, it doesn’t look like the indicator makes much of a difference in my model. It is likely because there are only 29 individuals over the age of 100 and 201,031 individuals under the age of 100 (In my dataset). Those 29 individuals will not have a big impact on the linear relationship between age and IAT, even though the first smoothed scatterplot made it look like it does.\nTo bring this point home, I can plot age and IAT with and without the individuals that are 100 years or older. Let me know if you find a better way to overlay these plots! (I have been a little stressed on time, and couldn’t find a quick answer.)\n\nggplot(iat1, aes(x = age, y = IAT_score)) +\n  geom_point() + geom_smooth(method = \"lm\") + xlim(0, 111) +\n  labs(title = \"With individuals 100 years or older\")\n\n\n\nggplot(iat1 %&gt;% filter(age &lt; 100), aes(x = age, y = IAT_score)) +\n  geom_point() + geom_smooth(method = \"lm\") + xlim(0, 111) +\n  labs(title = \"Without individuals 100 years or older\")\n\n\n\n\nI see no difference. Thus, I think it’s okay to leave age as is!!\n\n\n\n\n\n\nTasks\n\n\n\nNo tasks here! If you want to try out what I did above, you can!\n\n\n\n\n2.6 Step 5: Check for interactions\nNow we’re going to check if there are any interactions. I will walk you through a streamlined way to check for interactions between your explanatory variable and all the other variables in the model.\nFirst, I want you to revisit your work in Lab 3. Remind yourself of the variables that you identified as possible effect modifiers.\nAs you check for interactions, don’t forget to make your decisions based on your discussion/hypotheses in Lab 3. Always prioritize investigation of interactions that are justified clinically before investigating interactions only based on statistical significance.\n\n1vars = names(model.frame(prelim_model))[-1]\n\n.env &lt;- environment()\n2interactions &lt;- combn(vars, 2, function(x) paste(x, collapse=\" * \")) %&gt;%\n3    grep(., pattern = \"iam_unordered\", value = T)\n\n\n1\n\nCreate a vector of the variable names that are in your preliminary model. Note I use [-1] to remove IAT_score from my list. Please make sure to change prelim_model to the name of your model at this point.\n\n2\n\nHere we are just combining all our covariates into interactions that R can understand. This makes it so we don’t have to write it all ourselves.\n\n3\n\nMake sure to change the pattern = \"iam_unordered\" to be pattern = to your explanatory variable.\n\n\n\n\nNow that we’ve created the set up for all the possible interactions, we can run them through the lm() function and see the summary of the models. In the following code I use the lappy() function to fit an individual model for the main effects + each interaction listed in interactions.\n\n\n\n\n\n\nNote\n\n\n\nPlease note that this code takes a while to run. Once you run it and take note of the results, you can comment out or add #| eval: false to prevent it from running every time you render. You don’t need to show the results for this in your submitted work, but I want to see the code, and read about your decisions about from results.\n\n\n\nsummary = lapply(interactions,\n             function(int) summary(lm(reformulate(c(vars, int), \"IAT_score\", env=.env),\n                                      data = iat)))\nsummary\n\nYou can alse go straight to using the anova() function to compare the preliminary model.\n\nanova_res = lapply(interactions,\n             function(int) anova(lm(reformulate(c(vars, int), \"IAT_score\", env=.env),\n                                      data = iat),\n1                                 prelim_model))\nanova_res[1]\ng = anova_res[[1]]\ng$F\n\n\n1\n\nYou will to change this name for the preliminary model if you called it something different.\n\n\n\n\n[[1]]\nAnalysis of Variance Table\n\nModel 1: IAT_score ~ iam_unordered + identfat + comptomost + ind_m + ind_f + \n    ind_tmm + ind_twf + ind_gqnc + ind_other + race + ethn + \n    edu_14_f + age + iam_unordered * identfat\nModel 2: IAT_score ~ iam_unordered + identfat + comptomost + ind_m + ind_f + \n    ind_tmm + ind_twf + ind_gqnc + ind_other + race + ethn + \n    edu_14_f + age\n  Res.Df   RSS  Df Sum of Sq     F    Pr(&gt;F)    \n1 200990 31337                                  \n2 201014 31346 -24   -9.4223 2.518 5.558e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n[1]       NA 2.518048\n\n\n\n\n\n\n\n\nTasks\n\n\n\nUsing your discussion in Lab 3 and the results from the F-test on interactions:\n\nCreate a list of the interactions that you will include in your model.\nRun the preliminary final model that includes the main effects and interactions.\n\n\n\n\n\n2.7 Step 6: Assess model fit\nAt this point we may want to compare different models. While Steps 1-5 have been directing us towards a single model, you may have been interested in other models along the way. Maybe there were some interactions that you thought were interesting, but didn’t think of before. Maybe you would like to combine different groups for categorical variables.\nIf you are completely happy with your model, then you don’t have to do this step.\nYou might create a table like such:\n\nsum = summary(prelim_model)\nmodel_fit_stats = data.frame(Model = \"Preliminary main effects model\", Adjusted_R_sq = sum$adj.r.squared, AIC = AIC(prelim_model), BIC = BIC(prelim_model))\n\nmodel_fit_stats\n\n                           Model Adjusted_R_sq      AIC      BIC\n1 Preliminary main effects model    0.04511326 197006.6 197486.5\n\n\n\n\n\n\n\n\nTasks\n\n\n\nOptional: Create a table that displays some fo the model fit statistics to compare preliminary final models.\n\n\n\n\n2.8 Create a forest plot of your coefficient estimates\nIt’s often helpful to have a visualization of coefficient estimates. Forest plots are a nice way to show all the values together. Below I have started a forest plot using my prelim_model. You can make the plot with your final model.\nI used the plot_model() function to make the plot, and here’s a site that discusses some of it’s capabilities. The below plot is just a starting point!! You’ll need to clean up the variables, title, etc.\nYou may use another function to make the plots. I chose this one since it can handle the model as input.\n\nplot_model(prelim_model, show.values = TRUE, value.offset = 0.5) + ylim(-0.25, 0.25)\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\nHere are some other packages for forest plots:\n\nhttps://cran.r-project.org/web/packages/forestploter/vignettes/forestploter-intro.html\nhttps://larmarange.github.io/ggstats/articles/ggcoef_model.html\n\n\n\n\n\n\n\nTasks\n\n\n\nCreate a forest plot to visualize the coefficient estimates."
  },
  {
    "objectID": "labs/Lab_04.html",
    "href": "labs/Lab_04.html",
    "title": "Lab 4",
    "section": "",
    "text": "Please turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here.\n\n\n\n\n\n\nCaution\n\n\n\nThis is the instructions file. The link above will take you to the editing file where you can add your work and turn it in!! Please do not remove anything from the editing file!!\n\n\n\n\nThe main purpose of this lab is to perform model selection, identify one or more potential final models, and start our interpretation of our main relationship.\n\n\n\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning"
  },
  {
    "objectID": "labs/Lab_04.html#directions",
    "href": "labs/Lab_04.html#directions",
    "title": "Lab 4",
    "section": "",
    "text": "Please turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here.\n\n\n\n\n\n\nCaution\n\n\n\nThis is the instructions file. The link above will take you to the editing file where you can add your work and turn it in!! Please do not remove anything from the editing file!!\n\n\n\n\nThe main purpose of this lab is to perform model selection, identify one or more potential final models, and start our interpretation of our main relationship.\n\n\n\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning"
  },
  {
    "objectID": "labs/Lab_04.html#lab-activities",
    "href": "labs/Lab_04.html#lab-activities",
    "title": "Lab 4",
    "section": "2 Lab activities",
    "text": "2 Lab activities\nBefore starting this lab, you should go back to Lab 2, save a new .rda file that contains all the new variables from that Lab. Then you can load it here!\n\n2.1 Restate your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\n\n\n2.2 Step 1: Simple linear regressions / analysis\n\n\n\n\n\n\nTasks\n\n\n\n\nRun a simple linear regression model for each covariate against the IAT score (outcome).\nDisplay results from the test if each covariate explains enough variation of the outcome. This may be from three options in the instructions: summary()/anova() only, lapply(), or sapply()\n\nInterpretation of the results will be in the next step.\n\n\n\n\n2.3 Step 2: Preliminary variable selection\n\n\n\n\n\n\nTasks\n\n\n\n\nDecide which covariates will be included in the initial model and list them.\nRun the initial model and display the regression table.\n\nNo need to write out the model, but you may in addition to the list.\n\n\n\n\n2.4 Step 3: Assess change in coefficient\n\n\n\n\n\n\nTasks\n\n\n\nRemove variables from the initial model based on your common sense, change in coefficient, and/or p-values of the F-tests.\nYou do NOT need to show all your work here. You just need to include:\n\nA brief explanation of what variables were dropped and why (a sentence per variable), and\nAn example of your process with one variable is enough (including code that you ran)\n\n\n\n\n\n2.5 Step 4: Assess scale for continuous variables\n\n\n\n\n\n\nTasks\n\n\n\nNo tasks here! If you want to try out what I did above, you can!\n\n\n\n\n2.6 Step 5: Check for interactions\n\n\n\n\n\n\nTasks\n\n\n\nUsing your discussion in Lab 3 and the results from the F-test on interactions:\n\nCreate a list of the interactions that you will include in your model.\nRun the preliminary final model that includes the main effects and interactions.\n\n\n\n\n\n2.7 Step 6: Assess model fit\n\n\n\n\n\n\nTasks\n\n\n\nOptional: Create a table that displays some fo the model fit statistics to compare preliminary final models.\n\n\n\n\n2.8 Create a forest plot of your coefficient estimates\n\n\n\n\n\n\nTasks\n\n\n\nCreate a forest plot to visualize the coefficient estimates."
  },
  {
    "objectID": "labs/BMI_help.html",
    "href": "labs/BMI_help.html",
    "title": "BMI Variable Help",
    "section": "",
    "text": "Link to github page for qmd file\n\nLoading the needed packages:\n\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(here)\nif(!require(lubridate)) { install.packages(\"lubridate\"); library(lubridate) }\n\n\n\nLoading my IAT dataset (as it’s Rda file):\n\nload(file = here(\"../TA_files/Project/data/IAT_data.rda\"))\n\n\n\nSelecting the variables that I want to look at:\n\niat_prep = iat_2021_raw %&gt;% \n  select(IAT_score = D_biep.Thin_Good_all, \n         att7, iam_001, identfat_001, \n         myweight_002, myheight_002,\n         identthin_001, controlother_001, \n         controlyou_001, mostpref_001,\n         important_001, \n         birthmonth, birthyear, month, year, \n         raceomb_002, raceombmulti, ethnicityomb, \n         edu, edu_14, \n         genderIdentity, \n         birthSex)\n\n\n\nSelf-reported BMI\nI started investigating the BMI because I was curious how the paper [@elran-barak2018] used it and just wanted to check reproducibility. There are a few issues with the self-reported BMI that immediately stuck out:\n\nComponents of BMI (weight and height) were self-reported\n\nPeople told they are underweight often add pounds (REFERENCE)\nPeople told they are overweight often subtract pounds (REFERENCE)\n\nRaw data from weight and height are categorical. This is according to the codebook associated with this dataset. Please find your codebook file named Weight_IAT_public_2021_codebook.csv . You can find the value names for myweight_002 and myheight_002.\nFor example, in the weight variable,\n\nmost categories identify a lower limit to the weight in the group. One example group is weight is greater than or equal to 200 pounds and less than 205 pounds (labelled as “200 lb :: 91 kg”).\nthe first category for weight is “below 50lb:: 23kg” with 258 observations\nthe last category for weight is “above 440lb:: above 200kg” with 295 observations\n\nWhile the 5 groups of weight leading up the last category have 33, 28, 34, 20, and 89 observations, respectively.\n\nMy intention here is not the question anyone’s weight, but keep in mind that surveys sometimes have people selecting the first or last option because they are not taking the survey seriously\n\n\n\nMy exact steps\n\nI wanted to get a table of the counts within each weight group. I used the gt package to make a table of what I thought was a categorical variable. It looks like R interprets the numbered categories as numbers.\n\niat_prep %&gt;%\n  dplyr::select(myweight_002) %&gt;%\n  tbl_summary()\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 465,8861\n    \n  \n  \n    myweight_002\n23 (18, 29)\n        Unknown\n141,326\n  \n\n  \n    \n      1 Median (Q1, Q3)\n    \n  \n\n\n\n\nI will first check the class of the variable to make sure R is doing what I think it’s doing.\n\nclass(iat_prep$myweight_002)\n\n[1] \"integer\"\n\n\nSo R is interpreting the values as integers. I will need to make them categories to view them through gt commands.\nLet’s make it a category:\n\niat_prep2 = iat_prep %&gt;% \n  mutate(myweight = as.factor(myweight_002))\n\nNow we make the table:\n\niat_prep2 %&gt;%\n  dplyr::select(myweight) %&gt;%\n  tbl_summary()\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 465,8861\n    \n  \n  \n    myweight\n\n        1\n258 (&lt;0.1%)\n        2\n257 (&lt;0.1%)\n        3\n329 (0.1%)\n        4\n363 (0.1%)\n        5\n379 (0.1%)\n        6\n329 (0.1%)\n        7\n327 (0.1%)\n        8\n360 (0.1%)\n        9\n589 (0.2%)\n        10\n1,002 (0.3%)\n        11\n2,180 (0.7%)\n        12\n3,766 (1.2%)\n        13\n6,175 (1.9%)\n        14\n9,038 (2.8%)\n        15\n12,068 (3.7%)\n        16\n15,598 (4.8%)\n        17\n16,007 (4.9%)\n        18\n17,518 (5.4%)\n        19\n19,093 (5.9%)\n        20\n17,794 (5.5%)\n        21\n15,599 (4.8%)\n        22\n16,636 (5.1%)\n        23\n14,854 (4.6%)\n        24\n14,643 (4.5%)\n        25\n13,510 (4.2%)\n        26\n12,778 (3.9%)\n        27\n12,243 (3.8%)\n        28\n11,498 (3.5%)\n        29\n9,414 (2.9%)\n        30\n9,099 (2.8%)\n        31\n7,274 (2.2%)\n        32\n8,775 (2.7%)\n        33\n4,691 (1.4%)\n        34\n5,411 (1.7%)\n        35\n4,595 (1.4%)\n        36\n5,659 (1.7%)\n        37\n3,494 (1.1%)\n        38\n3,938 (1.2%)\n        39\n2,489 (0.8%)\n        40\n2,932 (0.9%)\n        41\n1,941 (0.6%)\n        42\n3,197 (1.0%)\n        43\n1,244 (0.4%)\n        44\n1,794 (0.6%)\n        45\n1,442 (0.4%)\n        46\n1,322 (0.4%)\n        47\n1,251 (0.4%)\n        48\n1,238 (0.4%)\n        49\n900 (0.3%)\n        50\n800 (0.2%)\n        51\n651 (0.2%)\n        52\n1,152 (0.4%)\n        53\n347 (0.1%)\n        54\n436 (0.1%)\n        55\n346 (0.1%)\n        56\n409 (0.1%)\n        57\n295 (&lt;0.1%)\n        58\n384 (0.1%)\n        59\n165 (&lt;0.1%)\n        60\n202 (&lt;0.1%)\n        61\n126 (&lt;0.1%)\n        62\n342 (0.1%)\n        63\n92 (&lt;0.1%)\n        64\n154 (&lt;0.1%)\n        65\n129 (&lt;0.1%)\n        66\n113 (&lt;0.1%)\n        67\n139 (&lt;0.1%)\n        68\n85 (&lt;0.1%)\n        69\n85 (&lt;0.1%)\n        70\n55 (&lt;0.1%)\n        71\n65 (&lt;0.1%)\n        72\n120 (&lt;0.1%)\n        73\n26 (&lt;0.1%)\n        74\n26 (&lt;0.1%)\n        75\n26 (&lt;0.1%)\n        76\n33 (&lt;0.1%)\n        77\n28 (&lt;0.1%)\n        78\n34 (&lt;0.1%)\n        79\n20 (&lt;0.1%)\n        80\n89 (&lt;0.1%)\n        81\n295 (&lt;0.1%)\n        Unknown\n141,326\n  \n\n  \n    \n      1 n (%)\n    \n  \n\n\n\n\nThe table is really long, so a histogram would work much better to visualize how many observations are in each category:\n\nggplot(data = iat_prep, aes(x = myweight_002)) + \n  geom_histogram() +\n  geom_vline(aes(xintercept = mean(iat_prep$myweight_002, \n                                   na.rm = T)), \n             color = \"red\", linewidth = 2)\n\nWarning: Use of `iat_prep$myweight_002` is discouraged.\nℹ Use `myweight_002` instead.\n\n\nWarning: Removed 141326 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\nWe need to convert the heights and weights to their cm and kg respectively. Since I only have a number category, I’ve gone into the codebook to find what each numbered category represents. If you put 8, you are 43 inches tall; 16:51 in; and 32:67in. Now I can use a line to see if I can create an equation to convert these values.\n\\[\n\\begin{align}\nin & = m\\times cat+b \\\\\n43 &= m \\times 8 + b \\\\\nb & = 43-8m \\\\\n\\\\\n51 &= 16m + b \\\\\n51 &= 16m + (43-8m) \\\\\nm &=1 \\\\\nb&=43-8m = 43-8=35 \\\\\n\\end{align}\n\\]\nThen we double check with third set of points:\n\\[\n\\begin{align}\n67 & = 1 \\times 32 + 35 \\\\\n67 & = 67 \\\\\n\\end{align}\n\\]\n\niat_prep$myheight_in = 1*iat_prep$myheight_002 + 35\n\nThen we need to convert height to meters since BMI is in \\(kg/m^2\\).\n\niat_prep$myheight_m = 0.0254*iat_prep$myheight_in\n\nOkay, now we need to do something similar for weight. Three more points to find the conversion: 10:90lb; 20:140lb; and 30: 190lb.\n\\[\n\\begin{align}\nlb & = m\\times cat+b \\\\\n90 &= m \\times 10 + b \\\\\nb & = 90-10m \\\\\n\\\\\n140 &= 20m + b \\\\\n140 &= 20m + (90-10m) \\\\\nm &=5 \\\\\nb&=90-10m = 90-50=40 \\\\\n\\end{align}\n\\]\nThen we double check with third set of points:\n\\[\n\\begin{align}\n190 & = 5 \\times 30 + 40 \\\\\n190 & = 190 \\\\\n\\end{align}\n\\]\n\niat_prep$myweight_lb = 5*iat_prep$myweight_002 + 40\n\nThen we need to convert height to meters since BMI is in \\(kg/m^2\\).\n\niat_prep$myweight_kg = 0.453592*iat_prep$myweight_lb\n\n\niat_prep$bmi = iat_prep$myweight_kg/(iat_prep$myheight_m)^2\n\n\nggplot(data = iat_prep, aes(x = bmi)) + \n  geom_histogram(binwidth = 1) +\n  geom_vline(aes(xintercept = mean(bmi, \n                                   na.rm = T)), \n             color = \"red\", linewidth = 2)\n\nWarning: Removed 142470 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nFrom histogram, looks like there are a couple observations at BMIs greater than 200. Let’s double check that.\n\nsummary(iat_prep$bmi)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   4.28   20.81   23.71   25.48   28.21  241.41  142470 \n\n\nOkay, so we now know the max is 241.41. I want to see the observations that have BMIs this large. I’ll take a look at their other values to see if there are any other issues.\n\niat_prep_bmi = iat_prep %&gt;% filter(bmi &gt; 200)\nhead(iat_prep_bmi, 10)\n\n     IAT_score att7 iam_001 identfat_001 myweight_002 myheight_002\n1  -0.61544208    4       1            5           81            2\n2   0.54476890    1       6            2           80            1\n3   0.70458996    1       2            3           80            2\n4   0.28206698    6       7            1           81            1\n5   0.33790313    5       7            3           81            2\n6   1.23311171    1       7            4           80            2\n7  -0.02357343    7       1            1           81            2\n8           NA    7       1            1           81            2\n9   0.33704837    4       1            1           81            2\n10 -0.47687442    4       4           NA           81            1\n   identthin_001 controlother_001 controlyou_001 mostpref_001 important_001\n1              1                5              5            4             5\n2              3                4              4            3             1\n3              3                3              2            4             5\n4              5                1              1            6             5\n5              4                3              2            1             4\n6              1                5              1            1             2\n7              5                1              1            2             5\n8              5                1              5            7             5\n9              5                1              1            4             4\n10            NA               NA              1           NA            NA\n   birthmonth birthyear month year raceomb_002    raceombmulti ethnicityomb edu\n1          12      1910     1 2021           8 [1,2,3,4,5,6,7]            1  12\n2          12      2009     1 2021           4                            1   1\n3          10      1916     1 2021           4                            1   1\n4          11      1910     2 2021          NA                            3   1\n5           4      2007     2 2021           6                            3  NA\n6           5      2001     2 2021           6                            2   5\n7           2      1980     2 2021           8 [1,2,3,4,5,6,7]            1   9\n8          NA        NA     2 2021          NA                           NA  NA\n9           5      1976     2 2021           5                            2   4\n10          9        NA     2 2021        -999                           NA  NA\n   edu_14 genderIdentity birthSex myheight_in myheight_m myweight_lb\n1      12            [2]        2          37     0.9398         445\n2       1            [1]        1          36     0.9144         440\n3       1            [1]        1          37     0.9398         440\n4       1            [6]        2          36     0.9144         445\n5      NA            [1]        1          37     0.9398         445\n6       5            [1]        1          37     0.9398         440\n7       9  [1,2,3,4,5,6]        2          37     0.9398         445\n8      NA                      NA          37     0.9398         445\n9       4            [1]        1          37     0.9398         445\n10     NA            [2]        2          36     0.9144         445\n   myweight_kg      bmi\n1     201.8484 228.5359\n2     199.5805 238.6963\n3     199.5805 225.9681\n4     201.8484 241.4087\n5     201.8484 228.5359\n6     199.5805 225.9681\n7     201.8484 228.5359\n8     201.8484 228.5359\n9     201.8484 228.5359\n10    201.8484 241.4087\n\n\nLooking at the subset of individuals with BMIs greater than 200, I am reminded that there is some serious quality control that needs to be done to this dataset. Other variable observations indicate that some of these rows are individuals who did not accurately fill out their survey. Right now, we keep them in our dataset, but we will need to examine them for outliers."
  },
  {
    "objectID": "labs/Project_template.html#statistical-methods",
    "href": "labs/Project_template.html#statistical-methods",
    "title": "Project Template: Title here",
    "section": "Statistical Methods",
    "text": "Statistical Methods"
  },
  {
    "objectID": "labs/Project_template.html#results",
    "href": "labs/Project_template.html#results",
    "title": "Project Template: Title here",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "labs/Project_template.html#discussion",
    "href": "labs/Project_template.html#discussion",
    "title": "Project Template: Title here",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "labs/Project_template.html#conclusion",
    "href": "labs/Project_template.html#conclusion",
    "title": "Project Template: Title here",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "labs/Project_template.html#references",
    "href": "labs/Project_template.html#references",
    "title": "Project Template: Title here",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "labs/Lab_02_work.html",
    "href": "labs/Lab_02_work.html",
    "title": "Lab 2 Work",
    "section": "",
    "text": "IMPORTANT TO READ\n\n\n\n\nPlease do not delete the rubric from your .qmd file. I will use it to circle the grades!\nPlease delete everything"
  },
  {
    "objectID": "labs/Lab_02_work.html#directions",
    "href": "labs/Lab_02_work.html#directions",
    "title": "Lab 2 Work",
    "section": "Directions",
    "text": "Directions\nPlease turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here.\nThe rest of this lab’s instructions are embedded into the lab activities.\n\nPurpose\nThe main purpose of this lab is to introduce our dataset, codebook, and variables. We will continue to think about the context of our research question, but our main focus is to become familiar with the data.\n\n\nGrading\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\nRubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nSome tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning (like target population, choosing variables, revisiting research question)"
  },
  {
    "objectID": "labs/Lab_02_work.html#lab-activities",
    "href": "labs/Lab_02_work.html#lab-activities",
    "title": "Lab 2 Work",
    "section": "Lab activities",
    "text": "Lab activities\n\n1. Access and download the data\nThis serves as good practice for accessing data that is online or needs to be downloaded from a collaborator.\nData can be accessed here. Under “Weight IAT 2004-2022” there are several drop down menus:\n\nI opened the first “Datasets & Codebooks,” then selected “OSF Storage (United States).” Once selected, the “Download as zip” option pops up in the top right part of the Files section.\n\nWe will be working with the Weight_IAT.public.2021.csv dataset. Please locate the zip file called Weight IAT.public.2021-CSV.zip . T0 download, you need to click the row of the zip file, but you can’t click the name of the zip file. If a link opens, then you clicked the name. If the row is highlighted blue and clickable “Download” and “View” buttons appear on the top right, then you selected it correctly! (See below image for what it should look like.)\n\nThen click the “Download” button to download! Note that the name does not have an underscore between “Weight” and “IAT.” I like to have my datasets named without spaces, so I will replace the space with an underscore.\nFor the codebook, perform the same process for the file named: Weight_IAT_public_2021_codebook.xlsx\nYou will need to unzip the actual data.\nMove the data to a folder that you can easily access as you work from this document. I like to have a folder named data to house my data.\n\n\n\n\n\n\nTask Summary\n\n\n\nDownload the 2021 data and codebook from the archives and store in accessible folder.\n\n\n\n\n2. Load data and needed packages\nFirst, load the packages that you will need in the remainder of this lab. You can add to this as you need to. At the top of your R code chunk, you can add the following option to repress the messages from the loading packages:\n\n\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(here)\nif(!require(lubridate)) { install.packages(\"lubridate\"); library(lubridate) }\n\nUsing R, load the data (csv file) into this document. Note that this is a csv file that we can load with basic R packages. Name your dataset something that feels intuitive to you and will distinguish it from other datasets that you work with.\n\n\n[1] \"/Users/wakim/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/Teaching/Classes/W25_BSTA_512_612/BSTA_512_W25_site\"\n\n\nLoading the csv file every time you render will take a long time. One way to speed this up is by saving the data as an rda file (R data file). Change the following R code to save the rda file. You will also need to remove the #| eval: false at the top of the code chunk once you have corrected the code. If you are confused on the syntax, don’t forget that you can use ?save for more information.\n\nsave(&lt;whatever you called the read csv file&gt;, file = \"Where you would like to save the file with its name\")\n\nCheck that you have an rda file where you saved it. Now use load() with the file path to load the rda data here.\n\nload(file = \"Where you would like to save the file with its name\")\n\nAt this point, if you think you loaded the file correctly, add #| eval: false to the code chunk where you loaded the csv file and back to the chunk where you saved the rda file.\nTake a glimpse at the data to make sure you loaded it correctly.\nHow many rows and columns are in the dataset? Do you think we will need all these variables for our analysis?\n\n\n\n\n\n\nTask Summary\n\n\n\nRead csv, save as rda, load rda, glimpse at data.\nHow many rows and columns are in the dataset? Do you think we will need all these variables for our analysis?\n\n\n\n\n3. Data wrangling\nAs you go through this process, it is important that you look at the codebook for more information on each variable.\n\n3.1 What’s our target population?\nAs many of you mentioned in Lab 1, individuals taking the IAT test are not necessarily representative of the world population. I want you to articulate the target population that you think our analysis can give information about. To what population can we generalize our analysis results? We can get very specific with this population, but try to restrict your population to 3-5 characteristics.\nAfter you articulate the population, I want to add one more restriction to our population: US residency. The sample includes individuals residing in many different countries. Since we are discussing attitudes and beliefs that is inherently connected to society and culture, I think it is important that we restrict our analysis and discussion to a country that we have some social experience in. Thus, let’s restrict our data to the US only by filtering the variable countryres to category 1 (corresponding to the US).\n\n\n\n\n\n\nTask\n\n\n\nDescribe our target population. Keep your description to 3-5 characteristics, not including our restriction on the US population.\n\n\n\n\n3.2 Restrict your analysis to 1 outcome and 9 possible covariates/predictors\nWe are going to restrict our analysis to the single outcome, IAT score, which is named D_biep.Thin_Good_all. You can rename this variable.\nWe will also restrict our analysis to the following 9 potential variables so our work is a little more manageable.\n\n\n\n\n\n\nTask\n\n\n\nFrom the following 7 attitudes and beliefs, please select 3 that you think will be the most important variables related to your research question. In 1-2 lines, briefly explain why you chose each variable. This can be informal and bulleted.\n\n\n(Make sure you chose the variable that is part of your research question!)\n\nSelf-perception of weight (iam_001)\nFat group identity (identfat_001 )\nThin group identity (identthen_001 )\nControllability of weight of others (controlother_001)\nControllability of weight of yourself (controlyou_001)\nAwareness of societal standards (mostpref_001 )\nInternalization of societal standards (important_001)\n\nWe will start our data exploration with the following 4 demographic variables:\n\nAge (we need to construct)\nRace (raceomb_002 or raceombmulti)\nEthnicity\nSex assigned at birth (birthSex)\n\nPlease pick 2 additional variables to include in your analysis:\n\nEducation (edu)\nGender (genderIdentity)\nSelf-reported BMI (through self-reported height and weight)\nPolitical identity\nReligion\n\nI have chosen these variables for a mixture of reasons. For example, I have left out variables about residence and occupation because those variables have hundreds of categories that would be overwhelming in linear regression. For the 4 required demographic variables, I chose age because I really want us to get practice with a continuous variable. I chose race and ethnicity because of the intertwined history of racism and anti-fat bias in Western countries (including the U.S. where most participants reside).\n\n\n\n\n\n\nA note of the available variables on race\n\n\n\nThe dataset has two separate race variables. One has mutually exclusive categories (raceomb_002) and the other allows participants to make multiple selections (raceombmulti). The former (raceomb_002) allows one participant to identify with only one race category.\nImportant lesson from We All Count about using a multiple selection race question. We can try out all these options!\n\n\nFinally, I chose sex assigned at birth because adults in 2021 in the US were likely raised in a society where your sex assigned at birth impacted the gender stereotypes that you were raised in, which could impact exposure to diet culture. This in addition to the many medical conditions associated with one’s sex assigned at birth that may affect weight. The reason why I am leaving gender as an optional variable is because the question on gender allows participants to chose multiple options. The binary sex assigned at birth will make our analysis a little easier from a statistics stand point. Unfortunately, we need to balance achievable learning objectives and the most appropriate variable. Since I have required race as a variable and has a multi-level option, I do not want to overload our analysis with another multi-level variable. Sex assigned at birth will not create more work for you (that is outside of the course objectives) while capturing medical conditions and some of the societal impact of diet culture. This is certainly a limitation in our analysis that we should address in our discussion. I do encourage you to look into gender if the binary sex assigned at birth does not feel right for you. I am happy to help!\n\n\n\n\n\n\nA word on self-reported BMI\n\n\n\nThis variable is rooted in racism and anti-fat bias. The American Medical Association made a few press releases on policies using BMI as a measure, with alternative measures (frankly, just other measures of fatness to use as a diagnostic tool instead of checking true indicators of health). However, I can think of a couple examples where BMI might help us understand some context in this research, so I have left it as an option. Although still self-reported, it might be interesting to see how BMI (which is the closest measurement available in this dataset to an “objective” measure of fatness) is related to individuals’ attitudes and beliefs. I am not saying there is anything to the relationship, but it might be worth checking out if you are interested.\nI will also say, in this dataset, there are MANY issues constructing the variable for BMI from height and weight. If you do not feel strongly about including it, I would suggest you avoid the variable self-reported BMI. It is not worth bringing in a racist and anti-fat variable into the dataset if you do not have a specific use for it. If you do plan to use it, please come to me for help as early as possible!\n\n\nIf you would like to investigate a variable outside the list, please let me know by emailing or chatting with me.\n\n\n\n\n\n\nTask\n\n\n\nUsing R, select your identified variables from your dataset. Your new dataset should have 10 columns for the 10 variables.\n\n\n\n\n [1] \" \"             \"[2]\"           \"[1]\"           \"[3]\"          \n [5] \"[1,3]\"         \"[5]\"           \"[1,5,6]\"       \"[2,5]\"        \n [9] \"[1,2]\"         \"[1,5]\"         \"[4]\"           \"[6]\"          \n[13] \"[1,2,3,4,5,6]\" \"[5,6]\"         \"[1,6]\"         \"[3,5]\"        \n[17] \"[4,5]\"         \"[2,6]\"         \"[2,4]\"         \"[2,5,6]\"      \n[21] \"[1,4]\"         \"[2,3]\"         \"[3,4,5,6]\"     \"[1,2,3,4]\"    \n[25] \"[1,3,5,6]\"     \"[1,3,5]\"       \"[2,3,4]\"       \"[3,5,6]\"      \n[29] \"[2,4,6]\"       \"[1,2,5]\"       \"[3,6]\"         \"[1,2,6]\"      \n[33] \"[1,4,6]\"       \"[2,3,5]\"       \"[1,3,4,6]\"     \"[1,2,3,4,5]\"  \n[37] \"[4,6]\"         \"[2,4,5]\"       \"[1,2,4]\"       \"[1,3,6]\"      \n[41] \"[1,2,3,4,6]\"   \"[3,4,5]\"       \"[1,2,3]\"       \"[1,3,4,5,6]\"  \n[45] \"[4,5,6]\"       \"[1,2,3,5]\"     \"[1,4,5]\"       \"[3,4]\"        \n[49] \"[2,3,4,5]\"     \"[1,4,5,6]\"     \"[1,2,5,6]\"     \"[1,3,4]\"      \n\n\n\n\n3.3 Manipulating variables that are coded as numeric variables\nMany variables in this dataset are coded as numeric values, but have specific categories linking up to the numbers. Using mutate() and cases() similar to our Data Management lesson, please create a new categorical variable with the specified categories from the codebook. Make sure that you create a variable with a new name! Since some of these variables are ordered categories, we will investigate if it’s appropriate to use the numeric or categorical version of the variable.\n\n\n\n\n\n\nExample of how I would create new variable for self-perception of weight (iam_001):\n\n\n\nBy looking at the codebook, I see that respondents answer the following question: “Currently, I am:”\n\n“Very underweight”\n“Moderately underweight”\n“Slightly underweight”\n“Neither underweight nor underweight”\n“Slightly overweight”\n“Moderately overweight”\n“Very overweight”\n\nIf I look at the data as is, I see that the variable is numeric.\n\niat_2021 %&gt;%\n  dplyr::select(iam_001) %&gt;%\n  tbl_summary()\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 465,8861\n    \n  \n  \n    iam_001\n\n        1\n2,023 (0.6%)\n        2\n7,902 (2.4%)\n        3\n24,399 (7.3%)\n        4\n148,081 (44%)\n        5\n88,566 (27%)\n        6\n43,090 (13%)\n        7\n18,978 (5.7%)\n        Unknown\n132,847\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\nAgain, I want to create a varaible with the answers instead of numbers, so I will change transform the variable to include the text:\n\niat_2021 = iat_2021 %&gt;%\n  mutate(iam_001_f = case_match(iam_001,\n                             7 ~ \"Very overweight\",\n                             6 ~ \"Moderately overweight\",\n                             5 ~ \"Slightly overweight\",\n                             4 ~ \"Neither underweight nor underweight\",\n                             3 ~ \"Slightly underweight\",\n                             2 ~ \"Moderately underweight\",\n                             1 ~ \"Very underweight\",\n                             .default = NA # to add NA if unknown\n                             ))\n\nI have called the new variable iam_001_f to indicate that the variable is not in factor form. You can also call it something like iam_001_cat to indicate the categorical form.\n\n\n\n\n\n\n\n\nTask\n\n\n\nIdentify and list the variables that are coded numerically and correspond to categories. Create a new variable for the categorical/factor version of the variable. It is up to you to check that your code ran properly!! If you are using multi-choice categorical variables (might include race, gender), then do not convert the variable yet!\n\n\n\n\n3.4 Creating age from birth date and test date\nThis dataset does not have an available “age” variable. However, we have enough information to determine each individual’s age from the test date and their self-reported birth date. We can use the lubridate package to configure the age. First, we need to use make_date() to construct the birth date and test date. Below, I have implemented make_date() to make the birth date.\n\n\n\n\n\n\nTask\n\n\n\nFrom the codebook, find the variables that we can use to construct the test date. Then use make_date() to create the test date.\n\n\n\niat_2021 = iat_2021 %&gt;%\n  mutate(birthdate = make_date(month = birthmonth, year = birthyear), \n         testdate = make_date(month = month, year = year))\n\nOnce the two dates are created, we can use further use lubridate to calculate the age in years. This code is a little complicated, so here is an example of how I have created age:\n\niat_2021 = iat_2021 %&gt;%\n  mutate(age = interval(start = birthdate, end = testdate) %&gt;%\n          as.period() %&gt;% year()) %&gt;%\n  select(-birthmonth, -birthyear, -year, -month, \n         -testdate, -birthdate)\n\nNote that the name of my dataset is iat_2021 and I feed it into mutate(). Within mutate(), I assigned age to the interval between the name of my birth date (birthdate) and the name of my test date (testdate). I need to convert the interval to a period of time (as.period()), then to a measurement of years (year()).\n\n\n\n\n\n\nTask\n\n\n\nFollowing the above example, create an age variable that measures the years between individuals’ birth and test date. Then remove the variables used to make age.\n\n\n\n\n3.5 If you chose BMI, create the variable\nRaw data from weight and height are categorical. This is according to the codebook associated with this dataset. Please find your codebook file named Weight_IAT_public_2021_codebook.csv . You can find the value names for myweight_002 and myheight_002.\n\nFor example, in the weight variable,\n\nmost categories identify a lower limit to the weight in the group. One example group is weight is greater than or equal to 200 pounds and less than 205 pounds (labelled as “200 lb :: 91 kg”).\nthe first category for weight is “below 50lb:: 23kg” with 258 observations\nthe last category for weight is “above 440lb:: above 200kg” with 295 observations\n\nWhile the 5 groups of weight leading up the last category have 33, 28, 34, 20, and 89 observations, respectively.\n\n\n\nI will post an extra resource outlining some of my work on the BMI variable.\n\n\n3.6 Make a new dataset with only complete cases\nHandling missing data is outside the scope of our class. There are many techniques to handling missing data, but we will use complete case analysis. This means we will only use observations that have information for every variable we chose. The function drop_na() will give you the complete cases. You can feed your dataset into the function and assign it as a new dataframe.\nFor example:\n\nnew_df = old_df %&gt;% drop_na()\n\niat_2021 = iat_2021 %&gt;%\n    mutate(iam_001_f = case_match(iam_001,\n                                  7 ~ \"Very overweight\",\n                                  6 ~ \"Moderately overweight\",\n                                  5 ~ \"Slightly overweight\",\n                                  4 ~ \"Neither underweight nor underweight\",\n                                  3 ~ \"Slightly underweight\",\n                                  2 ~ \"Moderately underweight\",\n                                  1 ~ \"Very underweight\",\n                                  .default = NA) %&gt;% \n             factor(levels = c(\"Very underweight\", \n                               \"Moderately underweight\", \n                               \"Slightly underweight\", \n                               \"Neither underweight nor underweight\", \n                               \"Slightly overweight\", \n                               \"Moderately overweight\", \n                               \"Very overweight\")))\n\n\n\n\n4. Some exploratory data analysis\n\n4.1 Peek at your outcome\nThis serves as a check to make sure we are all looking at the correct outcome: IAT score.\n\n\n\n\n\n\nTask\n\n\n\nPlease plot a histogram of the IAT scores. What do you notice about the outcome?\n\n\n\n\n4.2 Univariate exploratory data analysis\n\n\n\n\n\n\nTask\n\n\n\nUsing ggplot or tables, visualize your variables. Get a sense of each variable’s distribution. Do you notice anything out of the ordinary?\n\n\n\n\n4.3 Bivariate exploratory data analysis\n\n\n\n\n\n\nTask\n\n\n\nTake a look at the scatterplot, violin, or box plot of IAT score and your variable of interest. Use R and ggplot to make this plot. If your variable of interest is categorical, then make sure to use a violin or boxplot.\n\n\n\n\n\n5. Revisit your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate the research question that you proposed in Lab 1. What are your thoughts on the research question now that we looked at the data? Feel free to change it now that we’ve looked at the data. If you change your question, make sure 4.2 reflects the new research question.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn research, we typically do NOT change our research question after looking at the data! Researchers typically form their questions from other research and their expertise. We may not have expertise in this field and we have not been studying implicit bias, so I want to be a little more flexible with our analysis."
  },
  {
    "objectID": "labs/Project_report_instructions.html",
    "href": "labs/Project_report_instructions.html",
    "title": "Project Report Instructions",
    "section": "",
    "text": "Important\n\n\n\nInstructions and rubric are completely done! (3/15/2024)"
  },
  {
    "objectID": "labs/Project_report_instructions.html#directions",
    "href": "labs/Project_report_instructions.html#directions",
    "title": "Project Report Instructions",
    "section": "1 Directions",
    "text": "1 Directions\n\n\n\n\n\n\nProject template\n\n\n\nYou may use this project template to get started on the report. It is your responsibility to meet the formatting guidelines below!!\nDO NOT USE SITE PAGE (“Project Report Instructions”, current page) as your template!!\n\n\n\n1.1 Purpose\nProject reports serve as a great way to communicate the knowledge learned in a statistics class and connect it to context within research. It is important that we can take a step back from the numbers and analysis to see what questions linear regression can help us answer.\n\n\n1.2 Formatting guide\n\nThe report will be written in Quarto. Turn in both the qmd and html files\n\nNo code should appear in the html document\n\nThis means all R code chunks should have #| echo: false\nThis also means warnings and messages should be turned off\n\n\nThe report should be 10 - 14 paragraphs long\nTables and figures should NOT have variable names as they appear in the data frame\n\nVariable names should be understood by a reader\nVariable names should be written in full words\nInclude a title or caption for all figures\nFigure and tables appear on same page or close to same page where they are first referenced\nTables and figures are an appropriate size in the html - Nicky is able to read all words in figures and tables\n\nWriting, spelling, and grammar should be admissable\n\nThis means I can generally follow your thought/what you are trying to communicate\nSome spelling and grammar mistakes are allowed\n\nI will not take off points if there are a few sprinkled in\nIf every or close to every sentence has mistakes, then I will take off\n\n\nSectioning of the report\n\nMain sections that were required: Introduction, Statistical Methods, Results, Discussion, Conclusion, and References\nOther sections that might help group specific methods or results\n\nTitle information at the top of the html\n\nThis includes the title itself, your name, and the date\n\n\n\n\n\n\n\n\nThe project report is a separate file from the labs\n\n\n\nYou can save tables and figures from labs or separate files, then load them in the report\n\nSave R objects in analyses file:\n\nSuppose you named the Table 1 as table1\nsave(table1, file = \"table1.Rdata\")\n\nLoad R objects in report file: load(file = \"table1.Rdata\")\n\n\n\n\n\n1.3 Examples of reports\nThe following are examples of reports from BSTA 513 with the feedback that I gave them.\nPlease note that 513 uses a different type of outcome than our class. These examples are meant to help guide you with the formatting and some appropriate content.\nAlso note that these were converted to PDFs so I could write in feedback. Some of the tables and figure sizes were distorted. They need to be legible in the html.\n\nReport 1 with my feedback\nReport 2 with my feedback\n\nThe above reports have code showing in their html. Remember that I am asking you to hide all code, warnings, and messages.\n\n\n1.4 Grading\nThe project report is out of 36 points. Note that the Statistical Methods and Results sections are graded on an 8-point scale, while all other components are graded on a 4-point scale.\n\n1.4.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences with very few grammatical or spelling errors. With little editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences with some (around 2 per section) grammatical or spelling errors. With some editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences, but have many grammatical or spelling errors. With major editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences, but are very hard to follow due to grammar mistakes.\nLab not submitted on Sakai (or by email if late) with .html file. Report is not written with complete sentences. With major editing, the report can be distributed.\n\n\nFigures and work\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables look professional, are easily interpreted by the reader, and easily convey the intended message.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. For the most part, figures and tables look professional, are easily interpreted by the reader, and easily convey the intended message. A few mistakes in the figures are made.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables look semi-professional, are not so easily interpreted by the reader, and convey the intended message but after some work by the reader. Some mistakes in the figures are made.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables do not look professional, are not easily interpreted by the reader, and/or do not convey the intended message. Many mistakes in the figures are made.\nRequested output is not displayed, Missing one or more figures.\n\n\nIntroduction\nProvides a good background for the research question, includes motivation for the question, and references previous research that justifies this analysis.\nProvides a decent background for the research question and includes motivation for the question. Previous research is mentioned, but feels disconnected to the current analysis.\nProvides a decent background for the research question and includes motivation for the question. Previous research is mentioned, but feels disconnected to the current analysis.\nDoes not provide a background that connects to the research question. Motivation and previous research are not mentioned.\nNo introduction included.\n\n\nMethods (8 points)\nDescribes statistical methods concisely and highlights pertinent information to the reader (listed Sections below). Demonstrates proper analyses were performed.\nDescribes statistical methods and highlights pertinent information to the reader (listed Sections below). Details were omitted or added that were not needed to explain the overarching methods. Demonstrates proper analyses were performed.\nDescribes statistical methods and highlights pertinent information to the reader (listed Sections below). Details were omitted or added that were not needed to explain the overarching methods. Some incorrect analyses included in the description.\nDescribes statistical methods, but lacks clarity. Demonstrates a lack of understanding about the overall process of regression analysis. Incorrect analyses included in the description.\nNo methods included.\n\n\nResults (8 points)\nCorrectly interprets coefficients for the explanatory variable and identifies any other interesting trends. Highlights pertinent results to the reader (listed Sections below).\nCorrectly interprets coefficients, but does correctly incorporate the interaction (if in the model). Highlights pertinent results to the reader (listed Sections below).\nIncorrectly interprets coefficients. Highlights pertinent results to the reader (listed Sections below).\nIncorrectly interprets coefficients.Omits pertinent results to the reader (listed Sections below).\nNo results included.\n\n\nDiscussion\nThoroughly and concisely discusses limitations and considerations of the results, and their consequences.\nDiscusses limitations and considerations of the results and their consequences, but misses some big considerations.\nDiscusses limitations and considerations of the results, but does not discuss the consequences.\nDiscusses limitations and considerations of the results, but misses many considerations and does not discuss consequences.\nNo discussion included.\n\n\nConclusion and References\nFor the conclusion, main research question is answered and statistical caveats described to non-technical person. References are mostly cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is answered and statistical caveats described to non-technical person. References are sometimes cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is somewhat answered (but focus is not on the research question) and statistical caveats described to non-technical person. References are sometimes cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is somewhat answered (but not the focus at all) and statistical caveats are not described. References are not cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is not answered. Or references are not included at all.\n\n\n\n\nIn formatting, an example of a report with little editing needed is one that has zero to some grammar or spelling mistakes, no code chunks showing, and no output warnings nor messages showing.\nProfessional figures mean\n\nI can read the words and numbers in the html\n\nVariable names are converted from the data frame version to readable text\nFor example: iam_001 does not show up on axes, instead something like: Response to \"Currently, I am...\"\n\nColors are only used if conveying information\nIntended message of the figure is easily understood\n\nIf you are trying to show a trend of mean IAT vs. an ordered categorical variable, then the variable is ordered on the x-axis\n\n\nFor the references\n\nI will not be overly critical about the formatting\nBy consistency, I mean that you if you are citing things like (Last Name, Year) it doesn’t suddenly change to number citations.\nIf you would like to use Quarto’s citation tool, you can! I actually pair it with Zotero and it works beautifully! (But I would not embark on this if you haven’t used Zotero before)"
  },
  {
    "objectID": "labs/Project_report_instructions.html#sections",
    "href": "labs/Project_report_instructions.html#sections",
    "title": "Project Report Instructions",
    "section": "2 Sections",
    "text": "2 Sections\n\n2.1 Title\n\nPurpose: Create an identifiable name for your research project that includes the main research question’s variables and gives some context to the analysis or results\n\n\n\n2.2 Introduction\n\nLength: 1-2 paragraphs\nPurpose: Introduce the research question and why it is important to study\nThis section is non-technical.\n\nBy reading just the introduction and conclusion, someone without a technical background should have an idea of what they study was about, why it is important, and what the main results are\n\nYou may start with the introduction written in Lab 1, but you should edit it and make sure it flows into your report well!\nShould contain some references\n\n\n\n2.3 Statistical Methods\n\nLength: 3-5 paragraphs\nPurpose: Describe the analyses that were conducted and methods used to select variables and check diagnostics\nImportant to keep in mind: methods typically describe your approach and process, not the results of that process\n\nFor example: I might say “We investigated the linearity of each continuous covariate visually. If continuous variables were not linear, then we divided the variable into categories using existing guidelines from &lt;insert reference here&gt; or creating quartiles.”\n\nIn the methods section, I would NOT say: “We investigated the linearity of each continuous covariate visually. We found that age was not linearly related to IAT scores. Thus, we categorized age into the following groups: ___, ____, ____, ____, and ____.”\n\nThe last two sentences about age would be more appropriate in the Results section\n\n\n\nSome important methods to discuss (You may divide these into your sections, not necessarily with these names)\n\nGeneral approach to the dataset\n\n3-5 sentences\nDid you need to do any quality control?\nMissing data: we performed complete case analysis\n\n1 sentence\nCan be included in the Exploratory data analysis section\n\n\nVariables and variable creation\n\nThis includes a description of analyses for Table 1 and what statistics were used to summarize the variables\n\nMore on creation of Table 1, not discussing the results of Table 1\n\nIncludes (not required)\n\nIndicators for gender identity or race\nCreating BMI\nCategorizing a continuous variable (even if performed in model selection)\nUsing scoring for an ordered categorical variable (that is not your explanatory variable)\n\n1-2 sentences per variable\n\nModel building: we performed purposeful selection\n\n3-5 sentences\nIncludes\n\nDescribe purposeful selection: combining existing literature, clinical significance, and analysis\nHow did you build the model? Describe the process\nDid you consider confounders and effect modifiers?\n\n\nModel diagnostics\n\n2-5 sentences\nIncludes\n\nProcess of investigating model diagnostics\nBy the time you build the model, LINE assumptions should be met\nIf assumptions were not met, what process did you use to fix it?\n\n\n\n\n\n\n2.4 Results\n\nLength: ~3 paragraphs\nPurpose: Relay the results from our sample’s analysis typically focusing on the numbers and interpretations\nSome important results to discuss (also could be sections)\n\nSample data set statistics (Table 1)\n\n3-5 sentences\nInclude a brief description of the sample’s characteristics\nTable 1 should be referenced and appear here!\n\nFinal model\n\n1-2 sentences\nDescribe final model (or models if comparing a few)\n\nWhat variables were included in your final model?\nWhat interactions with your explanatory variable did you include?\n\n\nInterpret the model coefficients in the context of the research question\n\n1-2 paragraphs\nInterpreting the explanatory variable’s relationship with IAT score is the most important thing to report!!\n\nWhen doing this, make sure you account for ALL interactions: If your explanatory variable has multiple interactions and you are trying to interpret one, then what does that mean about the other variables involved in the other interactions? If this is confusing, please make an appointment with me!!\n\n\nResults of model diagnostics if there is anything worth noting\n\nTables & figures\n\nThe following are required tables or figures\n\nTable 1 summarizing participant characteristics both overall and stratified by your primary independent variable\nTable or figure with regression results\n\nCan be a forest plot\nIf you have A LOT of coefficient estimates, the forest plot may not work well!\n\n\n1-3 figures that you think are helpful in understanding the results, for example\n\nDAG explaining connection between variables (if you did this)\nTable or figure to compare model fit statistics (if you did this)\nTable or figure for unadjusted relationship between outcome and explanatory variables\n\n\n\n\n\n2.5 Discussion\n\nLength: 2-3 paragraphs\nPurpose: Discuss the results and give them context outside of the sample and its analysis\nSome important things to include\n\nInclude a paragraph on the limitations of the results\n\nYou don’t need to hit all the limitations, but think about the big ones (generalizability? independence of samples? large sample size vs. clinical significance? the way we handled variables?)\n\nAfter limitations, discuss the positive parts of the results\n\nWhat can we do with these results? What impact can it have?\n\nAny overarching trends that are worth noting? (Giebel et al. 2024)\n\nShould contain some references\n\n\n\n2.6 Conclusion\n\nLength: 1 short paragraph (more like ~3 sentences)\nPurpose: Describe the main conclusions to a non-technical audience\n\n\n\n2.7 References\n\nInclude your references here!\nYou introduction should have references, especially when discussing the social science behind the analysis\nYou must reference the IAT data source!!"
  },
  {
    "objectID": "homework/HW_02.html",
    "href": "homework/HW_02.html",
    "title": "Homework 2",
    "section": "",
    "text": "Caution\n\n\n\nThis homework is not ready to be worked on!! Nicky needs to update it."
  },
  {
    "objectID": "homework/HW_02.html#directions",
    "href": "homework/HW_02.html#directions",
    "title": "Homework 2",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nIf you are computing something by hand, you may take a picture of your work and insert the image in this file. You may also use LaTeX to write it inline.\nWrite all answers in complete sentences as if communicating the results to a collaborator. This means including a sentence summarizing results in the context of the research study.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_02.html#question-1",
    "href": "homework/HW_02.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\nThis homework assignment is based on data collected as part of an observational study of patients who suffered from stroke.\nDataset: The main goal was to study various psychological factors: optimism, fatalism, depression, spirituality, and their relationship with stroke severity and other health outcomes among the study participants. Data were collected using questionnaires during a baseline interview and also medical chart review. More information about this study can be found in the article Fatalism, optimism, spirituality, depressive symptoms and stroke outcome: a population based analysis.\nThe dataset that you will work with is called completedata.sas7bdat. It is SIMILAR but does not exactly match the data in the article. It contains information on complete cases (i.e. excludes participants who had missing data on one or more variables of interest) who suffered a stroke. The two variables we are interested in are:\n\nCovariate: Fatalism (larger values indicate that the individual feels less control of their life)\n\nScores range from 8 to 40\n\nOutcome: Depression (larger values imply increased depression)\n\nScores range from 0 to 27\n\n\nFor our homework purposes we will assume they are continuous.\n\nfatal_dep = read_sas(here(\"./data/completedata.sas7bdat\"))\n\n\nPart a\nPlot the data, with title and axis labels, for Depression (y-axis) vs. Fatalism (x-axis). Comment on what you see.\n\n\nPart b\nFit a linear regression model to estimate the association between the predictor Fatalism and the outcome Depression.\nInterpret the slope and intercept. Does the intercept make sense?\n\n\n\n\n\n\nNote\n\n\n\nMake sure to include the confidence interval. Whenever asked to interpret coefficients, you must include confidence intervals. Also, the “units” for fatalism and depression are scores.\n\n\n\n\nPart c\nIn your dataset, make a new variable FatalismC, equal to Fatalism centered at its median (C is for centered).\n\\[\n\\text{FatalismC} = \\text{Fatalism} - \\text{median of Fatalism}\n\\]\nThis is one way of centering a variable, and can be used when the intercept estimate does not make sense. (Hint: the mutate() function will work well here!)\nPlot the data, with title and axis labels, for Depression (y-axis) vs. FatalismC (x-axis).\n\n\nPart d\nRe-run the regression from Part b using this new variable for FatalismC. Interpret the new slope and intercept. Which of the following are the same as Part b: intercept, slope?\n\n\n\n\n\n\nNote\n\n\n\nMake sure to include the confidence interval. Whenever asked to interpret coefficients, you must include confidence intervals. Also, the “units” for the centered fatalism is still the score.\n\n\n\n\nPart e\nFrom the above interpretations, what would be the equivalent conclusion from a hypothesis test for the association between Depression and Fatalism?\n\n\n\n\n\n\nNote\n\n\n\nYou do not need to go through the whole process for the hypothesis test. You only need to state whether it is rejected or not and site the confidence interval as evidence."
  },
  {
    "objectID": "homework/HW_02.html#question-2",
    "href": "homework/HW_02.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nThis question and data are adapted from this textbook.\nIn an experiment designed to describe the dose–response curve for vitamin K, individual rats were depleted of their vitamin K reserves and then fed dried liver for 4 days at different dosage levels. The response of each rat was measured as the concentration of a clotting agent needed to clot a sample of its blood in 3 minutes. The results of the experiment on 12 rats are given in the following table; values are expressed in common logarithms for both dose and response.\n\nclot = read_excel(here(\"./data/CH05Q09.xls\"))\nclot %&gt;% gt() %&gt;%\n  cols_label(RAT = md(\"**Rat**\"),\n             LOGCONC = md(\"**Log10 Concentration (Y)**\"),\n             LOGDOSE = md(\"**Log10 Dose (X)**\"))\n\n\n\n\n\n  \n    \n      Rat\n      Log10 Concentration (Y)\n      Log10 Dose (X)\n    \n  \n  \n    1\n2.65\n0.18\n    2\n2.25\n0.33\n    3\n2.26\n0.42\n    4\n1.95\n0.54\n    5\n1.72\n0.65\n    6\n1.60\n0.75\n    7\n1.55\n0.83\n    8\n1.32\n0.92\n    9\n1.13\n1.01\n    10\n1.07\n1.04\n    11\n0.95\n1.09\n    12\n0.88\n1.15\n  \n  \n  \n\n\n\n\nUse the log-transformed values as given in the dataset.\nUse the following scatterplot to build your answers off of:\n\n\n\n\n\n\n\n\n\n\nPart a\nFit a linear regression model to the data and add the regression line to the plot.\n\n\nPart b\nUse R to create the ANOVA table for the regression described in the exercise.\n\n\nPart c\nUsing the F-test, determine whether there is an association between the log10 concentration and log10 dose.\n\n\n\n\n\n\nNote\n\n\n\nMake sure to include all needed steps for an F-test. Calculating the F test statistic (step 5) is not needed if you use the ANOVA table. Make sure your conclusion connects back the research context.\n\n\n\n\nPart d\nRewrite your hypothesis test in Part c to show the null and alternative models that we are testing. Did we reject the smaller (reduced) model?\n\n\n\n\n\n\nNote\n\n\n\nYou do not need to go through the hypothesis test process again. A quick statement on rejecting or not is okay.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you prefer to write out the models by hand, remember that you can take a picture of your work and insert it into this document. HW0 can be a good reference for how we’ve done this before."
  },
  {
    "objectID": "homework/HW_02.html#question-3",
    "href": "homework/HW_02.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\nWe will continue to work with the study and dataset from Question 2 above.\n\nPart a\nFind the correlation coefficient between the two variables. Is the value consistent with your description of the relationship in Question 2? Why or why not?\n\n\nPart b\nCalculate the coefficient of determination using linear regression summary output. Can we also calculate the coefficient of determination from the ANOVA in Question 2?\n\n\nPart c\nGive an interpretation of the coefficient of determination in the context of the study."
  },
  {
    "objectID": "homework/HW_01.html",
    "href": "homework/HW_01.html",
    "title": "Homework 1",
    "section": "",
    "text": "Caution\n\n\n\nThis homework is not ready to be worked on!! Nicky needs to update it."
  },
  {
    "objectID": "homework/HW_01.html#directions",
    "href": "homework/HW_01.html#directions",
    "title": "Homework 1",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the HW1 datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as your HW1 .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\nFor each question, make sure to include all code and resulting output in the html file to support your answers\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nWrite all answers in complete sentences as if communicating the results to a collaborator.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your .qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_01.html#questions",
    "href": "homework/HW_01.html#questions",
    "title": "Homework 1",
    "section": "Questions",
    "text": "Questions\nThe following questions were adapted from this textbook.\n\nQuestion 1\nPlease use R code to determine the following answers. (adapted from problem 3.3 in Applied Regression Analysis and Other Multivariable Methods)\n\n\n\n\n\n\nType ?pnorm in the console to get some information on a potentially helpful function.\n\n\n\n\nPart a\nFrom a normal distribution with mean 4 and standard deviation 6, what is \\(P(X&gt;2)\\)?\n\n\nPart b\nFrom a normal distribution with mean 4 and standard deviation 6, for what value (in place of ??) would \\(P(X&gt;??) = 0.1\\)?\n\n\n\nQuestion 2\nSuppose that the height (\\(H\\)) of assigned-male-at-birth (AMAB) patients registered at a clinic has the normal distribution with mean 70 inches and variance 4. (adapted from problem 3.11 in Applied Regression Analysis and Other Multivariable Methods)\n\nPart a\nFor a random sample of patients of size \\(n = 25\\), the expression \\(P(\\bar{H} &lt; 65)\\), in which \\(\\bar{H}\\) denotes the sample mean height, is equivalent to saying \\(P(Z &lt; ?)\\)\n\n\n\n\n\n\n\\(Z\\) is a standard normal random variable.\n\n\n\n\n\nPart b\nUsing the pnorm function, show that the probability expressions in Part a are equal.\n\n\nPart c\nFind an interval \\((a, b)\\) such that \\(P(a&lt; \\bar{H} &lt;b) = 0.80\\) for the same random sample in Part a.\n\n\n\nQuestion 3\nTest the null hypothesis that the true population average height is the same for two independent groups from one hospital versus the alternative hypothesis that these two population averages are different, using the following data:\n\nGroup 1: [69.25, 72.80, 68.73, 72.01, 70.36, 71.49, 72.73]\nGroup 2: [67.54, 68.51, 71.84, 70.59, 71.52, 71.50]\n\nYou may assume that the populations from which the data come are each normally distributed, with equal population variances. What conclusion should be drawn, with \\(\\alpha = 0.05\\)?\n\n\n\n\n\n\nPlease attempt this problem using R. Take a look at the information for the t.test function. You will need to set x, y, alternative, and var.equal=T. You can use the below groups coded in R.\n\n\n\n\ngrp1 = c(69.25, 72.80, 68.73, 72.01, 70.36, 71.49, 72.73)\ngrp2 = c(67.54, 68.51, 71.84, 70.59, 71.52, 71.50)\n\n\n\nQuestion 4\nThe choice of an alternative hypothesis (\\(H_A\\) or \\(H_1\\)) should depend primarily on (choose all that apply). Explain your reasoning.\n\nthe data obtained from the study.\nwhat the investigator is interested in determining.\nthe critical region.\nthe significance level.\nthe power of the test.\n\n\n\nQuestion 5\nVisit this site on dplyr.\nFor one of the functions that we have not discussed in class, please use it on the dds.discr dataset. Please write in words what you would like to perform, then write the code.\nNote: the dds.discr dataset is an .Rda file. Instead of using read_csv() or read_excel(), you can use load().\n\n\nQuestion 6\nThe accompanying data CH05Q01 gives the dry weights (Y) of 11 chick embryos ranging in age from 6 to 16 days (X ). Also given in the data are the values of the common logarithms of the weights (Z).\n\nLoad the dataset using the readxl package.\n\nThis readxl package was installed as a part of the tidyverse, however it does not get loaded when you load the tidyverse package and thus you need to do that separately.\nUse the command read_excel(), as shown below\n\n\n\nlibrary(readxl)\n# you might need to update the location of the data file\n# you can choose whatever name you like for the tibble when loading it into R's workspace \nch05q01 &lt;- read_excel(\"./data/CH05Q01.xls\")\n\n\nPart a\nCreate two scatterplots in R using ggplot:\n\nBetween age (X) and dry weight (Y)\nBetween age (X) and log10 dry weight (Z)\n\nObserve the following two scatter diagrams. Describe the relationships between age (X) and dry weight (Y) and between age and log10 dry weight (Z).\n\n\nPart b\nState the population simple linear regression models for these two regressions: Y regressed on X and Z regressed on X.\n\n\n\n\n\n\nThis is asking for the regression models BEFORE you find the values of the coefficients.\n\n\n\n\n\nPart c\nDetermine the least-squares estimates of each of the regression lines in part (b).\n\n\n\n\n\n\nNow get the regression coefficients using R and plug them into the regression models from (b). You can get the coefficients from the R output!\n\n\n\n\n\nPart d\nUsing ggplot, create the respective best-fit lines on your plots. Which of the two regression lines has the better fit? Based on your answers to parts (a)–(c), is it more appropriate to run a linear regression of Y on X or of Z on X? Explain.\n\n\nPart e (to be covered on Monday 1/22)\nFor the regression that you chose as being more appropriate in part (d), find 95% confidence intervals for the true slope and intercept. Interpret each interval with regard to the null hypothesis that the true value is 0.\n\n\n\n\n\n\nYou can get the CI’s from the R output\n\n\n\n\n\n\n\n\n\nUpdate 1/21/2024\n\n\n\nI removed some of the instructions because they were a little confusing. No need to go through the formal process of a hypothesis test.\n\n\n\n\nPart f (to be covered on Monday 1/22)\nFor the regression that you chose as being more appropriate in part (d), add 95% confidence and prediction bands. Using your sketch, find and interpret an approximate 95% confidence interval for the mean response of an 8-day-old chick.\n\n\n\n\n\n\nUpdate 1/21/2024\n\n\n\nI removed prediction bands because we did not cover them!\n\n\n\n\n\n\n\n\nUpdate 1/22/2024\n\n\n\nYou do NOT need to do this problem (Question 6, part f). We did NOT get to it on Monday, so I don’t expect you to do it for the homework."
  },
  {
    "objectID": "homework/HW_01.html#question-7",
    "href": "homework/HW_01.html#question-7",
    "title": "Homework 1",
    "section": "Question 7",
    "text": "Question 7\nGo to the R Documentation of the lm() function. Please answer the following questions about the function arguments and output.\n\nPart a\nIf you wanted to perform weighted least squares in R, what argument would you need to change?\n\n\nPart b\nWhat is the default method in lm()? Can it be used to solve OLS? (Hint: check out the Wiki page for the method)\n\n\nPart c\nIn linear regression, a singular fit means that the estimated standard deviation of your residuals is very close to zero. In the lm() function, will you get an error if you have a singular fit?"
  },
  {
    "objectID": "homework/HW_01.html#question-8",
    "href": "homework/HW_01.html#question-8",
    "title": "Homework 1",
    "section": "Question 8",
    "text": "Question 8\nQuick True/False question: Is ordinary least squares the only way to find the best fit line for linear regression?\nExplain your reasoning."
  },
  {
    "objectID": "homework/HW_04.html",
    "href": "homework/HW_04.html",
    "title": "Homework 4",
    "section": "",
    "text": "Caution\n\n\n\nThis homework is not ready to be worked on!! Nicky needs to update it."
  },
  {
    "objectID": "homework/HW_04.html#directions",
    "href": "homework/HW_04.html#directions",
    "title": "Homework 4",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW4.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: \"BSTA 512/612\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nIf you are computing something by hand, you may take a picture of your work and insert the image in this file. You may also use LaTeX to write it inline.\nWrite all answers in complete sentences as if communicating the results to a collaborator. This means including a sentence summarizing results in the context of the research study.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_04.html#questions",
    "href": "homework/HW_04.html#questions",
    "title": "Homework 4",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nFor this problem we will be using the penguins dataset from the palmerpenguins R package. We will look at the association between flipper length of penguins (measured in mm) and specific species of penguins.\nDescription from help file:\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\n\nMore info about the data.\n\n# first install the palmerpenguins package\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\ndata(penguins)\n\n# run the command below to learn more about the variables in the penguins dataset\n# ?penguins\n\n\nPart a\nCalculate the average flipper lengths stratified by each of the penguin species.\n\n\nPart b\nMake a scatterplot (with jigger) of flipper lengths by species, and include diamond-shape points for the averages of the flipper lengths for each of the species.\n\n\nPart c\nWrite out the fitted regression equation that models the flipper length by penguin species. Use LaTeX math markup or insert an image of your equation. Do not yet insert values for the regression coefficients, i.e. use the generic coefficients \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1\\). Use Adelie as the reference level.\n\n\nPart d\nRun the linear regression of flipper lengths vs. species in R, and display the regression table output. Which species did R choose as the reference level, and how did you determine this?\n\n\nPart e\nHow do we interpret each of the regression coefficients for this model? Write out a separate interpretation for each of the coefficients.\n\n\nPart f\nCalculate the mean flipper length (and the 95% CI) of penguins in the Chinstrap and Gentoo species using the predict() function.\n\n\n\nQuestion 2\nA team of nutrition experts investigated the influence of protein content in diet on the relationship between age (explanatory variable) and height (outcome, in centimeters) for children. Using the dataset, CH12Q03.xls, answer the following questions.\nThis question was adapted from this textbook.\n\nPart a\nUsing R, make a variable that is a factor for Diet. Make sure to check what values the original variable for Diet can take. How many indicator functions do you need to represent the categorical variable Diet (protein-rich vs. protein-poor)?\n\n\nPart b\nAt a level of significance \\(\\alpha = 0.10\\), test whether protein diet modifies the effect of age on height. Justify your answer (e.g., perform a hypothesis test for the interaction between diet and age).\nNote: recall that an effect modifier is an interaction.\n\n\nPart c\nIs it possible that diet is a confounder? Note: this will depend on your results from Part b.\n\n\nPart d\nWrite the fitted regression equation for our model in Part b. Write the respective regression lines for each specific diet group: protein rich and protein poor. Interpret the slope of each regression line (no need for a 95% CI here).\n\n\n\nQuestion 3\nAn experiment was conducted regarding a quantitative analysis of factors found in high-density lipoprotein (HDL) in a sample of human blood serum. Three variables thought to be predictive of, or associated with, HDL measurement (Y) were the total cholesterol (X1) and total triglyceride (X2) concentrations in the sample, plus the presence or absence of a certain sticky component of the serum called sinking pre-beta or SPB (X3), coded as 0 if absent and 1 if present. Using the dataset, CH09Q05.xls, answer the following questions.\n\nPart a\nUse \\(\\alpha= 0.05\\), test whether if there is a crude association between HDL measurement and total cholesterol. Note: testing for a crude association means we fit a simple linear regression model and see if the association is significant.\n\n\nPart b\nSometimes simple linear regression leads us to believe that there is no association between two variables, but missing interaction might be obscuring the association. Use \\(\\alpha= 0.1\\) to test whether total triglyceride is an effect modifier of the association between HDL and total cholesterol.\nNote: Since the data frame has the variables named as \\(Y\\), \\(X1\\), and \\(X2\\), you may use those in the regression equations, but when you are making a conclusion, please use the specific names of the variables to identify each. For example, \\(Y\\) is actually HDL.\nNote: The plan is to cover interactions between two continuous covariates on Wednesday 2/21. We will not interpret the interaction coefficient for this, but we can test the interaction coefficient. Similar to the interaction for a binary covariate and a continuous covariate, we only need to test one coefficient, \\(\\beta_3\\).\n\n\nPart c\nIs it possible that total triglyceride is a confounder? No need to test this explicity.\nNote: this will depend on your results from Part b."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf_key_info.html#key-dates",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html",
    "title": "Lesson 10: Categorical Covariates",
    "section": "",
    "text": "Understand why we need a new way to code categorical variables compared to continuous variables\nWrite the regression equation for a categorical variable using reference cell coding\nCalculate and interpret coefficients for reference cell coding\nChange the reference level in a categorical variable for reference cell coding\nCreate new variables and interpret coefficient for ordinal / scoring coding\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#lets-map-that-to-our-regression-analysis-process",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#lets-map-that-to-our-regression-analysis-process",
    "title": "Lesson 10: Categorical Covariates",
    "section": "",
    "text": "Model Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#still-looking-at-gapminder-life-expectancy-data",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#still-looking-at-gapminder-life-expectancy-data",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Still looking at Gapminder Life Expectancy data",
    "text": "Still looking at Gapminder Life Expectancy data\n\nWe will look at life expectancy vs. these world regions\nGapminder uses four world regions\n\nAfrica\nThe Americas\nAsia\nEurope"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#linear-regression-with-a-categorical-covariate",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#linear-regression-with-a-categorical-covariate",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Linear regression with a categorical covariate",
    "text": "Linear regression with a categorical covariate\n\n\nBad option for visualization:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_point() +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nGood option for visualization:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_jitter(size = 1, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\",\n       caption = \"Diamonds = region averages\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nGood option for visualization:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_boxplot() +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#linear-regression-with-a-categorical-covariate-1",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#linear-regression-with-a-categorical-covariate-1",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Linear regression with a categorical covariate",
    "text": "Linear regression with a categorical covariate\n\n\n\nWhen using a categorical covariate/predictor (that is not ordered),\n\nWe do NOT, technically, find a best-fit line\n\nInstead we model the means of the outcome\n\nFor the different levels of the categorical variable\n\nIn 511, we used Kruskal-Wallis test and our ANOVA table to test if groups means were statistically different from one another\nWe can do this using linear models AND we can include other variable in the model"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#there-are-different-ways-to-code-categorical-variables",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#there-are-different-ways-to-code-categorical-variables",
    "title": "Lesson 10: Categorical Covariates",
    "section": "There are different ways to code categorical variables",
    "text": "There are different ways to code categorical variables\n\nReference cell coding (sometimes called dummy coding)\n\nCompares each level of a variable to the omitted (reference) level\n\nEffect coding (sometimes called sum coding or deviation coding)\n\nCompares deviations from the grand mean\n\nOrdinal encoding (sometimes called scoring)\n\nCategories have a natural, even spaced ordering\n\n\nIf you want to learn more about these and other coding schemes:\n\nCoding Systems for Categorical Variables in Regression Analysis\nCategorical Data Encoding Techniques\nCoding Schemes for Categorical Variables"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#building-the-regression-equation-problem-with-a-single-coefficient",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#building-the-regression-equation-problem-with-a-single-coefficient",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Building the regression equation: problem with a single coefficient",
    "text": "Building the regression equation: problem with a single coefficient\n\n\nPreviously: simple linear regression\n\nOutcome \\(Y\\) = numerical variable\nPredictor \\(X\\) = numerical variable\n\nThe regression (best-fit) line is: \\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X \\]\n\nNew: what if the explanatory variable is categorical?\nNaively, we could write: \\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\)\nOr, with our variables: \\[\\widehat{\\textrm{LE}} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot \\textrm{WR} \\]\n\nBut what does \\(\\textrm{WR}\\) (world regions) mean in this equation?\n\nWhat values can it take? How do we represent each region?"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#building-the-regression-equation-how-do-we-map-categories-to-means",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#building-the-regression-equation-how-do-we-map-categories-to-means",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Building the regression equation: how do we map categories to means?",
    "text": "Building the regression equation: how do we map categories to means?\n\nIf we only have world region in our model and want to map it to an expected life expectancy…\n\n\n\n\nmeans_LE = gapm2 %&gt;%\n  group_by(four_regions) %&gt;%\n  summarise(mean = mean(LifeExpectancyYrs))\n\n\nWe want to create a function that can map each region to life expectancy\n\nIf in Africa: \\(\\widehat{LE} = 61.32\\)\nIf in the Americas: \\(\\widehat{LE} = 74.64\\)\nIf in Asia: \\(\\widehat{LE} = 71.70\\)\nIf in Europe: \\(\\widehat{LE} = 77.61\\)\n\nCan we make one equation for \\(\\widehat{LE}\\) by putting the “if” statements within the equation?"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#building-the-regression-equation-indicator-functions",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#building-the-regression-equation-indicator-functions",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Building the regression equation: Indicator functions",
    "text": "Building the regression equation: Indicator functions\n\nIn order to represent each region in the equation, we need to introduce a new function:\n\nIndicator function:\n\n\\[I(X = x) \\text{ or } I(x) =\n\\left\\{\n\\begin{array}{@{}ll@{}}\n1, & \\text{if}\\ X = x \\\\\n  0, & \\text{else}\n\\end{array}\\right. \\]\n\nThis basically a binary yes/no if \\(X\\) is a specific value \\(x\\)\n\nFor example, if we want to identify a country as being in the Americas region, we can make:\n\\[I(WR = \\text{Americas}) \\text{ or }I(\\text{Americas}) =\n\\left\\{\n\\begin{array}{@{}ll@{}}\n1, & \\text{if}\\ WR = \\text{Americas} \\\\\n  0, & \\text{else}\n\\end{array}\\right. \\]"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#poll-everywhere-question-1",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#poll-everywhere-question-1",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#building-the-regression-equation-indicators-in-our-equation",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#building-the-regression-equation-indicators-in-our-equation",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Building the regression equation: Indicators in our equation",
    "text": "Building the regression equation: Indicators in our equation\n\n\n\\[\\begin{aligned}\n\\widehat{\\textrm{LE}} = & 61.32 \\cdot I(\\text{Africa}) + 74.64 \\cdot I(\\text{Americas}) + \\\\ &71.7 \\cdot I(\\text{Asia}) + 77.61 \\cdot I(\\text{Europe})\n\\end{aligned}\\]\n\nHowever, a linear regression equation still requires an intercept!\n\nSo one of our regions need to become our “reference” group\nWe’ll use Africa as our reference\nThat means we need to adjust all the numbers\n\n\n\\[\\begin{aligned}\n\\widehat{\\textrm{LE}} = & 61.32 + 13.32 \\cdot I(\\text{Americas}) + \\\\ &10.38 \\cdot I(\\text{Asia}) + 16.29 \\cdot I(\\text{Europe}) \\\\\n\\widehat{\\textrm{LE}} = & \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) + \\\\ & \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#viewing-the-regression-equation-another-way",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#viewing-the-regression-equation-another-way",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Viewing the regression equation another way",
    "text": "Viewing the regression equation another way\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) + \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\\]\n\n\n\n\n\n\n\n\n\n\nWorld region\nRegression equation for WR\nAverage Life Expectancy for WR\n\n\n\n\nAfrica\n\\(\\begin{aligned} \\widehat{\\textrm{LE}} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 0 + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 \\end{aligned}\\)\n\\(\\widehat{\\textrm{LE}} = \\widehat\\beta_0\\)\n\n\nAmericas\n\\(\\begin{aligned} \\widehat{\\textrm{LE}} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 1+ \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 \\end{aligned}\\)\n\\(\\widehat{\\textrm{LE}} = \\widehat\\beta_0 + \\widehat\\beta_1\\)\n\n\nAsia\n\\(\\begin{aligned} \\widehat{\\textrm{LE}} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 0 + \\\\ & \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 \\cdot 0 \\end{aligned}\\)\n\\(\\widehat{\\textrm{LE}} = \\widehat\\beta_0 + \\widehat\\beta_2\\)\n\n\nEurope\n\\(\\begin{aligned} \\widehat{\\textrm{LE}} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 0 + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 1 \\end{aligned}\\)\n\\(\\widehat{\\textrm{LE}} = \\widehat\\beta_0 + \\widehat\\beta_3\\)"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#interpretation-of-regression-equation-coefficients",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#interpretation-of-regression-equation-coefficients",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Interpretation of regression equation coefficients",
    "text": "Interpretation of regression equation coefficients\n\nRemember: expected, mean, and average are interchangeable\n\n\n\n\n\n\n\n\nCoefficient\nInterpretation\n\n\n\n\n\\(\\widehat{\\beta}_0\\)\nExpected/mean/average life expectancy of Africa\n\n\n\\(\\widehat{\\beta}_1\\)\nDifference in mean life expectancy of the Americas and Africa -OR-\nMean difference in life expectancy of the Americas and Africa\n\n\n\\(\\widehat{\\beta}_2\\)\nDifference in mean life expectancy between Asia and Africa -OR-\nMean difference in life expectancy between Asia and Africa\n\n\n\\(\\widehat{\\beta}_3\\)\nDifference in mean life expectancy between Europe and Africa -OR-\nMean difference in life expectancy between Europe and Africa"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#poll-everywhere-question-2",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#poll-everywhere-question-2",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#regression-table-with-lm-function",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#regression-table-with-lm-function",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Regression table with lm() function",
    "text": "Regression table with lm() function\n\nmodel1 &lt;- lm(LifeExpectancyYrs ~ four_regions, data = gapm2)\ntidy(model1, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 38) %&gt;% \n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n61.32\n0.76\n80.26\n0.00\n59.81\n62.83\n    four_regionsAmericas\n13.32\n1.23\n10.83\n0.00\n10.89\n15.74\n    four_regionsAsia\n10.38\n1.08\n9.61\n0.00\n8.25\n12.51\n    four_regionsEurope\n16.29\n1.13\n14.37\n0.00\n14.05\n18.52\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{LE}} = 61.32 + 13.32 \\cdot I(\\text{Americas}) + 10.38 \\cdot I(\\text{Asia}) + 16.29 \\cdot I(\\text{Europe})\\]\n\nWhich world region did R choose as the reference level?\nHow you would calculate the mean life expectancies of world regions using only the results from the regression table?"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#bringing-in-the-numbersunits95-ci",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#bringing-in-the-numbersunits95-ci",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Bringing in the numbers/units/95% CI",
    "text": "Bringing in the numbers/units/95% CI\n\n\n\n\n\n\n\nCoefficient\nInterpretation\n\n\n\n\n\\(\\widehat{\\beta}_0\\)\nAverage life expectancy of countries in Africa is 61.32 years (95% CI: 59.81, 62.83).\n\n\n\\(\\widehat{\\beta}_1\\)\nThe difference in mean life expectancy between countries in the Americas and Africa is 13.32 (95% CI: 10.89, 15.74).\n\n\n\\(\\widehat{\\beta}_2\\)\nThe difference in mean life expectancy between countries in the Americas and Africa is 10.38 (95% CI: 8.25, 12.51).\n\n\n\\(\\widehat{\\beta}_3\\)\nThe difference in mean life expectancy between countries in Europe and Africa is 18.52 (95% CI: 14.05, 18.52).\n\n\n\n \n\nDon’t forget that we can use the confidence intervals to assess whether the mean difference with Africa is significant or not"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#we-can-also-use-r-to-report-each-regions-average-life-expectancy",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#we-can-also-use-r-to-report-each-regions-average-life-expectancy",
    "title": "Lesson 10: Categorical Covariates",
    "section": "We can also use R to report each region’s average life expectancy",
    "text": "We can also use R to report each region’s average life expectancy\nFind the 95% CI’s for the mean life expectancy for the Americas, Asia, and Europe\n\nUse the base R predict() function (see Lesson 4 for more info)\nRequires specification of a newdata “value”\n\n\nnewdata &lt;- data.frame(four_regions = c(\"Africa\", \"Americas\", \"Asia\", \"Europe\")) \n\n\n\n\n(pred = predict(model1, \n                newdata=newdata, \n                interval=\"confidence\"))\n\n       fit      lwr      upr\n1 61.32037 59.81287 62.82787\n2 74.63824 72.73841 76.53806\n3 71.70185 70.19435 73.20935\n4 77.60889 75.95751 79.26027\n\n\n\n\n\nInterpretations\n\n\n\nThe average life expectancy for countries in the Americas is 74.64 years (95% CI: 72.74, 76.54).\nThe average life expectancy for countries in Asia is 71.7 years (95% CI: 70.19, 73.21).\nThe average life expectancy for countries in Europe is 77.61 years (95% CI: 75.96, 79.26)."
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#another-way-to-look-at-coefficient-values",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#another-way-to-look-at-coefficient-values",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Another way to look at coefficient values",
    "text": "Another way to look at coefficient values\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) +  \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\\]\n\n\nCode\n# means of each level of `four_regions`\ngapm2_ave &lt;- gapm2 %&gt;% \n  group_by(four_regions) %&gt;% \n  summarise(\n    life_ave = mean(LifeExpectancyYrs))\n\n# mean of `africa`\nmean_africa &lt;- gapm2_ave %&gt;% \n  filter(four_regions == \"Africa\") %&gt;% \n  pull(life_ave)\n\n# differences in means between levels of `four_regions` and `africa`\ngapm2_ave_diff &lt;- gapm2_ave %&gt;% \n  mutate(`Difference with Africa` = life_ave - mean_africa) %&gt;%\n  rename(`World regions` = four_regions, \n         `Average life expectancy` = life_ave)\n\n# At the beginning of the Rmd we loaded knitr, which is where the kable command is from\n# library(knitr)\ngapm2_ave_diff %&gt;% kable(\n  digits = 1,\n  format = \"markdown\"\n  ) \n\n\n\n\n\nWorld regions\nAverage life expectancy\nDifference with Africa\n\n\n\n\nAfrica\n61.3\n0.0\n\n\nAmericas\n74.6\n13.3\n\n\nAsia\n71.7\n10.4\n\n\nEurope\n77.6\n16.3\n\n\n\n\n\n\\[\\widehat{\\textrm{LE}} = 61.32 + 13.32 \\cdot I(\\text{Americas}) + 10.38 \\cdot I(\\text{Asia}) + 16.29 \\cdot I(\\text{Europe})\\]"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#reference-levels",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#reference-levels",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Reference levels",
    "text": "Reference levels\nWhy is Africa not one of the variables in the regression equation?\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) +  \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\\]\n\nCategorical variables have to have at least 2 levels. If they have 2 levels, we call them binary\n\n \n\nWe choose one level as our reference level to which all other levels of the categorical variable are compared\n\nThe levels \\(\\text{Americas}, \\text{Asia}, \\text{Europe}\\) are compared to the level \\(\\text{Africa}\\)\n\n\n \n\nThe intercept of the regression equation is the mean of the outcome restricted to the reference level\n\nRecall that the intercept is the mean life expectancy of Africa, which was our reference level\n\n\n \n\nIf the categorical variable has \\(r\\) levels, then we need \\(r-1\\) variables/coefficients to model it!"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#we-can-change-the-reference-level-to-europe-12",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#we-can-change-the-reference-level-to-europe-12",
    "title": "Lesson 10: Categorical Covariates",
    "section": "We can change the reference level to Europe (1/2)",
    "text": "We can change the reference level to Europe (1/2)\n\nSuppose we want to compare the mean life expectancies of world regions to the \\(\\text{Europe}\\) level instead of \\(\\text{Africa}\\)\nBelow is the estimated regression equation for when \\(Africa\\) is the reference level\n\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) +  \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\\]\n\nUpdate the variables to make \\(Europe\\) the reference level:\n\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Africa}) +  \\widehat\\beta_2 \\cdot I(\\text{Americas}) + \\widehat\\beta_3 \\cdot I(\\text{Asia})\\]"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#we-can-change-the-reference-level-to-europe-22",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#we-can-change-the-reference-level-to-europe-22",
    "title": "Lesson 10: Categorical Covariates",
    "section": "We can change the reference level to Europe (2/2)",
    "text": "We can change the reference level to Europe (2/2)\n\nNow update the coefficients of the regression equation using the output below.\n\n\n\n\n\n\nWorld regions\nAverage life expectancy\nDifference with Europe\n\n\n\n\nAfrica\n61.32\n-16.29\n\n\nAmericas\n74.64\n-2.97\n\n\nAsia\n71.70\n-5.91\n\n\nEurope\n77.61\n0.00\n\n\n\n\n\n\\[\\widehat{\\textrm{LE}} = 77.61 -16.29 \\cdot I(\\text{Africa}) -2.97 \\cdot I(\\text{Americas}) -5.91 \\cdot I(\\text{Asia})\\]"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#r-change-reference-level-to-europe-12",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#r-change-reference-level-to-europe-12",
    "title": "Lesson 10: Categorical Covariates",
    "section": "R: Change reference level to europe (1/2)",
    "text": "R: Change reference level to europe (1/2)\n\nfour_regions data type was originally a character - check this with str()\n\n\nstr(gapm$four_regions) \n\n chr [1:195] \"asia\" \"europe\" \"africa\" \"europe\" \"africa\" \"americas\" ...\n\n\n\nIn order to change the reference level, we need to convert it to data type factor\n\nI also did this at the beginning to capitalize each region\n\n\n\ngapm_ex = gapm %&gt;% \n mutate(four_regions = factor(four_regions, \n                              levels = c(\"africa\", \"americas\", \"asia\", \"europe\"), \n                              labels = c(\"Africa\", \"Americas\", \"Asia\", \"Europe\")))\nstr(gapm_ex$four_regions) \n\n Factor w/ 4 levels \"Africa\",\"Americas\",..: 3 4 1 4 1 2 2 4 3 4 ...\n\nlevels(gapm_ex$four_regions) # order of factor levels\n\n[1] \"Africa\"   \"Americas\" \"Asia\"     \"Europe\""
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#r-change-reference-level-to-europe-22",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#r-change-reference-level-to-europe-22",
    "title": "Lesson 10: Categorical Covariates",
    "section": "R: Change reference level to europe (2/2)",
    "text": "R: Change reference level to europe (2/2)\n\nNow change the order of the factor levels\nCode below uses fct_relevel() from the forcats package that gets loaded as a part of the tidyverse\nAny levels not mentioned will be left in their existing order, after the explicitly mentioned levels.\n\n\ngapm2 &lt;- gapm2 %&gt;% \n  mutate(four_regions = \n      fct_relevel(four_regions, \"Europe\"))\n\nlevels(gapm2$four_regions)\n\n[1] \"Europe\"   \"Africa\"   \"Americas\" \"Asia\""
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#r-run-model-with-europe-as-the-reference-level",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#r-run-model-with-europe-as-the-reference-level",
    "title": "Lesson 10: Categorical Covariates",
    "section": "R: Run model with europe as the reference level",
    "text": "R: Run model with europe as the reference level\n\nlevels(gapm2$four_regions)\n\n[1] \"Europe\"   \"Africa\"   \"Americas\" \"Asia\"    \n\nmodel2 &lt;- lm(LifeExpectancyYrs ~ four_regions, data = gapm2)\ntidy(model2) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n77.61\n0.84\n92.72\n0.00\n    four_regionsAfrica\n−16.29\n1.13\n−14.37\n0.00\n    four_regionsAmericas\n−2.97\n1.28\n−2.33\n0.02\n    four_regionsAsia\n−5.91\n1.13\n−5.21\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\\widehat{\\textrm{LE}} &=  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Africa}) +  \\widehat\\beta_2 \\cdot I(\\text{Americas}) + \\widehat\\beta_3 \\cdot I(\\text{Asia}) \\\\ \\widehat{\\textrm{LE}} &= 77.61 -16.29 \\cdot I(\\text{Africa}) -2.97 \\cdot I(\\text{Americas}) -5.91 \\cdot I(\\text{Asia}) \\end{aligned}\\]"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#fitted-values-residuals",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#fitted-values-residuals",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Fitted values & residuals",
    "text": "Fitted values & residuals\n\n\nSimilar to as before:\n\nObserved values \\(y\\) are the values in the dataset\nFitted values \\(\\widehat{y}\\) are the values that fall on the best-fit line for a specific value of x are the means of the outcome stratified by the categorical predictor’s levels\nResiduals \\(y - \\widehat{y}\\) are the differences between the two"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#fitted-values-are-the-same-as-the-means",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#fitted-values-are-the-same-as-the-means",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Fitted values are the same as the means",
    "text": "Fitted values are the same as the means\n\nm1_aug &lt;- augment(model1)\n\nggplot(m1_aug, aes(x = four_regions, y = .fitted)) + geom_point() +\n  theme(axis.text = element_text(size = 22), axis.title = element_text(size = 22))"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#residual-plots-now-the-spread-within-each-region",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#residual-plots-now-the-spread-within-each-region",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Residual plots (now the spread within each region)",
    "text": "Residual plots (now the spread within each region)\n\nggplot(m1_aug, aes(x=.resid)) + geom_histogram() + \n  theme(axis.text = element_text(size = 22), title = element_text(size = 22))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#poll-everywhere-question-3",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#poll-everywhere-question-3",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#lets-look-at-life-expectancy-vs.-four-income-levels",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#lets-look-at-life-expectancy-vs.-four-income-levels",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Let’s look at life expectancy vs. four income levels",
    "text": "Let’s look at life expectancy vs. four income levels\n\nGapminder discusses individual income levels\n\n \n\nIncome levels for a country is based on average GDP per capita, and grouped into:\n\nLow income\nLower middle income\nUpper middle income\nHigh income"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#visualizing-the-ordinal-variable-income-levels",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#visualizing-the-ordinal-variable-income-levels",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Visualizing the ordinal variable, income levels",
    "text": "Visualizing the ordinal variable, income levels\n\n\n\n\n\n\n\n\n\n\n\n\nA few changes needed:\n\nPut the income levels in order\n\n\ngapm2 = gapm2 %&gt;%\n mutate(income_levels = factor(income_levels, \n            ordered = T, \n            levels = c(\"Low income\", \n            \"Lower middle income\", \n            \"Upper middle income\", \n            \"High income\")))\n\n\nMake the income levels readable\n\nHow to Rotate Axis Labels in ggplot2?"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#much-better-visualizing-the-ordinal-variable-income-levels",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#much-better-visualizing-the-ordinal-variable-income-levels",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Much better: Visualizing the ordinal variable, income levels",
    "text": "Much better: Visualizing the ordinal variable, income levels\n\n\n\nggplot(gapm2, aes(x = income_levels, y = LifeExpectancyYrs)) +\n  geom_jitter(size = 1, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  labs(x = \"Income levels\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. income levels\",\n       caption = \"Diamonds = Income level averages\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20), \n        axis.text.x=element_text(angle = 20, vjust = 1, hjust=1))"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#how-can-we-code-this-variable",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#how-can-we-code-this-variable",
    "title": "Lesson 10: Categorical Covariates",
    "section": "How can we code this variable?",
    "text": "How can we code this variable?\nWe have two options:\n\n\n\n\nTreat the levels as nominal, and use reference cell coding\n\n\n\nLike we did with world regions\nThis option will not break the linearity assumption\nFor \\(g\\) categories of the variable, we will have \\(g-1\\) coefficients to estimate\n\n\n\n\n\n\nUse the ordinal values to score the levels and treat as a numerical variable\n\n\n\nEven if a variable is inherently ordered, we need to check that linearity holds if categories are represented as numbers\nThis way of coding preserves more power in the model (less coefficients to estimate means more power)\nOnly one coefficient to estimate"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#some-important-considerations-when-scoring-ordinal-variables",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#some-important-considerations-when-scoring-ordinal-variables",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Some important considerations when scoring ordinal variables",
    "text": "Some important considerations when scoring ordinal variables\n\nEven if a variable is inherently ordered, we need to check that linearity holds if categories are represented as numbers\n\n \n\nAssumes differences between adjacent groups are equal\n\nIncome levels are pre-set groups by Gapminder\nMight be hard to interpret “every 1-level increase in income level”\n\n\n \n\nIs the variable part of the main relationship that you are investigating? (even if linearity holds)\n\nIf yes, consider leaving as reference cell coding unless the interpretation makes sense\nIf no, and just needed as an adjustment in your model, then power benefit of scoring might be worth it!"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#check-that-linearity-holds-for-income-levels",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#check-that-linearity-holds-for-income-levels",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Check that linearity holds for income levels",
    "text": "Check that linearity holds for income levels\n\n\n\nUsing visual assessment, linearity holds for our income levels\nWe can use the ordinal encoding for income levels\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#poll-everywhere-question-4",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#poll-everywhere-question-4",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#ordinal-coding-scoring",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#ordinal-coding-scoring",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Ordinal coding / Scoring",
    "text": "Ordinal coding / Scoring\n\nMap each income level to a number\nUsually start at 1\n\n\n\n\nIncome Level\nScore\n\n\n\n\nLow income\n1\n\n\nLower middle income\n2\n\n\nUpper middle income\n3\n\n\nHigh income\n4\n\n\n\n\ngapm2 = gapm2 %&gt;%\n  mutate(income_num = as.numeric(income_levels))\nstr(gapm2$income_num)\n\n num [1:187] 1 3 3 4 2 4 3 2 4 4 ..."
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#run-the-model-with-the-scored-income",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#run-the-model-with-the-scored-income",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Run the model with the scored income",
    "text": "Run the model with the scored income\n\nmod_inc2 = lm(LifeExpectancyYrs ~ income_num, data = gapm2)\ntidy(mod_inc2) %&gt;% gt() %&gt;% tab_options(table.font.size = 37) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n54.01\n1.06\n51.03\n0.00\n    income_num\n6.25\n0.37\n16.91\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{\\textrm{LE}} &=  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot \\text{Income level} \\\\\n\\widehat{\\textrm{LE}} &=  54.01 + 6.25 \\cdot \\text{Income level}\n\\end{aligned}\\]\n\nKeep in mind: We cannot calculate the expected outcome outside of the scoring values\n\nFor example, we cannot find the mean life expectancy for an income level of 1.5"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#interpreting-the-model",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#interpreting-the-model",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Interpreting the model",
    "text": "Interpreting the model\n\ntidy(mod_inc2, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 37) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n54.01\n1.06\n51.03\n0.00\n51.92\n56.10\n    income_num\n6.25\n0.37\n16.91\n0.00\n5.52\n6.98\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{LE}} =  54.01 + 6.25 \\cdot \\text{Income level}\\]\n\nInterpreting the intercept: At an income level of 0, mean life expectancy is 54.01 (95% CI: 51.92, 56.10).\n\nNote: this does not make sense because there is no income level of 0!\n\nInterpreting the coefficient for income: For every 1-level increase in income level, mean life expectancy increases 6.25 years (95% CI: 5.52, 6.98)."
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#what-if-life-expectancy-vs.-income-level-looked-like-this",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#what-if-life-expectancy-vs.-income-level-looked-like-this",
    "title": "Lesson 10: Categorical Covariates",
    "section": "What if life expectancy vs. income level looked like this?",
    "text": "What if life expectancy vs. income level looked like this?\n\n\n\nset.seed(40)\ngapm2 = gapm2 %&gt;% rowwise() %&gt;% \n  mutate(life_exp_sim = rnorm(1, mean = 23*log(8*(income_num-0.7)), sd=5))\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nNo longer maintaining the linearity assumption\nNeed to use reference cell coding\n\n \n\nWe would fit the following model: \\[\\begin{aligned}\n\\textrm{LE} = & \\beta_0 + \\beta_1 \\cdot I(\\text{Lower middle income}) + \\\\ & \\beta_2 \\cdot I(\\text{Upper middle income}) + \\\\ & \\beta_3 \\cdot I(\\text{High income}) + \\epsilon\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#if-time",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#if-time",
    "title": "Lesson 10: Categorical Covariates",
    "section": "If time…",
    "text": "If time…\nLet’s walk through categorical variables that have multiple selections\n\nSo each group is not mutually exclusive\nWe could make an indicator for each category, but individuals could be a part of multiple categories\n\n \n\nAlso, thinking about income levels - can we combine two groups to make one??"
  },
  {
    "objectID": "lessons/10_Cat_covariates/01_Review_key_info.html#key-dates",
    "href": "lessons/10_Cat_covariates/01_Review_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html",
    "href": "lessons/03_SLR/03_SLR.html",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "",
    "text": "Identify the aims of your research and see how they align with the intended purpose of simple linear regression\nIdentify the simple linear regression model and define statistics language for key notation\nIllustrate how ordinary least squares (OLS) finds the best model parameter estimates\nSolve the optimal coefficient estimates for simple linear regression using OLS\nApply OLS in R for simple linear regression of real data\n\n\n\n\n\n\n\nRows: 188 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (2): life_expectancy_years_2011, female_literacy_rate_2011\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n \nAverage life expectancy vs. female literacy rate\n \n\nEach point on the plot is for a different country\n\n \n\n\\(X\\) = country’s adult female literacy rate\n\n \n\n\\(Y\\) = country’s average life expectancy (years)\n\n\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]\n\n\n\n\n\n\nggplot(gapm, aes(x = female_literacy_rate_2011,\n                 y = life_expectancy_years_2011)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 3, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nData files\n\nCleaned: lifeexp_femlit_2011.csv\nNeeds cleaning: lifeexp_femlit_water_2011.csv\n\nData were downloaded from Gapminder\n2011 is the most recent year with the most complete data\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same.\nAdult literacy rate is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life.\n\n\n\n\n\nLoad data\n\n\ngapm_original &lt;- read_csv(here::here(\"data\", \"lifeexp_femlit_water_2011.csv\"))\n\nRows: 194 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, water_2011_quart\ndbl (3): life_expectancy_years_2011, female_literacy_rate_2011, water_basic_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nGlimpse of the data\n\n\nglimpse(gapm_original)\n\nRows: 194\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andor…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9, 76.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.9, 99.5,…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 92.6, 100.0, 40.3, 97.0, 99.5, …\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q2\", \"Q4\", \"Q1\", \"Q3\", \"Q4\", \"…\n\n\n\nNote the missing values for our variables of interest\n\n\n\n\n\nGet a sense of the summary statistics\n\n\ngapm_original %&gt;% \n  select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  summary()\n\n life_expectancy_years_2011 female_literacy_rate_2011\n Min.   :47.50              Min.   :13.00            \n 1st Qu.:64.30              1st Qu.:70.97            \n Median :72.70              Median :91.60            \n Mean   :70.66              Mean   :81.65            \n 3rd Qu.:76.90              3rd Qu.:98.03            \n Max.   :82.90              Max.   :99.80            \n NA's   :7                  NA's   :114              \n\n\n\n\n\n\nRemove rows with missing data for life expectancy and female literacy rate\n\n\ngapm &lt;- gapm_original %&gt;% \n  drop_na(life_expectancy_years_2011, female_literacy_rate_2011)\n\nglimpse(gapm)\n\nRows: 80\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\", \"Antigu…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8, 96.7, 9…\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q1\", \"Q3\", \"Q4\", \"Q3\", \"Q3\", \"…\n\n\n\nNo missing values now for our variables of interest\n\n\n\n\n\nAnd no more missing values when we look only at our two variables of interest\n\n\ngapm %&gt;% select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 life_expec…    80    48  81.8   72.4  65.9  75.8  9.95  6.30  69.9  7.95 0.889\n2 female_lit…    80    13  99.8   91.6  71.0  98.0 27.0  11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;\n\n\n\n\nNote\n\n\n\nRemoving the rows with missing data was not needed to run the regression model.\nI did this step since later we will be calculating the standard deviations of the explanatory and response variables for just the values included in the regression model. It’ll be easier to do this if we remove the missing values now."
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#lets-start-with-an-example",
    "href": "lessons/03_SLR/03_SLR.html#lets-start-with-an-example",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Let’s start with an example",
    "text": "Let’s start with an example\n\n\n\n\n\n\n\n\n \nAverage life expectancy vs. female literacy rate\n \n\nEach point on the plot is for a different country\n\n \n\n\\(X\\) = country’s adult female literacy rate\n\n \n\n\\(Y\\) = country’s average life expectancy (years)\n\n\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#reference-how-did-i-code-that",
    "href": "lessons/03_SLR/03_SLR.html#reference-how-did-i-code-that",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Reference: How did I code that?",
    "text": "Reference: How did I code that?\n\n\nggplot(gapm, aes(x = female_literacy_rate_2011,\n                 y = life_expectancy_years_2011)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 3, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#dataset-description",
    "href": "lessons/03_SLR/03_SLR.html#dataset-description",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Dataset description",
    "text": "Dataset description\n\nData files\n\nCleaned: lifeexp_femlit_2011.csv\nNeeds cleaning: lifeexp_femlit_water_2011.csv\n\nData were downloaded from Gapminder\n2011 is the most recent year with the most complete data\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same.\nAdult literacy rate is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life."
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#get-to-know-the-data-12",
    "href": "lessons/03_SLR/03_SLR.html#get-to-know-the-data-12",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Get to know the data (1/2)",
    "text": "Get to know the data (1/2)\n\nLoad data\n\n\ngapm_original &lt;- read_csv(here::here(\"data\", \"lifeexp_femlit_water_2011.csv\"))\n\n\nGlimpse of the data\n\n\nglimpse(gapm_original)\n\nRows: 194\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andor…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9, 76.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.9, 99.5,…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 92.6, 100.0, 40.3, 97.0, 99.5, …\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q2\", \"Q4\", \"Q1\", \"Q3\", \"Q4\", \"…\n\n\n\nNote the missing values for our variables of interest"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#get-to-know-the-data-22",
    "href": "lessons/03_SLR/03_SLR.html#get-to-know-the-data-22",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Get to know the data (2/2)",
    "text": "Get to know the data (2/2)\n\nGet a sense of the summary statistics\n\n\ngapm_original %&gt;% \n  select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  summary()\n\n life_expectancy_years_2011 female_literacy_rate_2011\n Min.   :47.50              Min.   :13.00            \n 1st Qu.:64.30              1st Qu.:70.97            \n Median :72.70              Median :91.60            \n Mean   :70.66              Mean   :81.65            \n 3rd Qu.:76.90              3rd Qu.:98.03            \n Max.   :82.90              Max.   :99.80            \n NA's   :7                  NA's   :114"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#remove-missing-values-12",
    "href": "lessons/03_SLR/03_SLR.html#remove-missing-values-12",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Remove missing values (1/2)",
    "text": "Remove missing values (1/2)\n\nRemove rows with missing data for life expectancy and female literacy rate\n\n\ngapm &lt;- gapm_original %&gt;% \n  drop_na(life_expectancy_years_2011, female_literacy_rate_2011)\n\nglimpse(gapm)\n\nRows: 80\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\", \"Antigu…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8, 96.7, 9…\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q1\", \"Q3\", \"Q4\", \"Q3\", \"Q3\", \"…\n\n\n\nNo missing values now for our variables of interest"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#remove-missing-values-22",
    "href": "lessons/03_SLR/03_SLR.html#remove-missing-values-22",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Remove missing values (2/2)",
    "text": "Remove missing values (2/2)\n\nAnd no more missing values when we look only at our two variables of interest\n\n\ngapm %&gt;% select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 life_expec…    80    48  81.8   72.4  65.9  75.8  9.95  6.30  69.9  7.95 0.889\n2 female_lit…    80    13  99.8   91.6  71.0  98.0 27.0  11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;\n\n\n\n\nNote\n\n\n\nRemoving the rows with missing data was not needed to run the regression model.\nI did this step since later we will be calculating the standard deviations of the explanatory and response variables for just the values included in the regression model. It’ll be easier to do this if we remove the missing values now."
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#questions-we-can-ask-with-a-simple-linear-regression-model",
    "href": "lessons/03_SLR/03_SLR.html#questions-we-can-ask-with-a-simple-linear-regression-model",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Questions we can ask with a simple linear regression model",
    "text": "Questions we can ask with a simple linear regression model\n\n\n\n\n\n\n\n\n\nHow do we…\n\ncalculate slope & intercept?\ninterpret slope & intercept?\ndo inference for slope & intercept?\n\nCI, p-value\n\ndo prediction with regression line?\n\nCI for prediction?\n\n\nDoes the model fit the data well?\n\nShould we be using a line to model the data?\n\nShould we add additional variables to the model?\n\nmultiple/multivariable regression\n\n\n\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#association-vs.-prediction",
    "href": "lessons/03_SLR/03_SLR.html#association-vs.-prediction",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Association vs. prediction",
    "text": "Association vs. prediction\n\n\n\n\nAssociation\n\n\n\nWhat is the association between countries’ life expectancy and female literacy rate?\nUse the slope of the line or correlation coefficient\n\n\n\n\n\n\nPrediction\n\n\n\nWhat is the expected average life expectancy for a country with a specified female literacy rate?    \n\n\n\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#three-types-of-study-design-there-are-more",
    "href": "lessons/03_SLR/03_SLR.html#three-types-of-study-design-there-are-more",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Three types of study design (there are more)",
    "text": "Three types of study design (there are more)\n\n\n\n\nExperiment\n\n\n\nObservational units are randomly assigned to important predictor levels\n\nRandom assignment controls for confounding variables (age, gender, race, etc.)\n“gold standard” for determining causality\nObservational unit is often at the participant-level\n\n\n\n\n\n\n\nQuasi-experiment\n\n\n\nParticipants are assigned to intervention levels without randomization\nNot common study design\n\n\n\n\n\n\nObservational\n\n\n\nNo randomization or assignment of intervention conditions\nIn general cannot infer causality\n\nHowever, there are casual inference methods…"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#lets-revisit-the-regression-analysis-process",
    "href": "lessons/03_SLR/03_SLR.html#lets-revisit-the-regression-analysis-process",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Let’s revisit the regression analysis process",
    "text": "Let’s revisit the regression analysis process\n   \n \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nPrediction vs interpretation\nComparing models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nParameter estimation\nInterpret model parameters\nHypothesis tests for coefficients\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of full and reduced models\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-2",
    "href": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-2",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#simple-linear-regression-model",
    "href": "lessons/03_SLR/03_SLR.html#simple-linear-regression-model",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nThe (population) regression model is denoted by:\n \n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n \n\n\nObservable sample data\n\n\\(Y\\) is our dependent variable\n\nAka outcome or response variable\n\n\\(X\\) is our independent variable\n\nAka predictor, regressor, exposure variable\n\n\n\nUnobservable population parameters\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable with a…\n\nNormal distribution with mean 0 and constant variance \\(\\sigma^2\\)\ni.e. \\(\\epsilon \\sim N(0, \\sigma^2)\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#simple-linear-regression-model-another-way-to-view-components",
    "href": "lessons/03_SLR/03_SLR.html#simple-linear-regression-model-another-way-to-view-components",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Simple Linear Regression Model (another way to view components)",
    "text": "Simple Linear Regression Model (another way to view components)\nThe (population) regression model is denoted by:\n \n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n \nComponents\n\n\n\n\\(Y\\)\nresponse, outcome, dependent variable\n\n\n\\(\\beta_0\\)\nintercept\n\n\n\\(\\beta_1\\)\nslope\n\n\n\\(X\\)\npredictor, covariate, independent variable\n\n\n\\(\\epsilon\\)\nresiduals, error term"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#if-the-population-parameters-are-unobservable-how-did-we-get-the-line-for-life-expectancy",
    "href": "lessons/03_SLR/03_SLR.html#if-the-population-parameters-are-unobservable-how-did-we-get-the-line-for-life-expectancy",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "If the population parameters are unobservable, how did we get the line for life expectancy?",
    "text": "If the population parameters are unobservable, how did we get the line for life expectancy?\n\n\n \n\nNote: the population model is the true, underlying model that we are trying to estimate using our sample data\n\nOur goal in simple linear regression is to estimate \\(\\beta_0\\) and \\(\\beta_1\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-3",
    "href": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-3",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#okay-so-how-do-we-estimate-the-regression-line",
    "href": "lessons/03_SLR/03_SLR.html#okay-so-how-do-we-estimate-the-regression-line",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Okay, so how do we estimate the regression line?",
    "text": "Okay, so how do we estimate the regression line?\n \nAt this point, we are going to move over to an R shiny app that I made.\n \nLet’s see if we can eyeball the best-fit line!"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#regression-line-best-fit-line",
    "href": "lessons/03_SLR/03_SLR.html#regression-line-best-fit-line",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Regression line = best-fit line",
    "text": "Regression line = best-fit line\n\n\n\\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X \\]\n\n\\(\\widehat{Y}\\) is the predicted outcome for a specific value of \\(X\\)\n\\(\\widehat{\\beta}_0\\) is the intercept of the best-fit line\n\\(\\widehat{\\beta}_1\\) is the slope of the best-fit line, i.e., the increase in \\(\\widehat{Y}\\) for every increase of one (unit increase) in \\(X\\)\n\nslope = rise over run"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#simple-linear-regression-model-1",
    "href": "lessons/03_SLR/03_SLR.html#simple-linear-regression-model-1",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\n\n\nPopulation regression model\n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n \nComponents\n\n\n\n\\(Y\\)\nresponse, outcome, dependent variable\n\n\n\\(\\beta_0\\)\nintercept\n\n\n\\(\\beta_1\\)\nslope\n\n\n\\(X\\)\npredictor, covariate, independent variable\n\n\n\\(\\epsilon\\)\nresiduals, error term\n\n\n\n\nEstimated regression line\n\n\\[\\widehat{Y} =  \\widehat{\\beta}_0 + \\widehat{\\beta}_1X\\]\n\n \nComponents\n\n\n\n\n\n\n\n\\(\\widehat{Y}\\)\nestimated expected response given predictor \\(X\\)\n\n\n\\(\\widehat{\\beta}_0\\)\nestimated intercept\n\n\n\\(\\widehat{\\beta}_1\\)\nestimated slope\n\n\n\\(X\\)\npredictor, covariate, independent variable"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#we-get-it-nicky-how-do-we-estimate-the-regression-line",
    "href": "lessons/03_SLR/03_SLR.html#we-get-it-nicky-how-do-we-estimate-the-regression-line",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "We get it, Nicky! How do we estimate the regression line?",
    "text": "We get it, Nicky! How do we estimate the regression line?\nFirst let’s take a break!!"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#it-all-starts-with-a-residual",
    "href": "lessons/03_SLR/03_SLR.html#it-all-starts-with-a-residual",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "It all starts with a residual…",
    "text": "It all starts with a residual…\n\n\n\nRecall, one characteristic of our population model was that the residuals, \\(\\epsilon\\), were Normally distributed: \\(\\epsilon \\sim N(0, \\sigma^2)\\)\nIn our population regression model, we had: \\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\nWe can also take the average (expected) value of the population model\nWe take the expected value of both sides and get:\n\n\\[\\begin{aligned}\n        E[Y] & = E[\\beta_0 + \\beta_1X + \\epsilon] \\\\\n        E[Y] & = E[\\beta_0] + E[\\beta_1X] + E[\\epsilon] \\\\\n        E[Y] & = \\beta_0 + \\beta_1X + E[\\epsilon] \\\\\n        E[Y|X] & = \\beta_0 + \\beta_1X \\\\\n\\end{aligned}\\]\n\nWe call \\(E[Y|X]\\) the expected value of \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#so-now-we-have-two-representations-of-our-population-model",
    "href": "lessons/03_SLR/03_SLR.html#so-now-we-have-two-representations-of-our-population-model",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "So now we have two representations of our population model",
    "text": "So now we have two representations of our population model\n\n\n\n\nWith observed \\(Y\\) values and residuals:\n\n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n\n\n\n\nWith the population expected value of \\(Y\\) given \\(X\\):\n\n\n\\[E[Y|X] = \\beta_0 + \\beta_1X\\]\n\n\n\n\nUsing the two forms of the model, we can figure out a formula for our residuals:\n\\[\\begin{aligned}\nY & = (\\beta_0 + \\beta_1X) + \\epsilon \\\\\nY & = E[Y|X] + \\epsilon \\\\\nY - E[Y|X] & = \\epsilon \\\\\n\\epsilon & = Y - E[Y|X]\n\\end{aligned}\\]\nAnd so we have our true, population model, residuals!\n\nThis is an important fact! For the population model, the residuals: \\(\\epsilon = Y - E[Y|X]\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#back-to-our-estimated-model",
    "href": "lessons/03_SLR/03_SLR.html#back-to-our-estimated-model",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Back to our estimated model",
    "text": "Back to our estimated model\nWe have the same two representations of our estimated/fitted model:\n\n\n\n\nWith observed values:\n\n\n\\[Y =  \\widehat{\\beta}_0 + \\widehat{\\beta}_1X + \\widehat{\\epsilon}\\]\n\n\n\n\n\nWith the estimated expected value of \\(Y\\) given \\(X\\):\n\n\n\\[\\begin{aligned}\n\\widehat{E}[Y|X] & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\widehat{E[Y|X]} & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\widehat{Y} & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\end{aligned}\\]\n\n\n\n\nUsing the two forms of the model, we can figure out a formula for our estimated residuals:\n\\[\\begin{aligned}\nY & = (\\widehat{\\beta}_0 + \\widehat{\\beta}_1X) + \\widehat\\epsilon \\\\\nY & = \\widehat{Y} + \\widehat\\epsilon \\\\\n\\widehat\\epsilon & = Y - \\widehat{Y}\n\\end{aligned}\\]\n\nThis is an important fact! For the estimated/fitted model, the residuals: \\(\\widehat\\epsilon = Y - \\widehat{Y}\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#individual-i-residuals-in-the-estimatedfitted-model",
    "href": "lessons/03_SLR/03_SLR.html#individual-i-residuals-in-the-estimatedfitted-model",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Individual \\(i\\) residuals in the estimated/fitted model",
    "text": "Individual \\(i\\) residuals in the estimated/fitted model\n\n\n\nObserved values for each individual \\(i\\): \\(Y_i\\)\n\nValue in the dataset for individual \\(i\\)\n\nFitted value for each individual \\(i\\): \\(\\widehat{Y}_i\\)\n\nValue that falls on the best-fit line for a specific \\(X_i\\)\nIf two individuals have the same \\(X_i\\), then they have the same \\(\\widehat{Y}_i\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#individual-i-residuals-in-the-estimatedfitted-model-1",
    "href": "lessons/03_SLR/03_SLR.html#individual-i-residuals-in-the-estimatedfitted-model-1",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Individual \\(i\\) residuals in the estimated/fitted model",
    "text": "Individual \\(i\\) residuals in the estimated/fitted model\n\n\n\nObserved values for each individual \\(i\\): \\(Y_i\\)\n\nValue in the dataset for individual \\(i\\)\n\nFitted value for each individual \\(i\\): \\(\\widehat{Y}_i\\)\n\nValue that falls on the best-fit line for a specific \\(X_i\\)\nIf two individuals have the same \\(X_i\\), then they have the same \\(\\widehat{Y}_i\\)\n\n\n\n\nResidual for each individual: \\(\\widehat\\epsilon_i = Y_i - \\widehat{Y}_i\\)\n\nDifference between the observed and fitted value"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-4",
    "href": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-4",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#so-what-do-we-do-with-the-residuals",
    "href": "lessons/03_SLR/03_SLR.html#so-what-do-we-do-with-the-residuals",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "So what do we do with the residuals?",
    "text": "So what do we do with the residuals?\n\nWe want to minimize the residuals\n\nAka minimize the difference between the observed \\(Y\\) value and the estimated expected response given the predictor ( \\(\\widehat{E}[Y|X]\\) )\n\nWe can use ordinary least squares (OLS) to do this in linear regression!\nIdea behind this: reduce the total error between the fitted line and the observed point (error between is called residuals)\n\nVague use of total error: more precisely, we want to reduce the sum of squared errors\nThink back to my R Shiny app!\nWe need to mathematically define this!\n\n\n \n \n\nNote: there are other ways to estimate the best-fit line!!\n\nExample: Maximum likelihood estimation"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#setting-up-for-ordinary-least-squares",
    "href": "lessons/03_SLR/03_SLR.html#setting-up-for-ordinary-least-squares",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Setting up for ordinary least squares",
    "text": "Setting up for ordinary least squares\n\n\n\nSum of Squared Errors (SSE)\n\n\\[ \\begin{aligned}\nSSE & = \\displaystyle\\sum^n_{i=1} \\widehat\\epsilon_i^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{Y}_i)^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - (\\widehat{\\beta}_0+\\widehat{\\beta}_1X_i))^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\n\\(\\widehat\\epsilon_i = Y_i - \\widehat{Y}_i\\)\n\\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1X_i\\)\n\n\n\n\n\n\nThen we want to find the estimated coefficient values that minimize the SSE!"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#steps-to-estimate-coefficients-using-ols",
    "href": "lessons/03_SLR/03_SLR.html#steps-to-estimate-coefficients-using-ols",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Steps to estimate coefficients using OLS",
    "text": "Steps to estimate coefficients using OLS\n\nSet up SSE (previous slide)\nMinimize SSE with respect to coefficient estimates\n\nNeed to solve a system of equations\n\nCompute derivative of SSE wrt \\(\\widehat\\beta_0\\)\nSet derivative of SSE wrt \\(\\widehat\\beta_0 = 0\\)\nCompute derivative of SSE wrt \\(\\widehat\\beta_1\\)\nSet derivative of SSE wrt \\(\\widehat\\beta_1 = 0\\)\nSubstitute \\(\\widehat\\beta_1\\) back into \\(\\widehat\\beta_0\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#minimize-sse-with-respect-to-coefficients",
    "href": "lessons/03_SLR/03_SLR.html#minimize-sse-with-respect-to-coefficients",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "2. Minimize SSE with respect to coefficients",
    "text": "2. Minimize SSE with respect to coefficients\n\nWant to minimize with respect to (wrt) the potential coefficient estimates ( \\(\\widehat\\beta_0\\) and \\(\\widehat\\beta_1\\))\nTake derivative of SSE wrt \\(\\widehat\\beta_0\\) and \\(\\widehat\\beta_1\\) and set equal to zero to find minimum SSE\n\n\\[\n\\dfrac{\\partial SSE}{\\partial \\widehat\\beta_0} = 0 \\text{ and } \\dfrac{\\partial SSE}{\\partial \\widehat\\beta_1} = 0\n\\]\n\nSolve the above system of equations in steps 3-6"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#compute-derivative-of-sse-wrt-widehatbeta_0",
    "href": "lessons/03_SLR/03_SLR.html#compute-derivative-of-sse-wrt-widehatbeta_0",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "3. Compute derivative of SSE wrt \\(\\widehat\\beta_0\\)",
    "text": "3. Compute derivative of SSE wrt \\(\\widehat\\beta_0\\)\n\n\n\\[\nSSE = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n\\]\n\\[\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0}& =\\frac{\\partial\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)^2}{\\partial{\\widehat{\\beta}}_0}=\n\\sum_{i=1}^{n}\\frac{{\\partial\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)}^2}{\\partial{\\widehat{\\beta}}_0} \\\\\n& =\\sum_{i=1}^{n}{2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)\\left(-1\\right)}=\\sum_{i=1}^{n}{-2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)} \\\\\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0} & = -2\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\nDerivative rule: derivative of sum is sum of derivative\nDerivative rule: chain rule"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#set-derivative-of-sse-wrt-widehatbeta_0-0",
    "href": "lessons/03_SLR/03_SLR.html#set-derivative-of-sse-wrt-widehatbeta_0-0",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "4. Set derivative of SSE wrt \\(\\widehat\\beta_0 = 0\\)",
    "text": "4. Set derivative of SSE wrt \\(\\widehat\\beta_0 = 0\\)\n\n\n\\[\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0} & =0 \\\\ -2\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right) & =0 \\\\\n\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right) & =0 \\\\ \\sum_{i=1}^{n}Y_i-n{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\sum_{i=1}^{n}X_i & =0 \\\\\n\\frac{1}{n}\\sum_{i=1}^{n}Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\frac{1}{n}\\sum_{i=1}^{n}X_i & =0 \\\\\n\\overline{Y}-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\overline{X} & =0 \\\\\n{\\widehat{\\beta}}_0 & =\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\n\\(\\overline{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_i\\)\n\\(\\overline{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_i\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#compute-derivative-of-sse-wrt-widehatbeta_1",
    "href": "lessons/03_SLR/03_SLR.html#compute-derivative-of-sse-wrt-widehatbeta_1",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "5. Compute derivative of SSE wrt \\(\\widehat\\beta_1\\)",
    "text": "5. Compute derivative of SSE wrt \\(\\widehat\\beta_1\\)\n\n\n\\[\nSSE = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n\\]\n\\[\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_1}& =\\frac{\\partial\\sum_{i=1}^{n}{(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i)}^2}{\\partial{\\widehat{\\beta}}_1}=\\sum_{i=1}^{n}\\frac{{\\partial(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i)}^2}{\\partial{\\widehat{\\beta}}_1} \\\\\n&=\\sum_{i=1}^{n}{2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)(-X_i)}=\\sum_{i=1}^{n}{-2X_i\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)} \\\\ &=-2\\sum_{i=1}^{n}{X_i\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)}\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\nDerivative rule: derivative of sum is sum of derivative\nDerivative rule: chain rule"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#set-derivative-of-sse-wrt-widehatbeta_1-0",
    "href": "lessons/03_SLR/03_SLR.html#set-derivative-of-sse-wrt-widehatbeta_1-0",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "6. Set derivative of SSE wrt \\(\\widehat\\beta_1 = 0\\)",
    "text": "6. Set derivative of SSE wrt \\(\\widehat\\beta_1 = 0\\)\n\n\n\\[\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_1} & =0 \\\\ \\sum_{i=1}^{n}\\left({X_iY}_i-{\\widehat{\\beta}}_0X_i-{\\widehat{\\beta}}_1{X_i}^2\\right)&=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i{\\widehat{\\beta}}_0}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1}&=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i\\left(\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}\\right)}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1} &=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i\\overline{Y}}+\\sum_{i=1}^{n}{{\\widehat{\\beta}}_1X_i\\overline{X}}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1} &=0 \\\\\n\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}+\\sum_{i=1}^{n}{({\\widehat{\\beta}}_1X_i\\overline{X}}-{X_i}^2{\\widehat{\\beta}}_1) &=0 \\\\\n\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}+{\\widehat{\\beta}}_1\\sum_{i=1}^{n}{X_i(\\overline{X}}-X_i) &=0 \\\\\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\n\\({\\widehat{\\beta}}_0=\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}\\)\n\\(\\overline{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_i\\)\n\\(\\overline{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_i\\)\n\n\n\n         \n \n\n\\[{\\widehat{\\beta}}_1 =\\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})}\\]"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#substitute-widehatbeta_1-back-into-widehatbeta_0",
    "href": "lessons/03_SLR/03_SLR.html#substitute-widehatbeta_1-back-into-widehatbeta_0",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "7. Substitute \\(\\widehat\\beta_1\\) back into \\(\\widehat\\beta_0\\)",
    "text": "7. Substitute \\(\\widehat\\beta_1\\) back into \\(\\widehat\\beta_0\\)\nFinal coefficient estimates for SLR\n\n\n\n\nCoefficient estimate for \\(\\widehat\\beta_1\\)\n\n\n\\[{\\widehat{\\beta}}_1 =\\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})}\\]\n\n\n\n\n\nCoefficient estimate for \\(\\widehat\\beta_0\\)\n\n\n\\[\\begin{aligned}\n{\\widehat{\\beta}}_0 & =\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X} \\\\\n{\\widehat{\\beta}}_0 & = \\overline{Y} - \\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})} \\overline{X} \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-5",
    "href": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-5",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#do-i-need-to-do-all-that-work-every-time",
    "href": "lessons/03_SLR/03_SLR.html#do-i-need-to-do-all-that-work-every-time",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Do I need to do all that work every time??",
    "text": "Do I need to do all that work every time??"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#regression-in-r-lm",
    "href": "lessons/03_SLR/03_SLR.html#regression-in-r-lm",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Regression in R: lm()",
    "text": "Regression in R: lm()\n\nLet’s discuss the syntax of this function\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#regression-in-r-lm-summary",
    "href": "lessons/03_SLR/03_SLR.html#regression-in-r-lm-summary",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Regression in R: lm() + summary()",
    "text": "Regression in R: lm() + summary()\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\n  (108 observations deleted due to missingness)\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#regression-in-r-lm-tidy",
    "href": "lessons/03_SLR/03_SLR.html#regression-in-r-lm-tidy",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Regression in R: lm() + tidy()",
    "text": "Regression in R: lm() + tidy()\n \n\ntidy(model1) %&gt;% \n  gt() %&gt;% \n  tab_options(table.font.size = 45)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n \n\nRegression equation for our model (which we saw a looong time ago):\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#how-do-we-interpret-the-coefficients",
    "href": "lessons/03_SLR/03_SLR.html#how-do-we-interpret-the-coefficients",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "How do we interpret the coefficients?",
    "text": "How do we interpret the coefficients?\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]\n\n\nIntercept\n\nThe expected outcome for the \\(Y\\)-variable when the \\(X\\)-variable is 0\nExample: The expected/average life expectancy is 50.9 years for a country with 0% female literacy.\n\nSlope\n\nFor every increase of 1 unit in the \\(X\\)-variable, there is an expected increase of, on average, \\(\\widehat\\beta_1\\) units in the \\(Y\\)-variable.\nWe only say that there is an expected increase and not necessarily a causal increase.\nExample: For every 1 percent increase in the female literacy rate, the expected/average life expectancy increases, on average, 0.232 years."
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#next-time",
    "href": "lessons/03_SLR/03_SLR.html#next-time",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Next time",
    "text": "Next time\n\nInference of our estimated coefficients\nInference of estimated expected \\(Y\\) given \\(X\\)\nPrediction\nHypothesis testing!\n\n\n\nLesson 3: SLR 1"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome_key_info.html#key-dates",
    "href": "lessons/00_Welcome/00_Welcome_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/01_Review/01_Review_key_info.html#key-dates",
    "href": "lessons/01_Review/01_Review_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "",
    "text": "Apply tools from SLR (Lesson 6: SLR Diagnostics) in MLR to evaluate LINE assumptions, including residual plots and QQ-plots\nApply tools involving standardized residuals, leverage, and Cook’s distance from SLR (Lesson 7: SLR Diagnostics 2) in MLR to flag potentially influential points\nUse Variance Inflation Factor (VIF) and it’s general form to detect and correct multicollinearity\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)\n\n\n\n\n\n\n\n\n\n\n\nOur final model contains\n\nFemale Literacy Rate FLR\nCO2 Emissions in quartiles CO2_q\nIncome levels in groups assigned by Gapminder income_levels1\nWorld regions four_regions\nMembership of global and economic groups members_oecd_g77\nFood Supply FoodSupplykcPPD\nClean Water Supply WaterSupplePct\n\n\n\n\n\nDisplay regression table for final model\ntidy(final_model) %&gt;% gt() %&gt;% tab_options(table.font.size = 32) %&gt;%  \n  fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n39.877\n4.889\n8.157\n0.000\n    FemaleLiteracyRate\n−0.073\n0.047\n−1.555\n0.125\n    CO2_q(0.806,2.54]\n1.099\n1.914\n0.574\n0.568\n    CO2_q(2.54,4.66]\n−0.292\n2.419\n−0.121\n0.904\n    CO2_q(4.66,35.2]\n−0.595\n2.524\n−0.236\n0.814\n    income_levels1Lower middle income\n5.441\n2.343\n2.322\n0.024\n    income_levels1Upper middle income\n6.111\n2.954\n2.069\n0.043\n    income_levels1High income\n7.959\n3.277\n2.429\n0.018\n    four_regionsAmericas\n9.003\n2.050\n4.391\n0.000\n    four_regionsAsia\n5.260\n1.637\n3.213\n0.002\n    four_regionsEurope\n6.855\n2.871\n2.387\n0.020\n    WaterSourcePrct\n0.166\n0.066\n2.496\n0.015\n    FoodSupplykcPPD\n0.004\n0.002\n1.825\n0.073\n    members_oecd_g77oecd\n1.119\n2.674\n0.418\n0.677\n    members_oecd_g77others\n1.047\n2.511\n0.417\n0.678\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nPart of the reason why we discussed model diagnostics in SLR was so that we could have accompanying visuals to help us understand\n\n \n\nWith 7 variables in out final model, it is hard to visualize outliers and influential points\n\n \n\nI highly encourage you revisit Lesson 6 and 7 (SLR Diagnostics) to help understand these notes\n\n\n\n\n\nRun final_model through augment() (final_model is input)\n\nSo we assigned final_model as the output of the lm() function\n\nWill give us values about each observation in the context of the fitted regression model\n\ncook’s distance (.cooksd), fitted value (.fitted, \\(\\widehat{Y}_i\\)), leverage (.hat), residual (.resid), standardized residuals (.std.resid)\n\n\n\naug = augment(final_model)\nhead(aug) %&gt;% relocate(.fitted, .resid, .std.resid, .hat, .cooksd, .after = LifeExpectancyYrs)\n\n# A tibble: 6 × 14\n  LifeExpectancyYrs .fitted .resid .std.resid  .hat  .cooksd FemaleLiteracyRate\n              &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;              &lt;dbl&gt;\n1              56.7    61.5 -4.78      -1.43  0.327 0.0663                 13  \n2              76.7    75.3  1.38       0.387 0.227 0.00293                95.7\n3              60.9    58.6  2.30       0.684 0.320 0.0147                 58.6\n4              76.9    74.7  2.21       0.620 0.238 0.00799                99.4\n5              76      76.9 -0.879     -0.233 0.145 0.000614               97.9\n6              73.8    74.6 -0.796     -0.214 0.168 0.000618               99.5\n# ℹ 7 more variables: CO2_q &lt;fct&gt;, income_levels1 &lt;fct&gt;, four_regions &lt;fct&gt;,\n#   WaterSourcePrct &lt;dbl&gt;, FoodSupplykcPPD &lt;dbl&gt;, members_oecd_g77 &lt;chr&gt;,\n#   .sigma &lt;dbl&gt;\n\n\nRDocumentation on the augment() function."
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#regression-analysis-process",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#regression-analysis-process",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "",
    "text": "Model Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#lets-remind-ourselves-of-the-final-model",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#lets-remind-ourselves-of-the-final-model",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "",
    "text": "Our final model contains\n\nFemale Literacy Rate FLR\nCO2 Emissions in quartiles CO2_q\nIncome levels in groups assigned by Gapminder income_levels1\nWorld regions four_regions\nMembership of global and economic groups members_oecd_g77\nFood Supply FoodSupplykcPPD\nClean Water Supply WaterSupplePct\n\n\n\n\n\nDisplay regression table for final model\ntidy(final_model) %&gt;% gt() %&gt;% tab_options(table.font.size = 32) %&gt;%  \n  fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n39.877\n4.889\n8.157\n0.000\n    FemaleLiteracyRate\n−0.073\n0.047\n−1.555\n0.125\n    CO2_q(0.806,2.54]\n1.099\n1.914\n0.574\n0.568\n    CO2_q(2.54,4.66]\n−0.292\n2.419\n−0.121\n0.904\n    CO2_q(4.66,35.2]\n−0.595\n2.524\n−0.236\n0.814\n    income_levels1Lower middle income\n5.441\n2.343\n2.322\n0.024\n    income_levels1Upper middle income\n6.111\n2.954\n2.069\n0.043\n    income_levels1High income\n7.959\n3.277\n2.429\n0.018\n    four_regionsAmericas\n9.003\n2.050\n4.391\n0.000\n    four_regionsAsia\n5.260\n1.637\n3.213\n0.002\n    four_regionsEurope\n6.855\n2.871\n2.387\n0.020\n    WaterSourcePrct\n0.166\n0.066\n2.496\n0.015\n    FoodSupplykcPPD\n0.004\n0.002\n1.825\n0.073\n    members_oecd_g77oecd\n1.119\n2.674\n0.418\n0.677\n    members_oecd_g77others\n1.047\n2.511\n0.417\n0.678"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#its-a-lot-to-visualize",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#its-a-lot-to-visualize",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "",
    "text": "Part of the reason why we discussed model diagnostics in SLR was so that we could have accompanying visuals to help us understand\n\n \n\nWith 7 variables in out final model, it is hard to visualize outliers and influential points\n\n \n\nI highly encourage you revisit Lesson 6 and 7 (SLR Diagnostics) to help understand these notes"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#remember-our-friend-augment",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#remember-our-friend-augment",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "",
    "text": "Run final_model through augment() (final_model is input)\n\nSo we assigned final_model as the output of the lm() function\n\nWill give us values about each observation in the context of the fitted regression model\n\ncook’s distance (.cooksd), fitted value (.fitted, \\(\\widehat{Y}_i\\)), leverage (.hat), residual (.resid), standardized residuals (.std.resid)\n\n\n\naug = augment(final_model)\nhead(aug) %&gt;% relocate(.fitted, .resid, .std.resid, .hat, .cooksd, .after = LifeExpectancyYrs)\n\n# A tibble: 6 × 14\n  LifeExpectancyYrs .fitted .resid .std.resid  .hat  .cooksd FemaleLiteracyRate\n              &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;              &lt;dbl&gt;\n1              56.7    61.5 -4.78      -1.43  0.327 0.0663                 13  \n2              76.7    75.3  1.38       0.387 0.227 0.00293                95.7\n3              60.9    58.6  2.30       0.684 0.320 0.0147                 58.6\n4              76.9    74.7  2.21       0.620 0.238 0.00799                99.4\n5              76      76.9 -0.879     -0.233 0.145 0.000614               97.9\n6              73.8    74.6 -0.796     -0.214 0.168 0.000618               99.5\n# ℹ 7 more variables: CO2_q &lt;fct&gt;, income_levels1 &lt;fct&gt;, four_regions &lt;fct&gt;,\n#   WaterSourcePrct &lt;dbl&gt;, FoodSupplykcPPD &lt;dbl&gt;, members_oecd_g77 &lt;chr&gt;,\n#   .sigma &lt;dbl&gt;\n\n\nRDocumentation on the augment() function."
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#summary-of-the-assumptions-and-their-diagnostic-tool",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#summary-of-the-assumptions-and-their-diagnostic-tool",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Summary of the assumptions and their diagnostic tool",
    "text": "Summary of the assumptions and their diagnostic tool\n\n\n\n\n\n\n\n\nAssumption\nWhat needs to hold?\nDiagnostic tool\n\n\n\n\nLinearity\n\\(\\text{}\\)\n\nRelationship between each \\(X\\) and \\(Y\\) is linear\n\n\nScatterplot of \\(Y\\) vs. \\(X\\)\n\n\\(\\text{}\\)\n\n\nIndependence\n\\(\\text{}\\)\n\nObservations are independent from each other\n\n\nStudy design\n\n\\(\\text{}\\)\n\n\nNormality\n\nResiduals (and thus \\(Y|X_1, X_2, ..., X_p\\))\nare normally distributed\n\n\nQQ plot of residuals\nDistribution of residuals\n\n\n\nEquality of variance\n\nVariance of residuals (and thus \\(Y|X_1, X_2, ..., X_p\\))\nis same across fitted values (homoscedasticity)\n\n\nResidual plot\n\n\\(\\text{}\\)"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#autoplot-to-examine-equality-of-variance-and-normality",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#autoplot-to-examine-equality-of-variance-and-normality",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "autoplot() to examine equality of variance and Normality",
    "text": "autoplot() to examine equality of variance and Normality\n\n\n\nlibrary(ggfortify)\nautoplot(final_model) + theme(text=element_text(size=20))"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#autoplot-to-examine-equality-of-variance-and-normality-1",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#autoplot-to-examine-equality-of-variance-and-normality-1",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "autoplot() to examine equality of variance and Normality",
    "text": "autoplot() to examine equality of variance and Normality\n\n\n\nlibrary(ggfortify)\nautoplot(final_model) + theme(text=element_text(size=20))\n\n\n\n\n\n\n\n\n\nLooks like 3 obs are flagged:\n\n17: Cote d’Ivoire\n59: South Africa\n61: Kingdom of Eswatini (formerly Swaziland in 2011)\n\nWithout them, QQ-plot and residual plot look good\n\nPoints on QQ-plot are close to identity line\nResiduals have pretty consistent spread across fitted values\n\nBut don’t take them out!!!\n\nInstead, discuss what may be missing in our regression model that is not capturing the characteristics of these countries"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#poll-everywhere-question-1",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#poll-everywhere-question-1",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#identifying-outliers",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#identifying-outliers",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Identifying outliers",
    "text": "Identifying outliers\n\n\n\n\nInternally standardized residual\n\n\n\\[\nr_i = \\frac{\\widehat\\epsilon_i}{\\sqrt{\\widehat\\sigma^2(1-h_{ii})}}\n\\]\n\n\n\n\nWe flag an observation if the standardized residual is “large”\n\nDifferent sources will define “large” differently\nPennState site uses \\(|r_i| &gt; 3\\)\nautoplot() shows the 3 observations with the highest standardized residuals\nOther sources use \\(|r_i| &gt; 2\\), which is a little more conservative\n\n\n\n\n\n\n \n\nggplot(data = aug) + \n  geom_histogram(aes(x = .std.resid))"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#countries-that-are-outliers-r_i-2",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#countries-that-are-outliers-r_i-2",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Countries that are outliers (\\(|r_i| > 2\\))",
    "text": "Countries that are outliers (\\(|r_i| &gt; 2\\))\n\nWe can identify the countries that are outliers\n\n\naug %&gt;% relocate(.std.resid, .after = country) %&gt;%\n  filter(abs(.std.resid) &gt; 2) %&gt;% arrange(desc(abs(.std.resid)))\n\n# A tibble: 6 × 15\n  country   .std.resid LifeExpectancyYrs FemaleLiteracyRate CO2_q income_levels1\n  &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt;              &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;         \n1 Swaziland      -2.96              48.9               87.3 (0.8… Lower middle …\n2 South Af…      -2.45              55.8               92.2 (4.6… Upper middle …\n3 Cote d'I…      -2.28              56.9               47.6 [0.0… Lower middle …\n4 Cape Ver…       2.07              72.7               80.3 (0.8… Lower middle …\n5 Sudan           2.05              66.5               63.2 [0.0… Lower middle …\n6 Vanuatu        -2.04              63.2               81.5 [0.0… Lower middle …\n# ℹ 9 more variables: four_regions &lt;fct&gt;, WaterSourcePrct &lt;dbl&gt;,\n#   FoodSupplykcPPD &lt;dbl&gt;, members_oecd_g77 &lt;chr&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#   .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#leverage-h_i",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#leverage-h_i",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Leverage \\(h_i\\)",
    "text": "Leverage \\(h_i\\)\n\nValues of leverage are: \\(0 \\leq h_i \\leq 1\\)\nWe flag an observation if the leverage is “high”\n\nOnly good for SLR: Some textbooks use \\(h_i &gt; 4/n\\) where \\(n\\) = sample size\nOnly good for SLR: Some people suggest \\(h_i &gt; 6/n\\)\nWorks for MLR: \\(h_i &gt; 3p/n\\) where \\(p\\) = number of regression coefficients\n\n\n\naug = aug %&gt;% relocate(.hat, .after = FemaleLiteracyRate)\naug %&gt;% arrange(desc(.hat))\n\n# A tibble: 72 × 15\n   country       LifeExpectancyYrs FemaleLiteracyRate  .hat CO2_q income_levels1\n   &lt;chr&gt;                     &lt;dbl&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;         \n 1 Mexico                     75.8               92.3 0.445 (2.5… Upper middle …\n 2 Tajikistan                 69.9               99.6 0.425 [0.0… Lower middle …\n 3 Bosnia and H…              76.9               96.7 0.367 (4.6… Upper middle …\n 4 Uzbekistan                 69                 99.2 0.363 (2.5… Lower middle …\n 5 Bangladesh                 71                 53.4 0.347 [0.0… Lower middle …\n 6 Afghanistan                56.7               13   0.327 [0.0… Low income    \n 7 Zimbabwe                   51.9               80.1 0.321 [0.0… Low income    \n 8 Angola                     60.9               58.6 0.320 (0.8… Lower middle …\n 9 Myanmar                    67.4               90.4 0.304 [0.0… Lower middle …\n10 Yemen                      67.7               48.5 0.296 (0.8… Lower middle …\n# ℹ 62 more rows\n# ℹ 9 more variables: four_regions &lt;fct&gt;, WaterSourcePrct &lt;dbl&gt;,\n#   FoodSupplykcPPD &lt;dbl&gt;, members_oecd_g77 &lt;chr&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#   .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#countries-with-high-leverage-h_i-3pn",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#countries-with-high-leverage-h_i-3pn",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Countries with high leverage (\\(h_i > 3p/n\\))",
    "text": "Countries with high leverage (\\(h_i &gt; 3p/n\\))\n\nWe can look at the countries that have high leverage: there are NONE\n\n\nn = nrow(gapm2); p = length(final_model$coefficients) - 1\naug %&gt;% \n  filter(.hat &gt; 3*p/n) %&gt;%\n  arrange(desc(.hat))\n\n# A tibble: 0 × 15\n# ℹ 15 variables: country &lt;chr&gt;, LifeExpectancyYrs &lt;dbl&gt;,\n#   FemaleLiteracyRate &lt;dbl&gt;, .hat &lt;dbl&gt;, CO2_q &lt;fct&gt;, income_levels1 &lt;fct&gt;,\n#   four_regions &lt;fct&gt;, WaterSourcePrct &lt;dbl&gt;, FoodSupplykcPPD &lt;dbl&gt;,\n#   members_oecd_g77 &lt;chr&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;, .sigma &lt;dbl&gt;,\n#   .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#identifying-points-with-high-cooks-distance",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#identifying-points-with-high-cooks-distance",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Identifying points with high Cook’s distance",
    "text": "Identifying points with high Cook’s distance\n\n\nThe Cook’s distance for the \\(i^{th}\\) observation is\n\\[d_i = \\frac{h_i}{2(1-h_i)} \\cdot r_i^2\\] where \\(h_i\\) is the leverage and \\(r_i\\) is the studentized residual\n\n\nAnother rule for Cook’s distance that is not strict:\n\nInvestigate observations that have \\(d_i &gt; 1\\)\n\nCook’s distance values are already in the augment tibble: .cooksd\n\n\n\n\nNo countries with high Cook’s distance\n\n\naug = aug %&gt;% relocate(.cooksd, .after = country)\naug %&gt;% arrange(desc(.cooksd)) %&gt;% filter(.cooksd &gt; 1)\n\n# A tibble: 0 × 15\n# ℹ 15 variables: country &lt;chr&gt;, .cooksd &lt;dbl&gt;, LifeExpectancyYrs &lt;dbl&gt;,\n#   FemaleLiteracyRate &lt;dbl&gt;, .hat &lt;dbl&gt;, CO2_q &lt;fct&gt;, income_levels1 &lt;fct&gt;,\n#   four_regions &lt;fct&gt;, WaterSourcePrct &lt;dbl&gt;, FoodSupplykcPPD &lt;dbl&gt;,\n#   members_oecd_g77 &lt;chr&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;, .sigma &lt;dbl&gt;,\n#   .std.resid &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#plotting-cooks-distance",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#plotting-cooks-distance",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Plotting Cook’s Distance",
    "text": "Plotting Cook’s Distance\n\n# plot(model) shows figures similar to autoplot()\n# adds on Cook's distance though\nplot(final_model, which = 4)"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#how-do-we-deal-with-influential-points",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#how-do-we-deal-with-influential-points",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "How do we deal with influential points?",
    "text": "How do we deal with influential points?\n\nIf an observation is influential, we can check data errors:\n\nWas there a data entry or collection problem?\nIf you have reason to believe that the observation does not hold within the population (or gives you cause to redefine your population)\n\nIf an observation is influential, we can check our model:\n\nDid you leave out any important predictors?\nShould you consider adding some interaction terms?\nIs there any nonlinearity that needs to be modeled?\n\nBasically, deleting an observation should be justified outside of the numbers!\n\nIf it’s an honest data point, then it’s giving us important information!\n\nMeans we will need to discuss the limitations of our model\n\nFor example: Think about measurements that might help explain life expectancy that are NOT in our model\n\nA really well thought out explanation from StackExchange"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#poll-everywhere-question-2",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#poll-everywhere-question-2",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#when-we-have-detected-problems-in-our-model",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#when-we-have-detected-problems-in-our-model",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "When we have detected problems in our model…",
    "text": "When we have detected problems in our model…\n\nWe have talked about influential points\nWe have talked about identifying issues with our LINE assumptions\n\n \nWhat are our options once we have identified issues in our linear regression model?\n\nAre we missing a crucial measure in our dataset?\nTry a transformation if there is an issue with linearity or normality\n\nAddressed in model selection\n\nTry a weighted least squares approach if unequal variance (oof, not enough time for us to get to)\nTry a robust estimation procedure if we have a lot of outlier issues (outside scope of class)"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#what-is-multicollinearity-adapted-from-parts-of-stat-501-page",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#what-is-multicollinearity-adapted-from-parts-of-stat-501-page",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "What is multicollinearity? (adapted from parts of STAT 501 page)",
    "text": "What is multicollinearity? (adapted from parts of STAT 501 page)\nSo far, we’ve been ignoring something very important: multicollinearity\n\n\n\n\n\n\nMulticollinearity\n\n\nTwo or more covariates in a multivariable regression model are highly correlated\n\n\n\n\n\n\n\nTypes of multicollinearity\n\nStructural multicollinearity\n\nMathematical artifact caused by creating new covariates from other covariates\nFor example: If we have age, and decide to transform age to include age-squared\n\nThen we have age and age-squared in the model: age-squared is perfectly predicted by age!\n\n\nData-based multicollinearity\n\nResult of a poorly designed experiment, reliance on purely observational data, or the inability to manipulate the system on which the data are collected."
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#poll-everywhere-question-3",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#poll-everywhere-question-3",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#why-is-multicollinearity-a-problem",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#why-is-multicollinearity-a-problem",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Why is multicollinearity a problem?",
    "text": "Why is multicollinearity a problem?\nIn linear regression…\n\nEstimated regression coefficient of any one variable depends on other predictors included in the model\n\nNot necessarily bad, but a big change might be an issue\n\nHypothesis tests for any coefficient may yield different conclusions depending on other predictors included in the model\nMarginal contribution of any one predictor variable in reducing the error sum of squares depends on other predictors included in the model\n\n \nWhen there is multicollinearity in our model:\n\nPrecision of the estimated regression coefficients or correlated covariates decreases a lot\n\nBasically, standard error increases and confidence intervals get wider, which means we’re not as confident in our estimate anymore\nBecause highly correlated covariates are not adding much more information, but are constraining our model more"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#did-you-notice-anything-about-all-the-consequences-of-multicollinearity",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#did-you-notice-anything-about-all-the-consequences-of-multicollinearity",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Did you notice anything about all the consequences of multicollinearity?",
    "text": "Did you notice anything about all the consequences of multicollinearity?\n\nAll consequences relate to estimating a regression coefficient precisely\n\nRecall that precision is linked to analysis goals of association and interpretability\nSee Lesson 12: Model Selection\n\n\n \n\nMulticollinearity is not really an issue when our goal is prediction\n\nHighly correlated covariates/predictors will not hurt our prediction of an outcome"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#how-do-we-detect-multicollinearity",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#how-do-we-detect-multicollinearity",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "How do we detect multicollinearity?",
    "text": "How do we detect multicollinearity?\n\n\n\nVariance inflation factors (VIF): quantifies how much the variance of the estimated coefficient for covariate \\(k\\) increases\n\nIncreases: from SLR with only covariate \\(k\\) to MLR with all other covariates\n\n\n \n\nGeneral rule of thumb\n\n\\(4 &lt; VIF &lt; 10\\): Warrent investigation (but most people aren’t investigating this…)\n\\(VIF &gt; 10\\): Requires correction\n\nInfluencing regression coefficient estimates\n\n\n\n\n\n\n\n\nVIF\n\n\n\\[\nVIF = \\dfrac{1}{1-R_k^2}\n\\]\n\\(R_k^2\\) is the \\(R^2\\)-value obtained by regressing the \\(k^{th}\\) covariate/predictor on the remaining predictors"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#lets-apply-it-to-our-final-model",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#lets-apply-it-to-our-final-model",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Let’s apply it to our final model",
    "text": "Let’s apply it to our final model\n\nNaive way to calculate this:\n\n\nlibrary(rms)\nrms::vif(final_model)\n\n               FemaleLiteracyRate                 CO2_q(0.806,2.54] \n                         4.863139                          2.979224 \n                 CO2_q(2.54,4.66]                  CO2_q(4.66,35.2] \n                         4.758904                          5.180216 \nincome_levels1Lower middle income income_levels1Upper middle income \n                         5.290718                          8.406927 \n        income_levels1High income              four_regionsAmericas \n                         7.293148                          2.531966 \n                 four_regionsAsia                four_regionsEurope \n                         2.096398                          7.771994 \n                  WaterSourcePrct                   FoodSupplykcPPD \n                         4.824266                          3.499250 \n             members_oecd_g77oecd            members_oecd_g77others \n                         2.720955                          5.125196 \n\n\n\nAll \\(VIF &lt; 10\\)\nProblem: multi-level covariates (CO2 Emissions and income level) have different VIF’s even though they should be considered one variable"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#lets-apply-it-to-our-final-model-correctly-12",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#lets-apply-it-to-our-final-model-correctly-12",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Let’s apply it to our final model correctly (1/2)",
    "text": "Let’s apply it to our final model correctly (1/2)\n\nCalculate the GVIF and, more importantly, the \\(GVIF^{1/(2\\cdot df)}\\)\nGVIF is the \\(R^2\\)-value for regressing a covariate’s group indicators on the remaining covariates\n\nCaptures the correlation between covariates better\n\n\\(GVIF^{1/(2\\cdot df)}\\) helps standardize GVIF based on how many levels each categorical covariate has\n\nI’ll refer to this as df-corrected GVIF or standardized GVIF\nIf continuous covariate, \\(GVIF^{1/(2\\cdot df)} = \\sqrt{GVIF}\\)\n\n\n\nlibrary(car)\ncar::vif(final_model)\n\n                        GVIF Df GVIF^(1/(2*Df))\nFemaleLiteracyRate  4.863139  1        2.205253\nCO2_q               8.223951  3        1.420736\nincome_levels1     11.045885  3        1.492336\nfour_regions       13.935918  3        1.551277\nWaterSourcePrct     4.824266  1        2.196421\nFoodSupplykcPPD     3.499250  1        1.870628\nmembers_oecd_g77    7.430919  2        1.651052"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#lets-apply-it-to-our-final-model-correctly-22",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#lets-apply-it-to-our-final-model-correctly-22",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Let’s apply it to our final model correctly (2/2)",
    "text": "Let’s apply it to our final model correctly (2/2)\n\nIf continuous covariate, \\(GVIF^{1/(2\\cdot df)} = \\sqrt{GVIF}\\)\nSo we can square \\(GVIF^{1/(2\\cdot df)}\\) and set VIF rules\nOR: we can correct any \\(GVIF^{1/(2\\cdot df)} &gt; \\sqrt{10} = 3.162\\)\n\n\ncar::vif(final_model)\n\n                        GVIF Df GVIF^(1/(2*Df))\nFemaleLiteracyRate  4.863139  1        2.205253\nCO2_q               8.223951  3        1.420736\nincome_levels1     11.045885  3        1.492336\nfour_regions       13.935918  3        1.551277\nWaterSourcePrct     4.824266  1        2.196421\nFoodSupplykcPPD     3.499250  1        1.870628\nmembers_oecd_g77    7.430919  2        1.651052\n\n\n\nAll of these covariates are okay! No multicollinearity to correct in this dataset!"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#but-what-if-we-do-need-to-make-corrections-for-multicollinearity",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#but-what-if-we-do-need-to-make-corrections-for-multicollinearity",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "But what if we do need to make corrections for multicollinearity?",
    "text": "But what if we do need to make corrections for multicollinearity?\n\nWe have been dealing with data-based multicollinearity in our example\nIf we had issues with multicollinearity, then what are our options?\n\nRemove the variable(s) with large VIF\nUse expert knowledge in the field to decide\n\nIf one variable has a large VIF, then there is usually another one or more variables with large VIFs\n\nBasically, all the covariates that are correlated will have large VIFs\n\nExample: our two largest GVIFs were for world region and income levels\n\nHypothetical: their \\(GVIF^{1/(2\\cdot df)} &gt; 3.162\\)\nRemove one of them\nI’m no expert, but from more of a data equity lens, there’s a lot of generalizations made about world regions\n\nI think relying on the income level of a country might give us more information as well"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#what-about-structural-multicollinearity",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#what-about-structural-multicollinearity",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "What about structural multicollinearity?",
    "text": "What about structural multicollinearity?\n\nStructural multicollinearity\n\nMathematical artifact caused by creating new covariates from other covariates\n\n\n \n\nFor example: If we have age, and decide to transform age to include age-squared\n\nThen we have age and age-squared in the model: age-squared is perfectly predicted by age!\nBy having the untransformed and transformed covariate in the model, they are inherently correlated!\n\n\n \n\nBest practice to reduce the correlation: center you covariate\n\nBy centering age, we no longer have a one-to-one connection between age and age-squared\nIf centered at 40yo: a 35 yo and a 45 yo will both have centered age of 5, and age-squared of 25\n\n\n \n\nCheck out the Penn State site for a work through of an example with VIFs"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#summary-of-multicollinearity",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#summary-of-multicollinearity",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Summary of multicollinearity",
    "text": "Summary of multicollinearity\n\nCorrelated covariates/predictors will hurt our model’s precision and interpretations of coefficients\n\n \n\nWe need to check for multicollinearity by using VIFs or GVIFs\n\n \n\nIf \\(VIF &gt; 10\\) or \\(GVIF^{1/(2\\cdot df)} &gt; 3.162\\), we need to do something about the covariates\n\nData based: remove one the of correlated variables\nStructural based: centering usually fixes it"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#regression-analysis-process-1",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#regression-analysis-process-1",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Regression analysis process",
    "text": "Regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Understand the motivation for model selection, including bias-variance trade off and alignment of research goals (association vs. prediction)\nExplain the general process or idea behind different model selection techniques\nKnow the formula for and function for various model fit statistics\n\n\n\n\nFirst, let’s think about the number of observations in our dataset\nFor example: In the Gapminder dataset, I can use an indicator for each country\n\nRemember that each country is an observation\nSo we have a perfectly fit model - a covariate for each observation\nBut we cannot generalize this to any other countries\nAnd we haven’t identified any meaningful relationships between life expectancy and other measured characteristics\n\nMore covariates in the model is not always better\n\nOverfitting the data limits our generalizability and prevents us from answering research questions\n\n\n\n\n\n\n\nSuppose we have \\(p = 30\\) covariates (in the true model) and n = 50 observations. We could consider the following two alternatives:\n\nWe could fit a model using all of the covariates.\n\nIn this case, \\(\\widehat\\beta\\) is unbiased for \\(\\beta\\) (in a linear model fit using OLS). But \\(\\widehat\\beta\\) has very high variance.\n\nWe could fit a model using only the five strongest covariates.\n\nIn this case, \\(\\widehat\\beta\\) will be biased for \\(\\beta\\), but it will have lower variance (compared to the estimate including all covariates)\n\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html\n\n\n\n\n\n\n\n\n\n\nRecall mean square error is a function of SSE (sum of squared residuals)\n\\[\nMSE = \\dfrac{1}{n} \\sum_{i=1}^{n} \\big(Y_i - \\widehat{Y}_i \\big)^2\n\\]\nMSE can also be written as a function of the bias and variance\n\\[\nMSE = \\text{bias}\\big(\\widehat\\beta\\big)^2 + \\text{variance}\\big(\\widehat\\beta\\big)\n\\]\nFor the same data:\n\nMore covariates in model: less bias, more variance\nLess covariates in model: more bias, less variance\n\nOut goal: find a model with just the right amount of covariates to balance bias and vairance\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html\n\n\n\n\n\n\n\n\n“Because models always fall far short of the complex reality under study, there are no best or optimal strategies for modeling.”\n\nFrom: Statistical Foundations for Model-Based Adjustments\n\nNot all statistical texts provide practical advice on model development\n\nA lot of resources include methods/code to compare models, but does not include much advice re: selecting which model to ultimately use.\nOther texts are sparse on details or incorporate simplistic approaches\n\nModel development strategy should align with research goals\n\nPrediction vs. Estimating Association\nStrategy may depend on study design and data set size\n\n\n\n\n\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nGoal: Understand one variable’s (or a group of variable’s) effect on the response after adjusting for other factors\nMainly interpret the coefficient of the variable that is the focus of the study\n\nInterpreting the coefficients of the other variables is not important, but can help bring context\n\nAny variables not selected for the final model have still been adjusted for, since they had a chance to be in the model\nExample: How is body mass of a penguin associated with flipper length?\n\n\n\n\n\n\nPrediction\n\n\n\nGoal: to calculate the most precise prediction of the response variable\nInterpreting coefficients is not important\nChoose only the variables that are strong predictors of the response variable\n\nExcluding irrelevant variables can help reduce widths of the prediction intervals\n\nExample: What is the flipper length of a penguin with body mass of 3000 g (and all its other characteristics)?\n\n\n\n\n\n\n\n\nMore information on the two analysis goals:\n\n\n\n\n\nIf you ever get the chance, check out Dr. Kristin Sainani’s series on Statistics\n\n\n\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nSelection of potential models is tied more with the research context with some incorporation of prediction scores\nPre-specification of multivariable model\nPurposeful model selection\n\n“Risk factor modeling”\n\nChange in Estimate (CIE) approaches\n\nWill learn in Survival Analysis (BSTA 514)\n\n\n\n\n\n\n\nPrediction\n\n\n\nSelection of potential models is fully dependent on prediction scores\nAutomated strategies\n\nStepwise selection (forward/backward)\n\nYou’ll see these a lot, but they’re not really good methods\n\nBest subset\nRegularization techniques (LASSO, Ridge, Elastic net)\n\n\n\n\n\n\n\nFor categorical outcomes, there are more prediction model selection strategies (will learn more in BSTA 513)\n\nExamples: Decision trees, Random forest, Neural networks, K-means"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#why-cant-i-just-throw-in-all-the-variables-into-my-model",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#why-cant-i-just-throw-in-all-the-variables-into-my-model",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "First, let’s think about the number of observations in our dataset\nFor example: In the Gapminder dataset, I can use an indicator for each country\n\nRemember that each country is an observation\nSo we have a perfectly fit model - a covariate for each observation\nBut we cannot generalize this to any other countries\nAnd we haven’t identified any meaningful relationships between life expectancy and other measured characteristics\n\nMore covariates in the model is not always better\n\nOverfitting the data limits our generalizability and prevents us from answering research questions"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#model-complexity-vs.-parsimony",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#model-complexity-vs.-parsimony",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Suppose we have \\(p = 30\\) covariates (in the true model) and n = 50 observations. We could consider the following two alternatives:\n\nWe could fit a model using all of the covariates.\n\nIn this case, \\(\\widehat\\beta\\) is unbiased for \\(\\beta\\) (in a linear model fit using OLS). But \\(\\widehat\\beta\\) has very high variance.\n\nWe could fit a model using only the five strongest covariates.\n\nIn this case, \\(\\widehat\\beta\\) will be biased for \\(\\beta\\), but it will have lower variance (compared to the estimate including all covariates)\n\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#bias-variance-trade-off",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#bias-variance-trade-off",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Recall mean square error is a function of SSE (sum of squared residuals)\n\\[\nMSE = \\dfrac{1}{n} \\sum_{i=1}^{n} \\big(Y_i - \\widehat{Y}_i \\big)^2\n\\]\nMSE can also be written as a function of the bias and variance\n\\[\nMSE = \\text{bias}\\big(\\widehat\\beta\\big)^2 + \\text{variance}\\big(\\widehat\\beta\\big)\n\\]\nFor the same data:\n\nMore covariates in model: less bias, more variance\nLess covariates in model: more bias, less variance\n\nOut goal: find a model with just the right amount of covariates to balance bias and vairance\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#model-selection-basics-slide-adjusted-from-jodi-lapidus",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#model-selection-basics-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "“Because models always fall far short of the complex reality under study, there are no best or optimal strategies for modeling.”\n\nFrom: Statistical Foundations for Model-Based Adjustments\n\nNot all statistical texts provide practical advice on model development\n\nA lot of resources include methods/code to compare models, but does not include much advice re: selecting which model to ultimately use.\nOther texts are sparse on details or incorporate simplistic approaches\n\nModel development strategy should align with research goals\n\nPrediction vs. Estimating Association\nStrategy may depend on study design and data set size"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#the-goals-of-association-vs.-prediction",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#the-goals-of-association-vs.-prediction",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Association / Explanatory / One variable’s effect\n\n\n\nGoal: Understand one variable’s (or a group of variable’s) effect on the response after adjusting for other factors\nMainly interpret the coefficient of the variable that is the focus of the study\n\nInterpreting the coefficients of the other variables is not important, but can help bring context\n\nAny variables not selected for the final model have still been adjusted for, since they had a chance to be in the model\nExample: How is body mass of a penguin associated with flipper length?\n\n\n\n\n\n\nPrediction\n\n\n\nGoal: to calculate the most precise prediction of the response variable\nInterpreting coefficients is not important\nChoose only the variables that are strong predictors of the response variable\n\nExcluding irrelevant variables can help reduce widths of the prediction intervals\n\nExample: What is the flipper length of a penguin with body mass of 3000 g (and all its other characteristics)?"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#model-building-for-association-vs.-prediction",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#model-building-for-association-vs.-prediction",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "More information on the two analysis goals:\n\n\n\n\n\nIf you ever get the chance, check out Dr. Kristin Sainani’s series on Statistics"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#model-selection-strategies-for-continuous-outcomes",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#model-selection-strategies-for-continuous-outcomes",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Association / Explanatory / One variable’s effect\n\n\n\nSelection of potential models is tied more with the research context with some incorporation of prediction scores\nPre-specification of multivariable model\nPurposeful model selection\n\n“Risk factor modeling”\n\nChange in Estimate (CIE) approaches\n\nWill learn in Survival Analysis (BSTA 514)\n\n\n\n\n\n\n\nPrediction\n\n\n\nSelection of potential models is fully dependent on prediction scores\nAutomated strategies\n\nStepwise selection (forward/backward)\n\nYou’ll see these a lot, but they’re not really good methods\n\nBest subset\nRegularization techniques (LASSO, Ridge, Elastic net)\n\n\n\n\n\n\n\nFor categorical outcomes, there are more prediction model selection strategies (will learn more in BSTA 513)\n\nExamples: Decision trees, Random forest, Neural networks, K-means"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#pre-specification-of-multivariable-model-slide-adjusted-from-jodi-lapidus",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#pre-specification-of-multivariable-model-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Pre-specification of multivariable model (slide adjusted from Jodi Lapidus)",
    "text": "Pre-specification of multivariable model (slide adjusted from Jodi Lapidus)\n\nIn a clinical trial, we often have to write and finalize a statistical analysis plan (SAP) before the trial starts\nIf we wish to compare treatment effects adjusted for covariates, all covariates typically specified in advance\n\nExample: Comparing effectiveness of 3-drug vs. 2-drug regimen for delaying AIDS onset or death. Covariates such as severity of HIV infection at baseline would have been specified in advance.\nVariables such as study site, as well as any randomization stratification variables are common covariates.\n\nIn these cases, only a limited number of multivariable models are fit and reported\n\nDo not perform all the model building steps outlined in Hosmer and Lemeshow texts"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#purposeful-model-selection-slide-adjusted-from-jodi-lapidus",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#purposeful-model-selection-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Purposeful model selection (slide adjusted from Jodi Lapidus)",
    "text": "Purposeful model selection (slide adjusted from Jodi Lapidus)\n\nCan use this type of model selection for any type of regression\nCareful, well-thought out variable selection process\n\nConsiders both confounding and interaction, as well as checking model assumptions, fit, etc.\n\nOften a reasonable strategy, especially in epidemiology and more exploratory clinical studies\n\nHowever, not always appropriate!\nE.g. clinical trials with model specified in advance. (pre-specified model)\n\n\n \n\nThis is the selection process that we will focus on in this class!"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#change-in-estimate-cie-approach-slide-adjusted-from-jodi-lapidus",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#change-in-estimate-cie-approach-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Change in estimate (CIE) approach (slide adjusted from Jodi Lapidus)",
    "text": "Change in estimate (CIE) approach (slide adjusted from Jodi Lapidus)\n\nCIE strategies select covariates on the basis of how much their control changes exposure effect estimates\n\nObserved change is presumed to measure confounding by the covariate.\n\nWhat estimate?\n\nH/L text suggest using coefficients from the model\nWe typically use the coefficient estimate from the explanatory variable that we are most interested in\n\nWhat magnitude change is ”important”?\n\nH/L text suggest 10%\n\nOne must choose an effect measure to judge change importance, where “importance” needs to be evaluated along a contextually meaningful scale\nAccurate assessment of confounding may require examining changes from removing entire sets of covariates\n\nAdd in or eliminate candidate confounders one at time?\nAdd in or eliminate candidate confounders in sets?"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#stepwise-selection-slide-adjusted-from-adrianna-westbrook",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#stepwise-selection-slide-adjusted-from-adrianna-westbrook",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Stepwise selection (slide adjusted from Adrianna Westbrook)",
    "text": "Stepwise selection (slide adjusted from Adrianna Westbrook)\n\nThis is an incredibly common approach that statisticians use, often because it is an older and more recognized method\n\nBUT IT IS ALSO ONE OF THE WORST MODEL SELECTION STRATEGIES!!\n\nMajor disadvantages to stepwise selection:\n\nProne to overfitting\nBiased estimates\nCements the wrong idea that we are looking for our “most significant” covariates\n\nPredictors/covariates are added or removed one at time if they are below a certain threshold (usually p-value below 0.10 to 0.20)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#stepwise-selection-two-common-approaches",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#stepwise-selection-two-common-approaches",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Stepwise selection: two common approaches",
    "text": "Stepwise selection: two common approaches\n\nI will introduce two of the approaches so that you understand the general process if a collaborator ever mentions stepwise selection\nForward selection:\n\nFor \\(p\\) cvariates potential covariates, run all simple linear regressions:\n\n\\(Y= \\beta_0 + \\beta_1 X_1 + \\epsilon\\) through \\(Y= \\beta_0 + \\beta_1 X_{p} + \\epsilon\\)\nInclude the \\(X_i\\) with the lowest p-value (assuming it is below the threshold)\n\nNow run \\(Y= \\beta_0 + \\beta_1 X_i + \\beta_2 X_1 + \\epsilon\\) through \\(Y= \\beta_0 + \\beta_1 X_i + \\beta_2 X_{p} + \\epsilon\\) and enter the next \\(X_j\\) with the lowest p-value\nContinue process until no more predictors come back with a p-value below the threshold\n\nBackward selection:\n\nStart with a full model (\\(Y= \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p + \\epsilon\\)) and remove predictor with the highest p-value (assuming it is above the threshold)\nRepeatedly remove the variable with the highest p-value until all remaining variables meet the stopping criteria (are below the threshold)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#best-subset-slide-adjusted-from-adrianna-westbrook",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#best-subset-slide-adjusted-from-adrianna-westbrook",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Best subset (slide adjusted from Adrianna Westbrook)",
    "text": "Best subset (slide adjusted from Adrianna Westbrook)\n\nI don’t see this approach very often\nQuite literally making subsets of the data and using the “best” one\nGeneral steps:\n\nRun every possible model fitting 1 to all possible \\(p\\) predictors/covariates\nYou can limit number of potential predictors\n\\(2^p\\) = total number of models where \\(p\\) = number of predictors\nYou will get the best fitting model within each category (i.e., 1 predictor model, 2 predictor model,…, \\(p\\) predictor model)\nThen have to find the best fitting model between the best models from each category\n\nMajor disadvantages to best subset:\n\nDoes not account for interactions\nNeeds to run a lot of models (takes A LOT of time)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#regularization-techniques",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#regularization-techniques",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Regularization techniques",
    "text": "Regularization techniques\n\nRegularization techniques (LASSO, ridge, elastic net) adds a penalization that shrinks (or regularizes) coefficients down to reduce overfitting\n\n\n\n\n\n\n\n\n\n\n\nLASSO (Least About Shrinkage and Selection Operator)\nRidge\nElastic Net\n\n\nPenalization\nL-1 Norm, uses absolute value\nL-2 Norm, uses squared value\nBest of both worlds, L-1 and L-2 used\n\n\nPro’s\nReduces overfitting, will shrink coefficient to zero\nReduces overfitting, handles collinearity, can handle k&gt;n\nReduces overfitting, handles collinearity, handles k&gt;n, shrinks coefficients to zero\n\n\nCon’s\nCannot handle k&gt;n, doesn’t handle multicollinearity well\nDoes not shrink coefficients to zero, difficult to interpret\nMore difficult for R to do than the other two (but not really that bad)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#introduction-to-model-fit-statistics",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#introduction-to-model-fit-statistics",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Introduction to model fit statistics",
    "text": "Introduction to model fit statistics\n\nSo far we have compared models using the F-test\nThe F-test is a great way to compare models that are nested\n\nBasically, this means that the “full” model contains all the covariates that the “reduced” model contains\nThe full model will have additional covariates, but the covariates in the reduced is a subset of the covariates in the full\n\nWhat if we want to compare models that are not nested?\n\nThere is a special group of fit statistics that can help us compare models\nNote: these are sometimes used in the model building process (within one strategy)\n\nHelpful if we want to compare selected models across strategies\nHelpful if we have a few “final” models with different covariates that we want to compare"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#common-model-fit-statistics",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#common-model-fit-statistics",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Common model fit statistics",
    "text": "Common model fit statistics\n\nThe following model fit statistics combine information about the SSE, the number of parameters in the model, and the sample size\nFor these fit statistics, smaller values indicate better model fit!\n\n\n\n\n\n\n\n\nFit statistic\nEquation\n\n\n\n\nR-squared / Adjusted R-squared\n\\(Adj. R^2 = 1 - \\frac{SSE/(n-p-1)}{SSY/(n-1)}\\)\n\n\nMallow’s \\(C_p\\)\n\\(C_p = \\Bigg[ \\dfrac{\\widehat\\sigma^2_p}{\\widehat\\sigma^2_{max}} - 1 \\Bigg](n-p) + p\\)\n\n\nAkaike information criterion (AIC)\n\\(AIC = n\\log(SSE) - n \\log(n) + 2(p+1)\\)\n\n\nBayesian information criterion (BIC)\n\\(BIC = n\\log(SSE) - n\\log(n) + log(n)\\cdot(p+1)\\)\n\n\n\n \n\nWe don’t need to know the exact formulas for them!"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#common-model-fit-statistics-1",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#common-model-fit-statistics-1",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Common model fit statistics",
    "text": "Common model fit statistics\n\nThere is no hypothesis testing for these fit statistics\n\nOnly helpful if you are comparing models\nWorks for nested and non-nested models\n\nCommon to report all or some of them\nAll of the fit statistics will not necessarily reach a consensus about the best fitting model\n\nEach weigh SSE, number of parameters, and number of observations differently"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html",
    "title": "Lesson 11: Interactions",
    "section": "",
    "text": "Define confounders and effect modifiers, and how they interact with the main relationship we model.\nInterpret the interaction component of a model with a binary categorical covariate and continuous covariate, and how the main variable’s effect changes.\nInterpret the interaction component of a model with a multi-level categorical covariate and continuous covariate, and how the main variable’s effect changes.\nInterpret the interaction component of a model with two categorical covariates, and how the main variable’s effect changes.\n\nNext time:\n\nInterpret the interaction component of a model with two continuous covariates, and how the main variable’s effect changes.\nWhen there are only two covariates in the model, test whether one is a confounder or effect modifier.\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\n\n\n\n\n\n\n\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#lets-map-that-to-our-regression-analysis-process",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#lets-map-that-to-our-regression-analysis-process",
    "title": "Lesson 11: Interactions",
    "section": "",
    "text": "Model Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#recall-our-data-and-the-main-relationship",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#recall-our-data-and-the-main-relationship",
    "title": "Lesson 11: Interactions",
    "section": "",
    "text": "Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#what-is-a-confounder",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#what-is-a-confounder",
    "title": "Lesson 11: Interactions",
    "section": "What is a confounder?",
    "text": "What is a confounder?\n\nA confounding variable, or confounder, is a factor/variable that wholly or partially accounts for the observed effect of the risk factor on the outcome\nA confounder must be…\n\nRelated to the outcome Y, but not a consequence of Y\nRelated to the explanatory variable X, but not a consequence of X"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#including-a-confounder-in-the-model",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#including-a-confounder-in-the-model",
    "title": "Lesson 11: Interactions",
    "section": "Including a confounder in the model",
    "text": "Including a confounder in the model\n\nIn the following model we have two variables, \\(X_1\\) and \\(X_2\\)\n\n\\[Y= \\beta_0 + \\beta_1X_{1}+ \\beta_2X_{2} + \\epsilon\\]\n\nAnd we assume that every level of the confounder, there is parallel slopes\nNote: to interpret \\(\\beta_1\\), we did not specify any value of \\(X_2\\); only specified that it be held constant\n\nImplicit assumption: effect of \\(X_1\\) is equal across all values of \\(X_2\\)\n\nThe above model assumes that \\(X_{1}\\) and \\(X_{2}\\) do not interact (with respect to their effect on \\(Y\\))\n\nepidemiology: no “effect modification”\nmeaning the effect of \\(X_{1}\\) is the same regardless of the values of \\(X_{2}\\)"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#where-have-we-modeled-a-confounder-before",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#where-have-we-modeled-a-confounder-before",
    "title": "Lesson 11: Interactions",
    "section": "Where have we modeled a confounder before?",
    "text": "Where have we modeled a confounder before?\n\n\n\nWe have seen a plot of Life expectancy vs. female literacy rate with different levels of food supply colored (Lesson 8)\nIn our plot and the model, we treat food supply as a confounder\nIf food supply is a confounder in the relationship between life expectancy and female literacy rate, then we only use main effects in the model:\n\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]\n\n\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\n(mr1_2d = ggPredict(mr1, interactive = T))"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-1",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-1",
    "title": "Lesson 11: Interactions",
    "section": "Poll everywhere question 1",
    "text": "Poll everywhere question 1"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#what-is-an-effect-modifier",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#what-is-an-effect-modifier",
    "title": "Lesson 11: Interactions",
    "section": "What is an effect modifier?",
    "text": "What is an effect modifier?\n\n\n\nAn additional variable in the model\n\nOutside of the main relationship between \\(Y\\) and \\(X_1\\) that we are studying\n\nAn effect modifier will change the effect of \\(X_1\\) on \\(Y\\) depending on its value\n\nAka: as the effect modifier’s values change, so does the association between \\(Y\\) and \\(X_1\\)\nSo the coefficient estimating the relationship between \\(Y\\) and \\(X_1\\) changes with another variable"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#how-do-we-include-an-effect-modifier-in-the-model",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#how-do-we-include-an-effect-modifier-in-the-model",
    "title": "Lesson 11: Interactions",
    "section": "How do we include an effect modifier in the model?",
    "text": "How do we include an effect modifier in the model?\n\nInteractions!!\nWe can incorporate interactions into our model through product terms: \\[Y  =  \\beta_0 + \\beta_1X_{1}+ \\beta_2X_{2} +\n\\beta_3X_{1}X_{2} + \\epsilon\\]\nTerminology:\n\nmain effect parameters: \\(\\beta_1,\\beta_2\\)\n\nThe main effect models estimate the average \\(X_{1}\\) and \\(X_{2}\\) effects\n\ninteraction parameter: \\(\\beta_3\\)"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#types-of-interactions-non-interactions",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#types-of-interactions-non-interactions",
    "title": "Lesson 11: Interactions",
    "section": "Types of interactions / non-interactions",
    "text": "Types of interactions / non-interactions\n\n\n\nCommon types of interactions:\n\nSynergism: \\(X_{2}\\) strengthens the \\(X_{1}\\) effect\nAntagonism:\\(X_{2}\\) weakens the \\(X_{1}\\) effect\n\n\n \n\nIf the interaction coefficient is not significant\n\nNo evidence of effect modification, i.e., the effect of \\(X_{1}\\) does not vary with \\(X_{2}\\)\n\n\n \n\nIf the main effect of \\(X_2\\) is also not significant\n\nNo evidence that \\(X_2\\) is a confounder"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#do-we-think-income-level-is-an-effect-modifier-for-female-literacy-rate",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#do-we-think-income-level-is-an-effect-modifier-for-female-literacy-rate",
    "title": "Lesson 11: Interactions",
    "section": "Do we think income level is an effect modifier for female literacy rate?",
    "text": "Do we think income level is an effect modifier for female literacy rate?\n\n\n\nLet’s say we only have two income groups: low income and high income\nWe can start by visualizing the relationship between life expectancy and female literacy rate by income level\n\n \n\nQuestions of interest: Is the effect of female literacy rate on life expectancy differ depending on income level?\n\nThis is the same as: Is income level is an effect modifier for female literacy rate?\n\n\n \n\nLet’s run an interaction model to see!\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#model-with-interaction-between-a-binary-categorical-and-continuous-variables",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#model-with-interaction-between-a-binary-categorical-and-continuous-variables",
    "title": "Lesson 11: Interactions",
    "section": "Model with interaction between a binary categorical and continuous variables",
    "text": "Model with interaction between a binary categorical and continuous variables\nModel we are fitting:\n\\[ LE = \\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{high income}) + \\beta_3 FLR \\cdot I(\\text{high income}) + \\epsilon\\]\n\n\\(LE\\) as life expectancy\n\\(FLR\\) as female literacy rate (continuous variable)\n\\(I(\\text{high income})\\) as the indicator that income level is “high income” (binary categorical variable)\n\nIn R:\n\nm_int_inc2 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2 +\n                  FemaleLiteracyRate*income_levels2, data = gapm_sub)\n\nOR\n\nm_int_inc2 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate*income_levels2, \n                data = gapm_sub)"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#displaying-the-regression-table-and-writing-fitted-regression-equation",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#displaying-the-regression-table-and-writing-fitted-regression-equation",
    "title": "Lesson 11: Interactions",
    "section": "Displaying the regression table and writing fitted regression equation",
    "text": "Displaying the regression table and writing fitted regression equation\n\ntidy(m_int_inc2, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n54.849\n2.846\n19.270\n0.000\n49.169\n60.529\n    FemaleLiteracyRate\n0.156\n0.039\n3.990\n0.000\n0.078\n0.235\n    income_levels2Higher income\n−16.649\n15.364\n−1.084\n0.282\n−47.308\n14.011\n    FemaleLiteracyRate:income_levels2Higher income\n0.228\n0.164\n1.392\n0.168\n−0.099\n0.555\n  \n  \n  \n\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{high income}) + \\widehat\\beta_3 FLR \\cdot I(\\text{high income}) \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot I(\\text{high income}) + 0.228 \\cdot FLR \\cdot I(\\text{high income})\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-2",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-2",
    "title": "Lesson 11: Interactions",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#comparing-fitted-regression-lines-for-each-income-level",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#comparing-fitted-regression-lines-for-each-income-level",
    "title": "Lesson 11: Interactions",
    "section": "Comparing fitted regression lines for each income level",
    "text": "Comparing fitted regression lines for each income level\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{high income}) + \\widehat\\beta_3 FLR \\cdot I(\\text{high income}) \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot I(\\text{high income}) + 0.228 \\cdot FLR \\cdot I(\\text{high income})\n\\end{aligned}\\]\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 FLR \\cdot 0 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot 0 + \\\\\n& 0.228 \\cdot FLR \\cdot 0 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR\\\\\n\\end{aligned}\\]\n\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 FLR \\cdot 1 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot 1 + \\\\ & 0.228 \\cdot FLR \\cdot 1 \\\\\n\\widehat{LE} = & (54.85 - 16.65 \\cdot 1) + \\\\ & (0.156 \\cdot FLR + 0.228 \\cdot FLR \\cdot 1) \\\\\n\\widehat{LE} = & (54.85 - 16.65) + (0.156 + 0.228) \\cdot FLR\\\\\n\\widehat{LE} = & 38.2 + 0.384 \\cdot FLR\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#lets-take-a-look-back-at-the-plot",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#lets-take-a-look-back-at-the-plot",
    "title": "Lesson 11: Interactions",
    "section": "Let’s take a look back at the plot",
    "text": "Let’s take a look back at the plot\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR  \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR\\\\\n\\end{aligned}\\]\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & (\\widehat\\beta_0 +\\widehat\\beta_2) + (\\widehat\\beta_1 +\\widehat\\beta_3) FLR \\\\\n\\widehat{LE} = & (54.85 - 16.65) + (0.156 + 0.228) \\cdot FLR\\\\\n\\widehat{LE} = & 38.2 + 0.384 \\cdot FLR\\\\\n\\end{aligned}\\]\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#interpretation-for-interaction-between-binary-categorical-and-continuous-variables",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#interpretation-for-interaction-between-binary-categorical-and-continuous-variables",
    "title": "Lesson 11: Interactions",
    "section": "Interpretation for interaction between binary categorical and continuous variables",
    "text": "Interpretation for interaction between binary categorical and continuous variables\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{high income}) + \\widehat\\beta_3 FLR \\cdot I(\\text{high income}) \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_2 \\cdot I(\\text{high income})\\bigg] + \\underbrace{\\bigg[\\widehat\\beta_1 + \\widehat\\beta_3 \\cdot I(\\text{high income}) \\bigg]}_\\text{FLR's effect} FLR \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in female literacy rate’s effect, comparing higher income to lower income levels\nwhere the “female literacy rate effect” equals the change in mean life expectancy per percent increase in female literacy with income level held constant, i.e. “adjusted female literacy rate effect”\n\nIn summary, the interaction term can be interpreted as “difference in adjusted female literacy rate effect comparing higher income to lower income levels”\nIt will be helpful to test the interaction to round out this interpretation!!"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-binary-categorical-and-continuous-variables",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-binary-categorical-and-continuous-variables",
    "title": "Lesson 11: Interactions",
    "section": "Test interaction between binary categorical and continuous variables",
    "text": "Test interaction between binary categorical and continuous variables\n\nWe run an F-test for a single coefficient (\\(\\beta_3\\)) in the below model (see lesson 9, MLR: Inference / F-test)\n\n\\[ LE = \\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{high income}) + \\beta_3 FLR \\cdot I(\\text{high income}) + \\epsilon\\]\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_3=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_3\\neq0\\)\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[\\begin{aligned}\nLE = & \\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{high income}) + \\\\ &\\epsilon\n\\end{aligned}\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}\nLE = & \\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{high income}) + \\\\ &\\beta_3 FLR \\cdot I(\\text{high income}) + \\epsilon\n\\end{aligned}\\]\n\n\n\n\n\n\n\nI’m going to be skipping steps so please look back at Lesson 9 for full steps (required in HW 4)"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-binary-categorical-and-continuous-variables-1",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-binary-categorical-and-continuous-variables-1",
    "title": "Lesson 11: Interactions",
    "section": "Test interaction between binary categorical and continuous variables",
    "text": "Test interaction between binary categorical and continuous variables\n\nFit the reduced and full model\n\n\nm_int_inc_red = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2, \n                   data = gapm_sub)\nm_int_inc_full = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2 +\n                  FemaleLiteracyRate*income_levels2, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\nanova(m_int_inc_red, m_int_inc_full) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2\n69.000\n2,407.667\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2 + FemaleLiteracyRate * income_levels2\n68.000\n2,340.948\n1.000\n66.719\n1.938\n0.168\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and income level (p = 0.168).\n\nIf significant, we say more: For higher income levels, for every one percent increase in female literacy rate, the mean life expectancy increases 0.384 years. For lower income levels, for every one percent increase in female literacy rate, the mean life expectancy increases 0.156 years. Thus, the female literacy rate almost doubles comparing high income to low income levels."
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#do-we-think-world-region-is-an-effect-modifier-for-female-literacy-rate",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#do-we-think-world-region-is-an-effect-modifier-for-female-literacy-rate",
    "title": "Lesson 11: Interactions",
    "section": "Do we think world region is an effect modifier for female literacy rate?",
    "text": "Do we think world region is an effect modifier for female literacy rate?\n\n\n\nWe can start by visualizing the relationship between life expectancy and female literacy rate by world region\nQuestions of interest: Does the effect of female literacy rate on life expectancy differ depending on world region?\n\nThis is the same as: Is world region is an effect modifier for female literacy rate?\n\nLet’s run an interaction model to see!\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#model-with-interaction-between-a-multi-level-categorical-and-continuous-variables",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#model-with-interaction-between-a-multi-level-categorical-and-continuous-variables",
    "title": "Lesson 11: Interactions",
    "section": "Model with interaction between a multi-level categorical and continuous variables",
    "text": "Model with interaction between a multi-level categorical and continuous variables\nModel we are fitting:\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\\\ & \\beta_5 FLR \\cdot I(\\text{Americas}) + \\beta_6 FLR \\cdot I(\\text{Asia})+ \\beta_7 FLR \\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]\n\n\\(LE\\) as life expectancy\n\\(FLR\\) as female literacy rate (continuous variable)\n\\(I(\\text{Americas})\\), \\(I(\\text{Asia})\\), \\(I(\\text{Europe})\\) as the indicator for each world region\n\nIn R:\n\nm_int_wr = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + four_regions +\n                  FemaleLiteracyRate*four_regions, data = gapm_sub)\n\nOR\n\nm_int_wr = lm(LifeExpectancyYrs ~ FemaleLiteracyRate*four_regions, \n                data = gapm_sub)"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#displaying-the-regression-table-and-writing-fitted-regression-equation-1",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#displaying-the-regression-table-and-writing-fitted-regression-equation-1",
    "title": "Lesson 11: Interactions",
    "section": "Displaying the regression table and writing fitted regression equation",
    "text": "Displaying the regression table and writing fitted regression equation\n\ntidy(m_int_wr, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n58.225\n3.377\n17.240\n0.000\n51.478\n64.972\n    FemaleLiteracyRate\n0.051\n0.053\n0.957\n0.342\n−0.055\n0.157\n    four_regionsAmericas\n−2.406\n17.913\n−0.134\n0.894\n−38.191\n33.379\n    four_regionsAsia\n2.283\n5.410\n0.422\n0.674\n−8.525\n13.091\n    four_regionsEurope\n63.628\n46.414\n1.371\n0.175\n−29.095\n156.350\n    FemaleLiteracyRate:four_regionsAmericas\n0.164\n0.197\n0.830\n0.410\n−0.231\n0.558\n    FemaleLiteracyRate:four_regionsAsia\n0.061\n0.073\n0.830\n0.410\n−0.086\n0.208\n    FemaleLiteracyRate:four_regionsEurope\n−0.519\n0.476\n−1.090\n0.280\n−1.471\n0.432\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 FLR \\cdot I(\\text{Americas}) + \\widehat\\beta_6 FLR \\cdot I(\\text{Asia})+ \\widehat\\beta_7 FLR \\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 58.23 + 0.051 \\cdot FLR    −2.41 \\cdot I(\\text{Americas}) + 2.28 \\cdot I(\\text{Asia}) + 63.63 \\cdot I(\\text{Europe}) + \\\\ & 0.164  \\cdot FLR \\cdot I(\\text{Americas}) + 0.061 \\cdot FLR \\cdot I(\\text{Asia}) −0.519    \\cdot FLR \\cdot I(\\text{Europe})\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#comparing-fitted-regression-lines-for-each-world-region",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#comparing-fitted-regression-lines-for-each-world-region",
    "title": "Lesson 11: Interactions",
    "section": "Comparing fitted regression lines for each world region",
    "text": "Comparing fitted regression lines for each world region\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 FLR \\cdot I(\\text{Americas}) + \\widehat\\beta_6 FLR \\cdot I(\\text{Asia})+ \\widehat\\beta_7 FLR \\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 58.23 + 0.051 \\cdot FLR    −2.41 \\cdot I(\\text{Americas}) + 2.28 \\cdot I(\\text{Asia}) + 63.63 \\cdot I(\\text{Europe}) + \\\\ & 0.164  \\cdot FLR \\cdot I(\\text{Americas}) + 0.061 \\cdot FLR \\cdot I(\\text{Asia}) −0.519    \\cdot FLR \\cdot I(\\text{Europe})\n\\end{aligned}\\]\n\n\n\n\nAfrica\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 + \\\\ & \\widehat\\beta_4 \\cdot 0 + \\widehat\\beta_5 FLR \\cdot 0 + \\\\ & \\widehat\\beta_6 FLR \\cdot 0+ \\widehat\\beta_7 FLR \\cdot 0 \\\\\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR\\\\\n\\end{aligned}\\]\n\n\n\n\n\nThe Americas\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\\\ & \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 \\cdot 0 + \\\\ & \\widehat\\beta_4 \\cdot 0 + \\widehat\\beta_5 FLR \\cdot 1 + \\\\ & \\widehat\\beta_6 FLR \\cdot 0+ \\widehat\\beta_7 FLR \\cdot 0 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_2\\big) + \\\\ &\\big(\\widehat\\beta_1 + \\widehat\\beta_5\\big)FLR \\\\\n\\end{aligned}\\]\n\n\n\n\n\nAsia\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 1 + \\\\ & \\widehat\\beta_4 \\cdot 0 + \\widehat\\beta_5 FLR \\cdot 0 + \\\\ & \\widehat\\beta_6 FLR \\cdot 1+ \\widehat\\beta_7 FLR \\cdot 0 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_3\\big) + \\\\ &\\big(\\widehat\\beta_1 + \\widehat\\beta_6\\big)FLR \\\\\n\\end{aligned}\\]\n\n\n\n\n\nEurope\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 + \\\\ & \\widehat\\beta_4 \\cdot 1 + \\widehat\\beta_5 FLR \\cdot 0 + \\\\ & \\widehat\\beta_6 FLR \\cdot 0+ \\widehat\\beta_7 FLR \\cdot 1 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_4\\big) + \\\\ & \\big(\\widehat\\beta_1 + \\widehat\\beta_7\\big)FLR \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-3",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-3",
    "title": "Lesson 11: Interactions",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#centering-continuous-variables-when-we-are-including-interactions",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#centering-continuous-variables-when-we-are-including-interactions",
    "title": "Lesson 11: Interactions",
    "section": "Centering continuous variables when we are including interactions",
    "text": "Centering continuous variables when we are including interactions\n\n\nFor Europe, the mean life expectancy had a regression line with a large intercept\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_4\\big) + \\big(\\widehat\\beta_1 + \\widehat\\beta_7\\big)FLR \\\\\n\\widehat{LE} = & (58.23 + 63.63) + (0.051 - 0.519)FLR \\\\\n\\widehat{LE} = & 121.86 -0.468FLR \\\\\n\\end{aligned}\\]\n\nCentering the continuous variables in a model (when they are involved in interactions) helps with:\n\nInterpretations of the coefficient estimates\nCorrelation between the main effect for the variable and the interaction that it is involved with\n\nTo be discussed in future lecture: leads to multicollinearity issues\n\n\nOther online sources about when and when not to center:\n\nThe why and when of centering continuous predictors in regression modeling\nWhen not to center a predictor variable in regression"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#itll-be-helpful-to-center-female-literacy-rate",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#itll-be-helpful-to-center-female-literacy-rate",
    "title": "Lesson 11: Interactions",
    "section": "It’ll be helpful to center female literacy rate",
    "text": "It’ll be helpful to center female literacy rate\n\n\nCentering female literacy rate: \\[ FLR^c = FLR - \\overline{FLR}\\]\nCentering in R:\n\n\ngapm_sub = gapm_sub %&gt;% \n  mutate(FLR_c = FemaleLiteracyRate - mean(FemaleLiteracyRate))\n\n\nI’m going to print the mean so I can use it for my interpretations\n\n\n(mean_FLR = mean(gapm_sub$FemaleLiteracyRate))\n\n[1] 82.03056\n\n\n\nNow all intercept values (in each respective world region) will be the mean life expectancy when female literacy rate is 82.03%\nWe will used center FLR for the rest of the lecture"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#now-we-refit-the-model-with-the-centered-flr",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#now-we-refit-the-model-with-the-centered-flr",
    "title": "Lesson 11: Interactions",
    "section": "Now we refit the model with the centered FLR",
    "text": "Now we refit the model with the centered FLR\n\n\nm_int_wr_flrc = lm(LifeExpectancyYrs ~ FLR_c*four_regions, \n                data = gapm_sub)\ntidy(m_int_wr_flrc, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n62.387\n1.626\n38.358\n0.000\n59.138\n65.637\n    FLR_c\n0.051\n0.053\n0.957\n0.342\n−0.055\n0.157\n    four_regionsAmericas\n11.032\n2.918\n3.781\n0.000\n5.203\n16.862\n    four_regionsAsia\n7.287\n2.042\n3.568\n0.001\n3.207\n11.367\n    four_regionsEurope\n21.038\n7.698\n2.733\n0.008\n5.659\n36.417\n    FLR_c:four_regionsAmericas\n0.164\n0.197\n0.830\n0.410\n−0.231\n0.558\n    FLR_c:four_regionsAsia\n0.061\n0.073\n0.830\n0.410\n−0.086\n0.208\n    FLR_c:four_regionsEurope\n−0.519\n0.476\n−1.090\n0.280\n−1.471\n0.432\n  \n  \n  \n\n\n\n\n\nWhat changed? What stayed the same? What’s the new intercept for Europe?"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#interpretation-for-interaction-between-multi-level-categorical-and-continuous-variables",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#interpretation-for-interaction-between-multi-level-categorical-and-continuous-variables",
    "title": "Lesson 11: Interactions",
    "section": "Interpretation for interaction between multi-level categorical and continuous variables",
    "text": "Interpretation for interaction between multi-level categorical and continuous variables\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 FLR \\cdot I(\\text{Americas}) + \\widehat\\beta_6 FLR \\cdot I(\\text{Asia})+ \\widehat\\beta_7 FLR \\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe})\\bigg] + \\\\ &\\underbrace{\\bigg[\\widehat\\beta_1 +  \\widehat\\beta_5 \\cdot I(\\text{Americas}) + \\widehat\\beta_6 \\cdot I(\\text{Asia})+ \\widehat\\beta_7 \\cdot I(\\text{Europe}) \\bigg]}_\\text{FLR's effect} FLR \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_5\\) = mean change in female literacy rate’s effect, comparing countries in the Americas to countries in Africa\n\\(\\beta_6\\) = mean change in female literacy rate’s effect, comparing countries in Asia to countries in Africa\n\\(\\beta_7\\) = mean change in female literacy rate’s effect, comparing countries in Europe to countries in Africa\n\nIt will be helpful to test the interaction to round out this interpretation!!"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-multi-level-categorical-continuous-variables",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-multi-level-categorical-continuous-variables",
    "title": "Lesson 11: Interactions",
    "section": "Test interaction between multi-level categorical & continuous variables",
    "text": "Test interaction between multi-level categorical & continuous variables\n\nWe run an F-test for a group of coefficients (\\(\\beta_5\\), \\(\\beta_6\\), \\(\\beta_7\\)) in the below model (see lesson 9)\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\\\ & \\beta_5 FLR \\cdot I(\\text{Americas}) + \\beta_6 FLR \\cdot I(\\text{Asia})+ \\beta_7 FLR \\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_5= \\beta_6 = \\beta_7 =0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_5\\neq0\\) and/or \\(\\beta_6\\neq0\\) and/or \\(\\beta_7\\neq0\\)\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{Americas}) + \\\\ & \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\epsilon \\end{aligned}\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\\\ & \\beta_4 I(\\text{Europe}) + \\beta_5 FLR \\cdot I(\\text{Americas}) + \\\\ & \\beta_6 FLR \\cdot I(\\text{Asia})+ \\beta_7 FLR \\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-multi-level-categorical-continuous-variables-1",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-multi-level-categorical-continuous-variables-1",
    "title": "Lesson 11: Interactions",
    "section": "Test interaction between multi-level categorical & continuous variables",
    "text": "Test interaction between multi-level categorical & continuous variables\n\nFit the reduced and full model\n\n\nm_int_wr_red = lm(LifeExpectancyYrs ~ FLR_c + four_regions, \n                   data = gapm_sub)\nm_int_wr_full = lm(LifeExpectancyYrs ~ FLR_c + four_regions+\n                  FLR_c*four_regions, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\nanova(m_int_wr_red, m_int_wr_full) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FLR_c + four_regions\n67.000\n1,705.881\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FLR_c + four_regions + FLR_c * four_regions\n64.000\n1,641.151\n3.000\n64.731\n0.841\n0.476\n  \n  \n  \n\n\n\n# newdata &lt;- data.frame(four_regions = c(\"Africa\", \"Americas\", \"Asia\", \"Europe\"), \n#                       FLR_c = c()) \n# (pred = predict(m_int_wr_full, \n#                 newdata=newdata, \n#                 interval=\"confidence\"))\n\n\nConclusion: There is not a significant interaction between female literacy rate and income level (p = 0.478)."
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#do-we-think-income-level-can-be-an-effect-modifier-for-world-region",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#do-we-think-income-level-can-be-an-effect-modifier-for-world-region",
    "title": "Lesson 11: Interactions",
    "section": "Do we think income level can be an effect modifier for world region?",
    "text": "Do we think income level can be an effect modifier for world region?\n\n\n\nTaking a break from female literacy rate to demonstrate interactions for two categorical variables\nWe can start by visualizing the relationship between life expectancy and world region by income level\nQuestions of interest: Does the effect of world region on life expectancy differ depending on income level?\n\nThis is the same as: Is income level an effect modifier for world region?\n\nLet’s run an interaction model to see!"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#model-with-interaction-between-a-multi-level-categorical-and-continuous-variables-1",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#model-with-interaction-between-a-multi-level-categorical-and-continuous-variables-1",
    "title": "Lesson 11: Interactions",
    "section": "Model with interaction between a multi-level categorical and continuous variables",
    "text": "Model with interaction between a multi-level categorical and continuous variables\nModel we are fitting:\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\\\ & \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]\n\n\\(LE\\) as life expectancy\n\\(I(\\text{high income})\\) as indicator of high income\n\\(I(\\text{Americas})\\), \\(I(\\text{Asia})\\), \\(I(\\text{Europe})\\) as the indicator for each world region\n\nIn R:\n\n# gapm_sub = gapm_sub %&gt;% mutate(income_levels2 = relevel(income_levels2, ref = \"Higher income\")) # for poll everywhere\n\nm_int_wr_inc = lm(LifeExpectancyYrs ~ income_levels2 + four_regions +\n                  income_levels2*four_regions, data = gapm_sub)\nm_int_wr_inc = lm(LifeExpectancyYrs ~ income_levels2*four_regions, \n                data = gapm_sub)"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#displaying-the-regression-table-and-writing-fitted-regression-equation-2",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#displaying-the-regression-table-and-writing-fitted-regression-equation-2",
    "title": "Lesson 11: Interactions",
    "section": "Displaying the regression table and writing fitted regression equation",
    "text": "Displaying the regression table and writing fitted regression equation\n\ntidy(m_int_wr_inc, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 25) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n60.850\n1.281\n47.488\n0.000\n58.290\n63.410\n    income_levels2Higher income\n2.100\n2.865\n0.733\n0.466\n−3.624\n7.824\n    four_regionsAmericas\n10.800\n3.844\n2.810\n0.007\n3.121\n18.479\n    four_regionsAsia\n7.467\n1.957\n3.815\n0.000\n3.556\n11.377\n    four_regionsEurope\n11.500\n2.865\n4.014\n0.000\n5.776\n17.224\n    income_levels2Higher income:four_regionsAmericas\n2.640\n4.896\n0.539\n0.592\n−7.141\n12.421\n    income_levels2Higher income:four_regionsAsia\n1.543\n3.956\n0.390\n0.698\n−6.360\n9.447\n    income_levels2Higher income:four_regionsEurope\n2.382\n4.020\n0.592\n0.556\n−5.649\n10.412\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\widehat\\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 60.85 + 2.10 \\cdot I(\\text{high income}) + 10.8 \\cdot I(\\text{Americas}) + 7.47\\cdot  I(\\text{Asia}) + 11.50 \\cdot I(\\text{Europe}) + \\\\ & 2.64 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + 1.54 \\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & 2.38 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-4",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-4",
    "title": "Lesson 11: Interactions",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#comparing-fitted-regression-means-for-each-world-region",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#comparing-fitted-regression-means-for-each-world-region",
    "title": "Lesson 11: Interactions",
    "section": "Comparing fitted regression means for each world region",
    "text": "Comparing fitted regression means for each world region\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\widehat\\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 60.85 + 2.10 \\cdot I(\\text{high income}) + 10.8 \\cdot I(\\text{Americas}) + 7.47\\cdot  I(\\text{Asia}) + 11.50 \\cdot I(\\text{Europe}) + \\\\ & 2.64 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + 1.54 \\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & 2.38 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\nAfrica\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 + \\widehat\\beta_4 \\cdot 0 + \\\\ & \\widehat\\beta_5 I(\\text{high income}) \\cdot 0 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 0+ \\\\& \\widehat\\beta_7 I(\\text{high income}) \\cdot 0 \\\\\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income})\\\\\n\\end{aligned}\\]\n\n\n\n\n\nThe Americas\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 \\cdot 0 + \\widehat\\beta_4 \\cdot 0 + \\\\ &  \\widehat\\beta_5 I(\\text{high income}) \\cdot 1 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 0+ \\\\ & \\widehat\\beta_7 I(\\text{high income}) \\cdot 0 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_2\\big) + \\\\ &\\big(\\widehat\\beta_1 + \\widehat\\beta_5\\big)I(\\text{high income}) \\\\\n\\end{aligned}\\]\n\n\n\n\n\nAsia\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 1 + \\widehat\\beta_4 \\cdot 0 + \\\\ & \\widehat\\beta_5 I(\\text{high income}) \\cdot 0 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 1+ \\\\ & \\widehat\\beta_7 I(\\text{high income}) \\cdot 0 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_3\\big) + \\\\ &\\big(\\widehat\\beta_1 + \\widehat\\beta_6\\big)I(\\text{high income}) \\\\\n\\end{aligned}\\]\n\n\n\n\n\nEurope\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 + \\widehat\\beta_4 \\cdot 1 + \\\\ & \\widehat\\beta_5 I(\\text{high income}) \\cdot 0 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 0+ \\\\ & \\widehat\\beta_7 I(\\text{high income}) \\cdot 1 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_4\\big) + \\\\ & \\big(\\widehat\\beta_1 + \\widehat\\beta_7\\big)I(\\text{high income}) \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#comparing-fitted-regression-means-for-each-income-level",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#comparing-fitted-regression-means-for-each-income-level",
    "title": "Lesson 11: Interactions",
    "section": "Comparing fitted regression means for each income level",
    "text": "Comparing fitted regression means for each income level\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\widehat\\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 60.85 + 2.10 \\cdot I(\\text{high income}) + 10.8 \\cdot I(\\text{Americas}) + 7.47\\cdot  I(\\text{Asia}) + 11.50 \\cdot I(\\text{Europe}) + \\\\ & 2.64 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + 1.54 \\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & 2.38 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 0 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot 0\\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot 0 \\cdot I(\\text{Asia})+ \\widehat\\beta_7 \\cdot 0\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 1 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot 1\\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot 1 \\cdot I(\\text{Asia})+ \\widehat\\beta_7 \\cdot 1\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & (\\widehat\\beta_0 + \\widehat\\beta_1)  + (\\widehat\\beta_2 + \\widehat\\beta_5) I(\\text{Americas}) + (\\widehat\\beta_3 + \\widehat\\beta_6)  I(\\text{Asia}) + \\\\ & (\\widehat\\beta_4 + \\widehat\\beta_7)  I(\\text{Europe}) \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#lets-take-a-look-back-at-the-plot-1",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#lets-take-a-look-back-at-the-plot-1",
    "title": "Lesson 11: Interactions",
    "section": "Let’s take a look back at the plot",
    "text": "Let’s take a look back at the plot\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\\\ & \\widehat\\beta_4 I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & (\\widehat\\beta_0 + \\widehat\\beta_1)  + (\\widehat\\beta_2 + \\widehat\\beta_5) I(\\text{Americas}) + \\\\& (\\widehat\\beta_3 + \\widehat\\beta_6)  I(\\text{Asia}) +  (\\widehat\\beta_4 + \\widehat\\beta_7)  I(\\text{Europe}) \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#interpretation-for-interaction-between-two-categorical-variables",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#interpretation-for-interaction-between-two-categorical-variables",
    "title": "Lesson 11: Interactions",
    "section": "Interpretation for interaction between two categorical variables",
    "text": "Interpretation for interaction between two categorical variables\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot I(\\text{high income})\\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\widehat\\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{high income})\\bigg]  + \\bigg[\\widehat\\beta_2 + \\widehat\\beta_5 \\cdot I(\\text{high income})\\bigg] I(\\text{Americas}) + \\\\ & \\bigg[\\widehat\\beta_3 + \\widehat\\beta_6 \\cdot I(\\text{high income})\\bigg]  I(\\text{Asia}) +  \\bigg[\\widehat\\beta_4 + \\widehat\\beta_7 \\cdot I(\\text{high income})\\bigg]  I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_1\\) = mean change in the Africa’s life expectancy, comparing high income to low income countries\n\\(\\beta_5\\) = mean change in the Americas’ effect, comparing high income to low income countries\n\\(\\beta_6\\) = mean change in Asia’s effect, comparing high income to low income countries\n\\(\\beta_7\\) = mean change in Europe’s effect, comparing high income to low income countries"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-two-categorical-variables",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-two-categorical-variables",
    "title": "Lesson 11: Interactions",
    "section": "Test interaction between two categorical variables",
    "text": "Test interaction between two categorical variables\n\nWe run an F-test for a group of coefficients (\\(\\beta_5\\), \\(\\beta_6\\), \\(\\beta_7\\)) in the below model (see lesson 9)\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\\\ & \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_5= \\beta_6 = \\beta_7 =0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_5\\neq0\\) and/or \\(\\beta_6\\neq0\\) and/or \\(\\beta_7\\neq0\\)\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\\\& \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\epsilon \\end{aligned}\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\\\ & \\beta_4 I(\\text{Europe}) + \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\\\ & \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-multi-level-categorical-continuous-variables-2",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-multi-level-categorical-continuous-variables-2",
    "title": "Lesson 11: Interactions",
    "section": "Test interaction between multi-level categorical & continuous variables",
    "text": "Test interaction between multi-level categorical & continuous variables\n\nFit the reduced and full model\n\n\nm_int_wr_inc_red = lm(LifeExpectancyYrs ~ income_levels2 + four_regions, \n                   data = gapm_sub)\nm_int_wr_inc_full = lm(LifeExpectancyYrs ~ income_levels2 + four_regions +\n                          income_levels2*four_regions, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\nanova(m_int_wr_inc_red, m_int_wr_inc_full) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ income_levels2 + four_regions\n67.000\n1,693.242\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ income_levels2 + four_regions + income_levels2 * four_regions\n64.000\n1,681.304\n3.000\n11.938\n0.151\n0.928\n  \n  \n  \n\n\n\n# newdata &lt;- data.frame(four_regions = c(\"Africa\", \"Americas\", \"Asia\", \"Europe\"), \n#                       FLR_c = c()) \n# (pred = predict(m_int_wr_full, \n#                 newdata=newdata, \n#                 interval=\"confidence\"))\n\n\nConclusion: There is not a significant interaction between female literacy rate and income level (p = 0.928)."
  },
  {
    "objectID": "lessons/12_Interactions_02/01_Review_key_info.html#key-dates",
    "href": "lessons/12_Interactions_02/01_Review_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "",
    "text": "Understand equations and visualizations that helps us build multiple linear regression model.\nFit MLR model (in R) and understand the difference between fitted regression plane and regression lines.\nIdentify the population multiple linear regression model and define statistics language for key notation.\nBased off of previous SLR work, understand how the population MLR is estimated.\nInterpret MLR (population) coefficient estimates with additional variable in model\n\n\n\n\nSLR helped us establish the foundation for a lot of regression\n\nBut we do not usually use SLR in analysis\n\n\nWhat did we learn in SLR??\n\n\n\n\nModel Fitting\n\n\n\nOrdinary least squares (OLS)\nlm() function in R\n\n\n\n\n\n\nModel Use\n\n\n\nInference for variance of residuals\nHypothesis testing for coefficients\nInterpreting population coefficient estimates\nCalculated the expected mean for specific \\(X\\) values\nInterpreted coefficient of determination\n\n\n\n\n\n\nModel Evaluation/Diagnostics\n\n\n\nLINE Assumptions\nInfluential points\nData Transformations\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#reminder-of-what-we-learned-in-the-context-of-slr",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#reminder-of-what-we-learned-in-the-context-of-slr",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "",
    "text": "SLR helped us establish the foundation for a lot of regression\n\nBut we do not usually use SLR in analysis\n\n\nWhat did we learn in SLR??\n\n\n\n\nModel Fitting\n\n\n\nOrdinary least squares (OLS)\nlm() function in R\n\n\n\n\n\n\nModel Use\n\n\n\nInference for variance of residuals\nHypothesis testing for coefficients\nInterpreting population coefficient estimates\nCalculated the expected mean for specific \\(X\\) values\nInterpreted coefficient of determination\n\n\n\n\n\n\nModel Evaluation/Diagnostics\n\n\n\nLINE Assumptions\nInfluential points\nData Transformations"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#lets-map-that-to-our-regression-analysis-process",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#lets-map-that-to-our-regression-analysis-process",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "",
    "text": "Model Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#simple-linear-regression-vs.-multiple-linear-regression",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#simple-linear-regression-vs.-multiple-linear-regression",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Simple Linear Regression vs. Multiple Linear Regression",
    "text": "Simple Linear Regression vs. Multiple Linear Regression\n\n\n\nSimple Linear Regression\n\n \n\nWe use one predictor to try to explain the variance of the outcome\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\\]\n\n\n\nMultiple Linear Regression\n\n \n\nWe use multiple predictors to try to explain the variance of the outcome\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_{k}X_{k}+ \\epsilon\n\\]\n\n \n\nHas \\(k+1\\) total coefficients (including intercept) for \\(k\\) predictors/covariates\nSometimes referred to as multivariable linear regression, but never multivariate\n\n\n\n \n\nThe models have similar “LINE” assumptions and follow the same general diagnostic procedure"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#going-back-to-our-life-expectancy-example",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#going-back-to-our-life-expectancy-example",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Going back to our life expectancy example",
    "text": "Going back to our life expectancy example\n\nLet’s say many other variables were measured for each country, including food supply\n\nFood Supply (kilocalories per person per day, kc PPD): the average kilocalories consumed by a person each day.\n\nIn SLR, we only had one predictor and one outcome in the model:\n\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same.\nAdult literacy rate is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life.\n\n\n \n\nDo we think adult female literacy rate is going to explain a lot of the variance of life expectancy between countries?"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#loading-the-new-ish-data",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#loading-the-new-ish-data",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Loading the (new-ish) data",
    "text": "Loading the (new-ish) data\n\n# Load the data - update code if the csv file is not in the same location on your computer\n# If you need to download the file, please go to ur shared folder under Data &gt; Slides\ngapm &lt;- read_excel(\"data/Gapminder_vars_2011.xlsx\", \n                   na = \"NA\")  # important!!!! \n\ngapm_sub &lt;- gapm %&gt;% \n  drop_na(LifeExpectancyYrs, FemaleLiteracyRate, FoodSupplykcPPD)\n\nglimpse(gapm_sub)\n\nRows: 72\nColumns: 18\n$ country                            &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\",…\n$ CO2emissions                       &lt;dbl&gt; 0.4120, 1.7900, 1.2500, 5.3600, 4.6…\n$ ElectricityUsePP                   &lt;dbl&gt; NA, 2210, 207, NA, 2900, 1810, 258,…\n$ FoodSupplykcPPD                    &lt;dbl&gt; 2110, 3130, 2410, 2370, 3160, 2790,…\n$ IncomePP                           &lt;dbl&gt; 1660, 10200, 5910, 18600, 19600, 70…\n$ LifeExpectancyYrs                  &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8,…\n$ FemaleLiteracyRate                 &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5,…\n$ population                         &lt;dbl&gt; 2.97e+07, 2.93e+06, 2.42e+07, 9.57e…\n$ WaterSourcePrct                    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8,…\n$ geo                                &lt;chr&gt; \"afg\", \"alb\", \"ago\", \"atg\", \"arg\", …\n$ four_regions                       &lt;chr&gt; \"asia\", \"europe\", \"africa\", \"americ…\n$ eight_regions                      &lt;chr&gt; \"asia_west\", \"europe_east\", \"africa…\n$ six_regions                        &lt;chr&gt; \"south_asia\", \"europe_central_asia\"…\n$ members_oecd_g77                   &lt;chr&gt; \"g77\", \"others\", \"g77\", \"g77\", \"g77…\n$ Latitude                           &lt;dbl&gt; 33.00000, 41.00000, -12.50000, 17.0…\n$ Longitude                          &lt;dbl&gt; 66.00000, 20.00000, 18.50000, -61.8…\n$ `World bank region`                &lt;chr&gt; \"South Asia\", \"Europe & Central Asi…\n$ `World bank, 4 income groups 2017` &lt;chr&gt; \"Low income\", \"Upper middle income\"…"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#can-we-improve-our-model-by-adding-food-supply-as-a-covariate",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#can-we-improve-our-model-by-adding-food-supply-as-a-covariate",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Can we improve our model by adding food supply as a covariate?",
    "text": "Can we improve our model by adding food supply as a covariate?\n\n\n\n\n\n\nSimple linear regression population model\n\n\n\\[\\begin{aligned}\n\\text{Life expectancy} & = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\epsilon \\\\\n\\text{LE} & = \\beta_0 + \\beta_1 \\text{FLR} + \\epsilon\n\\end{aligned}\\]\n\n\n\n\nMultiple linear regression population model (with added Food Supply)\n\n\n\\[\\begin{aligned}\n\\text{Life expectancy} & = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_2 \\text{Food supply} + \\epsilon \\\\\n\\text{LE} & = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#visualize-relationship-between-life-expectancy-female-literacy-rate-and-food-supply",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#visualize-relationship-between-life-expectancy-female-literacy-rate-and-food-supply",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Visualize relationship between life expectancy, female literacy rate, and food supply",
    "text": "Visualize relationship between life expectancy, female literacy rate, and food supply\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#visualize-relationship-in-3-d",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#visualize-relationship-in-3-d",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Visualize relationship in 3-D",
    "text": "Visualize relationship in 3-D"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#poll-everywhere-question-1",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#poll-everywhere-question-1",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#how-do-we-fit-a-multiple-linear-regression-model-in-r",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#how-do-we-fit-a-multiple-linear-regression-model-in-r",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "How do we fit a multiple linear regression model in R?",
    "text": "How do we fit a multiple linear regression model in R?\nNew population model for example:\n\\[\\text{Life expectancy} = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_2 \\text{Food supply} + \\epsilon\\]\n\n# Fit regression model:\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\ntidy(mr1, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{Life expectancy}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{Female literacy rate} + \\widehat{\\beta}_2 \\text{Food supply} \\\\\n\\widehat{\\text{Life expectancy}} &= 33.595 + 0.157\\ \\text{Female literacy rate}\n+ 0.008\\ \\text{Food supply}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#dont-forget-summary-to-extract-information",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#dont-forget-summary-to-extract-information",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Don’t forget summary() to extract information!",
    "text": "Don’t forget summary() to extract information!\n\nsummary(mr1)\n\n\nCall:\nlm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n    data = gapm_sub)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.715  -2.328   1.052   3.022   9.083 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        33.595479   4.472049   7.512 1.56e-10 ***\nFemaleLiteracyRate  0.156699   0.032158   4.873 6.75e-06 ***\nFoodSupplykcPPD     0.008482   0.001795   4.726 1.17e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.391 on 69 degrees of freedom\nMultiple R-squared:  0.563, Adjusted R-squared:  0.5503 \nF-statistic: 44.44 on 2 and 69 DF,  p-value: 3.958e-13"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#visualize-the-fitted-multiple-regression-model",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#visualize-the-fitted-multiple-regression-model",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Visualize the fitted multiple regression model",
    "text": "Visualize the fitted multiple regression model\n\n\n\nThe fitted model equation \\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X_1 + \\widehat{\\beta}_2 \\cdot X_2\\] has three variables (\\(Y, X_1,\\) and \\(X_2\\)) and thus we need 3 dimensions to plot it\n\n \n\nInstead of a regression line, we get a regression plane\n\nSee code in .qmd- file. I hid it from view in the html file."
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#regression-lines-for-varying-values-of-food-supply",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#regression-lines-for-varying-values-of-food-supply",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Regression lines for varying values of food supply",
    "text": "Regression lines for varying values of food supply\n\\[\\begin{aligned}\n\\widehat{\\text{Life expectancy}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{Female literacy rate} + \\widehat{\\beta}_2 \\text{Food supply} \\\\\n\\widehat{\\text{Life expectancy}} &= 33.595 + 0.157 \\text{ Female literacy rate}\n+ 0.008 \\text{ Food supply}\n\\end{aligned}\\]\n\n\n\nNote: when the food supply is held constant but the female literacy rate varies…\n\nthen the outcome values change along a line\n\nDifferent values of food supply give different lines\n\nThe intercepts change, but\nthe slopes stay the same (parallel lines)\n\n\n\n\n(mr1_2d = ggPredict(mr1, interactive = T))"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#how-do-we-calculate-the-regression-line-for-3000-kc-ppd-food-supply",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#how-do-we-calculate-the-regression-line-for-3000-kc-ppd-food-supply",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "How do we calculate the regression line for 3000 kc PPD food supply?",
    "text": "How do we calculate the regression line for 3000 kc PPD food supply?\n\n\n \n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\ \\text{FS}\\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\cdot 3000 \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 24 \\\\\n\\widehat{\\text{LE}} &= 57.6 + 0.157\\ \\text{FLR}\n\\end{aligned}\\]\n\n\n(mr1_2d = ggPredict(mr1, interactive = T))"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#poll-everwhere-question-2",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#poll-everwhere-question-2",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everwhere Question 2",
    "text": "Poll Everwhere Question 2"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#population-multiple-regression-model",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#population-multiple-regression-model",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Population multiple regression model",
    "text": "Population multiple regression model\n\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\nor on the individual (observation) level:\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}+ \\ldots + \\beta_k x_{ik} + \\epsilon_i,\\ \\ \\text{for}\\ i = 1, 2, \\ldots, n\\]\n\n\n\nObservable sample data\n\n\\(Y\\) is our dependent variable\n\nAka outcome or response variable\n\n\\(X_1, X_2, \\ldots, X_k\\) are our \\(k\\) independent variables\n\nAka predictors or covariates\n\n\n\n\n\nUnobservable population parameters\n\n\\(\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k\\) are unknown population parameters\n\nFrom our sample, we find the population parameter estimates: \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, \\ldots, \\widehat{\\beta}_k\\)\n\n\\(\\epsilon\\) is the random error\n\nAnd is still normally distributed\n\\(\\epsilon \\sim N(0, \\sigma^2)\\) where \\(\\sigma^2\\) is the population parameter of the variance"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#how-do-we-estimate-the-model-parameters",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#how-do-we-estimate-the-model-parameters",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "How do we estimate the model parameters?",
    "text": "How do we estimate the model parameters?\n\nWe need to estimate the population model coefficients \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, \\ldots, \\widehat{\\beta}_k\\)\nThis can be done using the ordinary least-squares method\n\nFind the \\(\\widehat{\\beta}\\) values that minimize the sum of squares due to error (\\(SSE\\))\n\n\n\\[ \\begin{aligned}\nSSE & = \\displaystyle\\sum^n_{i=1} \\widehat\\epsilon_i^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{Y}_i)^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - (\\widehat{\\beta}_0 +\\widehat{\\beta}_1 X_{i1}+ \\ldots+\\widehat{\\beta}_1 X_{ik}))^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0 -\\widehat{\\beta}_1 X_{i1}- \\ldots-\\widehat{\\beta}_1 X_{ik})^2\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#technical-side-note-not-needed-in-our-class",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#technical-side-note-not-needed-in-our-class",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Technical side note (not needed in our class)",
    "text": "Technical side note (not needed in our class)\n\nThe equations for calculating the \\(\\boldsymbol{\\widehat{\\beta}}\\) values is best done using matrix notation (not required for our class)\nWe will be using R to get the coefficients instead of the equation (already did this a few slides back!)\nHow we have represented the population regression model: \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\n\n\n\nHow to represent population model with matrix notation:\n\n\\[\\begin{aligned}\n\\boldsymbol{Y} &= \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\\\\n\\boldsymbol{Y}_{n \\times 1}& = \\boldsymbol{X}_{n \\times (k+1)}\\boldsymbol{\\beta}_{(k+1)\\times 1} + \\boldsymbol{\\epsilon}_{n \\times 1}\n\\end{aligned}\\]\n\n\\[\n\\boldsymbol{Y} = \\left[\\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\n\\end{array} \\right]_{n \\times 1}\n\\] \\[\n\\boldsymbol{\\epsilon} = \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n\n\\end{array} \\right]_{n \\times 1}  \n\\]\n\n\\[\n\\boldsymbol{X} = \\left[ \\begin{array}{ccccc} 1 &  X_{11} &  X_{12} & \\ldots & X_{1,k} \\\\\n1 &X_{21} &  X_{22} & \\ldots & X_{2,k} \\\\\n\\vdots&\\vdots & \\vdots &  \\ldots & \\vdots \\\\\n1 & X_{n1} &  X_{n2} & \\ldots & X_{n,k} \\end{array} \\right]_{n \\times (k+1)}\n\\]\n\\[\n\\boldsymbol{\\beta}  = \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1\\\\  \\vdots \\\\\n\\beta_{k}\n\\end{array} \\right]_{(k+1)\\times 1}\n\\]"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#line-model-assumptions",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#line-model-assumptions",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "LINE model assumptions",
    "text": "LINE model assumptions\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nThe mean value of \\(Y\\) given any combination of \\(X_1, X_2, \\ldots, X_k\\) values, is a linear function of \\(\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k\\):\n\\[\\mu_{Y|X_1, \\ldots, X_k} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k\\]\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nObservations (\\(X_1, X_2, \\ldots, X_k, Y\\)) are independent from one another\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\n\\(Y\\) has a normal distribution for any any combination of \\(X_1, X_2, \\ldots, X_k\\) values\n\nThus, the residuals are normally distributed\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nThe variance of \\(Y\\) is the same for any any combination of \\(X_1, X_2, \\ldots, X_k\\) values\n\\[\\sigma^2_{Y|X_1, X_2, \\ldots, X_k} = Var(Y|X_1, X_2, \\ldots, X_k) = \\sigma^2\\]"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#summary-of-the-line-assumptions",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#summary-of-the-line-assumptions",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Summary of the LINE assumptions",
    "text": "Summary of the LINE assumptions\n\nEquivalently, the residuals are independently and identically distributed (iid):\n\nnormal\nwith mean 0 and\nconstant variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#variation-explained-vs.-unexplained",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#variation-explained-vs.-unexplained",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Variation: Explained vs. Unexplained",
    "text": "Variation: Explained vs. Unexplained\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\bar{Y})^2 &= \\sum_{i=1}^n (\\widehat{Y}_i- \\bar{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY &= SSR + SSE\n\\end{aligned}\\]\n\n\\(Y_i - \\bar{Y}\\) = the deviation of \\(Y_i\\) around the mean \\(\\bar{Y}\\)\n\n(the total amount deviation unexplained at \\(X_{i1},\\ldots,X_{ik}\\) ).\n\n\\(\\widehat{Y}_i- \\bar{Y}\\) = the deviation of the fitted value \\(\\widehat{Y}_i\\) around the mean \\(\\bar{Y}\\)\n\n(the amount deviation explained by the regression at \\(X_{i1},\\ldots,X_{ik}\\) ).\n\n\\(Y_i - \\widehat{Y}_i\\) = the deviation of the observation \\(Y\\) around the fitted regression line\n\n(the amount deviation unexplained by the regression at \\(X_{i1},\\ldots,X_{ik}\\) )."
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#poll-everywhere-question-3",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#poll-everywhere-question-3",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#building-the-anova-table",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#building-the-anova-table",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Building the ANOVA table",
    "text": "Building the ANOVA table\nANOVA table (\\(k\\) = # of predictors, \\(n\\) = # of observations)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariation Source\ndf\nSS\nMS\ntest statistic\np-value\n\n\n\n\nRegression\n\\(k\\)\n\\(SSR\\)\n\\(MSR = \\frac{SSR}{k}\\)\n\\(F = \\frac{MSR}{MSE}\\)\n\\(F \\sim F_{(k, n-k-1)}\\)\n\n\nError\n\\(n-k-1\\)\n\\(SSE\\)\n\\(MSE = \\frac{SSE}{n-k-1}\\)\n\n\n\n\nTotal\n\\(n-1\\)\n\\(SSY\\)\n\n\n\n\n\n\n\n\n \n\nanova(mr1) %&gt;% tidy() %&gt;% gt() %&gt;%\n   tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1.000\n1,934.245\n1,934.245\n66.547\n0.000\n    FoodSupplykcPPD\n1.000\n649.319\n649.319\n22.339\n0.000\n    Residuals\n69.000\n2,005.556\n29.066\nNA\nNA"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#interpreting-the-estimated-population-coefficients",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#interpreting-the-estimated-population-coefficients",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Interpreting the estimated population coefficients",
    "text": "Interpreting the estimated population coefficients\n\nFor a population model: \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\epsilon\\]\n\nWhere \\(X_1\\) and \\(X_2\\) are continuous variables\nNo need to specify \\(Y\\) because it required to be continuous in linear regression\n\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB)."
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#poll-everywhere-question-4",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#poll-everywhere-question-4",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#getting-these-interpretations-from-our-regression-table",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#getting-these-interpretations-from-our-regression-table",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Getting these interpretations from our regression table",
    "text": "Getting these interpretations from our regression table\nWe fit the regression model in R and printed the regression table:\n\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model: \\(\\widehat{\\text{LE}} = 33.595 + 0.157 \\text{ FLR} + 0.008 \\text{ FS}\\)\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 0 kcal PPD (95% CI: 24.674, 41.517).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012)."
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#lets-just-examine-the-general-interpretation-vs.-the-example",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#lets-just-examine-the-general-interpretation-vs.-the-example",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Let’s just examine the general interpretation vs. the example",
    "text": "Let’s just examine the general interpretation vs. the example\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 0 kcal PPD (95% CI: 24.674, 41.517).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012)."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred_key_info.html#key-dates",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "",
    "text": "Describe the model assumptions made in linear regression using ordinary least squares\nDetermine if the relationship between our sampled X and Y is linear\nUse QQ plots to determine if our fitted model holds the normality assumption\nUse residual plots to determine if our fitted model holds the equality of variance assumption\n\n\n\n\nWe have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]\n\n\n\n\n\nRows: 188 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (2): life_expectancy_years_2011, female_literacy_rate_2011\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe residuals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "",
    "text": "We have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]\n\n\n\n\n\nRows: 188 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (2): life_expectancy_years_2011, female_literacy_rate_2011\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "",
    "text": "The residuals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#least-squares-model-assumptions-line",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#least-squares-model-assumptions-line",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Least-squares model assumptions: LINE",
    "text": "Least-squares model assumptions: LINE\n \nThese are the model assumptions made in ordinary least squares:\n \n\n[L] Linearity of relationship between variables\n\n\n[I] Independence of the \\(Y\\) values\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\n[E] Equality of variance of the residuals (homoscedasticity)"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#l-linearity",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#l-linearity",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "L: Linearity",
    "text": "L: Linearity\n\n\n\nThe relationship between the variables is linear (a straight line):\n\nThe mean value of \\(Y\\) given \\(X\\), \\(\\mu_{y|x}\\) or \\(E[Y|X]\\), is a straight-line function of \\(X\\)\n\n\n\\[\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\]\n\n\n\nWarning in geom_point(size = 3, se = FALSE): Ignoring unknown parameters: `se`\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#i-independence-of-observations",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#i-independence-of-observations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "I: Independence of observations",
    "text": "I: Independence of observations\n\nThe \\(Y\\)-values are statistically independent of one another\nExamples of when they are not independent, include\n\nrepeated measures (such as baseline, 3 months, 6 months)\ndata from clusters, such as different hospitals or families\n\nThis condition is checked by reviewing the study design and not by inspecting the data\n\n \n\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#poll-everywhere-question-1",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#poll-everywhere-question-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-normality",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-normality",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Normality",
    "text": "N: Normality\n\n\n\nFor any fixed value of \\(X\\), \\(Y\\) has normal distribution.\n\nNote: This is not about \\(Y\\) alone, but \\(Y|X\\)\n\nEquivalently, the measurement (random) errors \\(\\epsilon_i\\) ’s normally distributed\n\nThis is more often what we check"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#e-equality-of-variance-of-the-residuals",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#e-equality-of-variance-of-the-residuals",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "E: Equality of variance of the residuals",
    "text": "E: Equality of variance of the residuals\n\nThe variance of \\(Y\\) given \\(X\\) (\\(\\sigma_{Y|X}^2\\)), is the same for any \\(X\\)\n\nWe use just \\(\\sigma^2\\) to denote the common variance\n\nThis is also called homoscedasticity"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#summary-of-line-model-assumptions",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#summary-of-line-model-assumptions",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Summary of LINE model assumptions",
    "text": "Summary of LINE model assumptions\n\n\\(Y\\) values are independent (check study design!)\n\n\n\n\n\n\nThe distribution of \\(Y\\) given \\(X\\) is\n\nnormal\nwith mean \\(\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\)\nand common variance \\(\\sigma^2\\)\n\n\nThis means that the residuals are\n\nnormal\nwith mean = 0\nand common variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#how-do-we-determine-if-our-model-follows-the-line-assumptions",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#how-do-we-determine-if-our-model-follows-the-line-assumptions",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "How do we determine if our model follows the LINE assumptions?",
    "text": "How do we determine if our model follows the LINE assumptions?\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nCheck if there is a linear relationship between the mean response (Y) and the explanatory variable (X)\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nCheck that the observations are independent\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\nCheck that the responses (at each level X) are normally distributed\n\nUsually measured through the residuals\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nCheck that the variance (or standard deviation) of the responses is equal for all levels of X\n\nUsually measured through the residuals"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#l-linearity-of-relationship-between-variables",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#l-linearity-of-relationship-between-variables",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "L: Linearity of relationship between variables",
    "text": "L: Linearity of relationship between variables\n\n\n\n\n\nIs the association between the variables linear?\n\n\n\n\nDiagnostic tool: Scatterplot of \\(X\\) vs. \\(Y\\)\n\n\n\nWarning in geom_point(size = 3, se = FALSE): Ignoring unknown parameters: `se`\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#poll-everywhere-question-2",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#poll-everywhere-question-2",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#i-independence-of-the-residuals-y-values",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#i-independence-of-the-residuals-y-values",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "I: Independence of the residuals (\\(Y\\) values)",
    "text": "I: Independence of the residuals (\\(Y\\) values)\n\nAre the data points independent of each other?\n\n \n\nDiagnostic tool: reviewing the study design and not by inspecting the data"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-normality-of-the-residuals",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-normality-of-the-residuals",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Normality of the residuals",
    "text": "N: Normality of the residuals\n\nWe need to check if the errors/residuals (\\(\\epsilon_i\\)’s) are normally distributed\n\n \n\nDiagnostic tools:\n\nDistribution plots of residuals\nQQ plots of residuals\n\n\n \n\nExtra resource on how QQ plots are made"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-extract-models-residuals-in-r",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-extract-models-residuals-in-r",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Extract model’s residuals in R",
    "text": "N: Extract model’s residuals in R\n\nFirst extract the residuals’ values from the model output using the augment() function from the broom package.\nGet a tibble with the orginal data, as well as the residuals and some other important values.\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, \n                data = gapm)\naug1 &lt;- augment(model1) \n\nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-check-normality-with-usual-distribution-plots",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-check-normality-with-usual-distribution-plots",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Check normality with “usual” distribution plots",
    "text": "N: Check normality with “usual” distribution plots\nNote that below I save each figure as an object, and then combine them together in one row of output using grid.arrange() from the gridExtra package\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_boxplot()\n\ngrid.arrange(hist1, density1, box1, nrow = 1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-normal-qq-plots-qq-quantile-quantile",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-normal-qq-plots-qq-quantile-quantile",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Normal QQ plots (QQ = quantile-quantile)",
    "text": "N: Normal QQ plots (QQ = quantile-quantile)\n\nIt can be tricky to eyeball with a histogram or density plot whether the residuals are normal or not\nQQ plots are often used to help with this\n\n\n\n\nVertical axis: data quantiles\n\ndata points are sorted in order and\nassigned quantiles based on how many data points there are\n\nHorizontal axis: theoretical quantiles\n\nmean and standard deviation (SD) calculated from the data points\ntheoretical quantiles are calculated for each point, assuming the data are modeled by a normal distribution with the mean and SD of the data\n\n\n \n\nData are approximately normal if points fall on a line."
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-examples-of-normal-qq-plots-from-n100-observations",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-examples-of-normal-qq-plots-from-n100-observations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Examples of Normal QQ plots (from \\(n=100\\) observations)",
    "text": "N: Examples of Normal QQ plots (from \\(n=100\\) observations)\n\n\n\n\nNormal\n\nUniform\n\nT\n\nSkewed\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-examples-of-normal-qq-plots-from-n10-observations",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-examples-of-normal-qq-plots-from-n10-observations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Examples of Normal QQ plots (from \\(n=10\\) observations)",
    "text": "N: Examples of Normal QQ plots (from \\(n=10\\) observations)\n\n\n\n\nNormal\n\nUniform\n\nT\n\nSkewed\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-examples-of-normal-qq-plots-from-n1000-observations",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-examples-of-normal-qq-plots-from-n1000-observations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Examples of Normal QQ plots (from \\(n=1000\\) observations)",
    "text": "N: Examples of Normal QQ plots (from \\(n=1000\\) observations)\n\n\n\n\nNormal\n\nUniform\n\nT\n\nSkewed\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-we-can-compare-the-qq-plots-model-vs.-theoretical",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-we-can-compare-the-qq-plots-model-vs.-theoretical",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: We can compare the QQ plots: model vs. theoretical",
    "text": "N: We can compare the QQ plots: model vs. theoretical\n\n\n\nResiduals from Life Expectancy vs. Female Literacy Rate Regression\n\n\nggplot(aug1, \n      aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line() \n\n\n\n\n\n\n\n\n\n\nSimulated QQ plot of Normal Residuals with \\(n = 80\\)\n\n\nggplot() +\n  stat_qq(aes(\n    sample = rnorm(80))) + \n  geom_abline(\n    intercept = 0, slope = 1, \n    color = \"blue\")"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-shapiro-wilk-test-of-normality",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-shapiro-wilk-test-of-normality",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Shapiro-Wilk Test of Normality",
    "text": "N: Shapiro-Wilk Test of Normality\n\nGoodness-of-fit test for the normal distribution: Is there evidence that our residuals are from a normal distribution?\nHypothesis test:\n\n\\[\\begin{aligned}\nH_0 & : \\text{data are from a normally distributed population} \\\\\nH_1 & : \\text{data are NOT from a normally distributed population}\n\\end{aligned}\\]\n\n\n\nshapiro.test(aug1$.resid)\n\n\n    Shapiro-Wilk normality test\n\ndata:  aug1$.resid\nW = 0.90575, p-value = 2.148e-05\n\n\n\n\n\nConclusion\n\n\nReject the null. Data are not from a normal distribution."
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#e-equality-of-variance-of-the-residuals-1",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#e-equality-of-variance-of-the-residuals-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "E: Equality of variance of the residuals",
    "text": "E: Equality of variance of the residuals\n\nHomoscedasticity: How do we determine if the variance across X values is constant?\nDiagnostic tool: residual plot"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#e-creating-a-residual-plot",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#e-creating-a-residual-plot",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "E: Creating a residual plot",
    "text": "E: Creating a residual plot\n\n\n\n\\(x\\) = explanatory variable from regression model\n\n(or the fitted values for a multiple regression)\n\n\\(y\\) = residuals from regression model\n\n\nggplot(aug1, \n       aes(x = female_literacy_rate_2011, \n           y = .resid)) + \n  geom_point(size = 2) +\n  geom_abline( intercept = 0, slope = 0,\n    size = 2, color = \"#FF8021\") +\n  labs(title = \"Residual plot\") +\n  theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 30))"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#autoplot-can-be-a-helpful-tool",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#autoplot-can-be-a-helpful-tool",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "autoplot() can be a helpful tool",
    "text": "autoplot() can be a helpful tool\n\nlibrary(ggfortify)\nautoplot(model1) + theme(text=element_text(size=14))"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#summary-of-the-assumptions-and-their-diagnostic-tool",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#summary-of-the-assumptions-and-their-diagnostic-tool",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Summary of the assumptions and their diagnostic tool",
    "text": "Summary of the assumptions and their diagnostic tool\n\n\n\n\n\n\n\n\nAssumption\nWhat needs to hold?\nDiagnostic tool\n\n\n\n\nLinearity\n\\(\\text{}\\)\n\nRelationship between \\(X\\) and \\(Y\\) is linear\n\n\nScatterplot of \\(Y\\) vs. \\(X\\)\n\n\\(\\text{}\\)\n\n\nIndependence\n\\(\\text{}\\)\n\nObservations are independent from each other\n\n\nStudy design\n\n\\(\\text{}\\)\n\n\nNormality\n\\(\\text{}\\)\n\nResiduals (and thus \\(Y|X\\)) are normally distributed\n\n\nQQ plot of residuals\nDistribution of residuals\n\n\n\nEquality of variance\n\\(\\text{}\\)\n\nVariance of residuals (and thus \\(Y|X\\)) is same across \\(X\\) values (homoscedasticity)\n\n\nResidual plot\n\n\\(\\text{}\\)"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html",
    "title": "Lesson 13: Purposeful model selection",
    "section": "",
    "text": "Understand the overall steps for purposeful selection as a model building strategy\nApply purposeful selection to a dataset using R\nUse different approaches to assess the linear scale of continuous variables in logistic regression\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#regression-analysis-process",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#regression-analysis-process",
    "title": "Lesson 13: Purposeful model selection",
    "section": "",
    "text": "Model Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#section",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#section",
    "title": "Lesson 13: Purposeful model selection",
    "section": "",
    "text": "“Successful modeling of a complex data set is part science, part statistical methods, and part experience and common sense.”\n\n \n\nHosmer, Lemeshow, and Sturdivant Textbook, pg. 101"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#overall-process",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#overall-process",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Overall Process",
    "text": "Overall Process\n\nExploratory data analysis\nCheck unadjusted associations in simple linear regression\nEnter all covariates in model that meet some threshold\n\nOne textbook suggest \\(p&lt;0.2\\) or \\(p&lt;0.25\\): great for modest sized datasets\nPLEASE keep in mind sample size in your study\nCan also use magnitude of association rather than, or along with, p-value\n\nRemove those that no longer reach some threshold\n\nCompare magnitude of associations to unadjusted version (univariable)\n\nCheck scaling of continuous and coding of categorical covariates\nCheck for interactions\nAssess model fit\n\nModel assumptions, diagnostics, overall fit"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#process-with-snappier-step-names",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#process-with-snappier-step-names",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Process with snappier step names",
    "text": "Process with snappier step names\n\n\nPre-step:\n \nStep 1:\n \nStep 2:\n \nStep 3:\n \nStep 4:\n \nStep 5:\n \nStep 6:\n\nExploratory data analysis (EDA)\n \nSimple linear regressions / analysis\n \nPreliminary variable selection\n \nAssess change in coefficients\n \nAssess scale for continuous variables\n \nCheck for interactions\n \nAssess model fit"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis",
    "text": "Pre-step: Exploratory data analysis\n\nThings we have been doing over the quarter in class and in our project\nI will not discuss some of the methods mentioned in our lab and data management class\n\nI am only going to introduce additional exploratory functions\n\n\n \nA few things we can do:\n\nCheck the data\nStudy your variables\nMissing data?\nExplore simple relationships and assumptions"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-check-the-data",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-check-the-data",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Check the data",
    "text": "Pre-step: Exploratory data analysis: Check the data\n\n\n\nGet to know the potential values for the data\n\nCategories\nUnits\n\nThen make sure the summary of values makes sense\n\nIf minimum or maximum look outside appropriate range\nFor example: a negative value for a measurement that is inherently positive (like population or income)\n\n\n\n\n\n\nhttps://www.gapminder.org/data/documentation/"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-check-the-data-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-check-the-data-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Check the data",
    "text": "Pre-step: Exploratory data analysis: Check the data\n\n\n\nLook at a summary for the raw data\nTypical use:\n\n\nlibrary(skimr)\nskim(gapm)\n\n\nSome skim() help"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-check-the-data-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-check-the-data-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Check the data",
    "text": "Pre-step: Exploratory data analysis: Check the data\n\n\n\nLook at a summary for the raw data\nTypical use:\n\n\nlibrary(skimr)\nskim(gapm)\n\n\nSome skim() help\nNote that skim(gapm) looks different because I had to create factors\nI am breaking down the skim() function into the categorical and continuous variables only because I want to show them on the slides\n\n\n\n\n\nskim(gapm_sub1) %&gt;% yank(\"factor\")\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nfour_regions\n0\n1.00\nFALSE\n4\nAsi: 57, Afr: 54, Eur: 49, Ame: 35\n\n\nincome_levels1\n1\n0.99\nFALSE\n4\nHig: 56, Upp: 55, Low: 52, Low: 31\n\n\nincome_levels2\n1\n0.99\nFALSE\n2\nHig: 111, Low: 83"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-check-the-data-3",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-check-the-data-3",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Check the data",
    "text": "Pre-step: Exploratory data analysis: Check the data\n\nskim(gapm_sub1) %&gt;% yank(\"numeric\")\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nCO2emissions\n4\n0.98\n4.55\n6.10\n0.03\n0.64\n2.41\n6.22\n41.20\n▇▁▁▁▁\n\n\nElectricityUsePP\n58\n0.70\n4220.92\n5964.07\n31.10\n699.00\n2410.00\n5600.00\n52400.00\n▇▁▁▁▁\n\n\nFoodSupplykcPPD\n27\n0.86\n2825.06\n443.59\n1910.00\n2490.00\n2775.00\n3172.50\n3740.00\n▅▇▇▇▅\n\n\nIncomePP\n2\n0.99\n16704.45\n19098.61\n614.00\n3370.00\n10100.00\n22700.00\n129000.00\n▇▂▁▁▁\n\n\nLifeExpectancyYrs\n8\n0.96\n70.66\n8.44\n47.50\n64.30\n72.70\n76.90\n82.90\n▁▃▃▇▇\n\n\nFemaleLiteracyRate\n115\n0.41\n81.65\n21.95\n13.00\n70.97\n91.60\n98.03\n99.80\n▁▁▂▁▇\n\n\nWaterSourcePrct\n1\n0.99\n84.84\n18.64\n18.30\n74.90\n93.50\n99.07\n100.00\n▁▁▂▂▇\n\n\nLatitude\n0\n1.00\n19.11\n23.93\n-42.00\n4.00\n17.33\n40.00\n65.00\n▁▃▇▆▅\n\n\nLongitude\n0\n1.00\n21.98\n66.52\n-175.00\n-5.75\n21.00\n49.27\n179.14\n▁▃▇▃▂\n\n\npopulation_mill\n0\n1.00\n35.95\n136.87\n0.00\n1.73\n7.57\n24.50\n1370.00\n▇▁▁▁▁"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-study-your-variables",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-study-your-variables",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Study your variables",
    "text": "Pre-step: Exploratory data analysis: Study your variables\n\nStarted this a little bit in previous slide (skim()), but you may want to look at things like:\n\nSample size\nCounts of missing data\nMeans and standard deviations\nIQRs\nMedians\nMinimums and maximums\n\nCan also look at visuals\n\nContinuous variables: histograms (in `skimr() a little)\nCategorical variables: frequency plots"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-study-your-variables-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-study-your-variables-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Study your variables",
    "text": "Pre-step: Exploratory data analysis: Study your variables\n\nlibrary(Hmisc)\n\n\nAttaching package: 'Hmisc'\n\n\nThe following object is masked from 'package:plotly':\n\n    subplot\n\n\nThe following object is masked from 'package:gt':\n\n    html\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\nhist.data.frame(gapm %&gt;% select(-Longitude, -Latitude, -eight_regions, -six_regions, -geo, -`World bank, 4 income groups 2017`, -country, -population, -`World bank region`, -ElectricityUsePP))"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-missing-data",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-missing-data",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Missing data",
    "text": "Pre-step: Exploratory data analysis: Missing data\n\nWhy are there missing data?\nWhich variables and observations should be excluded because of missing data?\nWill I impute missing data?\n\n \n\nUnfortunately, we don’t have time to discuss missing data more thoroughly\n\nI will try to cover this topic more thoroughly in BSTA 513\n\n \n\nFor the Gapminder dataset, we chose to use complete cases"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-step-1-explore-simple-relationships-and-assumptions",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-step-1-explore-simple-relationships-and-assumptions",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step / Step 1 : Explore simple relationships and assumptions",
    "text": "Pre-step / Step 1 : Explore simple relationships and assumptions\n\ngapm2 %&gt;% ggpairs() # gapm2 is a new dataset with some variables selected\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 27 rows containing missing values\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 5 rows containing missing values\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 9 rows containing missing values\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 115 rows containing missing values\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 4 rows containing missing values\n\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\nRemoved 4 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 27 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 27 rows containing missing values\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 28 rows containing missing values\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 123 rows containing missing values\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 27 rows containing missing values\n\n\nWarning: Removed 27 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\nRemoved 27 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 8 rows containing missing values\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 115 rows containing missing values\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 2 rows containing missing values\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\nRemoved 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\nWarning: Removed 9 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 28 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 115 rows containing missing values\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 8 rows containing missing values\n\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\nRemoved 8 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\nWarning: Removed 115 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 123 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 115 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 115 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 115 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 115 rows containing missing values\n\n\nWarning: Removed 115 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\nRemoved 115 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 115 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning: Removed 1 row containing non-finite outside the scale range (`stat_boxplot()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_boxplot()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 27 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 115 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 27 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 115 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_bin()`)."
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-3",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-3",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nFor each covariate, we want to see how it relates to the outcome (without adjusting for other covariates)\nWe can partially do this with visualizations\n\nHelps us see the data we throw it into regression that makes assumptions (like our LINE assumptions)\nggpairs() can be a quick way to do it\nggplot() can make each plot\n\n+ geom_boxplot() to make boxplots by groups for categorical covariates\n+ geom_jitter() + stat_summary() to make non-overlaping points with group means for categorical covariates\n+ geom_point() to make scatterplots for continuous covariates\n\n\nWe need to run simple linear regression\n\nWe’re calling regression with multi-level categories “simple” even though there are multiple coefficients"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nLet’s think back to our Gapminder dataset\nAlways good to start with our main relationship: life expectancy vs. female literacy rate\n\nThrowback to Lesson 3 SLR when we first visualized and ran lm() for this relationship\n\n\n\nmodel_FLR = lm(LifeExpectancyYrs ~ FemaleLiteracyRate, data = gapm_sub)\n\n \n\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nmodel_FLR = lm(LifeExpectancyYrs ~ FemaleLiteracyRate, data = gapm_sub)\ntidy(model_FLR) %&gt;% gt() %&gt;% tab_options(table.font.size = 40) %&gt;% \n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n51.438\n2.739\n18.782\n0.000\n    FemaleLiteracyRate\n0.230\n0.032\n7.141\n0.000"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-4",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-4",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nLet’s do this with one other variable before I show you a streamlined version of SLR\n\n\nmodel_WR = lm(LifeExpectancyYrs ~ four_regions, data = gapm_sub)\n\n \n\n\n\n\nCode\nggplot(gapm_sub, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_jitter(size = 1, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\",\n       caption = \"Diamonds = region averages\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\n\nanova(model_WR) %&gt;% tidy() %&gt;% gt() %&gt;%\n   tab_options(table.font.size = 40) %&gt;%\n   fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    four_regions\n3.000\n2,743.042\n914.347\n33.680\n0.000\n    Residuals\n68.000\n1,846.077\n27.148\nNA\nNA\n  \n  \n  \n\n\n\n\n \n\nRecall from Lesson 5 (SLR: More inference + Evaluation):\n\nanova() with one model name will compare the model (model_WR) to the intercept model"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-3",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-3",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nIf we do a good job visualizing the relationship between our outcome and each covariate, then we can proceed to a streamlined version of the F-test for each relationship\nFirst, I will select the variables that we are considering for model selection:\n\n\ngapm2 = gapm_sub %&gt;% select(LifeExpectancyYrs, CO2emissions, FoodSupplykcPPD, \n                            IncomePP, FemaleLiteracyRate, WaterSourcePrct, \n                            four_regions, members_oecd_g77)\n\n\nWe need to make sure our dataset only contains the variables we are considering for the model:\n\n\ngapm3 = gapm2 %&gt;% select(-LifeExpectancyYrs)"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-4",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-4",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nNow I can run the lapply() function, which allows me to run the same function multiple times over all the columns in gapm3\nFor each covariate I am running: lm(gapm2$LifeExpectancyYrs ~ x) %&gt;% anova()\n\nSo I am fitting the simple linear regression and printing the ANOVA table with F-test (comparing model with a without the covariate)\n\n\n\nlapply( gapm3, function(x) lm(gapm2$LifeExpectancyYrs ~ x) %&gt;% anova() )\n\n$CO2emissions\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx          1  452.3  452.31  7.6536 0.007241 **\nResiduals 70 4136.8   59.10                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$FoodSupplykcPPD\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 1893.4 1893.44  49.168 1.188e-09 ***\nResiduals 70 2695.7   38.51                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$IncomePP\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 1220.3 1220.34  25.358 3.557e-06 ***\nResiduals 70 3368.8   48.13                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$FemaleLiteracyRate\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 1934.2 1934.24  50.999 6.895e-10 ***\nResiduals 70 2654.9   37.93                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$WaterSourcePrct\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 2988.2 2988.20  130.66 &lt; 2.2e-16 ***\nResiduals 70 1600.9   22.87                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$four_regions\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          3 2743.0  914.35   33.68 1.858e-13 ***\nResiduals 68 1846.1   27.15                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$members_oecd_g77\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          2 1103.7  551.85  10.925 7.553e-05 ***\nResiduals 69 3485.4   50.51                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nWe can scroll through the output to see the ANOVA table for each covariate"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-5",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-5",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nWe can also filter the ANOVA table to just show the p-value for each F-test\n\n\nsapply( gapm3, function(x) anova( lm(gapm2$LifeExpectancyYrs ~ x) )$`Pr(&gt;F)` )\n\n     CO2emissions FoodSupplykcPPD     IncomePP FemaleLiteracyRate\n[1,]  0.007241207    1.187753e-09 3.557341e-06       6.894997e-10\n[2,]           NA              NA           NA                 NA\n     WaterSourcePrct four_regions members_oecd_g77\n[1,]    1.148644e-17 1.857818e-13      7.55261e-05\n[2,]              NA           NA               NA\n\n\n\nRow 1 is the p-value for the F-test\n\nThis will help us in Step 2"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-2-preliminary-variable-selection",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-2-preliminary-variable-selection",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 2: Preliminary variable selection",
    "text": "Step 2: Preliminary variable selection\n\nIdentify candidates for your first multivariable model by performing an F-test on each covariate’s SLR\n\nUsing p-values from previous slide\nIf the p-value of the test is less than 0.25, then consider the variable a candidate\n\n\n \n\nCandidates for first multivariable model\n\nAll clinically important variables (regardless of p-value)\nVariables with univariate test with p-value &lt; 0.25\n\n\n \n\nWith more experience, you won’t need to rely on these strict rules as much"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-2-preliminary-variable-selection-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-2-preliminary-variable-selection-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 2: Preliminary variable selection",
    "text": "Step 2: Preliminary variable selection\n\nFrom the previous p-values from the F-test on each covariate’s SLR\n\nDecision: we keep all the covariates since they all have a p-value &lt; 0.25\n\n\n\nsapply( gapm3, function(x) anova( lm(gapm2$LifeExpectancyYrs ~ x) )$`Pr(&gt;F)` )\n\n     CO2emissions FoodSupplykcPPD     IncomePP FemaleLiteracyRate\n[1,]  0.007241207    1.187753e-09 3.557341e-06       6.894997e-10\n[2,]           NA              NA           NA                 NA\n     WaterSourcePrct four_regions members_oecd_g77\n[1,]    1.148644e-17 1.857818e-13      7.55261e-05\n[2,]              NA           NA               NA"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-2-preliminary-variable-selection-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-2-preliminary-variable-selection-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 2: Preliminary variable selection",
    "text": "Step 2: Preliminary variable selection\n\nFit an initial model including any independent variable with p-value &lt; 0.25 and clinically important variables\n\n\ninit_model = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP +\n               four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77, \n                 data = gapm2)\ntidy(init_model, conf.int = T) %&gt;% gt() %&gt;% tab_options(table.font.size = 30) %&gt;% \n  fmt_number(decimals = 4)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n37.5560\n4.4083\n8.5194\n0.0000\n28.7410\n46.3710\n    FemaleLiteracyRate\n0.0020\n0.0352\n0.0580\n0.9539\n−0.0684\n0.0725\n    CO2emissions\n−0.2860\n0.1340\n−2.1344\n0.0368\n−0.5539\n−0.0181\n    IncomePP\n0.0002\n0.0001\n2.4133\n0.0188\n0.0000\n0.0003\n    four_regionsAmericas\n9.8963\n2.0031\n4.9405\n0.0000\n5.8909\n13.9017\n    four_regionsAsia\n5.7849\n1.5993\n3.6172\n0.0006\n2.5870\n8.9829\n    four_regionsEurope\n7.1421\n2.6994\n2.6458\n0.0104\n1.7442\n12.5399\n    WaterSourcePrct\n0.1377\n0.0658\n2.0928\n0.0405\n0.0061\n0.2693\n    FoodSupplykcPPD\n0.0052\n0.0021\n2.4961\n0.0153\n0.0010\n0.0093\n    members_oecd_g77oecd\n−0.3317\n2.5476\n−0.1302\n0.8968\n−5.4259\n4.7625\n    members_oecd_g77others\n0.3341\n2.2986\n0.1453\n0.8849\n−4.2622\n4.9304"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 3: Assess change in coefficient",
    "text": "Step 3: Assess change in coefficient\n\n\n\nThis is where we start identifying covariates that we might remove\n\n \n\nI would start by using the p-value to guide me towards specific variables\n\nFemale literacy rate, but that’s our main covariate\nmembers_oecd_g77\nMaybe water source percent?\n\n\n \n\nSome people will say you can use the p-value alone\n\nI like to double check that those variables do not have a large effect on the other coefficients\n\n\n\n\ntidy(init_model) %&gt;% gt() %&gt;% tab_options(table.font.size = 33) %&gt;%  \n  fmt_number(decimals = 4)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n37.5560\n4.4083\n8.5194\n0.0000\n    FemaleLiteracyRate\n0.0020\n0.0352\n0.0580\n0.9539\n    CO2emissions\n−0.2860\n0.1340\n−2.1344\n0.0368\n    IncomePP\n0.0002\n0.0001\n2.4133\n0.0188\n    four_regionsAmericas\n9.8963\n2.0031\n4.9405\n0.0000\n    four_regionsAsia\n5.7849\n1.5993\n3.6172\n0.0006\n    four_regionsEurope\n7.1421\n2.6994\n2.6458\n0.0104\n    WaterSourcePrct\n0.1377\n0.0658\n2.0928\n0.0405\n    FoodSupplykcPPD\n0.0052\n0.0021\n2.4961\n0.0153\n    members_oecd_g77oecd\n−0.3317\n2.5476\n−0.1302\n0.8968\n    members_oecd_g77others\n0.3341\n2.2986\n0.1453\n0.8849"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 3: Assess change in coefficient",
    "text": "Step 3: Assess change in coefficient\n\nVery similar to the process we used when looking at confounders\n\n \n\nOne variable at a time, we run the multivariable model with and without the variable\n\nWe look at the p-value of the F-test for the coefficients of said variable\nWe look at the percent change for the coefficient (\\(\\Delta\\%\\)) of our explanatory variable\n\n\n \n\nGeneral rule: We can remove a variable if…\n\np-value &gt; 0.05 for the F-test of its own coefficients\nAND change in coefficient (\\(\\Delta\\%\\)) of our explanatory variable is &lt; 10%"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 3: Assess change in coefficient",
    "text": "Step 3: Assess change in coefficient\n\nLet’s try this out on members_oecd_g77\n\n\n\nDisplay the ANOVA table with F-statistic and p-value\nmodel_full = init_model\nmodel_red = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP +\n               four_regions + WaterSourcePrct + FoodSupplykcPPD, \n                 data = gapm2)\nanova(model_full, model_red) %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP + four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n61.000\n999.201\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP + four_regions + WaterSourcePrct + FoodSupplykcPPD\n63.000\n1,000.988\n−2.000\n−1.787\n0.055\n0.947\n  \n  \n  \n\n\n\n\n\n\\(\\widehat\\beta_{FLR, full} = 0.002\\), \\(\\widehat\\beta_{FLR, red} = 0.0036\\)\n\n\\[\n\\Delta\\% = 100\\% \\cdot \\frac{\\widehat\\beta_{FLR, full} - \\widehat\\beta_{FLR, red}}{\\widehat\\beta_{FLR, full}} = 100\\% \\cdot \\frac{0.002 - 0.0036}{0.002} = -74.41\\%\n\\]\n\nBased off the percent change, I would keep this in the model"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient-3",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient-3",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 3: Assess change in coefficient",
    "text": "Step 3: Assess change in coefficient\n\nLet’s try this out on water source percent (even though the p-value was &lt; 0.05)\n\n\n\nDisplay the ANOVA table with F-statistic and p-value\nmodel_full = init_model\nmodel_red = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP +\n               four_regions + members_oecd_g77 + FoodSupplykcPPD, \n                 data = gapm2)\nanova(model_full, model_red) %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP + four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n61.000\n999.201\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP + four_regions + members_oecd_g77 + FoodSupplykcPPD\n62.000\n1,070.944\n−1.000\n−71.744\n4.380\n0.041\n  \n  \n  \n\n\n\n\n\n\\(\\widehat\\beta_{FLR, full} = 0.002\\), \\(\\widehat\\beta_{FLR, red} = 0.034\\)\n\n\\[\n\\Delta\\% = 100\\% \\cdot \\frac{\\widehat\\beta_{FLR, full} - \\widehat\\beta_{FLR, red}}{\\widehat\\beta_{FLR, full}} = 100\\% \\cdot \\frac{0.002 - 0.034}{0.002} = -1561.06\\%\n\\]\n\nBased off the percent change (and p-value), I would keep this in the model"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-5",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-5",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient-4",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient-4",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 3: Assess change in coefficient",
    "text": "Step 3: Assess change in coefficient\n\nAt the end of this step, we have a preliminary main effects model\nWhere the variables are excluded that met the following criteria:\n\nP-value &gt; 0.05 for the F-test of its own coefficients\nChange in coefficient (\\(\\Delta\\%\\)) of our explanatory variable is &lt; 10%\n\nIn our example, the preliminary main effects model (end of Step 3) was the same as the initial model (end of Step 2)\nPreliminary main effects model includes:\n\nFemaleLiteracyRate\nCO2emissions\nIncomePP\nfour_regions\nmembers_oecd_g77\nFoodSupplykcPPD\nWaterSupplePct"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#recap-of-steps-1-3",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#recap-of-steps-1-3",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Recap of Steps 1-3",
    "text": "Recap of Steps 1-3\n\nPre-step: Exploratory data analysis\nStep 1: Simple linear regressions / analysis\n\nLook at each covariate with outcome\nPerform SLR for each covariate\n\nStep 2: Preliminary variable selection\n\nFrom SLR, decide which variables go into the initial model\nUse F-test to see if each covariate (on its own) explains enough variation in outcome\nEnd with initial model\n\nStep 3: Assess change in coefficients\n\nFrom the initial model at end of step 2, we take a variable out of the model if:\n\nP-value &gt; 0.05 for the F-test of its own coefficients\nChange in coefficient (\\(\\Delta\\%\\)) of our explanatory variable is &lt; 10%\n\nEnd with preliminary main effects model"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables",
    "text": "Step 4: Assess scale for continuous variables\n\nWe assume the linear regression model is linear for each continuous variable\nWe need to assess linearity for continuous variables in the model\n\nDo this through smoothed scatterplots that we introduced in Lesson 6 (SLR Diagnostics)\nResidual plots (can be used in SLR) does not help us in MLR\nEach term in MLR model needs to have linearity with outcome\n\nThree methods/approaches to address the violation of linearity assumption:\n\nApproach 1: Categorize continuous variable\nApproach 2: Fractional Polynomials\nApproach 3: Spline functions\n\nApproach will depend on the covariate!!\nFor our class, only implement Approach 1 or 2\nModel at the end of Step 4 is the main effects model"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables",
    "text": "Step 4: Assess scale for continuous variables\n\n\nResidual plot does not help us with linearity in MLR\nlibrary(ggfortify)\nautoplot(model_full) + theme(text=element_text(size=14))"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables: Smoothed scatterplots",
    "text": "Step 4: Assess scale for continuous variables: Smoothed scatterplots\n\nSmoother scatterplots only check linearity, not addressing linearity issues\n\n \n\nCan also identify extreme observations\n\nAgain, just want to flag these values\nCan influence the assessment of linearity when using fractional polynomials or spline functions\n\n\n \n\nHelps us decide if the continuous variable can stay as is in the model\n\nProblem: if not linear, then we need to represent the variable in a new way (Approaches 1-3)"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables: Smoothed scatterplots",
    "text": "Step 4: Assess scale for continuous variables: Smoothed scatterplots\n\nIn Gapminder dataset, we have 5 continuous variables:\n\nCO2 Emissions\nFood Supply\nIncome\nFemale Literacy Rate\nWater source percent\n\nPlot each of these agains the outcome, life expectancy"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables: Smoothed scatterplots",
    "text": "Step 4: Assess scale for continuous variables: Smoothed scatterplots\n\n\nWe can quickly look at ggpairs() to identify variables\ngapm2 %&gt;% select(where(is.numeric)) %&gt;% \n  relocate(LifeExpectancyYrs, .after = last_col()) %&gt;% ggpairs()"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-3",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-3",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables: Smoothed scatterplots",
    "text": "Step 4: Assess scale for continuous variables: Smoothed scatterplots\n\n\nTake a look at C02, Food Supply, and Income\nCO2 = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = CO2emissions)) + \n  geom_point() +\n  geom_smooth(se=F) + labs(x = \"CO2 Emissions (kt)\", y = \"Life Expectancy (yrs)\")\n\nFS = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = FoodSupplykcPPD)) + \n  geom_point() +\n  geom_smooth(se=F) + labs(x = \"Food Supply (kcal PPD)\", y = \"Life Expectancy (yrs)\")\n\nIncome = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = IncomePP)) + \n  geom_point() +\n  geom_smooth(se=F) + labs(x = \"Income (GDP per capita)\", y = \"Life Expectancy (yrs)\")\n\ngrid.arrange(CO2, FS, Income, nrow=1)\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nFood Supply looks admissible\nCO2 Emissions and Income do not look very linear, but I want to zoom into the area of the plots that have most of the data"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-4",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-4",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables: Smoothed scatterplots",
    "text": "Step 4: Assess scale for continuous variables: Smoothed scatterplots\n\n\nZoom into areas on plots with more data\nCO2 = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = CO2emissions)) + \n  geom_point() + xlim(0,10) +\n  geom_smooth(se=F) + labs(x = \"CO2 Emissions (kt)\", y = \"Life Expectancy (yrs)\")\n\nFS = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = FoodSupplykcPPD)) + \n  geom_point() +\n  geom_smooth(se=F) + labs(x = \"Food Supply (kcal PPD)\", y = \"Life Expectancy (yrs)\")\n\nIncome = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = IncomePP)) + \n  geom_point() + xlim(0,40000) +\n  geom_smooth(se=F) + labs(x = \"Income (GDP per capita)\", y = \"Life Expectancy (yrs)\")\n\ngrid.arrange(CO2, FS, Income, nrow=1)\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nFood Supply still looks admissible\nCO2 Emissions and Income not linear: will address this!!"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables",
    "text": "Step 4: Assess scale for continuous variables\n\nThree methods/approaches to address the violation of linearity assumption:\n \n\nApproach 1: Categorize continuous variable\n\n \n\nApproach 2: Fractional Polynomials\n\n \n\nApproach 3: Spline functions"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 1: Categorize continuous variable",
    "text": "Step 4: Approach 1: Categorize continuous variable\n\nCategorize continuous variables\n\nPercentiles, quartiles, quantiles\n\nCreate indicator variables corresponding to each quartile\n\nMeaningful thresholds\n\nExample: income level groups discussed by Gapminder\n\n\nDisadvantages:\n\nTakes some time to create new variables, especially with multiple continuous covariates\nStart with quartiles, but might be more appropriate to use different splits\n\nNo set rules on this\n\n\nAdvantage: graphical and visually helps"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 1: Categorize continuous variable",
    "text": "Step 4: Approach 1: Categorize continuous variable\n\n\n\nFor income, I would use Gapminder’s income level groups\n\nDiscussed in Lesson 10 Categorical Covariates (slide 43)\n\n\n \n\nExperts in the field have developed these income groups\n\nI think this is best solution for income (that was not meeting linearity as a continuous variable)"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 1: Categorize continuous variable",
    "text": "Step 4: Approach 1: Categorize continuous variable\n\n\n\nLet’s still try it out with CO2 Emissions (kt)\nI have plotted the quartile lines of food supply with red lines\n\n\n\n\nTake a look at the quartiles within the scatterplot\nvline_coordinates= data.frame(Quantile_Name=names(quantile(gapm2$CO2emissions)),\n                          quantile_values=as.numeric(quantile(gapm2$CO2emissions)))\n\nggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = CO2emissions)) + \n  geom_point(size = 3) +\n  #geom_smooth(se=F) + \n  labs(x = \"CO2 Emissions (kt)\", y = \"Life Expectancy (yrs)\") +\n  geom_vline(data = vline_coordinates, aes(xintercept = quantile_values), \n             color = \"red\", linetype = \"dashed\", size = 2) +\n    theme(axis.title = element_text(size = 25), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 25))"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable-3",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable-3",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 1: Categorize continuous variable",
    "text": "Step 4: Approach 1: Categorize continuous variable\n\nLet’s make the quartiles for CO2 emissions:\n\n\nlibrary(dvmisc)\n\nLoading required package: rbenchmark\n\n\n\nAttaching package: 'dvmisc'\n\n\nThe following object is masked from 'package:tidyr':\n\n    expand_grid\n\ngapm2 = gapm2 %&gt;% \n  mutate(CO2_q = quant_groups(CO2emissions, groups = 4) %&gt;% factor())\n\nObservations per group: 18, 18, 18, 18. 0 missing.\n\n\n\n\nTake a look at the quartile means within the scatterplot\nggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = CO2_q)) + \n  # geom_point(size = 3, aes(y = LifeExpectancyYrs, x = CO2emissions)) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  #geom_smooth(se=F) + \n  labs(x = \"CO2 Emissions (kt)\", y = \"Life Expectancy (yrs)\") +\n    theme(axis.title = element_text(size = 25), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 25))"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable-4",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable-4",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 1: Categorize continuous variable",
    "text": "Step 4: Approach 1: Categorize continuous variable\n\n\n \n\nLet’s fit a new model with the two new representations for income and CO2 emissions\n\n \n\nRemember, this is the main effects model if we decide to make CO2 into quartiles\n\n\nmain_eff_model = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 +\n               four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77, \n                 data = gapm2)\n\n\n\ntidy(main_eff_model) %&gt;% gt() %&gt;% tab_options(table.font.size = 33) %&gt;%  \n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n39.877\n4.889\n8.157\n0.000\n    FemaleLiteracyRate\n−0.073\n0.047\n−1.555\n0.125\n    CO2_q(0.806,2.54]\n1.099\n1.914\n0.574\n0.568\n    CO2_q(2.54,4.66]\n−0.292\n2.419\n−0.121\n0.904\n    CO2_q(4.66,35.2]\n−0.595\n2.524\n−0.236\n0.814\n    income_levels1Lower middle income\n5.441\n2.343\n2.322\n0.024\n    income_levels1Upper middle income\n6.111\n2.954\n2.069\n0.043\n    income_levels1High income\n7.959\n3.277\n2.429\n0.018\n    four_regionsAmericas\n9.003\n2.050\n4.391\n0.000\n    four_regionsAsia\n5.260\n1.637\n3.213\n0.002\n    four_regionsEurope\n6.855\n2.871\n2.387\n0.020\n    WaterSourcePrct\n0.166\n0.066\n2.496\n0.015\n    FoodSupplykcPPD\n0.004\n0.002\n1.825\n0.073\n    members_oecd_g77oecd\n1.119\n2.674\n0.418\n0.677\n    members_oecd_g77others\n1.047\n2.511\n0.417\n0.678"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-2-fractional-polynomials",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-2-fractional-polynomials",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 2: Fractional Polynomials",
    "text": "Step 4: Approach 2: Fractional Polynomials\n\nMain concepts and transformations presented in Lesson 7 SLR: Model Evaluation and Diagnostics (slide 33 on)\nIdea: test many transformations of a continuous covariate\n\nBased on Royston and Altman, Applied Statistics, 1994\n\n\n \n\nRecall Tukey’s transformation (power) ladder\n\nAnd can use R’s gladder() to see the transformations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower p\n-3\n-2\n-1\n-1/2\n0\n1/2\n1\n2\n3\n\n\n\n\n\n\\(\\frac{1}{x^3}\\)\n\\(\\frac{1}{x^2}\\)\n\\(\\frac{1}{x}\\)\n\\(\\frac{1}{\\sqrt{x}}\\)\n\\(\\log(x)\\)\n\\(\\sqrt{x}\\)\n\\(x\\)\n\\(x^2\\)\n\\(x^3\\)\n\n\n\n \n\nWe can run through each and test different models, or use the approach from Lesson 7\nThere is also a package we can use!\n\nmfp package in R contains the fp() function"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-2-fractional-polynomials-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-2-fractional-polynomials-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 2: Fractional Polynomials",
    "text": "Step 4: Approach 2: Fractional Polynomials\n\nlibrary(mfp)\n\nLoading required package: survival\n\nfp_model_CO2 = mfp(LifeExpectancyYrs ~ FemaleLiteracyRate + \n                     fp(CO2emissions, df = 4) + income_levels1 + four_regions +\n                     WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77,\n               data = gapm2, family = \"gaussian\")\n\nfp_model_CO2$fptable %&gt;% gt(rownames_to_stub = T) %&gt;% tab_options(table.font.size = 24)\n\n\n\n\n\n  \n    \n      \n      df.initial\n      select\n      alpha\n      df.final\n      power1\n      power2\n    \n  \n  \n    four_regionsAmericas\n1\n1\n0.05\n1\n1\n.\n    four_regionsAsia\n1\n1\n0.05\n1\n1\n.\n    four_regionsEurope\n1\n1\n0.05\n1\n1\n.\n    WaterSourcePrct\n1\n1\n0.05\n1\n1\n.\n    income_levels1Lower middle income\n1\n1\n0.05\n1\n1\n.\n    income_levels1Upper middle income\n1\n1\n0.05\n1\n1\n.\n    income_levels1High income\n1\n1\n0.05\n1\n1\n.\n    FoodSupplykcPPD\n1\n1\n0.05\n1\n1\n.\n    FemaleLiteracyRate\n1\n1\n0.05\n1\n1\n.\n    CO2emissions\n4\n1\n0.05\n1\n1\n.\n    members_oecd_g77oecd\n1\n1\n0.05\n1\n1\n.\n    members_oecd_g77others\n1\n1\n0.05\n1\n1\n."
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-2-fractional-polynomials-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-2-fractional-polynomials-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 2: Fractional Polynomials",
    "text": "Step 4: Approach 2: Fractional Polynomials\n\n\n\n\n\n\n\n\n  \n    \n      \n      df.initial\n      select\n      alpha\n      df.final\n      power1\n      power2\n    \n  \n  \n    four_regionsAmericas\n1\n1\n0.05\n1\n1\n.\n    four_regionsAsia\n1\n1\n0.05\n1\n1\n.\n    four_regionsEurope\n1\n1\n0.05\n1\n1\n.\n    WaterSourcePrct\n1\n1\n0.05\n1\n1\n.\n    income_levels1Lower middle income\n1\n1\n0.05\n1\n1\n.\n    income_levels1Upper middle income\n1\n1\n0.05\n1\n1\n.\n    income_levels1High income\n1\n1\n0.05\n1\n1\n.\n    FoodSupplykcPPD\n1\n1\n0.05\n1\n1\n.\n    FemaleLiteracyRate\n1\n1\n0.05\n1\n1\n.\n    CO2emissions\n4\n1\n0.05\n1\n1\n.\n    members_oecd_g77oecd\n1\n1\n0.05\n1\n1\n.\n    members_oecd_g77others\n1\n1\n0.05\n1\n1\n.\n  \n  \n  \n\n\n\n\n\n\nConclusion from fractional polynomial is that CO2 does not need to be transformed\nA little counter-intuitive to what we saw in quartiles\nThus, I think leaving CO2 emissions as quartiles is best!"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-3-spline-functions",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-3-spline-functions",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 3: Spline functions",
    "text": "Step 4: Approach 3: Spline functions\n\nSpline function is to fit a series of smooth curves that joined at specific points (called knots)"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-3-spline-functions-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-3-spline-functions-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 3: Spline functions",
    "text": "Step 4: Approach 3: Spline functions\n\nNeed to specify knots for spline functions\n\nMore knots are flexible, but requires more parameters to estimate\nIn most applications three to five knots are sufficient\n\n\n \n\nWithin our class, fractional polynomials will be sufficient\n\n \n\nIf you think this is cool, I highly suggest you look into Functional Data Analysis (FDA) or Functional Regression\n\nJeffrey Morris is a big name in that field\n\n\n \n\nIn R there are a few options to incorporate splines\n\npspline( ): More information\nsmoothHR(): More information"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-conclusion-main-effects-model",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-conclusion-main-effects-model",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4 Conclusion: main effects model",
    "text": "Step 4 Conclusion: main effects model\n\n\n\nWe concluded that we will use:\n\nIncome levels (categorical) that Gapminder created\nQuartiles for CO2 Emissions\n\n\n\n\ntidy(main_eff_model) %&gt;% gt() %&gt;% tab_options(table.font.size = 33) %&gt;%  \n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n39.877\n4.889\n8.157\n0.000\n    FemaleLiteracyRate\n−0.073\n0.047\n−1.555\n0.125\n    CO2_q(0.806,2.54]\n1.099\n1.914\n0.574\n0.568\n    CO2_q(2.54,4.66]\n−0.292\n2.419\n−0.121\n0.904\n    CO2_q(4.66,35.2]\n−0.595\n2.524\n−0.236\n0.814\n    income_levels1Lower middle income\n5.441\n2.343\n2.322\n0.024\n    income_levels1Upper middle income\n6.111\n2.954\n2.069\n0.043\n    income_levels1High income\n7.959\n3.277\n2.429\n0.018\n    four_regionsAmericas\n9.003\n2.050\n4.391\n0.000\n    four_regionsAsia\n5.260\n1.637\n3.213\n0.002\n    four_regionsEurope\n6.855\n2.871\n2.387\n0.020\n    WaterSourcePrct\n0.166\n0.066\n2.496\n0.015\n    FoodSupplykcPPD\n0.004\n0.002\n1.825\n0.073\n    members_oecd_g77oecd\n1.119\n2.674\n0.418\n0.677\n    members_oecd_g77others\n1.047\n2.511\n0.417\n0.678"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 5: Check for interactions",
    "text": "Step 5: Check for interactions\n\nCreate a list of interaction terms from variables in the “main effects model” that has clinical plausibility\n\n \n\nAdd the interaction variables, one at a time, to the main effects model, and assess the significance using a likelihood ratio test or Wald test\n\nMay keep interaction terms with p-value &lt; 0.10 (or 0.05)\n\n\n \n\nKeep the main effects untouched, only simplify the interaction terms\n\n \n\nUse methods from Step 2 (comparing model with all interactions to a smaller model with interactions) to determine which interactions to keep\n\n \n\nThe model by the end of Step 5 is called the preliminary final model"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 5: Check for interactions",
    "text": "Step 5: Check for interactions\n\nvars = names(model.frame(main_eff_model))[-1] \n\ninteractions = combn(vars, 2, function(x) paste(x, collapse=\" * \")) %&gt;% \n    grep(., pattern = \"FemaleLiteracyRate\", value = T) \n\n\nMLRs = lapply(interactions, function(int)\n  lm(reformulate(c(vars, int), \"LifeExpectancyYrs\"), data = gapm2))"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 5: Check for interactions",
    "text": "Step 5: Check for interactions\n\n\n\nMLRs[[1]] %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 33) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n37.326\n5.463\n6.833\n0.000\n    FemaleLiteracyRate\n−0.035\n0.058\n−0.602\n0.550\n    CO2_q(0.806,2.54]\n9.049\n7.248\n1.249\n0.217\n    CO2_q(2.54,4.66]\n7.843\n16.082\n0.488\n0.628\n    CO2_q(4.66,35.2]\n−5.980\n25.867\n−0.231\n0.818\n    income_levels1Lower middle income\n4.032\n2.661\n1.515\n0.136\n    income_levels1Upper middle income\n4.997\n3.239\n1.543\n0.129\n    income_levels1High income\n6.825\n3.549\n1.923\n0.060\n    four_regionsAmericas\n9.317\n2.193\n4.250\n0.000\n    four_regionsAsia\n5.412\n1.668\n3.246\n0.002\n    four_regionsEurope\n7.267\n2.992\n2.429\n0.018\n    WaterSourcePrct\n0.178\n0.070\n2.529\n0.014\n    FoodSupplykcPPD\n0.004\n0.002\n1.706\n0.094\n    members_oecd_g77oecd\n0.798\n2.731\n0.292\n0.771\n    members_oecd_g77others\n1.121\n2.588\n0.433\n0.667\n    FemaleLiteracyRate:CO2_q(0.806,2.54]\n−0.104\n0.091\n−1.144\n0.258\n    FemaleLiteracyRate:CO2_q(2.54,4.66]\n−0.104\n0.177\n−0.590\n0.557\n    FemaleLiteracyRate:CO2_q(4.66,35.2]\n0.038\n0.268\n0.141\n0.889"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-3",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-3",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 5: Check for interactions",
    "text": "Step 5: Check for interactions\nYou can alse go straight to using the anova() function to compare the preliminary model.\n\nanova_res = lapply(interactions,\n            function(int) anova(lm(reformulate(c(vars, int), \"LifeExpectancyYrs\"),\n                                    data = gapm2), main_eff_model)) \nanova_res[[1]] %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + FemaleLiteracyRate * CO2_q\n54.000\n919.287\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n57.000\n946.458\n−3.000\n−27.171\n0.532\n0.662"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-4",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-4",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 5: Check for interactions",
    "text": "Step 5: Check for interactions\n\nI went through all the ANOVA tables, and found the following significant interactions:\n\nNone!\n\n\n\nanova_res\n\n[[1]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * CO2_q\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq     F Pr(&gt;F)\n1     54 919.29                          \n2     57 946.46 -3   -27.171 0.532 0.6623\n\n[[2]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * income_levels1\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     54 933.66                           \n2     57 946.46 -3   -12.802 0.2468 0.8633\n\n[[3]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * four_regions\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     54 850.47                           \n2     57 946.46 -3   -95.987 2.0315 0.1203\n\n[[4]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * WaterSourcePrct\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     56 943.42                           \n2     57 946.46 -1   -3.0399 0.1804 0.6726\n\n[[5]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * FoodSupplykcPPD\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     56 915.40                           \n2     57 946.46 -1   -31.063 1.9003 0.1735\n\n[[6]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * members_oecd_g77\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     55 934.95                           \n2     57 946.46 -2   -11.513 0.3386 0.7142\n\n\n \n\nThink about it: does that track with what we saw in our interactions lecture?"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-6-assess-model-fit",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-6-assess-model-fit",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 6: Assess model fit",
    "text": "Step 6: Assess model fit\n\nAssess the adequacy of the model and check its fit\n\n \n\nMethods will be discussed next class\n\nCombination of diagnostics and model fit statistics!\nLook at model fit statistics in this lesson\nLook at diagnostics in Lesson 14: MLR Diagnostics\n\n\n \n\nIf the model is adequate and fits well, then it is the Final model"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-6-assess-model-fit-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-6-assess-model-fit-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 6: Assess model fit",
    "text": "Step 6: Assess model fit\n\nfinal_model = main_eff_model\nprelim_me_model = model_full\nsave(final_model, prelim_me_model, gapm2, file = here(\"./lessons/15_MLR_Diagnostics/final_mod.rda\"))\n\n\nOur final model contains\n\nFemale Literacy Rate FLR\nCO2 Emissions in quartiles CO2_q\nIncome levels in groups assigned by Gapminder income_levels\nWorld regions four_regions\nMembership of global and economic groups members_oecd_g77\n\nOECD: Organization for Economic Co-operation and Development\nG77: Group of 77\nOther\n\nFood Supply FoodSupplykcPPD\nClean Water Supply WaterSupplePct"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-6-assess-model-fit-model-fit-statistics",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-6-assess-model-fit-model-fit-statistics",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 6: Assess model fit: Model fit statistics",
    "text": "Step 6: Assess model fit: Model fit statistics\n\nWay I did it in the lab instructions\n\n\nsum_fm = summary(final_model)\nmodel_fit_stats = data.frame(Model = \"Final model\", \n                             Adjusted_R_sq = sum_fm$adj.r.squared, \n                             AIC = AIC(final_model), BIC = BIC(final_model))\n\nmodel_fit_stats %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      Model\n      Adjusted_R_sq\n      AIC\n      BIC\n    \n  \n  \n    Final model\n0.743\n421.804\n458.230\n  \n  \n  \n\n\n\n\n\nAnother (maybe faster?) way to do it (glance() in broom package)\n\n\nglance(final_model) %&gt;% mutate(Model = \"Final model\") %&gt;%\n  select(Model, adj.r.squared, AIC, BIC) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      Model\n      adj.r.squared\n      AIC\n      BIC\n    \n  \n  \n    Final model\n0.743\n421.804\n458.230"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-6-assess-model-fit-comparing-model-fits",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-6-assess-model-fit-comparing-model-fits",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 6: Assess model fit: Comparing model fits",
    "text": "Step 6: Assess model fit: Comparing model fits\n\nRemember the preliminary main effects model (at end of Step 3): same as final model but the continuous varaibles, income and CO2 emissions, were not categorized\nWe can compare model fit statistics of the preliminary main effects model and the final model\n\n\nfm_glance = glance(final_model) %&gt;% mutate(Model = \"Final model\") %&gt;%\n  select(Model, `Adj R-squared` = adj.r.squared, AIC, BIC) \npmem_glance = glance(prelim_me_model) %&gt;% \n  mutate(Model = \"Preliminary main effects model\") %&gt;%\n  select(Model, `Adj R-squared` = adj.r.squared, AIC, BIC) \nrbind(fm_glance, pmem_glance) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      Model\n      Adj R-squared\n      AIC\n      BIC\n    \n  \n  \n    Final model\n0.743\n421.804\n458.230\n    Preliminary main effects model\n0.747\n417.708\n445.028\n  \n  \n  \n\n\n\n\n\nRemember, adjusted \\(R^2\\), AIC, and BIC penalize models for more coefficients\nPreliminary main effects model: better fit statistics, but violates linearity assumption"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html",
    "href": "lessons/02_Data_Management/02_Data_Management.html",
    "title": "Data Management with the tidyverse",
    "section": "",
    "text": "Artwork by @allison_horst"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#section",
    "href": "lessons/02_Data_Management/02_Data_Management.html#section",
    "title": "Data Management with the tidyverse",
    "section": "",
    "text": "Artwork by @allison_horst"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#what-is-the-tidyverse",
    "href": "lessons/02_Data_Management/02_Data_Management.html#what-is-the-tidyverse",
    "title": "Data Management with the tidyverse",
    "section": "What is the tidyverse?",
    "text": "What is the tidyverse?\nThe tidyverse is a collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\n\n\nggplot2 - data visualisation\ndplyr - data manipulation\ntidyr - tidy data\nreadr - read rectangular data\npurrr - functional programming\ntibble - modern data frames\nstringr - string manipulation\nforcats - factors\nand many more …"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#tidy-data1",
    "href": "lessons/02_Data_Management/02_Data_Management.html#tidy-data1",
    "title": "Data Management with the tidyverse",
    "section": "Tidy data1",
    "text": "Tidy data1\n\n\n\n\n\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nSource: R for Data Science. Grolemund and Wickham."
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#pipe-operator-magrittr",
    "href": "lessons/02_Data_Management/02_Data_Management.html#pipe-operator-magrittr",
    "title": "Data Management with the tidyverse",
    "section": "Pipe operator (magrittr)",
    "text": "Pipe operator (magrittr)\n\nThe pipe operator (%&gt;%) allows us to step through sequential functions in the same way we follow if-then statements or steps from instructions\n\n \n\nI want to find my keys, then start my car, then drive to work, then park my car.\n\n \n\n\nNested\n\npark(drive(start_car(find(\"keys\")), \n           to = \"work\"))\n\n\nPiped\n\nfind(\"keys\") %&gt;%\n  start_car() %&gt;%\n  drive(to = \"work\") %&gt;%\n  park()"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#recoding-a-binary-variable-with-pipe-operator",
    "href": "lessons/02_Data_Management/02_Data_Management.html#recoding-a-binary-variable-with-pipe-operator",
    "title": "Data Management with the tidyverse",
    "section": "Recoding a binary variable with pipe operator",
    "text": "Recoding a binary variable with pipe operator\n \n\nLet’s say I want a variable transmission to show the category names that are assigned to numeric values in the code. I want 0 to be coded as automatic and 1 to be coded as manual.\n\n \n\n\nBase R:\n\nmtcars$transmission &lt;-\n  ifelse(\n    mtcars$am == 0,\n    \"automatic\",\n    \"manual\"\n  )\n\n\nTidyverse:\n\nmtcars &lt;- mtcars %&gt;%\n  mutate(\n    transmission = case_when(\n      am == 0 ~ \"automatic\",\n      am == 1 ~ \"manual\"\n    )\n  )\n\n \n\nmutate() creates new columns that are functions of existing variables"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#recoding-a-multi-level-variable",
    "href": "lessons/02_Data_Management/02_Data_Management.html#recoding-a-multi-level-variable",
    "title": "Data Management with the tidyverse",
    "section": "Recoding a multi-level variable",
    "text": "Recoding a multi-level variable\n \n\nLet’s say I want a variable gear to show the category names that are assigned to numeric values in the code. I want 3 to be coded as gear three, 4 to be coded as gear four, 5 to be coded as gear five.\n\n \n\n\nBase R:\n\nmtcars$gear_char &lt;-\n  ifelse(\n    mtcars$gear == 3,\n    \"three\",\n    ifelse(\n      mtcars$gear == 4,\n      \"four\",\n      \"five\"\n    )\n  )\n\n\nTidyverse:\n\nmtcars &lt;- mtcars %&gt;%\n  mutate(\n    gear_char = case_when(\n      gear == 3 ~ \"three\",\n      gear == 4 ~ \"four\",\n      gear == 5 ~ \"five\"\n    )\n  )"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#ggplot2-in-tidyverse",
    "href": "lessons/02_Data_Management/02_Data_Management.html#ggplot2-in-tidyverse",
    "title": "Data Management with the tidyverse",
    "section": "ggplot2 in tidyverse",
    "text": "ggplot2 in tidyverse\n\n\n\n\n\n\n\n\n\nWe talked about this in our review notes\n\nI want to revisit it: always helps to have more examples!\nThis example is closer to the multivariable work we’ll do in this class!\n\n\n \n\nggplot2 is tidyverse’s data visualization package\n\n \n\nThe gg in “ggplot2” stands for Grammar of Graphics\n\n \n\nIt is inspired by the book Grammar of Graphics by Leland Wilkinson"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#tidyverse-visualizing-multiple-variables",
    "href": "lessons/02_Data_Management/02_Data_Management.html#tidyverse-visualizing-multiple-variables",
    "title": "Data Management with the tidyverse",
    "section": "Tidyverse: Visualizing multiple variables",
    "text": "Tidyverse: Visualizing multiple variables\n \n\nggplot(\n  mtcars,\n  aes(x = disp, y = mpg, color = transmission)) +\n  geom_point()"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#poll-everywhere-question-1",
    "href": "lessons/02_Data_Management/02_Data_Management.html#poll-everywhere-question-1",
    "title": "Data Management with the tidyverse",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#tidyverse-visualizing-even-more-variables",
    "href": "lessons/02_Data_Management/02_Data_Management.html#tidyverse-visualizing-even-more-variables",
    "title": "Data Management with the tidyverse",
    "section": "Tidyverse: Visualizing even more variables",
    "text": "Tidyverse: Visualizing even more variables\n\nggplot(\n  mtcars,\n  aes(x = disp, y = mpg, color = transmission)) +\n  geom_point() +\n  facet_wrap(~ cyl)"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#base-r-visualizing-even-more-variables",
    "href": "lessons/02_Data_Management/02_Data_Management.html#base-r-visualizing-even-more-variables",
    "title": "Data Management with the tidyverse",
    "section": "Base R: Visualizing even more variables",
    "text": "Base R: Visualizing even more variables\n\nmtcars$trans_color &lt;- ifelse(mtcars$transmission == \"automatic\", \"green\", \"blue\")\nmtcars_cyl4 = mtcars[mtcars$cyl == 4, ]\nmtcars_cyl6 = mtcars[mtcars$cyl == 6, ]\nmtcars_cyl8 = mtcars[mtcars$cyl == 8, ]\npar(mfrow = c(1, 3), mar = c(2.5, 2.5, 2, 0), mgp = c(1.5, 0.5, 0))\nplot(mpg ~ disp, data = mtcars_cyl4, col = trans_color, main = \"Cyl 4\")\nplot(mpg ~ disp, data = mtcars_cyl6, col = trans_color, main = \"Cyl 6\")\nplot(mpg ~ disp, data = mtcars_cyl8, col = trans_color, main = \"Cyl 8\")\nlegend(\"topright\", legend = c(\"automatic\", \"manual\"), pch = 1, col = c(\"green\", \"blue\"))"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#important-functions-for-data-management",
    "href": "lessons/02_Data_Management/02_Data_Management.html#important-functions-for-data-management",
    "title": "Data Management with the tidyverse",
    "section": "Important functions for data management",
    "text": "Important functions for data management\n \nData manipulation\n\npivot_longer() and pivot_wider()\nrename()\nmutate()\nfilter()\nselect()\n\nSummarizing data\n\ntbl_summary()\ngroup_by()\nsummarize()\nacross()"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#example-for-pivot_longer-instructional-staff-employment-trends",
    "href": "lessons/02_Data_Management/02_Data_Management.html#example-for-pivot_longer-instructional-staff-employment-trends",
    "title": "Data Management with the tidyverse",
    "section": "Example for pivot_longer(): Instructional staff employment trends",
    "text": "Example for pivot_longer(): Instructional staff employment trends\nThe American Association of University Professors (AAUP) is a nonprofit membership association of faculty and other academic professionals. This report by the AAUP shows trends in instructional staff employees between 1975 and 2011, and contains an image very similar to the one given below."
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#data",
    "href": "lessons/02_Data_Management/02_Data_Management.html#data",
    "title": "Data Management with the tidyverse",
    "section": "Data",
    "text": "Data\nEach row in this dataset represents a faculty type, and the columns are the years for which we have data. The values are percentage of hires of that type of faculty for each year.\n   \n\n(staff &lt;- read_csv(here(\"./data/instructional-staff.csv\")))\n\n# A tibble: 5 × 12\n  faculty_type    `1975` `1989` `1993` `1995` `1999` `2001` `2003` `2005` `2007`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Full-Time Tenu…   29     27.6   25     24.8   21.8   20.3   19.3   17.8   17.2\n2 Full-Time Tenu…   16.1   11.4   10.2    9.6    8.9    9.2    8.8    8.2    8  \n3 Full-Time Non-…   10.3   14.1   13.6   13.6   15.2   15.5   15     14.8   14.9\n4 Part-Time Facu…   24     30.4   33.1   33.2   35.5   36     37     39.3   40.5\n5 Graduate Stude…   20.5   16.5   18.1   18.8   18.7   19     20     19.9   19.5\n# ℹ 2 more variables: `2009` &lt;dbl&gt;, `2011` &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#poll-everywhere-question-2",
    "href": "lessons/02_Data_Management/02_Data_Management.html#poll-everywhere-question-2",
    "title": "Data Management with the tidyverse",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#recreate-the-visualization",
    "href": "lessons/02_Data_Management/02_Data_Management.html#recreate-the-visualization",
    "title": "Data Management with the tidyverse",
    "section": "Recreate the visualization",
    "text": "Recreate the visualization\n \n\nIn order to recreate this visualization we need to first reshape the data:\n\none variable for faculty type\none variable for year\n\n\n \n\nConvert the data from the wide format to long format\n\npivot_longer()"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#pivot_-functions",
    "href": "lessons/02_Data_Management/02_Data_Management.html#pivot_-functions",
    "title": "Data Management with the tidyverse",
    "section": "pivot_*() functions",
    "text": "pivot_*() functions"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#poll-everywhere-question-3",
    "href": "lessons/02_Data_Management/02_Data_Management.html#poll-everywhere-question-3",
    "title": "Data Management with the tidyverse",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#pivot-staff-data-and-mutate-percentage",
    "href": "lessons/02_Data_Management/02_Data_Management.html#pivot-staff-data-and-mutate-percentage",
    "title": "Data Management with the tidyverse",
    "section": "Pivot staff data and mutate percentage",
    "text": "Pivot staff data and mutate percentage\n\n(staff_long &lt;- staff %&gt;%\n  pivot_longer(\n    cols = -faculty_type,    # columns to pivot\n    names_to = \"year\",       # name of new column for variable names\n    values_to = \"percentage\" # name of new column for values\n  ) %&gt;%\n  mutate(percentage = as.numeric(percentage))\n)\n\n# A tibble: 55 × 3\n   faculty_type              year  percentage\n   &lt;chr&gt;                     &lt;chr&gt;      &lt;dbl&gt;\n 1 Full-Time Tenured Faculty 1975        29  \n 2 Full-Time Tenured Faculty 1989        27.6\n 3 Full-Time Tenured Faculty 1993        25  \n 4 Full-Time Tenured Faculty 1995        24.8\n 5 Full-Time Tenured Faculty 1999        21.8\n 6 Full-Time Tenured Faculty 2001        20.3\n 7 Full-Time Tenured Faculty 2003        19.3\n 8 Full-Time Tenured Faculty 2005        17.8\n 9 Full-Time Tenured Faculty 2007        17.2\n10 Full-Time Tenured Faculty 2009        16.8\n# ℹ 45 more rows"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#a-meh-plot-over-the-years",
    "href": "lessons/02_Data_Management/02_Data_Management.html#a-meh-plot-over-the-years",
    "title": "Data Management with the tidyverse",
    "section": "A “meh” plot over the years",
    "text": "A “meh” plot over the years\n\nggplot(staff_long, aes(x = percentage, y = year, fill = faculty_type)) +\n  geom_col()"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#more-improvement",
    "href": "lessons/02_Data_Management/02_Data_Management.html#more-improvement",
    "title": "Data Management with the tidyverse",
    "section": "More improvement",
    "text": "More improvement\n\n\n\nstaff_long %&gt;%\n  mutate( \n    part_time = if_else(faculty_type == \"Part-Time Faculty\",\n                        \"Part-Time Faculty\", \"Other Faculty\"),\n    year = as.numeric(year)) %&gt;% \n  ggplot(\n    aes(x = year, y = percentage/100, group = faculty_type, color = part_time)) +\n  geom_line() +\n  scale_color_manual(values = c(\"gray\", \"red\")) + \n  scale_y_continuous(labels = label_percent(accuracy = 1)) + \n  theme_minimal() +\n  labs(\n    title = \"Instructional staff employment trends\",\n    x = \"Year\", y = \"Percentage\", color = NULL) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#all-that-just-to-show-one-helpful-function",
    "href": "lessons/02_Data_Management/02_Data_Management.html#all-that-just-to-show-one-helpful-function",
    "title": "Data Management with the tidyverse",
    "section": "All that just to show one helpful function",
    "text": "All that just to show one helpful function\nNow we can move onto the other functions mentioned:\n \nData manipulation\n\npivot_longer() and pivot_wider()\nrename()\nmutate()\nfilter()\nselect()\n\nSummarizing data\n\ntbl_summary()\ngroup_by()\nsummarize()\nacross()"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#lets-look-back-at-the-dds.discr-dataset-that-i-briefly-used-last-class",
    "href": "lessons/02_Data_Management/02_Data_Management.html#lets-look-back-at-the-dds.discr-dataset-that-i-briefly-used-last-class",
    "title": "Data Management with the tidyverse",
    "section": "Let’s look back at the dds.discr dataset that I briefly used last class",
    "text": "Let’s look back at the dds.discr dataset that I briefly used last class\n   \n\nWe will load the data (This is a special case! dds.discr is a built-in R dataset)\n\n\ndata(\"dds.discr\")\n\n\nNow, let’s take a glimpse at the dataset:\n\n\nglimpse(dds.discr)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ gender       &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ ethnicity    &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#rename-one-of-the-first-things-i-usually-do",
    "href": "lessons/02_Data_Management/02_Data_Management.html#rename-one-of-the-first-things-i-usually-do",
    "title": "Data Management with the tidyverse",
    "section": "rename(): one of the first things I usually do",
    "text": "rename(): one of the first things I usually do\n\nI notice that two variables have values that don’t necessarily match the variable name\n\nFemale and male are not genders\n“White not Hispanic” combines race and ethnicity into one category\n\n\n\nI want to rename gender to SAB (sex assigned at birth) and rename ethnicity to R_E (race and ethnicity)\n\n \n\ndds.discr1 = dds.discr %&gt;% \n  rename(SAB = gender, \n         R_E = ethnicity)\n\nglimpse(dds.discr1)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ SAB          &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ R_E          &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#mutate-constructing-new-variables-from-what-you-have",
    "href": "lessons/02_Data_Management/02_Data_Management.html#mutate-constructing-new-variables-from-what-you-have",
    "title": "Data Management with the tidyverse",
    "section": "mutate(): constructing new variables from what you have",
    "text": "mutate(): constructing new variables from what you have\n\nWe’ve seen a couple examples for mutate() so far (mostly because its used so often!)\nWe haven’t seen an example where we make a new variable from two variables\n\n\nI want to make a variable that is the ratio of expenditures over age\n\n \n\ndds.discr2 = dds.discr1 %&gt;%\n  mutate(exp_to_age = expenditures/age)\n\nglimpse(dds.discr2)\n\nRows: 1,000\nColumns: 7\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ SAB          &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ R_E          &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…\n$ exp_to_age   &lt;dbl&gt; 124.2941, 1133.0811, 484.6667, 336.8421, 339.3846, 304.40…"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#poll-everywhere-question-4",
    "href": "lessons/02_Data_Management/02_Data_Management.html#poll-everywhere-question-4",
    "title": "Data Management with the tidyverse",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#filter-keep-rows-that-match-a-condition",
    "href": "lessons/02_Data_Management/02_Data_Management.html#filter-keep-rows-that-match-a-condition",
    "title": "Data Management with the tidyverse",
    "section": "filter(): keep rows that match a condition",
    "text": "filter(): keep rows that match a condition\n\nWhat if I want to subset the data frame? (keep certain rows of observations)\n\n\nI want to look at the data for people who between 50 and 60 years old\n\n \n\ndds.discr3 = dds.discr2 %&gt;%\n  filter(age &gt;= 50 & age &lt;= 60)\n\nglimpse(dds.discr3)\n\nRows: 23\nColumns: 7\n$ id           &lt;int&gt; 15970, 19412, 29506, 31658, 36123, 39287, 39672, 43455, 4…\n$ age.cohort   &lt;fct&gt; 51+, 51+, 51+, 51+, 51+, 51+, 51+, 51+, 51+, 51+, 51+, 51…\n$ age          &lt;int&gt; 51, 60, 56, 60, 59, 59, 54, 57, 52, 57, 55, 52, 59, 54, 5…\n$ SAB          &lt;fct&gt; Female, Female, Female, Female, Male, Female, Female, Mal…\n$ expenditures &lt;int&gt; 54267, 57702, 48215, 46873, 42739, 44734, 52833, 48363, 5…\n$ R_E          &lt;fct&gt; White not Hispanic, White not Hispanic, White not Hispani…\n$ exp_to_age   &lt;dbl&gt; 1064.0588, 961.7000, 860.9821, 781.2167, 724.3898, 758.20…"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#select-keep-or-drop-columns-using-their-names-and-types",
    "href": "lessons/02_Data_Management/02_Data_Management.html#select-keep-or-drop-columns-using-their-names-and-types",
    "title": "Data Management with the tidyverse",
    "section": "select(): keep or drop columns using their names and types",
    "text": "select(): keep or drop columns using their names and types\n\nWhat if I want to remove or keep certain variables?\n\n\nI want to only have age and expenditure in my data frame\n\n \n\ndds.discr4 = dds.discr2 %&gt;%\n  select(age, expenditures)\n\nglimpse(dds.discr4)\n\nRows: 1,000\nColumns: 2\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#tbl_summary-table-summary-12",
    "href": "lessons/02_Data_Management/02_Data_Management.html#tbl_summary-table-summary-12",
    "title": "Data Management with the tidyverse",
    "section": "tbl_summary() : table summary (1/2)",
    "text": "tbl_summary() : table summary (1/2)\n\nWhat if I want one of those fancy summary tables that are at the top of most research articles? (lovingly called “Table 1”)\n\n\n\n\nlibrary(gtsummary)\ntbl_summary(dds.discr2)\n\n\n\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 1,0001\n    \n  \n  \n    id\n55,385 (31,759, 76,205)\n    age.cohort\n\n        0-5\n82 (8.2%)\n        6-12\n175 (18%)\n        13-17\n212 (21%)\n        18-21\n199 (20%)\n        22-50\n226 (23%)\n        51+\n106 (11%)\n    age\n18 (12, 26)\n    SAB\n\n        Female\n503 (50%)\n        Male\n497 (50%)\n    expenditures\n7,026 (2,898, 37,718)\n    R_E\n\n        American Indian\n4 (0.4%)\n        Asian\n129 (13%)\n        Black\n59 (5.9%)\n        Hispanic\n376 (38%)\n        Multi Race\n26 (2.6%)\n        Native Hawaiian\n3 (0.3%)\n        Other\n2 (0.2%)\n        White not Hispanic\n401 (40%)\n    exp_to_age\n462 (273, 938)\n  \n  \n  \n    \n      1 Median (Q1, Q3); n (%)"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#tbl_summary-table-summary-22",
    "href": "lessons/02_Data_Management/02_Data_Management.html#tbl_summary-table-summary-22",
    "title": "Data Management with the tidyverse",
    "section": "tbl_summary() : table summary (2/2)",
    "text": "tbl_summary() : table summary (2/2)\n\nLet’s make this more presentable\n\n \n\n\n\ndds.discr2 %&gt;%\n  select(-id, -age.cohort, -exp_to_age) %&gt;%\n  tbl_summary(label = c(age ~ \"Age\", \n                        R_E ~ \"Race/Ethnicity\", \n                        SAB ~ \"Sex Assigned at Birth\", \n                        expenditures ~ \"Expenditures\") ,\n              statistic = list(all_continuous() ~ \"{mean} ({sd})\"))\n\n\n\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 1,0001\n    \n  \n  \n    Age\n23 (18)\n    Sex Assigned at Birth\n\n        Female\n503 (50%)\n        Male\n497 (50%)\n    Expenditures\n18,066 (19,543)\n    Race/Ethnicity\n\n        American Indian\n4 (0.4%)\n        Asian\n129 (13%)\n        Black\n59 (5.9%)\n        Hispanic\n376 (38%)\n        Multi Race\n26 (2.6%)\n        Native Hawaiian\n3 (0.3%)\n        Other\n2 (0.2%)\n        White not Hispanic\n401 (40%)\n  \n  \n  \n    \n      1 Mean (SD); n (%)"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#group_by-group-by-one-or-more-variables",
    "href": "lessons/02_Data_Management/02_Data_Management.html#group_by-group-by-one-or-more-variables",
    "title": "Data Management with the tidyverse",
    "section": "group_by(): group by one or more variables",
    "text": "group_by(): group by one or more variables\n\nWhat if I want to quickly look at group differences?\nIt will not change how the data look, but changes the actions of following functions\n\n\nI want to group my data by sex assigned at birth.\n\n \n\ndds.discr5 = dds.discr2 %&gt;%\n  group_by(SAB)\nglimpse(dds.discr5)\n\nRows: 1,000\nColumns: 7\nGroups: SAB [2]\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ SAB          &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ R_E          &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…\n$ exp_to_age   &lt;dbl&gt; 124.2941, 1133.0811, 484.6667, 336.8421, 339.3846, 304.40…\n\n\n\nLet’s see how the groups change something like the summarize() function in the next slide"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#summarize-summarize-your-data-or-grouped-data-into-one-row",
    "href": "lessons/02_Data_Management/02_Data_Management.html#summarize-summarize-your-data-or-grouped-data-into-one-row",
    "title": "Data Management with the tidyverse",
    "section": "summarize(): summarize your data or grouped data into one row",
    "text": "summarize(): summarize your data or grouped data into one row\n\nWhat if I want to calculate specific descriptive statistics for my variables?\nThis function is often best used with group_by()\nIf only presenting the summaries, functions like tbl_summary() is better\nsummarize() creates a new data frame, which means you can plot and manipulate the summarized data\n\n \n\n\nOver whole sample:\n\ndds.discr2 %&gt;% \n  summarize(\n    ave = mean(expenditures),\n    SD = sd(expenditures),\n    med = median(expenditures))\n\n# A tibble: 1 × 3\n     ave     SD   med\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 18066. 19543.  7026\n\n\n\nGrouped by sex assigned at birth:\n\ndds.discr2 %&gt;% \n  group_by(SAB) %&gt;% \n  summarize(\n    ave = mean(expenditures),\n    SD = sd(expenditures),\n    med = median(expenditures))\n\n# A tibble: 2 × 4\n  SAB       ave     SD   med\n  &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 Female 18130. 20020.  6400\n2 Male   18001. 19068.  7219"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#across-apply-a-function-across-multiple-columns",
    "href": "lessons/02_Data_Management/02_Data_Management.html#across-apply-a-function-across-multiple-columns",
    "title": "Data Management with the tidyverse",
    "section": "across(): apply a function across multiple columns",
    "text": "across(): apply a function across multiple columns\n\nLike group_by(), this function is often paired with another transformation function\n\n\nI want all my integer values to have two significant figures.\n\n \n\ndds.discr6 = dds.discr2 %&gt;%\n  mutate(across(where(is.integer), signif, digits = 2))\n\nglimpse(dds.discr6)\n\nRows: 1,000\nColumns: 7\n$ id           &lt;dbl&gt; 10000, 10000, 10000, 11000, 11000, 11000, 11000, 11000, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;dbl&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ SAB          &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;dbl&gt; 2100, 42000, 1500, 6400, 4400, 4600, 3900, 3900, 5000, 29…\n$ R_E          &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…\n$ exp_to_age   &lt;dbl&gt; 124.2941, 1133.0811, 484.6667, 336.8421, 339.3846, 304.40…"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#dplyr-resources",
    "href": "lessons/02_Data_Management/02_Data_Management.html#dplyr-resources",
    "title": "Data Management with the tidyverse",
    "section": "dplyr resources",
    "text": "dplyr resources\n\nMore dpylr functions to reference!\n\nAdditional details and examples are available in the vignettes:\n\ncolumn-wise operations vignette\nrow-wise operations vignette\n\n \nand the dplyr 1.0.0 release blog posts:\n\nworking across columns\nworking within rows"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#r-programming-class-at-ohsu",
    "href": "lessons/02_Data_Management/02_Data_Management.html#r-programming-class-at-ohsu",
    "title": "Data Management with the tidyverse",
    "section": "R programming class at OHSU!",
    "text": "R programming class at OHSU!\nYou can check out Dr. Jessica Minnier’s R class page if you want more notes, videos, etc."
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#the-larger-tidy-ecosystem",
    "href": "lessons/02_Data_Management/02_Data_Management.html#the-larger-tidy-ecosystem",
    "title": "Data Management with the tidyverse",
    "section": "The larger tidy ecosystem",
    "text": "The larger tidy ecosystem\nJust to name a few…\n\njanitor\nkableExtra\npatchwork\ngghighlight\ntidybayes"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#credit-to-mine-çetinkaya-rundel",
    "href": "lessons/02_Data_Management/02_Data_Management.html#credit-to-mine-çetinkaya-rundel",
    "title": "Data Management with the tidyverse",
    "section": "Credit to Mine Çetinkaya-Rundel",
    "text": "Credit to Mine Çetinkaya-Rundel\n\nThese notes were built from Mine’s notes\n\nMost pages and code were left as she made them\nI changed a few things to match our class\n\nPlease see her Github repository for the original notes\n\n\n\nData Management"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#footnotes",
    "href": "lessons/02_Data_Management/02_Data_Management.html#footnotes",
    "title": "Data Management with the tidyverse",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSource: R for Data Science. Grolemund and Wickham.↩︎"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management_key_info.html#key-dates",
    "href": "lessons/02_Data_Management/02_Data_Management_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval_key_info.html#key-dates",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html",
    "title": "SLR: More inference + Evaluation",
    "section": "",
    "text": "Identify different sources of variation in an Analysis of Variance (ANOVA) table\nUsing the F-test, determine if there is enough evidence that population slope \\(\\beta_1\\) is not 0\nCalculate and interpret the coefficient of determination\nDescribe the model assumptions made in linear regression using ordinary least squares\n\n\n\n\n\nLesson 1 of SLR:\n\nFit regression line\nCalculate slope & intercept\nInterpret slope & intercept\n\nLesson 2 of SLR:\n\nEstimate variance of the residuals\nInference for slope & intercept: CI, p-value\nConfidence bands of regression line for mean value of Y|X\n\n\n\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#so-far-in-our-regression-example",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#so-far-in-our-regression-example",
    "title": "SLR: More inference + Evaluation",
    "section": "",
    "text": "Lesson 1 of SLR:\n\nFit regression line\nCalculate slope & intercept\nInterpret slope & intercept\n\nLesson 2 of SLR:\n\nEstimate variance of the residuals\nInference for slope & intercept: CI, p-value\nConfidence bands of regression line for mean value of Y|X\n\n\n\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#lets-revisit-the-regression-analysis-process",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#lets-revisit-the-regression-analysis-process",
    "title": "SLR: More inference + Evaluation",
    "section": "",
    "text": "Model Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#getting-to-the-f-test",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#getting-to-the-f-test",
    "title": "SLR: More inference + Evaluation",
    "section": "Getting to the F-test",
    "text": "Getting to the F-test\nThe F statistic in linear regression is essentially a proportion of the variance explained by the model vs. the variance not explained by the model\n\nStart with visual of explained vs. unexplained variation\nFigure out the mathematical representations of this variation\nLook at the ANOVA table to establish key values measuring our variance from our model\nBuild the F-test"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#explained-vs.-unexplained-variation",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#explained-vs.-unexplained-variation",
    "title": "SLR: More inference + Evaluation",
    "section": "Explained vs. Unexplained Variation",
    "text": "Explained vs. Unexplained Variation\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\\[ \\begin{aligned}\nY_i - \\overline{Y} & = (Y_i - \\hat{Y}_i) + (\\hat{Y}_i- \\overline{Y})\\\\\n\\text{Total unexplained variation} & = \\text{Residual variation after regression} + \\text{Variation explained by regression}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#more-on-the-equation",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#more-on-the-equation",
    "title": "SLR: More inference + Evaluation",
    "section": "More on the equation",
    "text": "More on the equation\n\n\n\\[Y_i - \\overline{Y} = (Y_i - \\hat{Y}_i) + (\\hat{Y}_i- \\overline{Y})\\]\n\n\\(Y_i - \\overline{Y}\\) = the deviation of \\(Y_i\\) around the mean \\(\\overline{Y}\\)\n\n(the total amount deviation unexplained at \\(X_i\\) ).\n\n\\(Y_i - \\hat{Y}_i\\) = the deviation of the observation \\(Y\\) around the fitted regression line\n\n(the amount deviation unexplained by the regression at \\(X_i\\) ).\n\n\\(\\hat{Y}_i- \\overline{Y}\\) = the deviation of the fitted value \\(\\hat{Y}_i\\) around the mean \\(\\overline{Y}\\)\n\n(the amount deviation explained by the regression at \\(X_i\\) )"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question-1",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question-1",
    "title": "SLR: More inference + Evaluation",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#how-is-this-actually-calculated-for-our-fitted-model",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#how-is-this-actually-calculated-for-our-fitted-model",
    "title": "SLR: More inference + Evaluation",
    "section": "How is this actually calculated for our fitted model?",
    "text": "How is this actually calculated for our fitted model?\n\\[ \\begin{aligned}\nY_i - \\overline{Y} & = (Y_i - \\hat{Y}_i) + (\\hat{Y}_i- \\overline{Y})\\\\\n\\text{Total unexplained variation} & = \\text{Variation due to regression} + \\text{Residual variation after regression}\n\\end{aligned}\\]\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 & = \\sum_{i=1}^n (\\hat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 \\\\\nSSY & = SSR + SSE\n\\end{aligned}\\] \\[\\text{Total Sum of Squares} = \\text{Sum of Squares due to Regression} + \\text{Sum of Squares due to Error (residuals)}\\]\n\n\n\n\nANOVA table:\n\n\n\n\n\n\n\n\n\n\n\nVariation Source\ndf\nSS\nMS\ntest statistic\np-value\n\n\n\n\nRegression\n\\(1\\)\n\\(SSR\\)\n\\(MSR = \\frac{SSR}{1}\\)\n\\(F = \\frac{MSR}{MSE}\\)\n\n\n\nError\n\\(n-2\\)\n\\(SSE\\)\n\\(MSE = \\frac{SSE}{n-2}\\)\n\n\n\n\nTotal\n\\(n-1\\)\n\\(SSY\\)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#analysis-of-variance-anova-table-in-r",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#analysis-of-variance-anova-table-in-r",
    "title": "SLR: More inference + Evaluation",
    "section": "Analysis of Variance (ANOVA) table in R",
    "text": "Analysis of Variance (ANOVA) table in R\n\n# Fit regression model:\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n             data = gapm)\n\nanova(model1)\n\nAnalysis of Variance Table\n\nResponse: life_expectancy_years_2011\n                          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nfemale_literacy_rate_2011  1 2052.8 2052.81  54.414 1.501e-10 ***\nResiduals                 78 2942.6   37.73                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n   tab_options(table.font.size = 40) %&gt;%\n   fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n1.000\n2,052.812\n2,052.812\n54.414\n0.000\n    Residuals\n78.000\n2,942.635\n37.726\nNA\nNA"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#what-is-the-f-statistic-testing",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#what-is-the-f-statistic-testing",
    "title": "SLR: More inference + Evaluation",
    "section": "What is the F statistic testing?",
    "text": "What is the F statistic testing?\n\\[F = \\frac{MSR}{MSE}\\]\n\nIt can be shown that\n\n\\[E(MSE)=\\sigma^2\\ \\text{and}\\ E(MSR) = \\sigma^2 + \\beta_1^2\\sum_{i=1}^n (X_i- \\overline{X})^2\\]\n\nRecall that \\(\\sigma^2\\) is the variance of the residuals\nThus if\n\n\\(\\beta_1 = 0\\), then \\(F \\approx \\frac{\\hat{\\sigma}^2}{\\hat{\\sigma}^2} = 1\\)\n\\(\\beta_1 \\neq 0\\), then \\(F \\approx \\frac{\\hat{\\sigma}^2 + \\hat{\\beta}_1^2\\sum_{i=1}^n (X_i- \\overline{X})^2}{\\hat{\\sigma}^2} &gt; 1\\)\n\nSo the \\(F\\) statistic can also be used to test \\(\\beta_1\\)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#f-test-vs.-t-test-for-the-population-slope",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#f-test-vs.-t-test-for-the-population-slope",
    "title": "SLR: More inference + Evaluation",
    "section": "F-test vs. t-test for the population slope",
    "text": "F-test vs. t-test for the population slope\nThe square of a \\(t\\)-distribution with \\(df = \\nu\\) is an \\(F\\)-distribution with \\(df = 1, \\nu\\)\n\\[T_{\\nu}^2 \\sim F_{1,\\nu}\\]\n\nWe can use either F-test or t-test to run the following hypothesis test:\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote that the F-test does not support one-sided alternative tests, but the t-test does!"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#planting-a-seed-about-the-f-test",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#planting-a-seed-about-the-f-test",
    "title": "SLR: More inference + Evaluation",
    "section": "Planting a seed about the F-test",
    "text": "Planting a seed about the F-test\nWe can think about the hypothesis test for the slope…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_1\\neq0\\)\n\n\n\n\nin a slightly different way…\n\n\n\n\n\n\nNull model (\\(\\beta_1=0\\))\n\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\nSmaller (reduced) model\n\n\n\n\n\n\nAlternative model (\\(\\beta_1\\neq0\\))\n\n\n\n\\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)\nLarger (full) model\n\n\n\n\n\n\nIn multiple linear regression, we can start using this framework to test multiple coefficient parameters at once\n\nDecide whether or not to reject the smaller reduced model in favor of the larger full model\nCannot do this with the t-test!"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question-2",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question-2",
    "title": "SLR: More inference + Evaluation",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#f-test-general-steps-for-hypothesis-test-for-population-slope-beta_1",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#f-test-general-steps-for-hypothesis-test-for-population-slope-beta_1",
    "title": "SLR: More inference + Evaluation",
    "section": "F-test: general steps for hypothesis test for population slope \\(\\beta_1\\)",
    "text": "F-test: general steps for hypothesis test for population slope \\(\\beta_1\\)\n\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nOften, we are curious if the coefficient is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=1\\) and denominator \\(df=n-2\\).\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic for \\(\\widehat\\beta_1\\) is\n\\[F = \\frac{MSR}{MSE}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{1, n-2} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject: \\(P(F_{1, n-2} &gt; F) &lt; \\alpha\\)\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at the \\(100\\alpha\\%\\) significiance level. There is (sufficient/insufficient) evidence that there is significant association between (\\(Y\\)) and (\\(X\\)) (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1",
    "title": "SLR: More inference + Evaluation",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\)\n\nSteps 1-4 are setting up our hypothesis test: not much change from the general steps\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nWe are testing if the slope is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=1\\) and denominator \\(df=n-2 = 80-2\\).\n\n\nnobs(model1)\n\n[1] 80"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-24",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-24",
    "title": "SLR: More inference + Evaluation",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/4)\n\n\n\nCompute the value of the test statistic\n\n\n\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n1\n2052.812\n2052.81234\n54.4136\n1.501286e-10\n    Residuals\n78\n2942.635\n37.72609\nNA\nNA\n  \n  \n  \n\n\n\n\n\nOption 1: Calculate the test statistic using the values in the ANOVA table\n\n\\[F = \\frac{MSR}{MSE} = \\frac{2052.81}{37.73}=54.414\\]\n\nOption 2: Get the test statistic value (F) from the ANOVA table\n\n\nI tend to skip this step because I can do it all with step 6"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-34",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-34",
    "title": "SLR: More inference + Evaluation",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (3/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (3/4)\n\n\n\nCalculate the p-value\n\n\n\n\nAs per Step 4, test statistic \\(F\\) can be modeled by a \\(F\\)-distribution with \\(df1 = 1\\) and \\(df2 = n-2\\).\n\nWe had 80 countries’ data, so \\(n=80\\)\n\nOption 1: Use pf() and our calculated test statistic\n\n\n# p-value is ALWAYS the right tail for F-test\npf(54.414, df1 = 1, df2 = 78, lower.tail = FALSE)\n\n[1] 1.501104e-10\n\n\n\nOption 2: Use the ANOVA table\n\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n1\n2052.812\n2052.81234\n54.4136\n1.501286e-10\n    Residuals\n78\n2942.635\n37.72609\nNA\nNA"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-44",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-44",
    "title": "SLR: More inference + Evaluation",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (4/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (4/4)\n\n\n\nWrite conclusion for the hypothesis test\n\n\n\nWe reject the null hypothesis that the slope is 0 at the \\(5\\%\\) significance level. There is sufficient evidence that there is significant association between female life expectancy and female literacy rates (p-value &lt; 0.0001)."
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#did-you-notice-anything-about-the-p-value",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#did-you-notice-anything-about-the-p-value",
    "title": "SLR: More inference + Evaluation",
    "section": "Did you notice anything about the p-value?",
    "text": "Did you notice anything about the p-value?\nThe p-value of the t-test and F-test are the same!!\n\nFor the t-test:\n\n\ntidy(model1) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\nFor the F-test:\n\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n1\n2052.812\n2052.81234\n54.4136\n1.501286e-10\n    Residuals\n78\n2942.635\n37.72609\nNA\nNA\n  \n  \n  \n\n\n\n\nThis is true when we use the F-test for a single coefficient!"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#correlation-coefficient-from-511",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#correlation-coefficient-from-511",
    "title": "SLR: More inference + Evaluation",
    "section": "Correlation coefficient from 511",
    "text": "Correlation coefficient from 511\n\n\nCorrelation coefficient \\(r\\) can tell us about the strength of a relationship\n\nIf \\(r = -1\\), then there is a perfect negative linear relationship between \\(X\\) and \\(Y\\)\nIf \\(r = 1\\), then there is a perfect positive linear relationship between \\(X\\) and \\(Y\\)\nIf \\(r = 0\\), then there is no linear relationship between \\(X\\) and \\(Y\\)\n\nNote: All other values of \\(r\\) tell us that the relationship between \\(X\\) and \\(Y\\) is not perfect. The closer \\(r\\) is to 0, the weaker the linear relationship."
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#correlation-coefficient",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#correlation-coefficient",
    "title": "SLR: More inference + Evaluation",
    "section": "Correlation coefficient",
    "text": "Correlation coefficient\n\n\nThe (Pearson) correlation coefficient \\(r\\) of variables \\(X\\) and \\(Y\\) can be computed using the formula:\n\\[\\begin{aligned}\nr  & = \\frac{\\sum_{i=1}^n (X_i - \\overline{X})(Y_i - \\overline{Y})}{\\Big(\\sum_{i=1}^n (X_i - \\overline{X})^2 \\sum_{i=1}^n (Y_i - \\overline{Y})^2\\Big)^{1/2}} \\\\\n& = \\frac{SSXY}{\\sqrt{SSX \\cdot SSY}}\n\\end{aligned}\\]\nwe have the relationship\n\\[\\widehat{\\beta}_1 = r\\frac{SSY}{SSX},\\ \\ \\text{or},\\ \\  r = \\widehat{\\beta}_1\\frac{SSX}{SSY}\\]"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#coefficient-of-determination-r2",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#coefficient-of-determination-r2",
    "title": "SLR: More inference + Evaluation",
    "section": "Coefficient of determination: \\(R^2\\)",
    "text": "Coefficient of determination: \\(R^2\\)\nIt can be shown that the square of the correlation coefficient \\(r\\) is equal to\n\\[R^2 = \\frac{SSR}{SSY} = \\frac{SSY - SSE}{SSY}\\]\n\n\\(R^2\\) is called the coefficient of determination.\nInterpretation: The proportion of variation in the \\(Y\\) values explained by the regression model\n\\(R^2\\) measures the strength of the linear relationship between \\(X\\) and \\(Y\\):\n\n\\(R^2 = \\pm 1\\): Perfect relationship\n\nHappens when \\(SSE = 0\\), i.e. no error, all points on the line\n\n\\(R^2 = 0\\): No relationship\n\nHappens when \\(SSY = SSE\\), i.e. using the line doesn’t not improve model fit over using \\(\\overline{Y}\\) to model the \\(Y\\) values."
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question",
    "title": "SLR: More inference + Evaluation",
    "section": "Poll Everywhere Question",
    "text": "Poll Everywhere Question"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-correlation-coeffiicent-r-and-coefficient-of-determination-r2",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-correlation-coeffiicent-r-and-coefficient-of-determination-r2",
    "title": "SLR: More inference + Evaluation",
    "section": "Life expectancy example: correlation coeffiicent \\(r\\) and coefficient of determination \\(R^2\\)",
    "text": "Life expectancy example: correlation coeffiicent \\(r\\) and coefficient of determination \\(R^2\\)\n\n\n\n(r = cor(x = gapm$life_expectancy_years_2011, \n    y = gapm$female_literacy_rate_2011,\n    use =  \"complete.obs\"))\n\n[1] 0.6410434\n\nr^2\n\n[1] 0.4109366\n\n(sum_m1 = summary(model1)) # for R^2 value\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\nsum_m1$r.squared\n\n[1] 0.4109366\n\n\n\n   \n\n\nInterpretation\n\n\n41.1% of the variation in countries’ average life expectancy is explained by the linear model with female literacy rate as the independent variable."
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#what-does-r2-not-measure",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#what-does-r2-not-measure",
    "title": "SLR: More inference + Evaluation",
    "section": "What does \\(R^2\\) not measure?",
    "text": "What does \\(R^2\\) not measure?\n\n\n\n\\(R^2\\) is not a measure of the magnitude of the slope of the regression line\n\nExample: can have \\(R^2 = 1\\) for many different slopes!!\n\n\\(R^2\\) is not a measure of the appropriateness of the straight-line model\n\nExample: figure"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#least-squares-model-assumptions-eline",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#least-squares-model-assumptions-eline",
    "title": "SLR: More inference + Evaluation",
    "section": "Least-squares model assumptions: eLINE",
    "text": "Least-squares model assumptions: eLINE\nThese are the model assumptions made in ordinary least squares:\n \n\ne xistence: For any \\(X\\), there exists a distribution for \\(Y\\)\n\n \n\nL inearity of relationship between variables\n\n \n\nI ndependence of the \\(Y\\) values\n\n \n\nN ormality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n \n\nE quality of variance of the residuals (homoscedasticity)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#e-existence-of-ys-distribution",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#e-existence-of-ys-distribution",
    "title": "SLR: More inference + Evaluation",
    "section": "e: Existence of Y’s distribution",
    "text": "e: Existence of Y’s distribution\n\nFor any fixed value of the variable \\(X\\), \\(Y\\) is a\n\nrandom variable with a certain probability distribution\nhaving finite\n\nmean and\nvariance\n\n\nThis leads to the normality assumption\nNote: This is not about \\(Y\\) alone, but \\(Y|X\\)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#l-linearity",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#l-linearity",
    "title": "SLR: More inference + Evaluation",
    "section": "L: Linearity",
    "text": "L: Linearity\n\nThe relationship between the variables is linear (a straight line):\n\nThe mean value of \\(Y\\) given \\(X\\), \\(\\mu_{y|x}\\) or \\(E[Y|X]\\), is a straight-line function of \\(X\\)\n\n\n\\[\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\]\n\n\nWarning in geom_point(size = 3, se = FALSE): Ignoring unknown parameters: `se`\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#i-independence-of-observations",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#i-independence-of-observations",
    "title": "SLR: More inference + Evaluation",
    "section": "I: Independence of observations",
    "text": "I: Independence of observations\n\nThe \\(Y\\)-values are statistically independent of one another\n\n \n\nExamples of when they are not independent, include\n \n\nrepeated measures (such as baseline, 3 months, 6 months)\n\n \n\ndata from clusters, such as different hospitals or families\n\n\n \n\nThis condition is checked by reviewing the study design and not by inspecting the data\n\n \n\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question-3",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question-3",
    "title": "SLR: More inference + Evaluation",
    "section": "Poll Everywhere Question",
    "text": "Poll Everywhere Question"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#n-normality",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#n-normality",
    "title": "SLR: More inference + Evaluation",
    "section": "N: Normality",
    "text": "N: Normality\n\nFor any fixed value of \\(X\\), \\(Y\\) has normal distribution.\n\nNote: This is not about \\(Y\\) alone, but \\(Y|X\\)\n\nEquivalently, the measurement (random) errors \\(\\epsilon_i\\) ’s normally distributed\n\nThis is more often what we check\n\nWe will discuss how to assess this in practice in Chapter 14 (Regression Diagnostics)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#e-equality-of-variance-of-the-residuals",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#e-equality-of-variance-of-the-residuals",
    "title": "SLR: More inference + Evaluation",
    "section": "E: Equality of variance of the residuals",
    "text": "E: Equality of variance of the residuals\n\nThe variance of \\(Y\\) given \\(X\\) (\\(\\sigma_{Y|X}^2\\)), is the same for any \\(X\\)\n\nWe use just \\(\\sigma^2\\) to denote the common variance\n\nThis is also called homoscedasticity\nWe will discuss how to assess this in practice in Chapter 14 (Regression Diagnostics)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#summary-of-eline-model-assumptions",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#summary-of-eline-model-assumptions",
    "title": "SLR: More inference + Evaluation",
    "section": "Summary of eLINE model assumptions",
    "text": "Summary of eLINE model assumptions\n\n\\(Y\\) values are independent (check study design!)\n\n\n\n\n\nThe distribution of \\(Y\\) given \\(X\\) is\n\nnormal\nwith mean \\(\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\)\nand common variance \\(\\sigma^2\\)\n\n\n\n\nThis means that the residuals are\n\nnormal\nwith mean = 0\nand common variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#anscombes-quartet",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#anscombes-quartet",
    "title": "SLR: More inference + Evaluation",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-1",
    "href": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-1",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "syllabus.html#key-course-info",
    "href": "syllabus.html#key-course-info",
    "title": "BSTA 512/612 Syllabus",
    "section": "Key Course Info",
    "text": "Key Course Info\n\nIf an assignment on Sakai is closed or you are submitting late work, please email me AND the TAs your work\nIf you are emailing for an extension, please email me AND the TAs.\nLatework policy\n\nFor homework: can be turned in any time before course ends, but you will not receive feedback on late work\nFor labs: one “no questions asked” extension\n\nAttendance policy: attend in person and fill out exit tickets for 12 out of the 19 classes\nThe class will end on March 17, 2025. All coursework MUST be completed by March 21, 2025 at 11pm."
  },
  {
    "objectID": "project.html#poster-and-presentation",
    "href": "project.html#poster-and-presentation",
    "title": "Project Central",
    "section": "Poster and presentation",
    "text": "Poster and presentation\nLast Year’s Report Instructions\n\nI am currently updating the Poster instructions\n\nPoster due 3/17\n\nReading and listening sources\nIf you are interested in sources that discuss the social complexities of anti-fat bias, feel free to take a look at the following sources. Please be aware that these resources will discuss anti-fat bias and related histories, including racism and sexism.\n\nArticle: Implicit and explicit anti-fat bias: The role of weight-related attitudes and beliefs\nPodcast: Anti-Fat Bias by Maintenance Phase\nBook: Fearing the Black Body: The Racial Origins of Fat Phobia\n\nMultnomah County Library has unlimited loans for the audiobook\n\nBlog: Dances with Fat\n\nYou can subscribe to Ragen’s weekly newsletter for free\n\n\nIf you have additional sources that you would like to share, please send them to me!"
  },
  {
    "objectID": "labs/Project_poster_instructions.html",
    "href": "labs/Project_poster_instructions.html",
    "title": "Project Poster Instructions",
    "section": "",
    "text": "Important\n\n\n\nInstructions and rubric are still in progress!\nThese instructions were partially developed using ChatGPT by feeding in my previous project report instructions and asking ChatGPT to edit for a poster."
  },
  {
    "objectID": "labs/Project_poster_instructions.html#directions",
    "href": "labs/Project_poster_instructions.html#directions",
    "title": "Project Poster Instructions",
    "section": "1 Directions",
    "text": "1 Directions\n\n\n\n\n\n\nProject template\n\n\n\nYou may use this project template to get started on the report. It is your responsibility to meet the formatting guidelines below!!\nDO NOT USE SITE PAGE (“Project Report Instructions”, current page) as your template!!\n\n\n\n1.1 Purpose\nA scientific poster serves as a visual and concise way to communicate research findings. For this project, your poster should highlight your linear regression analysis and results while ensuring the context and methods are clearly explained. Posters should balance visuals (e.g., tables, figures) with text to engage an audience effectively.\n\n\n1.2 Formatting guide\n\nThe report will be written in Quarto. Turn in both the qmd and html files\n\nNo code should appear in the html document\n\nThis means all R code chunks should have #| echo: false\nThis also means warnings and messages should be turned off\n\n\nThe report should be 10 - 14 paragraphs long\nTables and figures should NOT have variable names as they appear in the data frame\n\nVariable names should be understood by a reader\nVariable names should be written in full words\nInclude a title or caption for all figures\nFigure and tables appear on same page or close to same page where they are first referenced\nTables and figures are an appropriate size in the html - Nicky is able to read all words in figures and tables\n\nWriting, spelling, and grammar should be admissable\n\nThis means I can generally follow your thought/what you are trying to communicate\nSome spelling and grammar mistakes are allowed\n\nI will not take off points if there are a few sprinkled in\nIf every or close to every sentence has mistakes, then I will take off\n\n\nSectioning of the report\n\nMain sections that were required: Introduction, Statistical Methods, Results, Discussion, Conclusion, and References\nOther sections that might help group specific methods or results\n\nTitle information at the top of the html\n\nThis includes the title itself, your name, and the date\n\n\n\n\n\n\n\n\nThe project report is a separate file from the labs\n\n\n\nYou can save tables and figures from labs or separate files, then load them in the report\n\nSave R objects in analyses file:\n\nSuppose you named the Table 1 as table1\nsave(table1, file = \"table1.Rdata\")\n\nLoad R objects in report file: load(file = \"table1.Rdata\")\n\n\n\n\n\n1.3 Examples of reports\nThe following are examples of reports from BSTA 513 with the feedback that I gave them.\nPlease note that 513 uses a different type of outcome than our class. These examples are meant to help guide you with the formatting and some appropriate content.\nAlso note that these were converted to PDFs so I could write in feedback. Some of the tables and figure sizes were distorted. They need to be legible in the html.\n\nReport 1 with my feedback\nReport 2 with my feedback\n\nThe above reports have code showing in their html. Remember that I am asking you to hide all code, warnings, and messages.\n\n\n1.4 Grading\nThe project report is out of 36 points. Note that the Statistical Methods and Results sections are graded on an 8-point scale, while all other components are graded on a 4-point scale.\n\n1.4.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences with very few grammatical or spelling errors. With little editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences with some (around 2 per section) grammatical or spelling errors. With some editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences, but have many grammatical or spelling errors. With major editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences, but are very hard to follow due to grammar mistakes.\nLab not submitted on Sakai (or by email if late) with .html file. Report is not written with complete sentences. With major editing, the report can be distributed.\n\n\nFigures and work\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables look professional, are easily interpreted by the reader, and easily convey the intended message.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. For the most part, figures and tables look professional, are easily interpreted by the reader, and easily convey the intended message. A few mistakes in the figures are made.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables look semi-professional, are not so easily interpreted by the reader, and convey the intended message but after some work by the reader. Some mistakes in the figures are made.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables do not look professional, are not easily interpreted by the reader, and/or do not convey the intended message. Many mistakes in the figures are made.\nRequested output is not displayed, Missing one or more figures.\n\n\nIntroduction\nProvides a good background for the research question, includes motivation for the question, and references previous research that justifies this analysis.\nProvides a decent background for the research question and includes motivation for the question. Previous research is mentioned, but feels disconnected to the current analysis.\nProvides a decent background for the research question and includes motivation for the question. Previous research is mentioned, but feels disconnected to the current analysis.\nDoes not provide a background that connects to the research question. Motivation and previous research are not mentioned.\nNo introduction included.\n\n\nMethods (8 points)\nDescribes statistical methods concisely and highlights pertinent information to the reader (listed Sections below). Demonstrates proper analyses were performed.\nDescribes statistical methods and highlights pertinent information to the reader (listed Sections below). Details were omitted or added that were not needed to explain the overarching methods. Demonstrates proper analyses were performed.\nDescribes statistical methods and highlights pertinent information to the reader (listed Sections below). Details were omitted or added that were not needed to explain the overarching methods. Some incorrect analyses included in the description.\nDescribes statistical methods, but lacks clarity. Demonstrates a lack of understanding about the overall process of regression analysis. Incorrect analyses included in the description.\nNo methods included.\n\n\nResults (8 points)\nCorrectly interprets coefficients for the explanatory variable and identifies any other interesting trends. Highlights pertinent results to the reader (listed Sections below).\nCorrectly interprets coefficients, but does correctly incorporate the interaction (if in the model). Highlights pertinent results to the reader (listed Sections below).\nIncorrectly interprets coefficients. Highlights pertinent results to the reader (listed Sections below).\nIncorrectly interprets coefficients.Omits pertinent results to the reader (listed Sections below).\nNo results included.\n\n\nDiscussion\nThoroughly and concisely discusses limitations and considerations of the results, and their consequences.\nDiscusses limitations and considerations of the results and their consequences, but misses some big considerations.\nDiscusses limitations and considerations of the results, but does not discuss the consequences.\nDiscusses limitations and considerations of the results, but misses many considerations and does not discuss consequences.\nNo discussion included.\n\n\nConclusion and References\nFor the conclusion, main research question is answered and statistical caveats described to non-technical person. References are mostly cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is answered and statistical caveats described to non-technical person. References are sometimes cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is somewhat answered (but focus is not on the research question) and statistical caveats described to non-technical person. References are sometimes cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is somewhat answered (but not the focus at all) and statistical caveats are not described. References are not cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is not answered. Or references are not included at all.\n\n\n\n\nIn formatting, an example of a report with little editing needed is one that has zero to some grammar or spelling mistakes, no code chunks showing, and no output warnings nor messages showing.\nProfessional figures mean\n\nI can read the words and numbers in the html\n\nVariable names are converted from the data frame version to readable text\nFor example: iam_001 does not show up on axes, instead something like: Response to \"Currently, I am...\"\n\nColors are only used if conveying information\nIntended message of the figure is easily understood\n\nIf you are trying to show a trend of mean IAT vs. an ordered categorical variable, then the variable is ordered on the x-axis\n\n\nFor the references\n\nI will not be overly critical about the formatting\nBy consistency, I mean that you if you are citing things like (Last Name, Year) it doesn’t suddenly change to number citations.\nIf you would like to use Quarto’s citation tool, you can! I actually pair it with Zotero and it works beautifully! (But I would not embark on this if you haven’t used Zotero before)"
  },
  {
    "objectID": "labs/Project_poster_instructions.html#sections",
    "href": "labs/Project_poster_instructions.html#sections",
    "title": "Project Poster Instructions",
    "section": "2 Sections",
    "text": "2 Sections\n\n2.1 Title\n\nPurpose: Create an identifiable name for your research project that includes the main research question’s variables and gives some context to the analysis or results\n\n\n\n2.2 Introduction\n\nLength: 1-2 paragraphs\nPurpose: Introduce the research question and why it is important to study\nThis section is non-technical.\n\nBy reading just the introduction and conclusion, someone without a technical background should have an idea of what they study was about, why it is important, and what the main results are\n\nYou may start with the introduction written in Lab 1, but you should edit it and make sure it flows into your report well!\nShould contain some references\n\n\n\n2.3 Statistical Methods\n\nLength: 3-5 paragraphs\nPurpose: Describe the analyses that were conducted and methods used to select variables and check diagnostics\nImportant to keep in mind: methods typically describe your approach and process, not the results of that process\n\nFor example: I might say “We investigated the linearity of each continuous covariate visually. If continuous variables were not linear, then we divided the variable into categories using existing guidelines from &lt;insert reference here&gt; or creating quartiles.”\n\nIn the methods section, I would NOT say: “We investigated the linearity of each continuous covariate visually. We found that age was not linearly related to IAT scores. Thus, we categorized age into the following groups: ___, ____, ____, ____, and ____.”\n\nThe last two sentences about age would be more appropriate in the Results section\n\n\n\nSome important methods to discuss (You may divide these into your sections, not necessarily with these names)\n\nGeneral approach to the dataset\n\n3-5 sentences\nDid you need to do any quality control?\nMissing data: we performed complete case analysis\n\n1 sentence\nCan be included in the Exploratory data analysis section\n\n\nVariables and variable creation\n\nThis includes a description of analyses for Table 1 and what statistics were used to summarize the variables\n\nMore on creation of Table 1, not discussing the results of Table 1\n\nIncludes (not required)\n\nIndicators for gender identity or race\nCreating BMI\nCategorizing a continuous variable (even if performed in model selection)\nUsing scoring for an ordered categorical variable (that is not your explanatory variable)\n\n1-2 sentences per variable\n\nModel building: we performed purposeful selection\n\n3-5 sentences\nIncludes\n\nDescribe purposeful selection: combining existing literature, clinical significance, and analysis\nHow did you build the model? Describe the process\nDid you consider confounders and effect modifiers?\n\n\nModel diagnostics\n\n2-5 sentences\nIncludes\n\nProcess of investigating model diagnostics\nBy the time you build the model, LINE assumptions should be met\nIf assumptions were not met, what process did you use to fix it?\n\n\n\n\n\n\n2.4 Results\n\nLength: ~3 paragraphs\nPurpose: Relay the results from our sample’s analysis typically focusing on the numbers and interpretations\nSome important results to discuss (also could be sections)\n\nSample data set statistics (Table 1)\n\n3-5 sentences\nInclude a brief description of the sample’s characteristics\nTable 1 should be referenced and appear here!\n\nFinal model\n\n1-2 sentences\nDescribe final model (or models if comparing a few)\n\nWhat variables were included in your final model?\nWhat interactions with your explanatory variable did you include?\n\n\nInterpret the model coefficients in the context of the research question\n\n1-2 paragraphs\nInterpreting the explanatory variable’s relationship with IAT score is the most important thing to report!!\n\nWhen doing this, make sure you account for ALL interactions: If your explanatory variable has multiple interactions and you are trying to interpret one, then what does that mean about the other variables involved in the other interactions? If this is confusing, please make an appointment with me!!\n\n\nResults of model diagnostics if there is anything worth noting\n\nTables & figures\n\nThe following are required tables or figures\n\nTable 1 summarizing participant characteristics both overall and stratified by your primary independent variable\nTable or figure with regression results\n\nCan be a forest plot\nIf you have A LOT of coefficient estimates, the forest plot may not work well!\n\n\n1-3 figures that you think are helpful in understanding the results, for example\n\nDAG explaining connection between variables (if you did this)\nTable or figure to compare model fit statistics (if you did this)\nTable or figure for unadjusted relationship between outcome and explanatory variables\n\n\n\n\n\n2.5 Discussion\n\nLength: 2-3 paragraphs\nPurpose: Discuss the results and give them context outside of the sample and its analysis\nSome important things to include\n\nInclude a paragraph on the limitations of the results\n\nYou don’t need to hit all the limitations, but think about the big ones (generalizability? independence of samples? large sample size vs. clinical significance? the way we handled variables?)\n\nAfter limitations, discuss the positive parts of the results\n\nWhat can we do with these results? What impact can it have?\n\nAny overarching trends that are worth noting? (Giebel et al. 2024)\n\nShould contain some references\n\n\n\n2.6 Conclusion\n\nLength: 1 short paragraph (more like ~3 sentences)\nPurpose: Describe the main conclusions to a non-technical audience\n\n\n\n2.7 References\n\nInclude your references here!\nYou introduction should have references, especially when discussing the social science behind the analysis\nYou must reference the IAT data source!!"
  }
]