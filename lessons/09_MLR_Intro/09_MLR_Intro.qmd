---
title: "Lesson 9: Introduction to Multiple Linear Regression (MLR)"
author: "Nicky Wakim"
title-slide-attributes:
    data-background-color: "#213c96"
date: "02/05/2025"
format: 
  revealjs:
    theme: "../simple_NW.scss"
    chalkboard: true
    slide-number: true
    show-slide-number: all
    width: 1955
    height: 1100
    footer: "Lesson 9: MLR Intro"
    html-math-method: mathjax
    highlight-style: ayu
execute:
  echo: true
  freeze: auto  # re-render only when source changes
---

```{r}
#| label: "setup" 
#| include: false
#| message: false
#| warning: false

library(tidyverse)
library(janitor)
library(knitr)
library(broom)
library(rstatix)
library(gt)
library(readxl)
#----------
# new packages
# install.packages("describedata")
library(describedata) # gladder()
library(gridExtra)   # grid.arrange()
library(ggfortify)  # autoplot(model)
# New Day 6
library(gtsummary)

# New Day 7
library(plotly) # for plot_ly() command
library(GGally) # for ggpairs() command 
library(ggiraphExtra)   # for ggPredict() command

```

# Learning Objectives

1.  Understand the population multiple linear regression model through equations and visuals.
2.  Fit MLR model (in `R`) and understand the difference between fitted regression plane and regression lines.
3.  Interpret MLR (population) coefficient estimates with additional variable in model
4.  Based off of previous SLR work, understand how the population MLR is estimated.


```{css, echo=FALSE}
.reveal code {
  max-height: 100% !important;
}
```

## Reminder of what we learned in the context of SLR

-   SLR helped us establish the foundation for a lot of regression

    -   But we do not usually use SLR in analysis

**What did we learn in SLR??**

::: columns
::: {.column width="31%"}
::: RAP2
::: RAP2-title
Model Fitting
:::

::: RAP2-cont
-   Ordinary least squares (OLS)
-   `lm()` function in R
:::
:::
:::

::: {.column width="36%"}
::: RAP4
::: RAP4-title
Model Use
:::

::: RAP4-cont
-   Inference for variance of residuals
-   Hypothesis testing for coefficients
-   Interpreting population coefficient estimates
-   Calculated the expected mean for specific $X$ values
-   Interpreted coefficient of determination
:::
:::
:::

::: {.column width="32%"}
::: RAP3
::: RAP3-title
Model Evaluation/Diagnostics
:::

::: RAP3-cont
-   LINE Assumptions
-   Influential points
-   Data Transformations
:::
:::
:::
:::

## Let's map that to our regression analysis process

::: box
![](../img_slides/arrow2.png){.absolute top="13.5%" right="62.1%" width="155"} ![](../img_slides/arrow2.png){.absolute top="13.5%" right="28.4%" width="155"}![](../img_slides/arrow_back4.png){.absolute top="7.5%" right="30.5%" width="820"} ![](../img_slides/arrow_down.png){.absolute top="60.5%" right="48%" width="85"}

::: columns
::: {.column width="30%"}
::: RAP1
::: RAP1-title
Model Selection
:::

::: RAP1-cont
-   Building a model

-   Selecting variables

-   Prediction vs interpretation

-   Comparing potential models
:::
:::
:::

::: {.column width="4%"}
:::

::: {.column width="30%"}
::: RAP2
::: RAP2-title
Model Fitting
:::

::: RAP2-cont
-   Find best fit line

-   Using OLS in this class

-   Parameter estimation

-   Categorical covariates

-   Interactions
:::
:::
:::

::: {.column width="4%"}
:::

::: {.column width="30%"}
::: RAP3
::: RAP3-title
Model Evaluation
:::

::: RAP3-cont
-   Evaluation of model fit
-   Testing model assumptions
-   Residuals
-   Transformations
-   Influential points
-   Multicollinearity
:::
:::
:::
:::
:::

::: RAP4
::: RAP4-title
Model Use (Inference)
:::

::: RAP4-cont
::: columns
::: {.column width="50%"}
-   Inference for coefficients
-   Hypothesis testing for coefficients
:::

::: {.column width="50%"}
-   Inference for expected $Y$ given $X$
:::
:::
:::
:::

## 

![](../img_slides/all_mod_wrong.jpg){fig-align="center"}

# Learning Objectives

::: lob
1.  Understand the population multiple linear regression model through equations and visuals.
:::

2.  Fit MLR model (in `R`) and understand the difference between fitted regression plane and regression lines.
3.  Interpret MLR (population) coefficient estimates with additional variable in model
4.  Based off of previous SLR work, understand how the population MLR is estimated.

## Simple Linear Regression vs. Multiple Linear Regression

::: columns
::: {.column width="50%"}
::: L1
Simple Linear Regression
:::

 

::: L2
We use **one predictor** to try to explain the variance of the outcome

$$
Y = \beta_0 + \beta_1 X + \epsilon
$$
:::
:::

::: {.column width="50%"}
::: E1
Multiple Linear Regression
:::

 

::: E2
We use **multiple predictors** to try to explain the variance of the outcome

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_{k}X_{k}+ \epsilon
$$
:::

 

-   Has $k+1$ total coefficients (including intercept) for $k$ predictors/covariates
-   Sometimes referred to as ***multivariable*** linear regression, but *never multivariate*
:::
:::

 

-   The models have similar "LINE" assumptions and follow the same general diagnostic procedure

## Population multiple regression model

::: heq
$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2+ \ldots + \beta_k X_k + \epsilon$$
:::

or on the individual (observation) level:

$$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}+ \ldots + \beta_k x_{ik} + \epsilon_i,\ \ \text{for}\ i = 1, 2, \ldots, n$$

::: columns
::: column
#### Observable sample data

-   $Y$ is our dependent variable

    -   Aka outcome or response variable

-   $X_1, X_2, \ldots, X_k$ are our $k$ independent variables

    -   Aka predictors or covariates
:::

::: column
#### Unobservable population parameters

-   $\beta_0, \beta_1, \beta_2, \ldots, \beta_k$ are **unknown** population **parameters**
    -   From our sample, we find the population parameter estimates: $\widehat{\beta}_0, \widehat{\beta}_1, \widehat{\beta}_2, \ldots, \widehat{\beta}_k$
-   $\epsilon$ is the random error
    -   And is still normally distributed
    -   $\epsilon \sim N(0, \sigma^2)$ where $\sigma^2$ is the population parameter of the variance
:::
:::

## Going back to our life expectancy example

-   Let's say many other variables were measured for each country, including food supply

    -   [**Food Supply**](https://www.fao.org/faostat/en/#home) (kilocalories per person per day, kc PPD): the average kilocalories consumed by a person each day.

-   In SLR, we only had one predictor and one outcome in the model:

    -   Outcome: [**Life expectancy**](https://www.gapminder.org/data/documentation/gd004/) = the average number of years a newborn child would live if current mortality patterns were to stay the same.

    -   Predictor: [**Adult literacy rate**](http://data.uis.unesco.org/)(predictor) is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life.

 

-   Do we think adult female literacy rate is going to explain a lot of the variance of life expectancy between countries?

## Loading the data

```{r}
# Load the data - update code if the file is not in the same location
# on your computer
gapm <- read_excel("data/Gapminder_vars_2011.xlsx", 
                   na = "NA")  # important!!!! 

gapm_sub <- gapm %>% 
  drop_na(LifeExpectancyYrs, FemaleLiteracyRate, FoodSupplykcPPD)
  # above drops rows with NAs in any of the three variables

glimpse(gapm_sub)
```

## Can we improve our model by adding food supply as a covariate?

::: columns
::: {.column width="10%"}
:::

::: {.column width="80%"}
::: definition
::: def-title
Simple linear regression population model
:::

::: def-cont
$$\begin{aligned}
\text{Life expectancy} & = \beta_0 + \beta_1 \text{Female literacy rate} + \epsilon \\
\text{LE} & = \beta_0 + \beta_1 \text{FLR} + \epsilon
\end{aligned}$$
:::
:::

::: fact
::: fact-title
Multiple linear regression population model (with added Food Supply)
:::

::: fact-cont
$$\begin{aligned}
\text{Life expectancy} & = \beta_0 + \beta_1 \text{Female literacy rate} + \beta_2 \text{Food supply} + \epsilon \\
\text{LE} & = \beta_0 + \beta_1 \text{FLR} + \beta_2 \text{FS} + \epsilon
\end{aligned}$$
:::
:::
:::

::: {.column width="10%"}
:::
:::

## Visualize relationship between life expectancy, female literacy rate, and food supply

```{r fig.width=8, fig.height=3}
#| echo: false

plot_LE_FLR <- ggplot(gapm_sub, aes(x = FemaleLiteracyRate,
                 y = LifeExpectancyYrs)) +
  geom_point() +
  geom_smooth() +
  geom_smooth(method = "lm", color = "#F14124") +
  labs(title = "Life expectancy vs.\n female literacy rate", 
       x = "Female Litaracy Rate (%)", 
       y = "Life Expectancy (yrs)") +
  theme(axis.title = element_text(size = 10), 
        axis.text = element_text(size = 10), 
        title = element_text(size = 10))

plot_LE_FS <- ggplot(gapm_sub, aes(x = FoodSupplykcPPD,
                 y = LifeExpectancyYrs)) +
  geom_point() +
  geom_smooth() +
  geom_smooth(method = "lm", color = "#F14124") +
  labs(title = "Life expectancy vs. \n food supply", 
       x = "Food supply (kcal PPD)", 
       y = "Life Expectancy (yrs)") +
  theme(axis.title = element_text(size = 10), 
        axis.text = element_text(size = 10), 
        title = element_text(size = 10))

plot_FLR_FS <- ggplot(gapm_sub, aes(x = FoodSupplykcPPD,
                 y = FemaleLiteracyRate)) +
  geom_point() +
  geom_smooth() +
  geom_smooth(method = "lm", color = "#F14124") +
  labs(title = "Female literacy rate vs. \n food supply", 
       y = "Female Litaracy Rate (%)", 
       x = "Food supply (kcal PPD)") +
  theme(axis.title = element_text(size = 10), 
        axis.text = element_text(size = 10), 
        title = element_text(size = 10))

grid.arrange(plot_LE_FLR, plot_LE_FS, plot_FLR_FS,
             nrow = 1)
```

## Visualize relationship in 3-D

::: columns
::: {.column width="25%"}
:::

::: {.column width="50%"}
```{r fig.height=8, fig.width=8, warning=F}
#| echo: false
#| fig-align: center
# plotly package is require for plot_ly function, which is loaded at beginning of Rmd 
# z = response variable
# x & y are predictor variables

dim3_scatter <- plot_ly(gapm_sub, 
                       x = ~FemaleLiteracyRate, 
                       y = ~FoodSupplykcPPD, 
                       z = ~LifeExpectancyYrs) %>%
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'Female literacy rate'),
                     yaxis = list(title = 'Food supply (kc PPD)'),
                     zaxis = list(title = 'Life expectancy')))
dim3_scatter
```
:::

::: {.column width="25%"}
:::
:::

## Poll Everywhere Question 1

# Learning Objectives

1.  Understand the population multiple linear regression model through equations and visuals.

::: lob
2.  Fit MLR model (in `R`) and understand the difference between fitted regression plane and regression lines.
:::

3.  Interpret MLR (population) coefficient estimates with additional variable in model
4.  Based off of previous SLR work, understand how the population MLR is estimated.

## How do we fit a multiple linear regression model in R?

New population model for example:

$$\text{LE} = \beta_0 + \beta_1 \text{FLR} + \beta_2 \text{FS} + \epsilon$$

```{r}
# Fit regression model:
mr1 <- gapm_sub %>% 
  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)
tidy(mr1, conf.int=T) %>% gt() %>% tab_options(table.font.size = 35) %>% fmt_number(decimals = 3)
```

```{r}
#| echo: false

mr1_tidy = tidy(mr1, conf.int=T)
```

Fitted multiple regression model:

$$\begin{aligned}
\widehat{\text{LE}} &= \widehat{\beta}_0 + \widehat{\beta}_1 \text{FLR} + \widehat{\beta}_2 \text{FS} \\
\widehat{\text{LE}} &= `r round(mr1_tidy$estimate[1], 3)` + `r round(mr1_tidy$estimate[2], 3)` \ \text{FLR} 
+ `r round(mr1_tidy$estimate[3], 3)`\ \text{FS}
\end{aligned}$$

## Don't forget `summary()` to extract information!

```{r}
summary(mr1)
```

## Visualize the fitted multiple regression model

::: columns
::: {.column width="40%"}
-   The fitted model equation $$\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_1 \cdot X_1 + \widehat{\beta}_2 \cdot X_2$$ has three variables ($Y, X_1,$ and $X_2$) and thus we need 3 dimensions to plot it

 

-   Instead of a regression line, we get a **regression plane**
    -   *See code in `.qmd`- file. I hid it from view in the html file.*
:::

::: {.column width="60%"}
```{r fig.height=10, fig.width=10, echo=F, warning=FALSE}
#| fig-align: center
# setup hideous grid required by plotly
mr1 <- gapm_sub %>% 
  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)
x_grid <- seq(from = min(gapm_sub$FemaleLiteracyRate, na.rm = TRUE), 
              to = max(gapm_sub$FemaleLiteracyRate, na.rm = TRUE), 
              length = 100)
y_grid <- seq(from = min(gapm_sub$FoodSupplykcPPD, na.rm = TRUE), 
              to = max(gapm_sub$FoodSupplykcPPD, na.rm = TRUE), 
              length = 200)
z_grid <- expand.grid(x_grid, y_grid) %>%
  as_tibble() %>%
  rename(x_grid = Var1, y_grid = Var2) %>%
  mutate(z = coef(mr1)[1] + coef(mr1)[2]*x_grid + coef(mr1)[3]*y_grid) %>%
  .[["z"]] %>%
  matrix(nrow=length(x_grid)) %>%
  t()

# Plot points
plot_ly() %>%
  add_markers(
    x = gapm_sub$FemaleLiteracyRate,
    y = gapm_sub$FoodSupplykcPPD,
    z = gapm_sub$LifeExpectancyYrs,
    hoverinfo = 'text',
    text = ~paste("x1 - Female literacy rate: ",
                  gapm_sub$FemaleLiteracyRate,
                  "</br> x2 - Food supply (kc PPD): ",
                  gapm_sub$FoodSupplykcPPD,
                  "</br> y - Life expectancy: ",
                  gapm_sub$LifeExpectancyYrs)
  ) %>%
  # Label axes
  layout(
    scene = list(
      xaxis = list(title = "x1 - Female literacy rate"),
      yaxis = list(title = "x2 - Food supply (kc PPD)"),
      zaxis = list(title = "y - Life expectancy")
    )
  ) %>%
  # Add regression plane
  add_surface(
    x = x_grid,
    y = y_grid,
    z = z_grid
  )

```
:::
:::

## Regression lines for varying values of food supply

$$\begin{aligned}
\widehat{\text{LE}} &= \widehat{\beta}_0 + \widehat{\beta}_1 \text{FLR} + \widehat{\beta}_2 \text{FS} \\
\widehat{\text{LE}} &= `r round(mr1_tidy$estimate[1], 3)` + `r round(mr1_tidy$estimate[2], 3)` \ \text{FLR} 
+ `r round(mr1_tidy$estimate[3], 3)`\ \text{FS}
\end{aligned}$$

::: columns
::: {.column width="35%"}
-   Note: when the food supply is held constant but the female literacy rate varies...
    -   then the outcome values change along a **line**
-   Different values of food supply give different lines
    -   The intercepts change, but
    -   the slopes stay the same (parallel lines)
:::

::: {.column width="65%"}
```{r}
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: center

(mr1_2d = ggPredict(mr1, interactive = T))
```
:::
:::

## How do we calculate the regression line for 3000 kc PPD food supply?

::: columns
::: {.column width="40%"}
 

$$\begin{aligned}
\widehat{\text{LE}} &= `r round(mr1_tidy$estimate[1], 3)` + `r round(mr1_tidy$estimate[2], 3)` \ \text{FLR} 
+ `r round(mr1_tidy$estimate[3], 3)`\ \text{FS}\\
\widehat{\text{LE}} &= `r round(mr1_tidy$estimate[1], 3)` + `r round(mr1_tidy$estimate[2], 3)` \ \text{FLR} 
+ `r round(mr1_tidy$estimate[3], 3)`\cdot 3000\\
\widehat{\text{LE}} &= `r round(mr1_tidy$estimate[1], 3)` + `r round(mr1_tidy$estimate[2], 3)` \ \text{FLR}
+ `r round(mr1_tidy$estimate[3]*3000, 3)` \\
\widehat{\text{LE}} &= `r round(mr1_tidy$estimate[1] + mr1_tidy$estimate[3]*3000, 3)` + `r round(mr1_tidy$estimate[2], 3)` \ \text{FLR}
\end{aligned}$$
:::

::: {.column width="60%"}
```{r}
#| fig-width: 6
#| fig-height: 4
#| fig-align: center

(mr1_2d = ggPredict(mr1, interactive = T))
```
:::
:::

## Poll Everwhere Question 2

# Learning Objectives

1.  Understand the population multiple linear regression model through equations and visuals.
2.  Fit MLR model (in `R`) and understand the difference between fitted regression plane and regression lines.

::: lob
3.  Interpret MLR (population) coefficient estimates with additional variable in model
:::

4.  Based off of previous SLR work, understand how the population MLR is estimated.

## Interpreting the estimated population coefficients

-   For a population model: $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2+ \epsilon$$
    -   Where $X_1$ and $X_2$ are continuous variables
    -   No need to specify $Y$ because it required to be continuous in linear regression

::: columns
::: {.column width="33%"}
::: fact
::: fact-title
General interpretation for $\widehat{\beta}_0$
:::

::: fact-cont
The expected $Y$-variable is ($\widehat\beta_0$ units) when the $X_1$-variable is 0 $X_1$-units and $X_2$-variable is 0 $X_1$-units (95% CI: LB, UB).
:::
:::
:::

::: {.column width="33%"}
::: definition
::: def-title
General interpretation for $\widehat{\beta}_1$
:::

::: def-cont
For every increase of 1 $X_1$-unit in the $X_1$-variable, adjusting/controlling for $X_2$-variable, there is an expected increase/decrease of $|\widehat\beta_1|$ units in the $Y$-variable (95%: LB, UB).
:::
:::
:::

::: {.column width="33%"}
::: proof1
::: proof-title
General interpretation for $\widehat{\beta}_2$
:::

::: proof-cont
For every increase of 1 $X_2$-unit in the $X_2$-variable, adjusting/controlling for $X_1$-variable, there is an expected increase/decrease of $|\widehat\beta_2|$ units in the $Y$-variable (95%: LB, UB).
:::
:::
:::
:::


## Interpreting the estimated population coefficient: $\widehat{\beta}_0$ {#color-slide1}

```{css, echo=FALSE}
#color-slide1 h2 {
 color: #34AC8B;
}
```

- For an estimated model: $\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_1 X_2 + \widehat{\beta}_2 X_2$

$$\begin{aligned}
\widehat{Y} = &\widehat{\beta}_0 + \widehat{\beta}_1 0 + \widehat{\beta}_2 0\\
\widehat{Y} = &\widehat{\beta}_0 \\
\end{aligned}$$

**Interpretation:** The expected $Y$-variable is ($\widehat\beta_0$ units) when the $X_1$-variable is 0 $X_1$-units and $X_2$-variable is 0 $X_1$-units (95% CI: LB, UB).

## Interpreting the estimated population coefficient: $\widehat{\beta}_1$ {#color-slide2}

```{css, echo=FALSE}
#color-slide2 h2 {
 color: #4FADF3;
}
```

- We will use: $x_{1a}$ and $x_{1b} = x_{1a} + 1$, with the implication that $\Delta{x_1} = x_{1b} - x_{1a} = 1$

- Our goal is to get to a statement with $\widehat{\beta}_1$ alone:

$$\begin{aligned}
\widehat{Y}|x_{1a} = &\widehat{\beta}_0 + \widehat{\beta}_1 x_{1a} + \widehat{\beta}_2 X_2\\
\widehat{Y}|x_{1b} = &\widehat{\beta}_0 + \widehat{\beta}_1 x_{1b} + \widehat{\beta}_2 X_2\\
\widehat{Y}|x_{1b} - \widehat{Y}|x_{1a} = & \bigg[\widehat{\beta}_0 + \widehat{\beta}_1 x_{1b} + \widehat{\beta}_2 X_2\bigg] - \bigg[\widehat{\beta}_0 + \widehat{\beta}_1 x_{1a} + \widehat{\beta}_2 X_2\bigg] \\
\widehat{Y}|x_{1b} - \widehat{Y}|x_{1a} = &  \widehat{\beta}_1 x_{1b} - \widehat{\beta}_1 x_{1a}\\
\widehat{Y}|x_{1b} - \widehat{Y}|x_{1a} = &  \widehat{\beta}_1 (x_{1b} - x_{1a})\\
\widehat{Y}|x_{1b} - \widehat{Y}|x_{1a} = &  \widehat{\beta}_1\\
\end{aligned}$$

**Interpretation:** For every increase of 1 $X_1$-unit in the $X_1$-variable, adjusting/controlling for $X_2$-variable, there is an expected increase/decrease of $|\widehat\beta_1|$ units in the $Y$-variable (95%: LB, UB).


## Interpreting the estimated population coefficient: $\widehat{\beta}_2$ {#color-slide3}

```{css, echo=FALSE}
#color-slide3 h2 {
 color: #FF8021;
}
```

- We can so the same for $X_2$: $x_{2a}$ and $x_{2b} = x_{2a} + 1$, with the implication that $\Delta{x_2} = x_{2b} - x_{2a} = 1$

- Our goal is to get to a statement with $\widehat{\beta}_1$ alone:

$$\begin{aligned}
\widehat{Y}|x_{2a} = &\widehat{\beta}_0 + \widehat{\beta}_1 x_{1a} + \widehat{\beta}_2 X_2\\
\widehat{Y}|x_{2b} = &\widehat{\beta}_0 + \widehat{\beta}_1 x_{1b} + \widehat{\beta}_2 X_2\\
\widehat{Y}|x_{2b} - \widehat{Y}|x_{2a} = & \bigg[\widehat{\beta}_0 + \widehat{\beta}_1 X_1 + \widehat{\beta}_2 x_{2b} \bigg] - \bigg[\widehat{\beta}_0 + \widehat{\beta}_1 X_1 + \widehat{\beta}_2 x_{2a}\bigg] \\
\widehat{Y}|x_{2b} - \widehat{Y}|x_{2a} = &  \widehat{\beta}_2 x_{2b} - \widehat{\beta}_2 x_{2a}\\
\widehat{Y}|x_{2b} - \widehat{Y}|x_{2a} = &  \widehat{\beta}_2 (x_{2b} - x_{2a})\\
\widehat{Y}|x_{2b} - \widehat{Y}|x_{2a} = &  \widehat{\beta}_2\\
\end{aligned}$$

**Interpretation:** For every increase of 1 $X_2$-unit in the $X_2$-variable, adjusting/controlling for $X_1$-variable, there is an expected increase/decrease of $|\widehat\beta_2|$ units in the $Y$-variable (95%: LB, UB).


## Poll Everywhere Question 3

## Getting these interpretations from our regression table

We fit the regression model in R and printed the regression table:

```{r}
mr1 <- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, 
          data = gapm_sub)
```

```{r}
#| echo: false
tidy(mr1, conf.int=T) %>% gt() %>% tab_options(table.font.size = 30) %>% fmt_number(decimals = 3)
```

Fitted multiple regression model: $\widehat{\text{LE}} = 33.595 + 0.157 \text{ FLR} + 0.008 \text{ FS}$

::: columns
::: {.column width="33%"}
::: fact
::: fact-title
Interpretation for $\widehat{\beta}_0$
:::

::: fact-cont
The average life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 kcal PPD (95% CI: 24.674, 41.517).
:::
:::
:::

::: {.column width="33%"}
::: definition
::: def-title
Interpretation for $\widehat{\beta}_1$
:::

::: def-cont
For every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).
:::
:::
:::

::: {.column width="33%"}
::: proof1
::: proof-title
Interpretation for $\widehat{\beta}_2$
:::

::: proof-cont
For every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012).
:::
:::
:::
:::

## What we need in our interpretations of coefficients (reference)

::: columns
::: column
-   Units of Y

-   Units of X

-   If discussing intercept: Mean or average or expected before Y

-   If discussing coefficient for continuous covariate: Mean or average or expected before difference, increase, or decrease

    -   OR: Mean or average or expected before Y
    -   NOT: predicted
    -   Only need before difference or Y!!

-   Confidence interval
:::

::: column
-   If other covariates in the model

    -   Discussing intercept: Must state that variables are equal to 0

        -   or at their centered value if centered!

    -   Discussing coefficient for covariate: Must state "adjusting for all other variables", "Controlling for all other variables", or "Holding all other variables constant"

        -   If only one other variable in the model, then replace "all other variables" with the single variable name
:::
:::

# Learning Objectives

1.  Understand the population multiple linear regression model through equations and visuals.
2.  Fit MLR model (in `R`) and understand the difference between fitted regression plane and regression lines.
3.  Interpret MLR (population) coefficient estimates with additional variable in model

::: lob
4.  Based off of previous SLR work, understand how the population MLR is estimated.
:::

## How do we estimate the model parameters?

-   We need to estimate the population model coefficients $\widehat{\beta}_0, \widehat{\beta}_1, \widehat{\beta}_2, \ldots, \widehat{\beta}_k$
-   This can be done using the **ordinary least-squares method**
    -   Find the $\widehat{\beta}$ values that **minimize** the sum of squares due to error ($SSE$)

$$ \begin{aligned}
SSE & = \displaystyle\sum^n_{i=1} \widehat\epsilon_i^2 \\
SSE & = \displaystyle\sum^n_{i=1} (Y_i - \widehat{Y}_i)^2 \\
SSE & = \displaystyle\sum^n_{i=1} (Y_i - (\widehat{\beta}_0 +\widehat{\beta}_1 X_{i1}+ \ldots+\widehat{\beta}_1 X_{ik}))^2 \\
SSE & = \displaystyle\sum^n_{i=1} (Y_i - \widehat{\beta}_0 -\widehat{\beta}_1 X_{i1}- \ldots-\widehat{\beta}_1 X_{ik})^2
\end{aligned}$$

## Technical side note (not needed in our class)

-   The equations for calculating the $\boldsymbol{\widehat{\beta}}$ values is best done using matrix notation (not required for our class)

-   We will be using `R` to get the coefficients instead of the equation (already did this a few slides back!)

-   How we have represented the population regression model: $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2+ \ldots + \beta_k X_k + \epsilon$$

::: columns
::: {.column width="40%"}
-   How to represent population model with matrix notation:

$$\begin{aligned}
\boldsymbol{Y} &= \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} \\
\boldsymbol{Y}_{n \times 1}& = \boldsymbol{X}_{n \times (k+1)}\boldsymbol{\beta}_{(k+1)\times 1} + \boldsymbol{\epsilon}_{n \times 1}
\end{aligned}$$

- $\boldsymbol{X}$ is often called the design matrix
  - Each row represents an individual 
  - Each column represents a covariate
:::

::: {.column width="20%"}
$$
\boldsymbol{Y} = \left[\begin{array}{c} Y_1 \\ Y_2 \\ \vdots \\ Y_n
\end{array} \right]_{n \times 1} 
$$ $$
\boldsymbol{\epsilon} = \left[\begin{array}{c} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n
\end{array} \right]_{n \times 1}  
$$
:::

::: {.column width="40%"}
$$
\boldsymbol{X} = \left[ \begin{array}{ccccc} 1 &  X_{11} &  X_{12} & \ldots & X_{1,k} \\
1 &X_{21} &  X_{22} & \ldots & X_{2,k} \\
\vdots&\vdots & \vdots &  \ldots & \vdots \\
1 & X_{n1} &  X_{n2} & \ldots & X_{n,k} \end{array} \right]_{n \times (k+1)}
$$

$$
\boldsymbol{\beta}  = \left[\begin{array}{c} \beta_0 \\ \beta_1\\  \vdots \\
\beta_{k}
\end{array} \right]_{(k+1)\times 1}
$$
:::
:::

## LINE model assumptions

::: columns
::: column
::: definition
::: def-title
**\[L\] Linearity** of relationship between variables
:::

::: def-cont
The mean value of $Y$ given any combination of $X_1, X_2, \ldots, X_k$ values, is a linear function of $\beta_0, \beta_1, \beta_2, \ldots, \beta_k$:

$$\mu_{Y|X_1, \ldots, X_k} = \beta_0 + \beta_1 X_1 + \beta_2 X_2+ \ldots + \beta_k X_k$$
:::
:::
:::

::: column
::: proof1
::: proof-title
**\[I\] Independence** of the $Y$ values
:::

::: proof-cont
Observations ($X_1, X_2, \ldots, X_k, Y$) are independent from one another
:::
:::
:::

::: column
::: theorem
::: thm-title
**\[N\] Normality** of the $Y$'s given $X$ (residuals)
:::

::: thm-cont
$Y$ has a normal distribution for any any combination of $X_1, X_2, \ldots, X_k$ values

-   Thus, the residuals are normally distributed
:::
:::
:::

::: column
::: fact
::: fact-title
**\[E\] Equality** of variance of the residuals (homoscedasticity)
:::

::: fact-cont
The variance of $Y$ is the same for any any combination of $X_1, X_2, \ldots, X_k$ values

$$\sigma^2_{Y|X_1, X_2, \ldots, X_k} = Var(Y|X_1, X_2, \ldots, X_k) = \sigma^2$$
:::
:::
:::
:::

## Summary of the LINE assumptions

-   Equivalently, the **residuals** are independently and identically distributed (iid):
    -   normal
    -   with mean 0 and
    -   constant variance $\sigma^2$

 
    
- Residuals are still $\widehat{\epsilon}_i=Y_i - \widehat{Y}_i$ for each observation
  - It's just that $\widehat{Y}_i$ is now calculated with many covariates ($X_1, X_2, \ldots, X_k$)

## Variation: Explained vs. Unexplained

$$\begin{aligned}
\sum_{i=1}^n (Y_i - \overline{Y})^2 &= \sum_{i=1}^n (\widehat{Y}_i- \overline{Y})^2 + \sum_{i=1}^n (Y_i - \widehat{Y}_i)^2 \\
SSY &= SSR + SSE
\end{aligned}$$

-   $Y_i - \overline{Y}$ = the deviation of $Y_i$ around the mean $\overline{Y}$
    -   the **total** amount deviation
-   $\widehat{Y}_i- \overline{Y}$ = the deviation of the fitted value $\widehat{Y}_i$ around the mean $\overline{Y}$
    -   the amount deviation **explained** by the regression at $X_{i1},\ldots,X_{ik}$
-   $Y_i - \widehat{Y}_i$ = the deviation of the observation $Y$ around the fitted regression line
    -   the amount deviation **unexplained** by the regression at $X_{i1},\ldots,X_{ik}$

## Poll Everywhere Question 4

## Building the ANOVA table {visibility="hidden"}

ANOVA table ($k$ = \# of predictors, $n$ = \# of observations)

::: columns
::: {.column width="8%"}
:::

::: {.column width="84%"}
| Variation Source | df      | SS    | MS                        | test statistic        | p-value                 |
|------------|------------|------------|------------|------------|------------|
| Regression       | $k$     | $SSR$ | $MSR = \frac{SSR}{k}$     | $F = \frac{MSR}{MSE}$ | $P(F_{(k, n-k-1)}>F)$ |
| Error            | $n-k-1$ | $SSE$ | $MSE = \frac{SSE}{n-k-1}$ |                       |                         |
| Total            | $n-1$   | $SSY$ |                           |                       |                         |
:::
:::

 

```{r}
#| eval: false
#| echo: false
anova(mr1) %>% tidy() %>% gt() %>%
   tab_options(table.font.size = 40) %>% fmt_number(decimals = 3)
```

## SLR: Another way to think of SSY, SSR, and SSE

-   Let's create a data frame of each component within the SS's

    -   Deviation in SSY: $Y_i - \overline{Y}$
    -   Deviation in SSR: $\widehat{Y}_i- \overline{Y}$
    -   Deviation in SSE: $Y_i - \widehat{Y}_i$

-   Using our simple linear regression model as an example:

```{r}
slr1 <- gapm_sub %>% 
  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)
aug_slr1 = augment(slr1)
SS_dev_slr = gapm_sub %>% select(LifeExpectancyYrs) %>%
  mutate(SSY_dev = LifeExpectancyYrs - mean(LifeExpectancyYrs),
         y_hat = aug_slr1$.fitted, 
         SSR_dev = y_hat - mean(LifeExpectancyYrs), 
         SSE_dev = aug_slr1$.resid)
```

## *SLR*: Plot the components of each sum of squares

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Code to make the below plots"

SSY_plot = ggplot(SS_dev_slr, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y)))
SSR_plot = ggplot(SS_dev_slr, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y)))
SSE_plot = ggplot(SS_dev_slr, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i]))
grid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)
```

::: columns
::: column
```{r}
#| fig-align: right
#| fig-width: 6
#| fig-height: 8
#| echo: false

SSY_plot_slr = ggplot(SS_dev_slr, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y))) + theme(axis.title.x = element_text(size = 22))
SSR_plot_slr = ggplot(SS_dev_slr, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y))) + theme(axis.title.x = element_text(size = 22))
SSE_plot_slr = ggplot(SS_dev_slr, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i])) + theme(axis.title.x = element_text(size = 22))
grid.arrange(SSY_plot_slr, SSR_plot_slr, SSE_plot_slr, nrow = 3)
```
:::

::: column
$$SSY = \sum_{i=1}^n (Y_i - \overline{Y})^2 = `r round(var(SS_dev_slr$SSY_dev), 2)`$$ 

 

$$SSR = \sum_{i=1}^n (\widehat{Y}_i- \overline{Y})^2 = `r round(var(SS_dev_slr$SSR_dev), 2)`$$

 

$$SSE =\sum_{i=1}^n (Y_i - \widehat{Y}_i)^2 = `r round(var(SS_dev_slr$SSE_dev), 2)`$$
:::
:::

## *MLR:* Another way to think of SSY, SSR, and SSE

-   Let's create a data frame of each component within the SS's

    -   Deviation in SSY: $Y_i - \overline{Y}$
    -   Deviation in SSR: $\widehat{Y}_i- \overline{Y}$
    -   Deviation in SSE: $Y_i - \widehat{Y}_i$

-   Using our simple linear regression model as an example:

```{r}
mr1 <- gapm_sub %>% 
  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)
aug_mlr1 = augment(mr1)
SS_df = gapm_sub %>% select(LifeExpectancyYrs) %>%
  mutate(SSY_dev = LifeExpectancyYrs - mean(LifeExpectancyYrs),
         y_hat = aug_mlr1$.fitted, 
         SSR_dev = y_hat - mean(LifeExpectancyYrs), 
         SSE_dev = aug_mlr1$.resid)
```

## *MLR:* Plot the components of each sum of squares

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Code to make the below plots"

SSY_plot = ggplot(SS_df, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y)))
SSR_plot = ggplot(SS_df, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y)))
SSE_plot = ggplot(SS_df, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i]))
grid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)
```

::: columns
::: column
```{r}
#| fig-align: right
#| fig-width: 6
#| fig-height: 8
#| echo: false

SSY_plot = ggplot(SS_df, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y))) + theme(axis.title.x = element_text(size = 22))
SSR_plot = ggplot(SS_df, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y))) + theme(axis.title.x = element_text(size = 22))
SSE_plot = ggplot(SS_df, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i])) + theme(axis.title.x = element_text(size = 22))
grid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)
```
:::

::: column
$$SSY = \sum_{i=1}^n (Y_i - \overline{Y})^2 = `r round(var(SS_df$SSY_dev), 2)`$$ 

 

$$SSR = \sum_{i=1}^n (\widehat{Y}_i- \overline{Y})^2 = `r round(var(SS_df$SSR_dev), 2)`$$

 

$$SSE =\sum_{i=1}^n (Y_i - \widehat{Y}_i)^2 = `r round(var(SS_df$SSE_dev), 2)`$$
:::
:::

## What did you notice in the plots?

::: columns
::: {.column width="50%"}
::: L1
Simple Linear Regression
:::
:::
::: {.column width="50%"}
::: E1
Multiple Linear Regression
:::
:::
:::

::: columns
::: {.column width="35%"}
```{r}
#| fig-align: center
#| fig-width: 6
#| fig-height: 8
#| echo: false

grid.arrange(SSY_plot_slr, SSR_plot_slr, SSE_plot_slr, nrow = 3)
```
:::

::: {.column width="15%"}
$$SSY = `r round(var(SS_dev_slr$SSY_dev), 2)`$$ 

 

 

$$SSR = `r round(var(SS_dev_slr$SSR_dev), 2)`$$

 

 

$$SSE =`r round(var(SS_dev_slr$SSE_dev), 2)`$$
:::

::: {.column width="35%"}
```{r}
#| fig-align: center
#| fig-width: 6
#| fig-height: 8
#| echo: false

grid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)
```
:::
::: {.column width="15%"}
$$SSY = `r round(var(SS_df$SSY_dev), 2)`$$ 

 

 

$$SSR = `r round(var(SS_df$SSR_dev), 2)`$$

 

 

$$SSE =`r round(var(SS_df$SSE_dev), 2)`$$
:::
:::

- Next class: we can determine if model fit is better by comparing the SSE's of different models
