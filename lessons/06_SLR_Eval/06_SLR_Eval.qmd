---
title: "Lesson 6: SLR: More inference"
author: "Nicky Wakim"
title-slide-attributes:
    data-background-color: "#213c96"
date: "01/27/2025"
categories: ["Week 1"]
format: 
  revealjs:
    theme: "../simple_NW.scss"
    chalkboard: true
    slide-number: true
    show-slide-number: all
    width: 1955
    height: 1100
    footer: "Lesson 6: SLR 3"
    html-math-method: mathjax
    highlight-style: ayu
execute:
  echo: true
  freeze: auto  # re-render only when source changes
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| label: "setup" 
#| include: false
#| message: false
#| warning: false

library(tidyverse)    
library(openintro)
library(janitor)
library(rstatix)
library(knitr)
library(gtsummary)
library(moderndive)
library(gt)
library(broom) 
library(here) 
library(pwr) 
library(gridExtra) # NEW!!!
library(readxl)

# terminal: for icons
# quarto install extension quarto-ext/fontawesome

# set ggplot theme for slides 
theme_set(theme_gray(base_size = 22))
# theme_update(text = element_text(size=16))  # set global text size for ggplots
```

```{r}
#| include: false
#| message: false
#| warning: false
gapm1 <- read_excel(here("data/Gapminder_vars_2011.xlsx"), na = "NA") 
gapm <- gapm1 %>% drop_na(LifeExpectancyYrs, FemaleLiteracyRate)
```

```{r}
#| include: false
#| message: false
#| warning: false
# Fit regression model:
model1 <- gapm %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)

reg_table = tidy(model1) %>% gt() %>% 
 tab_options(table.font.size = 40) %>%
 fmt_number(decimals = 3)
```

# Learning Objectives

1.  Identify different sources of variation in an Analysis of Variance
    (ANOVA) table

2.  Using the F-test, determine if there is enough evidence that
    population slope $\beta_1$ is not 0
    
3.  Using the F-test, determine if there is enough evidence for association between an outcome and a categorical variable

4.  Calculate and interpret the coefficient of determination


## So far in our regression example...

::: columns
::: {.column width="49%"}
**Lesson 3: SLR 1**

-   Fit regression line
-   Calculate slope & intercept
-   Interpret slope & intercept

**Lesson 4: SLR 2**

-   Estimate variance of the residuals
-   Inference for slope & intercept: CI, p-value
-   Confidence bands of regression line for mean value of Y\|X

**Lesson 5: Categorical Covariates**

-   Inference for different categories
:::

::: {.column width="2%"}
:::

::: {.column width="49%"}
```{r}
#| fig-height: 8
#| fig-width: 11
#| echo: false

ggplot(gapm, aes(x = FemaleLiteracyRate,
                 y = LifeExpectancyYrs)) +
  geom_point(size = 4) +
  geom_smooth(method = "lm", se = FALSE, size = 3, colour="#F14124") +
  labs(x = "Female literacy rate (%)", 
       y = "Life expectancy (years)",
       title = "Relationship between life expectancy and \n the female literacy rate in 2011") +
    theme(axis.title = element_text(size = 30), 
        axis.text = element_text(size = 25), 
        title = element_text(size = 30))

```

```{=tex}
\begin{aligned}
\widehat{Y} &= \widehat\beta_0 + \widehat\beta_1 \cdot X\\
\widehat{\text{LE}} &= 50.9 + 0.232 \cdot \text{FLR}
\end{aligned}
```
:::
:::


```{css, echo=FALSE}
.reveal code {
  max-height: 100% !important;
}
```
## Process of regression data analysis

::: box
![](../img_slides/arrow2.png){.absolute top="13.5%" right="62.1%" width="155"} ![](../img_slides/arrow2.png){.absolute top="13.5%" right="28.4%" width="155"}![](../img_slides/arrow_back4.png){.absolute top="7.5%" right="30.5%" width="820"} ![](../img_slides/arrow_down.png){.absolute top="60.5%" right="48%" width="85"}

::: columns
::: {.column width="30%"}
::: RAP1
::: RAP1-title
Model Selection
:::

::: RAP1-cont
-   Building a model

-   Selecting variables

-   Prediction vs interpretation

-   Comparing potential models
:::
:::
:::

::: {.column width="4%"}
:::

::: {.column width="30%"}
::: RAP2
::: RAP2-title
Model Fitting
:::

::: RAP2-cont
-   Find best fit line

-   Using OLS in this class

-   Parameter estimation

-   Categorical covariates

-   Interactions
:::
:::
:::

::: {.column width="4%"}
:::

::: {.column width="30%"}
::: RAP3
::: RAP3-title
Model Evaluation
:::

::: RAP3-cont
-   Evaluation of model fit
-   Testing model assumptions
-   Residuals
-   Transformations
-   Influential points
-   Multicollinearity
:::
:::
:::
:::
:::

::: RAP4
::: RAP4-title
Model Use (Inference)
:::

::: RAP4-cont
::: columns
::: {.column width="50%"}
-   Inference for coefficients
-   Hypothesis testing for coefficients
:::

::: {.column width="50%"}
-   Inference for expected $Y$ given $X$
-   Prediction of new $Y$ given $X$
:::
:::
:::
:::


# Learning Objectives

::: lob
1.  Identify different sources of variation in an Analysis of Variance
    (ANOVA) table
:::

2.  Using the F-test, determine if there is enough evidence that
    population slope $\beta_1$ is not 0
    
3.  Using the F-test, determine if there is enough evidence for association between an outcome and a categorical variable

4.  Calculate and interpret the coefficient of determination


## Getting to the F-test

The F statistic in linear regression is essentially a proportion of the
variance explained by the model vs. the variance not explained by the
model

1.  Start with visual of explained vs. unexplained variation

2.  Figure out the mathematical representations of this variation

3.  Look at the ANOVA table to establish key values measuring our
    variance from our model

4.  Build the F-test

## Explained vs. Unexplained Variation

```{r}
#| echo: false
#| fig-align: center

regression_points <- augment(model1)
# summary(model1)
# sum(model1$residuals^2)

my <- mean(gapm$LifeExpectancyYrs, na.rm=T)

ggplot(regression_points, 
       aes(x = FemaleLiteracyRate,
                 y = LifeExpectancyYrs)) +
  geom_hline(yintercept = my, color = "#A7EA52", size = 3) +
  geom_segment(aes(
    xend = FemaleLiteracyRate, 
    yend = .fitted), 
    alpha = 1, 
    color = "#4FADF3", 
    size = 2) +
  geom_smooth(method = "lm", se = FALSE, color = "#F14124", size=3) +
  # > Color adjustments made here...
  geom_point(color = "black", size = 4) +  # Color mapped here
  #scale_color_gradient2(low = "#213c96", mid = "white", high = "#F14124") +  # Colors to use here
    #guides(color = "none") +
  geom_point(aes(y = .fitted), shape = 1, size = 4) +
labs(x = "Female literacy rate (%)", 
       y = "Life expectancy (years)",
       title = "Life expectancy vs. female literacy rate in 2011") +
    theme(axis.title = element_text(size = 20), 
        axis.text = element_text(size = 20), 
        title = element_text(size = 20))  
```

$$ \begin{aligned}
Y_i - \overline{Y} & = (Y_i - \widehat{Y}_i) + (\widehat{Y}_i- \overline{Y})\\
\text{Total variation} & = \text{Residual variation after regression} + \text{Variation explained by regression}
\end{aligned}$$

## More on the equation

::: columns
::: {.column width="40%"}
$$Y_i - \overline{Y} = (Y_i - \widehat{Y}_i) + (\widehat{Y}_i- \overline{Y})$$

-   $Y_i - \overline{Y}$ = the deviation of $Y_i$ around the mean
    $\overline{Y}$
    -   (the **total** amount deviation)
-   $Y_i - \widehat{Y}_i$ = the deviation of the observation $Y$ around the
    fitted regression line
    -   (the amount deviation **unexplained** by the regression at $X_i$
        ).
-   $\widehat{Y_i}- \overline{Y}$ = the deviation of the fitted value
    $\widehat{Y}_i$ around the mean $\overline{Y}$
    -   (the amount deviation **explained** by the regression at $X_i$ )
:::

::: {.column width="60%"}
   

![](../img_slides/SS.png){fig-align="center"}
:::
:::

## Another way of thinking about the different deviations

::: columns
::: {.column width="50%"}
![](../img_slides/SSY_SSE_SSR.png){fig-align="center" width="1200"}
:::

::: {.column width="50%"}
:::
:::

## Poll Everywhere Question 1

## How is this actually calculated for our fitted model? (1/2)

$$ \begin{aligned}
Y_i - \overline{Y} & = (Y_i - \widehat{Y}_i) + (\widehat{Y}_i- \overline{Y})\\
\text{Total variation} & = \text{Variation explained by regression} + \text{Residual variation after regression}
\end{aligned}$$

$$\begin{aligned}
\sum_{i=1}^n (Y_i - \overline{Y})^2 & = \sum_{i=1}^n (\widehat{Y}_i- \overline{Y})^2 + \sum_{i=1}^n (Y_i - \widehat{Y}_i)^2 \\
SSY & = SSR + SSE 
\end{aligned}$$
$$\text{Total Sum of Squares} = \text{Sum of Squares explained by Regression} + \text{Sum of Squares due to Error (residuals)}$$

::: columns
::: {.column width="9%"}
:::

::: {.column width="82%"}
ANOVA table:

| Variation Source | df    | SS    | MS                      | test statistic        | p-value |
|------------|------------|------------|------------|------------|------------|
| Regression       | $1$   | $SSR$ | $MSR = \frac{SSR}{1}$   | $F = \frac{MSR}{MSE}$ |         |
| Error            | $n-2$ | $SSE$ | $MSE = \frac{SSE}{n-2}$ |                       |         |
| Total            | $n-1$ | $SSY$ |                         |                       |         |
:::

::: {.column width="9%"}
:::
:::

## How is this actually calculated for our fitted model? (2/2)

$$\begin{aligned}
\sum_{i=1}^n (Y_i - \overline{Y})^2 & = \sum_{i=1}^n (\widehat{Y}_i- \overline{Y})^2 + \sum_{i=1}^n (Y_i - \widehat{Y}_i)^2 \\
SSY & = SSR + SSE 
\end{aligned}$$
$$\text{Total Sum of Squares} = \text{Sum of Squares explained by Regression} + \text{Sum of Squares due to Error (residuals)}$$

::: columns
::: {.column width="9%"}
:::

::: {.column width="82%"}
ANOVA table:

| Variation Source | df    | SS    | MS                      | test statistic        | p-value |
|------------|------------|------------|------------|------------|------------|
| Regression       | $1$   | $SSR$ | $MSR = \frac{SSR}{1}$   | $F = \frac{MSR}{MSE}$ |         |
| Error            | $n-2$ | $SSE$ | $MSE = \frac{SSE}{n-2}$ |                       |         |
| Total            | $n-1$ | $SSY$ |                         |                       |         |
:::

::: {.column width="9%"}
:::
:::

 

::: hl3
**F-statistic**: Proportion of variation that is explained by the model to variation not explained by the model
:::

## Analysis of Variance (ANOVA) table in R

```{r}
# Fit regression model:
model1 <- gapm %>% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)

anova(model1)
anova(model1) %>% tidy() %>% gt() %>%
   tab_options(table.font.size = 40) %>%
   fmt_number(decimals = 3)
```

# Learning Objectives

1.  Identify different sources of variation in an Analysis of Variance
    (ANOVA) table

::: lob
2.  Using the F-test, determine if there is enough evidence that
    population slope $\beta_1$ is not 0
:::
    
3.  Using the F-test, determine if there is enough evidence for association between an outcome and a categorical variable

4.  Calculate and interpret the coefficient of determination


## What is the F statistic testing?

$$F = \frac{MSR}{MSE}$$

-   It can be shown that

$$E(MSE)=\sigma^2\ \text{and}\ E(MSR) = \sigma^2 + \beta_1^2\sum_{i=1}^n (X_i- \overline{X})^2$$

-   Recall that $\sigma^2$ is the variance of the population residuals
-   Thus if
    -   $\beta_1 = 0$, then
        $F \approx \frac{\widehat{\sigma}^2}{\widehat{\sigma}^2} = 1$
    -   $\beta_1 \neq 0$, then
        $F \approx \frac{\widehat{\sigma}^2 + \widehat{\beta}_1^2\sum_{i=1}^n (X_i- \overline{X})^2}{\widehat{\sigma}^2} > 1$
-   So the $F$ statistic can also be used to test $\beta_1$

## F-test vs. t-test for the population slope

The square of a $t$-distribution with $df = \nu$ is an $F$-distribution
with $df = 1, \nu$

$$T_{\nu}^2 \sim F_{1,\nu}$$

-   We can use either F-test or t-test to run the following hypothesis
    test:

```{=tex}
\begin{align}
H_0 &: \beta_1 = 0\\
\text{vs. } H_A&: \beta_1 \neq 0
\end{align}
```

-   Note that the F-test does not support one-sided alternative tests,
    but the t-test does!
    
    - F-test cannot handle alternatives like $\beta_1 > 0$ nor $\beta_2 < 0$

## Planting a seed about the F-test

We can think about the hypothesis test for the slope...

::: columns
::: {.column width="17%"}
:::

::: {.column width="33%"}
::: proof1
::: proof-title
Null $H_0$
:::

::: proof-cont
$\beta_1=0$
:::
:::
:::

::: {.column width="33%"}
::: definition
::: def-title
Alternative $H_1$
:::

::: def-cont
$\beta_1\neq0$
:::
:::
:::
:::

in a slightly different way...

::: columns
::: {.column width="17%"}
:::

::: {.column width="33%"}
::: proof1
::: proof-title
Null model ($\beta_1=0$)
:::

::: proof-cont
-   $Y = \beta_0 + \epsilon$
-   Smaller (reduced) model
:::
:::
:::

::: {.column width="33%"}
::: definition
::: def-title
Alternative model ($\beta_1\neq0$)
:::

::: def-cont
-   $Y = \beta_0 + \beta_1 X + \epsilon$
-   Larger (full) model
:::
:::
:::
:::

-   In multiple linear regression, we can start using this framework to
    test multiple coefficient parameters at once

    -   Decide whether or not to reject the smaller reduced model in
        favor of the larger full model

    -   Cannot do this with the t-test when we have multiple coefficients!

## Poll Everywhere Question 2

## F-test: general steps for hypothesis test for population **slope** $\beta_1$

::: columns
::: {.column width="48%"}
::: highlight-container
::: highlight
1.  For today's class, we are assuming that we have met the underlying
    assumptions
:::
:::

::: highlight-container
::: highlight
2.  State the null hypothesis.
:::
:::

Often, we are curious if the coefficient is 0 or not:

```{=tex}
\begin{align}
H_0 &: \beta_1 = 0\\
\text{vs. } H_A&: \beta_1 \neq 0
\end{align}
```
::: highlight-container
::: highlight
3.  Specify the significance level.
:::
:::

Often we use $\alpha = 0.05$

::: highlight-container
::: highlight
4.  Specify the test statistic and its distribution under the null
:::
:::

The test statistic is $F$, and follows an F-distribution with numerator
$df=1$ and denominator $df=n-2$.
:::

::: {.column width="4%"}
:::

::: {.column width="48%"}
::: highlight-container
::: highlight
5.  Compute the value of the test statistic
:::
:::

The calculated **test statistic** for $\widehat\beta_1$ is

$$F = \frac{MSR}{MSE}$$

::: highlight-container
::: highlight
6.  Calculate the p-value
:::
:::

We are generally calculating: $P(F_{1, n-2} > F)$

::: highlight-container
::: highlight
7.  Write conclusion for hypothesis test
:::
:::

-   Reject: $P(F_{1, n-2} > F) < \alpha$

We (reject/fail to reject) the null hypothesis that the slope is 0 at
the $100\alpha\%$ significiance level. There is
(sufficient/insufficient) evidence that there is significant association
between ($Y$) and ($X$) (p-value = $P(F_{1, n-2} > F)$).
:::
:::

## Life expectancy example: hypothesis test for population **slope** $\beta_1$

-   Steps 1-4 are setting up our hypothesis test: not much change from
    the general steps

::: highlight-container
::: highlight
1.  For today's class, we are assuming that we have met the underlying
    assumptions
:::
:::

::: highlight-container
::: highlight
2.  State the null hypothesis.
:::
:::

We are testing if the slope is 0 or not:

```{=tex}
\begin{align}
H_0 &: \beta_1 = 0\\
\text{vs. } H_A&: \beta_1 \neq 0
\end{align}
```
::: highlight-container
::: highlight
3.  Specify the significance level.
:::
:::

Often we use $\alpha = 0.05$

::: highlight-container
::: highlight
4.  Specify the test statistic and its distribution under the null
:::
:::

::: columns
::: {.column width="70%"}
The test statistic is $F$, and follows an F-distribution with numerator
$df=1$ and denominator $df=n-2 = 80-2$.
:::

::: {.column width="30%"}
```{r}
nobs(model1)
```
:::
:::

## Life expectancy example: hypothesis test for population **slope** $\beta_1$ (2/4)

::: highlight-container
::: highlight
5.  Compute the value of the test statistic
:::
:::

```{r}
anova(model1) %>% tidy() %>% gt() %>%
  tab_options(table.font.size = 40)
```

-   **Option 1:** Calculate the test statistic using the values in the
    ANOVA table

$$F = \frac{MSR}{MSE} = \frac{2052.81}{37.73}=54.414$$

-   **Option 2:** Get the test statistic value (F) from the ANOVA table

::: hl
I tend to skip this step because I can do it all with step 6
:::

## Life expectancy example: hypothesis test for population **slope** $\beta_1$ (3/4)

::: highlight-container
::: highlight
6.  Calculate the p-value
:::
:::

-   As per Step 4, test statistic $F$ can be modeled by a
    $F$-distribution with $df1 = 1$ and $df2 = n-2$.

    -   We had 80 countries' data, so $n=80$

-   **Option 1:** Use `pf()` and our calculated test statistic

```{r}
# p-value is ALWAYS the right tail for F-test
pf(54.414, df1 = 1, df2 = 78, lower.tail = FALSE)
```

-   **Option 2:** Use the ANOVA table

```{r}
anova(model1) %>% tidy() %>% gt() %>%
  tab_options(table.font.size = 40)
```

## Life expectancy example: hypothesis test for population **slope** $\beta_1$ (4/4)

::: highlight-container
::: highlight
7.  Write conclusion for the hypothesis test
:::
:::

We reject the null hypothesis that the slope is 0 at the $5\%$
significance level. There is sufficient evidence that there is
significant association between female life expectancy and female
literacy rates (p-value \< 0.0001).

## Did you notice anything about the p-value?

The p-value of the t-test and F-test are the same!!

-   For the t-test:

```{r}
tidy(model1) %>% gt() %>%
  tab_options(table.font.size = 40)
```

-   For the F-test:

```{r}
anova(model1) %>% tidy() %>% gt() %>%
  tab_options(table.font.size = 40)
```

This is true when we use the F-test for a single coefficient!

# Learning Objectives

1.  Identify different sources of variation in an Analysis of Variance
    (ANOVA) table

2.  Using the F-test, determine if there is enough evidence that
    population slope $\beta_1$ is not 0

::: lob
3.  Using the F-test, determine if there is enough evidence for association between an outcome and a categorical variable
:::

4.  Calculate and interpret the coefficient of determination


## Testing association between continuous outcome and categorical variable

- Before we used the F-test (or t-test) to determine association between two continuous variables
- We CANNOT use the t-test to determine association between a continuous outcome and a multi-level categorical variable
- We CAN use the F-test to do this!

 

- We can use the t-test or F-test for a categorical variable with only 2 levels

## Poll Everywhere Question 3
    
## Building a very important toolkit: three types of tests

::: fact
::: fact-title
Overall test (in a couple classes)
:::

::: fact-cont
Does at least one of the covariates/predictors contribute significantly to the prediction of Y?
:::
:::

::: example
::: ex-title
Test for addition of a single variable (covariate subset test)
:::

::: ex-cont
Does the addition of one particular covariate add significantly to the prediction of Y achieved by other covariates already present in the model?
:::
:::

::: proposition
::: prop-title
Test for addition of group of variables (covariate subset test) (in a couple classes)
:::

::: prop-cont
Does the addition of some group of covariates add significantly to the prediction of Y achieved by other covariates already present in the model?
:::
:::

## When running a F-test for linear models...

-   We need to define a larger, full model (more parameters)
-   We need to define a smaller, reduced model (fewer parameters)
-   Use the F-statistic to decide whether or not we reject the smaller model
    -   The F-statistic compares the SSE of each model to determine if the full model explains a significant amount of additional variance

::: columns
::: {.column width="30%"}
 

$$
F = \dfrac{\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\frac{SSE(F)}{df_F}}
$$
:::

::: {.column width="70%"}
-   $SSE(R) \geq SSE(F)$
-   Numerator measures difference in **unexplained** variation between the models
    -   Big difference = added parameters greatly reduce the unexplained variation (increase explained variation)
    -   Smaller difference = added parameters don't reduce the unexplained variation
-   Take ratio of difference to the unexplained variation in the full model
:::
:::

## We can extend our look at the F-test

We can create a hypothesis test for more than one coefficient at a time...

::: columns
::: {.column width="17%"}
:::

::: {.column width="33%"}
::: proof1
::: proof-title
Null $H_0$
:::

::: proof-cont
$\beta_1=\beta_2=0$
:::
:::
:::

::: {.column width="33%"}
::: definition
::: def-title
Alternative $H_1$
:::

::: def-cont
$\beta_1\neq0$ and/or $\beta_2\neq0$
:::
:::
:::
:::

in a slightly different way...

::: columns
::: {.column width="17%"}
:::

::: {.column width="33%"}
::: proof1
::: proof-title
Null model
:::

::: proof-cont
-   $Y = \beta_0 + \epsilon$
-   Smaller (reduced) model
:::
:::
:::

::: {.column width="33%"}
::: definition
::: def-title
Alternative\* model
:::

::: def-cont
-   $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$
-   Larger (full) model
:::
:::
:::
:::

\*This is **not quite** the alternative, but if we reject the null, then this is the model we move forward with

## Let's say we want to test the association between life expectancy and world region

```{r}
#| echo: false
means_LE = gapm %>%
  group_by(four_regions) %>%
  summarise(mean = mean(LifeExpectancyYrs))
```

::: columns
::: {.column width="50%"}

$$\begin{aligned}
\widehat{\textrm{LE}} = & `r round(means_LE$mean[1], 2)` + `r round(means_LE$mean[2]-means_LE$mean[1], 2)` \cdot I(\text{Americas}) + \\ &`r round(means_LE$mean[3]-means_LE$mean[1], 2)` \cdot I(\text{Asia}) + `r round(means_LE$mean[4]-means_LE$mean[1], 2)` \cdot I(\text{Europe}) \\
\widehat{\textrm{LE}} = & \widehat\beta_0 + \widehat\beta_1 \cdot I(\text{Americas}) + \\ & \widehat\beta_2 \cdot I(\text{Asia}) + \widehat\beta_3 \cdot I(\text{Europe})
\end{aligned}$$

- We need to figure out if the model with world region explains significantly more variation than the model without world region!
:::

::: {.column width="50%"}
```{r fig.height=8, fig.width=8, warning=F, fig.align='center'}
#| echo: false
ggplot(gapm, aes(x = four_regions, y = LifeExpectancyYrs)) +
  geom_jitter(size = 1, alpha = .6, width = 0.2) +
  stat_summary(fun = mean, geom = "point", size = 8, shape = 18) +
  labs(x = "World region", 
       y = "Country life expectancy (years)",
       title = "Life expectancy vs. world region",
       caption = "Diamonds = region averages") +
  theme(axis.title = element_text(size = 22), 
        axis.text = element_text(size = 22), 
        title = element_text(size = 22))
```
:::
:::


## F-test: general steps for hypothesis test for $j$-level categorical variable

::: columns
::: {.column width="48%"}
::: highlight-container
::: highlight
1.  For today's class, we are assuming that we have met the underlying
    assumptions
:::
:::

::: highlight-container
::: highlight
2.  State the null hypothesis.
:::
:::

Often, we are curious if the coefficient is 0 or not:

```{=tex}
\begin{align}
H_0 :& \beta_1 = ... = \beta_j = 0\\
\text{vs. } H_A:& \beta_1 \neq 0 \text{ and/or } \\
&\beta_2 \neq 0 ... \text{ and/or } \beta_j \neq 0
\end{align}
```
::: highlight-container
::: highlight
3.  Specify the significance level.
:::
:::

Often we use $\alpha = 0.05$

::: highlight-container
::: highlight
4.  Specify the test statistic and its distribution under the null
:::
:::

The test statistic is $F$, and follows an F-distribution with numerator
$df=1$ and denominator $df=n-(k+1)$.
:::

::: {.column width="4%"}
:::

::: {.column width="48%"}
::: highlight-container
::: highlight
5.  Compute the value of the test statistic
:::
:::

The calculated **test statistic** for $\widehat\beta_1$ is

$$F = \frac{MSR}{MSE}$$

::: highlight-container
::: highlight
6.  Calculate the p-value
:::
:::

We are generally calculating: $P(F_{1, n-(j+1)} > F)$

::: highlight-container
::: highlight
7.  Write conclusion for hypothesis test
:::
:::

-   Reject: $P(F_{1, n-(j+1)} > F) < \alpha$

We (reject/fail to reject) the null hypothesis that the slope is 0 at
the $100\alpha\%$ significiance level. There is
(sufficient/insufficient) evidence that there is significant association
between ($Y$) and ($X$) (p-value = $P(F_{1, n-(j+1)} > F)$).
:::
:::

## Life expectancy example: hypothesis test for world region

-   Steps 1-4 are setting up our hypothesis test: not much change from
    the general steps

::: highlight-container
::: highlight
1.  For today's class, we are assuming that we have met the underlying
    assumptions
:::
:::

::: highlight-container
::: highlight
2.  State the null hypothesis.
:::
:::

We are testing if the slope is 0 or not:

```{=tex}
\begin{align}
H_0 :& \beta_1 = \beta_2 = \beta_3 0\\
\text{vs. } H_A:& \beta_1 \neq 0 \text{ and/or } \beta_2 \neq 0 \text{ and/or } \beta_3 \neq 0
\end{align}
```
::: highlight-container
::: highlight
3.  Specify the significance level.
:::
:::

Often we use $\alpha = 0.05$

::: highlight-container
::: highlight
4.  Specify the test statistic and its distribution under the null
:::
:::

::: columns
::: {.column width="70%"}
The test statistic is $F$, and follows an F-distribution with numerator
$df=j$ and denominator $df=n-(j+1) = 80-(3+1)$.
:::

::: {.column width="30%"}
```{r}
nobs(model1)
```
:::
:::

## Life expectancy example: hypothesis test for world region (2/4) 
::: highlight-container
::: highlight
5.  Compute the value of the test statistic
:::
:::

```{r}
model2 <- gapm %>% lm(formula = LifeExpectancyYrs ~ four_regions)
anova(model2) %>% tidy() %>% gt() %>% tab_options(table.font.size = 40)
```

```{r}
#| echo: false
anova_wr = anova(model2) %>% tidy()
```

-   **Option 1:** Calculate the test statistic using the values in the
    ANOVA table

$$F = \frac{MSR}{MSE} = \frac{`r anova_wr$meansq[1]`}{`r anova_wr$meansq[2]`} = `r anova_wr$statistic[1]`$$

-   **Option 2:** Get the test statistic value (F) from the ANOVA table

::: hl
I tend to skip this step because I can do it all with step 6
:::

## Life expectancy example: hypothesis test for population **slope** $\beta_1$ (3/4)

::: highlight-container
::: highlight
6.  Calculate the p-value
:::
:::

-   As per Step 4, test statistic $F$ can be modeled by a
    $F$-distribution with $df1 = 3$ and $df2 = n-4$.

    -   We had 80 countries' data, so $n=80$

-   **Option 1:** Use `pf()` and our calculated test statistic

```{r}
# p-value is ALWAYS the right tail for F-test
pf(33.5331, df1 = 3, df2 = 76, lower.tail = FALSE)
```

-   **Option 2:** Use the ANOVA table

```{r}
anova(model2) %>% tidy() %>% gt() %>%
  tab_options(table.font.size = 40)
```

## Life expectancy example: hypothesis test for population **slope** $\beta_1$ (4/4)

::: highlight-container
::: highlight
7.  Write conclusion for the hypothesis test
:::
:::

We reject the null hypothesis that all three coefficients are equal to 0 at the $5\%$
significance level. There is sufficient evidence that there is
association between female life expectancy and the country's world region (p-value \< 0.0001).


# Learning Objectives

1.  Identify different sources of variation in an Analysis of Variance
    (ANOVA) table

2.  Using the F-test, determine if there is enough evidence that
    population slope $\beta_1$ is not 0
    
3.  Using the F-test, determine if there is enough evidence for association between an outcome and a categorical variable

::: lob
4.  Calculate and interpret the coefficient of determination
:::

## Correlation coefficient from 511

::: columns
::: {.column width="45%"}
Correlation coefficient $r$ can tell us about the strength of a
relationship **between two continuous variables**

-   If $r = -1$, then there is a perfect negative linear relationship
    between $X$ and $Y$

-   If $r = 1$, then there is a perfect positive linear relationship
    between $X$ and $Y$

-   If $r = 0$, then there is no linear relationship between $X$ and $Y$

Note: All other values of $r$ tell us that the relationship between $X$
and $Y$ is not perfect. The closer $r$ is to 0, the weaker the linear
relationship.
:::

::: {.column width="55%"}
![](../img_slides/corr_coef.png){fig-align="center" width="878"}
:::
:::

## Correlation coefficient ($r$ or $R$)

::: columns
::: column
The (Pearson) correlation coefficient $r$ of variables $X$ and $Y$ can
be computed using the formula:

$$\begin{aligned}
r  & = \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\Big(\sum_{i=1}^n (X_i - \overline{X})^2 \sum_{i=1}^n (Y_i - \overline{Y})^2\Big)^{1/2}} \\
& = \frac{SSXY}{\sqrt{SSX \cdot SSY}}
\end{aligned}$$

we have the relationship

$$\widehat{\beta}_1 = r\frac{SSY}{SSX},\ \ \text{or},\ \  r = \widehat{\beta}_1\frac{SSX}{SSY}$$
:::
::: column
```{r}
#| fig-height: 8
#| fig-width: 11
#| echo: false

mx <- mean(gapm$FemaleLiteracyRate, na.rm=T)
my <- mean(gapm$LifeExpectancyYrs, na.rm=T)


ggplot(gapm, aes(x = FemaleLiteracyRate,
                 y = LifeExpectancyYrs)) +
  geom_point(size = 4) +
  geom_vline(xintercept = mx, color = "#FF8021", size = 3) +
   geom_hline(yintercept = my, color = "#A7EA52", size = 3) +
  labs(x = "Female literacy rate (%)", 
       y = "Life expectancy (years)",
       title = "Relationship between life expectancy and \n the female literacy rate in 2011") +
    theme(axis.title = element_text(size = 30), 
        axis.text = element_text(size = 25), 
        title = element_text(size = 30))

```
:::
:::

## Coefficient of determination: $R^2$

It can be shown that the square of the correlation coefficient $r$ is
equal to

$$R^2 = \frac{SSR}{SSY} = \frac{SSY - SSE}{SSY}$$

-   $R^2$ is called the **coefficient of determination**.
-   **Interpretation**: The proportion of variation in the $Y$ values explained by the
        regression model
-   $R^2$ measures the strength of the **linear** relationship between $X$
    and $Y$:
    -   $R^2 = \pm 1$: Perfect relationship
        -   Happens when $SSE = 0$, i.e. no error, all points on the
            line
    -   $R^2 = 0$: No relationship
        -   Happens when $SSY = SSE$, i.e. using the line doesn't not
            improve model fit over using $\overline{Y}$ to model the $Y$
            values.

## Life expectancy example: correlation coeffiicent $r$ and coefficient of determination $R^2$

```{r}
(r = cor(x = gapm$LifeExpectancyYrs, y = gapm$FemaleLiteracyRate, 
         use =  "complete.obs"))
```
::: columns
::: {.column width="78%"}
```{r}
summary(model1) # for R^2 value
```
:::

::: {.column width="22%"}

```{r}
r^2
```
   

::: fact
::: fact-title
Interpretation
:::

::: fact-cont
41.1% of the variation in countries' life expectancy is
explained by the linear model with female literacy rate as the
independent variable.
:::
:::
:::
:::

## What does $R^2$ not measure?

::: columns
::: {.column width="37%"}
-   $R^2$ is not a measure of the magnitude of the slope of the
    regression line

    -   Example: can have $R^2 = 1$ for many different slopes!!

-   $R^2$ is not a measure of the appropriateness of the straight-line
    model

    -   Example: figure
:::

::: {.column width="63%"}
![](../img_slides/anscombe.png){fig-align="center"}
:::
:::