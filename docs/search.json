[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSTA 512/612: Linear Models",
    "section": "",
    "text": "BSTA 512/612: Linear Models\n\nWinter 2025\n \nWelcome to BSTA 512/612! In this course, we will focus on linear models, and build our understanding of regression analysis. We will build some theoretical understanding in order to interpret and apply regression models appropriately. We will learn how to build a regression model, interpret the model, and diagnose potential issues with our model.  \n\n\n\n\n\n\n\n \n\n\n\n\n\n OneDrive Folder\n\n\n Echo360 link\n\n\n\n\n\n\n\nInstructor\n Dr. Nicky Wakim\n Vanport 622A\n wakim@ohsu.edu\n Office Hours W 3-4pm\n\n\n\nTA Office Hours\n\n\nKatie\nLiv\nMiyuki\n\n M 4:30-5:30\n F 10-11\n Th 10-11\n\n\n\n\n\nCourse details\n Mondays, Wednesdays\n Jan 6 - March 17\n 1:00 PM - 2:50 PM\n In-person, RPV 1215\n\n\n\nContacting me\nE-mail is the best way to get in contact with me. I will try to respond to all course-related e-mails within 24 hours Monday-Friday.\n\n\n\n\n\n\n\n\n\n View the source on GitHub"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Homework",
    "section": "",
    "text": "Homework\nAssignment\nAssignment due (@11pm)\nAnswers\nSolutions (.qmd)\nSolutions (.html)*\n\n\n\n\n0\n\n1/9\n\n\n\n\n\n1\n\n1/23\n\n\n\n\n\n2\n\n1/31\n\n\n\n\n\n3\n\n2/14\n\n\n\n\n\n4\n\n2/28\n\n\n\n\n\n5\n\n3/14\n*Please note that you need to download the .html file to see the LaTeX math properly."
  },
  {
    "objectID": "homework.html#file-naming",
    "href": "homework.html#file-naming",
    "title": "Homework",
    "section": "File Naming",
    "text": "File Naming\n\nFor HW Assignments, please use the following file naming: “Lastname_Firstinitial_HW0.qmd” and “Lastname_Firstinitial_HW0.html”"
  },
  {
    "objectID": "homework.html#rubrics",
    "href": "homework.html#rubrics",
    "title": "Homework",
    "section": "Rubrics",
    "text": "Rubrics\n\n\n\n\n\n\n\n\n\n1 point\n0 points\n\n\n\n\nCompletion\n75% of the question parts are thoroughly worked out and have an answer. “Question parts” means the sub-questions labeled “Part _”\nLess than 75% of question parts are thoroughly worked out. Attempts do not count as thoroughly worked out."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nDate\nLes-son\nTopic\nTB 1\nTB 2\nKey Info\nSlides HTML\nSlides PDF\nSlides Notes\nExit tix\nRecord-ing\nMuddy Points\n\n\n\n\n1\n1/6\n0\nWelcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nReview\n\n\n\n\n\n\n\n\n\n\n\n\n1/8\n2\nData and File Management\n\n\n\n\n\n\n\n\n\n\n\n\n1/9\n\nHW 0 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\n1/13\n3\nSimple Linear Regression\n3.1-3.2, 3.4\n4.1-4.2\n\n\n\n\n\n\n\n\n\n\n1/15\n4\nSLR: Inference and Prediction\n3.3\n4.3, 4.5-4.7\n\n\n\n\n\n\n\n\n\n\n1/16\n\nLab 1 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n1/20\n\nNo Class, MLKJ Day\n\n\n\n\n\n\n\n\n\n\n\n\n1/22\n5\nSLR: Categorical Covariates\n3.8-3.9\n4.4\n\n\n\n\n\n\n\n\n\n\n1/23\n\nHW 1 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\n1/27\n6\nSLR: More inference\n6.10\nNA\n\n\n\n\n\n\n\n\n\n\n1/29\n7\nSLR: Checking model assumptions\n5.1-5.2\n5.13-5.17\n\n\n\n\n\n\n\n\n\n\n1/31\n\nHW 2 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5\n2/3\n8\nSLR: Diagnostics\n\n5.21-5.22, 5.25.2\n\n\n\n\n\n\n\n\n\n\n2/5\n9\nMultiple Linear Regression\n3.5-3.6, 4.1\n5.1-5.2, 5.5-5.6\n\n\n\n\n\n\n\n\n\n\n2/7\n\nLab 2 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\n2/10\n10\nMLR: Using the F-test\n6.9-6.10\n\n\n\n\n\n\n\n\n\n\n\n2/12\n\nLab 2 Review\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\nInteractions, Part 1\n\n5.10\n\n\n\n\n\n\n\n\n\n\n2/14\n\nHW 3 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\n2/17\n\nNo Class, President’s Day\n\n\n\n\n\n\n\n\n\n\n\n\n2/19\n12\nInteractions, Part 2\n\n5.10\n\n\n\n\n\n\n\n\n\n\n2/21\n\nLab 3 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\n2/24\n13\nModel Selection\n\n\n\n\n\n\n\n\n\n\n\n\n2/26\n14\nPurposeful Selection\n\n\n\n\n\n\n\n\n\n\n\n\n2/28\n\nHW 4 due 11pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\n3/3\n15\nMLR: Diagnostics\n\n\n\n\n\n\n\n\n\n\n\n\n3/5\n\nCatch-up day\n\n\n\n\n\n\n\n\n\n\n\n\n3/7\n\nLab 4 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10\n3/10\n\nCatch-up day, virtual\n\n\n\n\n\n\n\n\n\n\n\n\n3/12\n\nCatch-up day, virtual\n\n\n\n\n\n\n\n\n\n\n\n\n3/14\n\nHW 5 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\n3/17\n\nProject Day!!\n\n\n\n\n\n\n\n\n\n\n\n\n3/17\n\nProject due 11pm"
  },
  {
    "objectID": "syllabus.html#description",
    "href": "syllabus.html#description",
    "title": "BSTA 512/612 Syllabus",
    "section": "Description",
    "text": "Description\nWelcome to BSTA 512/612! In this course, we will focus on linear models, and build our understanding of regression analysis. We will build some theoretical understanding in order to interpret and apply regression models appropriately. We will learn how to build a regression model, interpret the model, and diagnose potential issues with our model.\n\nCourse Learning Objectives\nAt the end of this course, students should be able to…\n\nAnalyze real-world data to answer questions about multivariable relationships for a continuous outcome\nBuild, fit, and evaluate linear regression models\nAssess whether a proposed model is appropriate and describe its limitations\nUse R and Quarto to write reproducible reports\nCommunicate results from statistical analyses to a general audience\n\nThese learning objectives were adapted from Maria Tackett’s Regression Analysis course."
  },
  {
    "objectID": "syllabus.html#instructors",
    "href": "syllabus.html#instructors",
    "title": "BSTA 512/612 Syllabus",
    "section": "Instructors",
    "text": "Instructors\nHere is the instructor page. This also has office hours!"
  },
  {
    "objectID": "syllabus.html#meeting-times",
    "href": "syllabus.html#meeting-times",
    "title": "BSTA 512/612 Syllabus",
    "section": "Meeting Times",
    "text": "Meeting Times\nMondays          1:00 PM – 2:50 PM PST in RPV 1205/1215\nWednesdays    1:00 PM – 2:50 PM PST in RPV 1205/1215\n\nKnown Exceptions\nWe will be in RPV 1217 on the following days:\n\nWednesday, Feb 5\nMonday, Feb 10\nWednesday, Feb 25\nWednesday, March 5\nMonday, March 10\nWednesday, March 12"
  },
  {
    "objectID": "syllabus.html#materials",
    "href": "syllabus.html#materials",
    "title": "BSTA 512/612 Syllabus",
    "section": "Materials",
    "text": "Materials\n\nTextbooks\nIn lieu of a formally published textbook, we will be referencing the following two online textbooks. Similar to our class, the books integrate R into their lessons.\n\nA Progressive Introduction to Linear Models by Joshua French\nIntroduction to Regression Methods for Public Health Using R by Ramzi W. Nahhas\n\n\nSupplemental Readings (Optional)\n\nAn Introduction to R (free pdf available)\n\n\n\n\nOnline Resources\n\nSakai\nWhile most course materials will be delivered online through this website, assignments will be turned in through Sakai, OHSU’s course management system. I will include a link on this website to the Sakai assignment page. \n\n\nWebex\nWebex software will be used for virtual office hours. To give everyone the best possible experience with Webex, I recommend the following best practices:\n\nPlease stay muted until you want to participate\nDuring office hours, please send a message in chat with your question or with a statement like “I have a question.” This makes sure I or the TA can address everyone’s questions in order. \nI encourage you to attend office hours with your video on. This helps me recognize you, and keep mental notes on what techniques/concepts I emphasize to facilitate your specific understanding. \n\n\n\nPoll Everywhere\nWe will use the Poll Everywhere tool as an interactive feature of the course. Poll Everywhere is a web-based application that allows students to participate by responding via text messages or by visiting a web page on an internet-enabled device (smartphone, tablet, laptop). Instructions will be displayed on-screen. The poll that is embedded within the presentation will update in real time. While there is no cost to use this software, standard text messaging rates will apply if you use your phone. Please make sure that you have a Poll Everywhere account before our first class. You are not required to use your OHSU/PSU email to make an account. \nDuring lectures I will pose questions to the class. These questions are designed to provide real-time feedback to both students and the instructor on how well students are grasping the material. This is meant to be an interactive, learning activity with NO contribution to your grade. Your identity will never be connected to your answers, so I encourage you to answer honestly.\n\n\nPennState STAT 501 Website\nPennState has a class offered to online MS students that has some overlap with our class. They have all their course notes posted on this page. This is a great source if you would like to see class notes with different phrasing.\nNot all of our topics are covered in their notes, but the most important ones are. If you are having trouble finding our course’s concepts on their page, please make ask me at Office Hours, after class, or in a private meeting. I do not explicitly state corresponding sections under our schedule because I believe it is important for you to develop skills involving resources and learning key words that can help you find answers. \n\n\nR: Statistical Computing Software\nStudents will use statistical software to complete homework assignments. Students are required to use R/RStudio for this course. R can be freely downloaded. Helpful documentation on installing R is available. I encourage you to install R prior to attending our first lecture. Please email me if you need help installing R or RStudio.\nYou will need to download the following three things:\n\nR https://www.r-project.org/\nRstudio https://posit.co/download/rstudio-desktop/\nQuarto https://quarto.org/docs/get-started/\n\n\nAdditional R Resources\nYour learning and practicing of R will hopefully not be limited to this course. One of the best aspects of programming in R is that many resources are freely available online. Here are just a few additional resources you may explore beyond this class to continue your training in R.\n\n\nUseful online R resources\n\nR for the rest of us\nStatistical tools for high-throughput data analysis. ggplot2 essentials\nR-bloggers\nStack Overflow for troubleshooting\nR Graphical Manual\nQuick-R. Accessing the power of R\nR for SAS, STATA, and SPSS Users\nggplot2\nLearn R 4 free\nJoin a local R user groups\nLearning Machines\n\n\n\n\nOnline R courses to complement or refresh material from class\n\nR for the rest of us\nCoursera: R programming\nedX: R basics\nData Carpentry: For Biologists\nData Carpentry: For Ecologists\nPsychiatric R\nR coder"
  },
  {
    "objectID": "syllabus.html#assessment",
    "href": "syllabus.html#assessment",
    "title": "BSTA 512/612 Syllabus",
    "section": "Assessment",
    "text": "Assessment\nThe course is structured around the following four components:\n\n\n\n\n\n\n\n\n\nComponent\nModality\nFrequency\nDescription\n\n\nLecture\nIn person\nTwice, Weekly\nCourse content is provided through in-person lectures. Lectures will consist of didactic lessons, interactive examples, and PollEverywhere questions. Sessions will be recorded through Explain Everything and posted to Sakai. Attending or viewing the lecture within 7 days of the original lecture date is mandatory. Class attendance will be taken through an Exit Ticket. If viewing the lecture asynchronously, you must take the Exit Ticket to verify your attendance.\n\n\nHomework\nOnline\nWeekly\nThe course includes 7 homework assignments. They are an opportunity for you to engage with important concepts, practice coding, and apply calculating skills. Homework assignments should be submitted online, and will be graded by me. Students are encouraged to work in groups for homework assignments, but each person should do their own summary and hand in their work. Homework assignments will be due on Thursday at 11 PM.\n\n\nProject (Labs and Poster)\nOnline\n4 labs, 1 final poster\nThe project will be a combination of submitted labs that will span the quarter and one final poster submitted at the end of the quarter. This is meant to translate the tools learned in the course to the work one may do in the workforce. This will help instill the procedure for shaping research goals, model selection, analyzing data, and interpreting meaningful results. Labs will guide you through the needed analysis and background for the project. The final poster will summarize your work over the labs by giving context and results to your research question. Students will work independently on each lab.\n\n\n\n\nTypes of assessments\nThis class will use a combination of formative and summative assessments to build and test our knowledge. Below I define each of these types of assessments:\n\nFormative assessment: Activity or work meant to help students learn and practice. Feedback on these assessments are meant to help the instructor and student identify gaps in knowledge and highlight accomplishments.\nSummative assessment: Work meant to test how well students have achieved learning objectives. Grading of these assessments are meant to gauge how well a student grasps the learning objectives and will be able to use their knowledge outside of the classroom."
  },
  {
    "objectID": "syllabus.html#assessment-breakdown",
    "href": "syllabus.html#assessment-breakdown",
    "title": "BSTA 512/612 Syllabus",
    "section": "Assessment Breakdown",
    "text": "Assessment Breakdown\n\nGrading & Requirements\nLetter grades will be assigned roughly according to the following scheme: A (&gt;=93%), A- (90-92%), B+ (88-89%), B(83-87%), B- (82-80%), C+(78-79%), C(73-77%), C- (70-72%), D (60 – 69%), F(&lt;60%).\nGrades will be based on homework assignments, midterm exam, class “attendance”, and final exam, as follows:\n\n\n\n\n\n\n\n\n\n\nCourse activity\nType of Assessment\nDue Date\nPercentage of final grade (BSTA 512)\nPercentage of final grade (BSTA 612)\n\n\nHomework\nFormative\nEvery 1-2 weeks\n48%\n43%\n\n\nProject Labs\nFormative/summative\nEvery 1-2 weeks\n35%\n35%\n\n\nProject Poster and Presentation\nSummative\n3/17\n10%\n10%\n\n\nExit tickets (Attendance)\nN/A\nTwice Weekly\n5%\n5%\n\n\nMid-Quarter Feedback\nN/A\nTBD\n2%\n2%\n\n\n612 Readings\nFormative\nApprox. every other week\n0%\n5%\n\n\n\n\n\nHomework grading\nNo student has the same amount of time available to dedicate to homework. This class may not be a priority to you, you may be taking several other courses, or you may need to dedicate time to other activities. Homeworks are formative assessments, meaning its purpose is to help you learn and practice. To reduce the pressure on you to have perfect or complete homework, I have a very simple grading policy: Your homework will be given a check mark if you turn in 75% of the questions parts completed (whether or not the 75% is correct). I highly encourage you to stay up-to-date with the homeworks and put in as much effort as you can. This will be the most helpful work in this class!\nIf you turn in the homework on time, the TAs will give you feedback (on one or more complete problems). There is no penalty for turning in the homework late, but you will not get feedback on your work. Please make sure to check the solutions or go to office hours to assess your work.\n\n\nViewing Grades in Sakai\nPoints you receive for graded activities will be posted to the Sakai Gradebook. Click on the Gradebook link on the left navigation to view your points."
  },
  {
    "objectID": "syllabus.html#course-instructor-evaluations",
    "href": "syllabus.html#course-instructor-evaluations",
    "title": "BSTA 512/612 Syllabus",
    "section": "Course & Instructor Evaluations",
    "text": "Course & Instructor Evaluations\n\nOngoing Course Feedback\nThroughout the duration of the course, you are also welcome to informally and anonymously submit your feedback through this Microsoft Form or Class Exit Tickets. This form will be available on Sakai. Students can submit feedback at any time and this form will be reviewed regularly by me. Your responses will be anonymous unless you elect to leave your email address. If I have done anything to make you feel uncomfortable, please give me feedback so I can change my behavior. Ultimately, this class is for you, and my individual social identity/behavior should not inhibit your learning. Thank you for your help making BSTA 512/612 a more successful class! Examples of ongoing feedback are:\n\nNicky talks a little fast during lecture time. May you speak slower?\nDuring Office Hours, Dr. Wakim made a face when I asked a question. This face made me feel self-conscious about my question.\nDr. W asked me a question about my experience that made me feel like a monolith. Please do not assume I can speak on behalf of my social identity groups.\nThe in-class examples do not make me more interested in the material.\n\n\n\nMid-quarter Feedback\nDuring the middle of the quarter, I will ask you to submit guided, anonymous feedback. Completion of feedback will be count towards your grade. To insure anonymity, I will ask you to sign a separate, written statement that you completed the feedback.\n\n\nFinal Course Feedback\nAt the conclusion of the course, you will be asked to complete a formal online review of the course and the instructor. Your feedback on this University evaluation is critical to improving future student learning in this course as well as providing metrics relevant to the instructor’s career advancement (or lack of). Since our class is on the smaller side, everyone’s participation is needed for feedback to be released."
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "BSTA 512/612 Syllabus",
    "section": "Schedule",
    "text": "Schedule\nPlease refer to the Schedule page. I will make changes to this schedule if we need more or less time on a concept. You do not need to read the corresponding chapters in the textbook for each class."
  },
  {
    "objectID": "syllabus.html#how-to-succeed-in-this-course",
    "href": "syllabus.html#how-to-succeed-in-this-course",
    "title": "BSTA 512/612 Syllabus",
    "section": "How to succeed in this course",
    "text": "How to succeed in this course\nEvery professor has different expectations when assigning certain work or providing certain resources. I want to walk through each class resource and assignment so that you know what you can do to succeed in this class. For resources, I want you to optimize the opportunities to learn. For assignments, I want you to know the strategies that students can use to learn the most and prepare for future exams.\n\nResources\n\n\n\n\n\n\n\n\nResource\nWhat is it?\nHow do I use it?\n\n\nOffice Hours\nBlocks of time a professor or TA dedicates for questions. The teaching staff will be located in a specific room. Several students may enter the space at a time and will ask specific or broad questions. If many students attend office hours, a queue will be created so that students can be served equally.\nThe main use of office hours is to ask questions about an assignment or lecture notes. You are welcome to sit and do homework in office hours. OH are also an informal way of meeting fellow students to collaborate with.\n\n\nLectures and lecture recordings\nTime shared between the professor and students where the professor conveys important class material. Material discussed in lectures include concepts, calculations, code, and examples. Lectures are a mix of presentation of information, working through examples together, interactive activities, and in-class polls.\nStudents should attend lectures in person if possible. You should attempt to understand new material presented by following the presentation slides, taking notes on additional details that may conveyed verbally, and working through examples with the professor. Students are encouraged to ask questions when you don’t understand the material at any point in the lecture.\n\n\nTextbooks\nWritten and published material that explains concepts, steps through calculations, provides examples, and provides practice problems. The listed textbooks is the basis for this course. While I am to cover all topics in class, the textbook provides alternative explanations and additional examples.\nWhile coming to class having read the accompanying textbook chapters helps understanding during class, I do not expect students to have read it. I see the textbook as a good resource if you are struggling with a specific topic after class, in need of an example while working on homework, or want additional practice when studying for the exam.\n\n\nWebsite\nThe course website is designed by me so that you have access to all the course materials in a more organized and flexible way. All resources delivered from me to you will be available on the website. Any assignments turned in will be through Sakai.\nYou can navigate through different course resources and information using the left-side tabs or top navigation bar. Course materials, like lecture notes, homework, data examples, and recordings, can be found under each week’s page under the schedule tab. You can also find the individual resources under the “Course Materials” tab on the left. Links to turn in assignments through Sakai will be given on the website. Please explore the tabs and get a sense of the organization.\n\n\nSakai\nSakai is a learning management system for higher ed. This is the university sanctioned LMS where we will submit assignments.\nYou will turn in assignments through Sakai under the “Submissions” tab. Generally, there will be a link to each assignment on the course website. You can also view your grades under “Gradebook” and links to Webex under “Webex.”\n\n\n\n\n\nAssignments\n\n\n\n\n\n\n\n\n\nAssignment\nType of assessment\nBefore you submit/take it\nAfter it is graded\n\n\nHomework\nFormative\n\nWork out each problem on your own as much as you can\nTalk through problems with a peer\nGo to Office Hours for help\nWrite down work that shows your thought process\nSearch your issue on Stack Exchange/Stack Overflow\n\n\nReview the solutions\nReview your mistakes\nFor solutions that involve writing sentences, check with me or a TA if your answer fits the solution\nGo to Office Hours to ask about your solutions\n\n\n\nProject Labs and Poster\nFormative and Summtive\n\nStart the lab as early as possible\nWork on R coding and check with classmates on work\nCome to Office Hours for help with specific R work\nFor the report, compile your work from the labs, and decide what is important in the analysis.\n\n\nThis will be graded at the end of the semester, so you will not have a chance to interact with my feedback as much\nIf you have questions about your grade, you may email me\nKeep the project paper for future reference\nYou can add this project to your resume!\n\n\n\nClass Exit Tickets\nN/A\n\nBring appropriate electronic device to participate in polls\nComplete the survey during the last 5 minutes of class or after class within 7 days\n\n\nReview muddiest and clearest points from the week\n\n\n\n\nIf you would like any other course resources explained in this format, please request it through the Ongoing Course Feedback."
  },
  {
    "objectID": "syllabus.html#course-policies-and-resources",
    "href": "syllabus.html#course-policies-and-resources",
    "title": "BSTA 512/612 Syllabus",
    "section": "Course Policies and Resources",
    "text": "Course Policies and Resources\n\nLate Work Policy\nI encourage you to make your best effort to submit all assignments on time, but I understand circumstances arise that are beyond our control. Please see this Swansea University’s page on extenuating circumstances for some examples. Not all circumstances are covered here, so please reach out if you have questions. \n\nThe class will end on March 21, 2025. All coursework is expected to be completed by then. If you have extenuating circumstances, and need additional time to complete class assignments, please contact me. Together, we will come up with a plan for completion and to sort out registrar logistics.\nIf you have extenuating circumstances that may jeopardize your ability to do work for several weeks, please contact me. We will come up with a plan to keep you on track in the course and prevent any delay in your education.\nFor homework, there is a due date posted, but you may turn in the assignment any time before the class ends. I will give you the check regardless of when you submit the assignment. However, if you would like feedback on the homework, you must turn it in on time OR email me and the TAs asking for feedback for your late homework.\nFor labs, you will have ONE no-questions-asked, 3-day extension. Please use this wisely! You just need to send me a quick email saying “I am using my no-questions-asked extension for Lab __.”\n\nBeyond the one no-questions-asked, 3-day extension, I ask you to email me directly. You can explain your circumstances and may ask me for an extension, but I won’t necessarily grant one.\n\nIf you have a emergency involving your self, family, pet, friend, classmate, or anything/one deemed important to you, please do not worry about immediately contacting me. We can work something out after your emergency. If I contact you during an emergency, it is only because I am worried, and you do NOT need to respond until you are able. \n\n\n\nRegrade Policy\nIf you think a question was incorrectly graded, first compare your answer to the answer key. If you believe a re-grade would be appropriate, write an email to me containing the question and a short explanation as to why the question(s) was/were incorrectly graded. Deadline: One week after assignments were returned to class (late requests will not be considered).\n\n\nAttendance Policy\nYou are expected to attend class, participate in-class polls, and complete the exit ticket. For students who miss class or need a review, I will make video and audio recordings of lectures available. There are no guarantees against technical or other challenges for the recording availability or quality.\nYou have three options to attend class:\n\nAttend in-person synchronously and fill out exit ticket\nWatch online synchronously and fill out exit ticket\nWatch online asynchronously within 7 days and fill out exit ticket\n\nThis is meant to keep you on track within the course and prevent a pile up of material. Make sure to complete the exit ticket at the end of class or viewing to demonstrate attendance.\n\n\nPlagiarism and Attribution\nPlease note that this section has been motivated by Dr. Steven Bedrick’s Course Policies and Grading site for BMI 525. (Note that this is a good example of informal attribution of someone else’s work.)\nIn this class, it is easy to use ChatGPT or other AI tools to solve your homework for you. Many problems follow a basic structure that is especially easy for ChatGPT to solve. In this class, you may use ChatGPT to help with your homework. You may even ask for direct answers. However, there are a few things I do not want you to do:\n\nDo not copy ChatGPT’s answer directly into your homework. Your homework is graded for full credit if you turn it in, in any state, so turning in ChatGPT’s answers is unacceptable. I rather see half-written answers that show what you’re thinking than see a correct answer from ChatGPT.\nDo not stop once ChatGPT answered a question. If it gives an explanation, interact with it! Make sure you understand the thought process of ChatGPT. Try writing out the process to help cement it in your head. Check the answer with what we learn in class.\nDo not use ChatGPT on our quizzes! Hence, you need to really understand how to solve these problems even if you use ChatGPT on the homework.\n\nAt the end of the day, ChatGPT is a resource that will be available to you in a job and outside of school. Thus, we should use it as a tool in school as well! Let me know if ChatGPT helped you understand something! I would love to incorporate it into future classes!\n\n\n\n\n\n\nImportant\n\n\n\nYou can think of this class as assembling a toolbox. When a handyperson starts working for the first time, they need to buy their tools. For their first few jobs, they might need help finding their tools, or remembering which tool is best used for what action. Eventually, they get to know their tools well, and using them appropriately becomes second nature.\nFor now, ChatGPT can help us find and use our tools, but we need to work towards using them as second nature!"
  },
  {
    "objectID": "syllabus.html#course-expectations",
    "href": "syllabus.html#course-expectations",
    "title": "BSTA 512/612 Syllabus",
    "section": "Course Expectations",
    "text": "Course Expectations\n\nInstructor Expectations\nCommitment to your learning and your success\nI believe that everyone has the ability to be successful in this course and I have put a lot of effort into designing the course in a way that maximizes your learning to ensure your success. Please talk to me before or after class or stop by my office if there is anything you want to discuss or about which you are unclear. I want to be supportive of your learning and growth.\nInclusive & supportive learning community\nI believe that learning happens best when we all learn together, as a community. This means creating a space characterized by generous listening, civility, humility, patience, and hospitality. I will attempt to promote a safe climate where we examine content from multiple cultural perspectives, and I will strive to create and maintain a classroom atmosphere in which you feel free to both listen to others and express your views and ask questions to increase your learning.\nOpenness to feedback\nI appreciate straightforward feedback from you regarding how well the class is meeting your needs. Let me know if material is not clear or when its relevance to the student learning outcomes for the course is not apparent. In particular, let me know if you identify bias or stereotyping in my teaching materials as I will seek to continuously improve. Please also let me know if there’s an aspect of the class you find particularly interesting, helpful, or enjoyable!\nResponsiveness\nI will monitor email as well as the discussion board daily and try respond to all messages within 24 hours Monday-Friday.\nClear guidelines and prompt feedback on assignments\nI will provide clear instructions for all assignments, and a grading rubric when applicable. I will provide detailed feedback on your submissions and will update grades promptly in Sakai.\n\n\nStudent Expectations and Resources\nAttend class\nYou are expected to attend all scheduled class meetings synchronously or watch the recording within 7 days. Attendance is taken through exit tickets. If you have issues accessing the poll on a specific day, please let me know. \nParticipate\nI encourage you to participate actively in class and online discussions. I will expect all students, and all instructors, to be respectful of each other’s contributions, whether I agree with them or not. Professional interactions are expected.\nBuild rapport\nIf you find that you have any trouble keeping up with assignments or other aspects of the course, make sure you let me know as early as possible. As you will find, building rapport and effective relationships are key to becoming an effective professional. Make sure that you are proactive in informing me when difficulties arise during the quarter so that I can help you find a solution.\nComplete assignments\nAll assignments for this course will be submitted electronically through Sakai unless otherwise instructed.  I encourage you to make your best effort to submit all assignments on time, but I understand that sometimes circumstances arise that are beyond our control. If you need an extension, please contact me in congruence with the Late Policy.\nSeek help if you need it\nI believe it is important to support the physical and emotional well‐being of my students. If you are experiencing physical or mental health issues, I encourage you to use the resources on campus such as those listed below. If you have a health issue that is affecting your performance or participation in the course, and/or if you need help connecting with these resources, please contact me.\n\nStudent Health and Wellness Center (SHW), Website, 503-494-8665 (OHSU Students only)\nStudent Health and Counseling (SHAC), Website, 503-725-2800\n\nInform your instructor of any accommodations needed\nYou should speak with or email me before or during the first week of classes regarding any special needs. Students seeking academic accommodations should register with the appropriate service under the School policies below.\nSome religious holidays may occur on regularly scheduled class days. Because available class hours are so limited in number, we will have to hold class on all such days. Class video recordings will be available and you are encouraged to engage with the material outside of the regular class time. You are also encouraged to come to office hours with questions from the session.\nCommit to integrity\nAs a student in this course (and at PSU or OHSU) you are expected to maintain high degrees of professionalism, commitment to active learning and participation in this class and also integrity in your behavior in and out of the classroom.\nCheating and other forms of academic misconduct will not be tolerated in this course and will be dealt with firmly. Student academic misconduct refers to behavior that includes plagiarism, cheating on assignments, fabrication of data, falsification of records or official documents, intentional misuse of equipment or materials (including library materials), or aiding and abetting the perpetration of such acts. Preparation of exams, assigned on an individual basis, must represent each student’s own individual effort. When used, resource materials should be cited in conventional reference format."
  },
  {
    "objectID": "syllabus.html#course-communications",
    "href": "syllabus.html#course-communications",
    "title": "BSTA 512/612 Syllabus",
    "section": "Course Communications",
    "text": "Course Communications\nSakai announcements\nFor important/urgent matters, I will communicate with you using announcements via Sakai that will be delivered to your OHSU Email account as well as displayed in the Sakai course site Announcements section.\nE-mail\nE-mail should be used only for messages that are private in nature. Please send private messages to my OHSU email address (wakim@ohsu.edu). Messages sent through Sakai Inbox will not be answered."
  },
  {
    "objectID": "syllabus.html#further-student-resources",
    "href": "syllabus.html#further-student-resources",
    "title": "BSTA 512/612 Syllabus",
    "section": "Further Student Resources",
    "text": "Further Student Resources\n\nSPH Writing Lab\nThe School of Public Health Writing Support serves graduate students (master’s and PhD) in SPH, offering help on all professional writing tasks, including class papers, dissertations, job application documents, personal statements, and grant applications, to name a few. Leslie Bienen, MFA, DVM offers one-on-one writing support and other workshops. Appointments are virtual for the time being. You can make an appointment by contacting writingsupportsph@pdx.edu or making an appointment through Calendly.\n\n\nStudent Wellness\nI am committed to supporting the physical and emotional well-being of my students. Both PSU and OHSU have designated centers for student health. For OHSU, students can visit the Behavioral Health site, where you can find more information including the number to make an appointment. All student visits are free. OHSU students also have access to PSU’s Counseling Services through the school’s Student Health & Counseling. Information on additional student resources for OHSU students are available on the OHSU Health and Wellness Resource page. \n\n\nSupport for Food Insecurity\nStudents across the country experience food insecurity at alarming rates. OHSU and PSU both provide a list of resources to help combat food insecurity. Of note, the Committee to Improve Student Food Security (CISFS) at PSU provides a Free Food Market on the second Monday of each month. OHSU also provides SNAP Enrollment Assistance. The Supplemental Nutrition Assistance Program (SNAP) allocates money towards food for individuals below a certain income level. If you make less than $2,430 monthly, you may wish to enroll.\n\n\nSupport for Students with Children\nStudents who have children can use the PSU resource: Resource Center for Students with Children. Resources are mostly focused on students with younger children. There are several great resources available, including: family-friendly study spaces, new baby starter packs, free kids clothing, and further information on financial resources for childcare."
  },
  {
    "objectID": "syllabus.html#school-policies-and-resources",
    "href": "syllabus.html#school-policies-and-resources",
    "title": "BSTA 512/612 Syllabus",
    "section": "School Policies and Resources",
    "text": "School Policies and Resources\n\nSchool of Public Health Handbook\nAll students are responsible for following the policies and expectations outlined in the student handbook for their program of study. Students are responsible for their own academic work and are expected to have read and practice principles of academic honesty, as presented in the handbook.\n\n\nStudent Access & Accommodations\nThe School of Public Health values diversity and inclusion; we are committed to fostering mutual respect and full participation for all students. My goal is to create a learning environment that is equitable, usable, inclusive, and welcoming. If any aspects of instruction or course design result in barriers to your inclusion or learning, please notify me. \n\nIf you are already registered with disability services at either OHSU or PSU and you are taking a course at the opposite institution, you need to contact the office you’re registered with to transfer your accommodations.\nIf you are not already registered with a disability services office, and you have, or think you may have, a disability that may affect your work in this class, and feel you need accommodations, use the following table for guidance about which office to contact to initiate accommodations.\n\nResource Table\n\n\n\nEnrollment University and Standing\nWhere to Seek Accommodations\n\n\nUndergraduate School of Public Health major\nPSU’s Disability Resource Center\n\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\n\n\n\nAll PSU-registering Dual Degree (MSW/MPH and MURP/MPH) Graduate School of Public Health Majors and all PSU-registering PhD students admitted prior to fall 2016.\nPSU’s Disability Resource Center\n\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\nwww.pdx.edu/drc\n\n\n\nGraduate School of Public Health major (irrespective of institution at which you register)\nOHSU’s Office for Student Access\n(503) 494-0082\nStudentAccess@OHSU.edu\nOHSU Auditorium Building 330\n\n\nNon-SPH major, PSU-enrolled student\nPSU’s Disability Resource Center\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\nwww.pdx.edu/drc\n\n\nNon-SPH major, OHSU-enrolled student\nOHSU’s Office for Student Access\n(503) 494-0082\nStudentAccess@OHSU.edu\nOHSU Auditorium Building 330\n\n\n\n \nFor more information related accessibility and accommodations, please see the “Statement Regarding Students with Disabilities” within the Institutional Policies section of this syllabus.\n\n\nTitle IX\nThe School of Public Health is committed to providing an environment free of all forms of prohibited discrimination and discriminatory harassment. The School of Public Health students who have questions about an incident related to Title IX are welcome to contact either the OHSU or PSU’s Title IX Coordinator and they will direct you to the appropriate resource or office. Title IX pertains to any form of sex/gender discrimination, discriminatory harassment, sexual harassment or sexual violence.\n\nPSU’s Title IX Coordinator is Julie Caron, she may be reached at titleixccordinator@pdx.edu or 503-725-4410. Julie’s office is located at 1600 SW 4th Ave, In the Richard and Maureen Neuberger Center RMNC - Suite 830.\nThe OHSU Title IX Coordinator’s may be reachedat 503-494-0258 or titleix@ohsu.edu and is located at 2525 SW 3rd St.\n\nPlease note that faculty and the Title IX Coordinators will keep the information you disclose private but are not confidential. If you would like to speak with a confidential advocate, who will not disclose the information to a university official without your written consent, you may contact an advocate at PSU or OHSU.\n\nPSU’s confidential advocates are available in Women’s Resource Center (serving all genders) in Smith Student Memorial Union 479. You may schedule an appointment by (503-725-5672) or schedule on line at https://psuwrc.youcanbook.me. For more information about resources at PSU, please see PSU’s Response to Sexual Misconduct website.\nOHSU’s advocates are available through the Confidential Advocacy Program (CAP) at 833-495-CAPS (2277) or by email CAPsupport@ohsu.edu, but please note, email is not a secure form of communication. Also visit www.ohsu.edu/CAP.\n\nAt OHSU, if you encounter any harassment, or discrimination based on race, color, religion, age, national origin or ancestry, veteran or military status, sex, marital status, pregnancy or parenting status, sexual orientation, gender identity or expression, disability or any other protected status, please contact the Affirmative Action and Equal Opportunity (AAEO) Department at 503-494-5148 or aaeo@ohsu.edu.\nAt PSU, you may contact the Office of Equity and Compliance if you experience any form of discrimination or discriminatory harassment as listed above at equityandcompliance@pdx.edu or by calling 503-725-5919.\n\n\nTechnical Support\nThe OHSU ITG Help Desk is available to assist students with email account or network account access issues between 6 a.m. and 6 p.m., Monday through Friday at 503-494-2222. For technical support in using the Sakai Course Management System, please contact the Sakai Help Desk at 877-972-5249 or email us at sakai@ohsu.edu"
  },
  {
    "objectID": "syllabus.html#ohsu-competencies",
    "href": "syllabus.html#ohsu-competencies",
    "title": "BSTA 512/612 Syllabus",
    "section": "OHSU Competencies",
    "text": "OHSU Competencies\n\nList of OHSU Graduation Core Competencies\n\nProfessional Knowledge and Skills\nProfessionalism\nInformation Literacy\nCommunication\nTeamwork\nCommunity Engagement, Social Justice and Equity\nPatient Centered Care\n\nTo access a descriptive list of OHSU Graducation Core Competencies: OHSU Graduation Core Competencies"
  },
  {
    "objectID": "syllabus.html#institutional-policies-and-resources",
    "href": "syllabus.html#institutional-policies-and-resources",
    "title": "BSTA 512/612 Syllabus",
    "section": "Institutional Policies and Resources",
    "text": "Institutional Policies and Resources\n\nStatement Regarding Students with Disabilities\nOHSU is committed to inclusive and accessible learning environments in compliance with federal and state law. If you have a disability or think you may have a disability (mental health, attention-related, learning, vision, hearing, physical or health impacts) contact the Office for Student Access at (503) 494-0082 or OHSU Student Access to have a confidential conversation about academic accommodations. Information is also available at Student Access Website. Because accommodations may take time to implement and cannot be applied retroactively, it is important to have this discussion as soon as possible.\nPortland State students also have similar resources available via the PSU Disability Resource Center (website http://www.pdx.edu/drc ). Please contact the DRC at tel. (503) 725-4150 or email at drc@pdx.edu\n\n\nStudent Evaluation of Courses\nCourse evaluation results are extremely important and used to help improve courses and the learning experience of future students. Responses will always remain anonymous and will only be available to instructors after grades have been posted. The results of scaled questions and comments go to both the instructor and their unit head/supervisor. Refer to Student Evaluation of Courses and Instructional Effectiveness, *Policy No. 02-50-035.\n*To access the OHSU Student Evaluation of Courses and Instructional Effectiveness Policy, you must log into the OHSU O2 website.\n\n\nCopyright Information\nCopyright laws and fair use policies protect the rights of those who have produced the material. The copy in this course has been provided for private study, scholarship, or research. Other uses may require permission from the copyright holder. The user of this work is responsible for adhering to copyright law of the U.S. (Title 17, U.S. Code). To help you familiarize yourself with copyright and fair use policies, the University encourages you to visit its Copyright Web Page\nSakai course web sites contain material protected by copyrights held by the instructor, other individuals or institutions. Such material is used for educational purposes in accord with copyright law and/or with permission given by the owners of the original material. You may download one copy of the materials on any single computer for non-commercial, personal, or educational purposes only, provided that you (1) do not modify it, (2) use it only for the duration of this course, and (3) include both this notice and any copyright notice originally included with the material. Beyond this use, no material from the course web site may be copied, reproduced, re-published, uploaded, posted, transmitted, or distributed in any way without the permission of the original copyright holder. The instructor assumes no responsibility for individuals who improperly use copyrighted material placed on the web site.\n\n\nSyllabi Changes and Retention\nSyllabi are considered to be a learning agreement between students and the faculty of record. Information contained in syllabi, other than the minimum requirements, may be subject to change as deemed appropriate by the faculty of record in concurrence with the academic program and the Office of the Provost. Refer to the *Course Syllabi Policy, 02-50-050.\n*To access the OHSU Course Syllabus Policy, you must log into the OHSU O2 website.\n\n\nCommitment to Diversity & Inclusion\nOHSU is committed to creating and fostering a learning and working environment based on open communication and mutual respect. If you encounter sexual harassment, sexual misconduct, sexual assault, or discrimination based on race, color, religion, age, national origin, veteran’s status, ancestry, sex, marital status, pregnancy or parenting status, sexual orientation, gender identity, disability or any other protected status please contact the Affirmative Action and Equal Opportunity Department at 503-494-5148 or aaeo@ohsu.edu. Inquiries about Title IX compliance or sex/gender discrimination and harassment may be directed to the OHSU Title IX Coordinator at 503-494-0258 or titleix@ohsu.edu.\n\n\nModified Operations, Policy 01-40-010\nPortland Campus:  Marquam Hill and South Waterfront\nStudents should review O2 or call OHSU’s weather alert line at 503-494-9021 for the most up-to-date information on OHSU-wide modified operations which include but are not limited to delays or closures for inclement weather.\nIf your home institution is not on the Portland campus (Marquam Hill or South Waterfront, contact your home institution for more information.\n\n\nOHSU Resources Available to Students*:\nRemote Learning Resources\nThe Remote Learning webpage on O2 contains concise, practical resources, and strategies for students that need to quickly transition to a fully remote instructional format.\nRegistrar’s Office\nMackenzie Hall, Rm. 1120\n503-494-7800; Email the Registrar\nStudent Registration Information: \nTo Register for Classes\nOHSU ITG Help Desk\nRegular staff hours are 6 a.m. to 6 p.m., Monday through Friday, but phones are answered seven days a week, 24 hours a day. Call 503 494-2222.\nTeaching and Learning Center\nAcademic Support Counseling and Sakai Course Management System, please contact the TLC Help Desk at 877-972-5249 or email TLC Help Desk\nStudent Academic Support Services\nFor resources on improving student’s study strategies, time management, motivation, test-taking skills and more, Please access the Student Academic Support Services Sakai page. For one-on-one appointments or to arrange a workshop for students, please contact Emily Hillhouse.\nConfidential Advocacy Program\nSupport for OHSU employees, students, and volunteers who have experienced any form of sexual misconduct, including sexual harassment, sexual assault, intimate-partner violence, stalking, relationship/dating violence, and other forms — regardless of when or where it took place. Contact Us.\nConcourse Syllabus Management\nFor help with accessing your Concourse Syllabus:  Please contact the Sakai help Desk for all other Concourse inquiries please visit the Concourse Support - Sakai or please contact the Mark Rivera at rivermar@ohsu.edu or call 503-494-0934\nPublic Safety\nOHSU Public Safety-Portland Campus (Marquam Hill and South Waterfront)\n\nEmergency on Campus: 503-494-4444 (Portland)\nNon-emergency: 503-494-7744; Contact Public Safety\n\nStudent Health & Wellness Center \nBaird Hall, Rm. 18 (Primary Care) and Rm. 6 (Behavioral Health)\n503-494-8665; For urgent care after hours, 503-494-8311 and ask for the Nurse on call.\nWellness Center Information  \nWellness Center Website\nIf your home institution is not on the Portland campus, contact your home institution student support services for more information.\nOmbudsman Office\nGaines Hall, Rm. 117\n707 SW Gaines Street, Portland, OR 97239\n503-494-5397; Contact Ombudsman; Ombudsman Website\nLibrary: Biomedical Information Communication Center\nBICC Library Hours of Operation\n\n\nPrivacy While Learning\nStudents may be asked to take classes remotely through videoconferencing software like WebEx. Some of these remote classes will be recorded. Any recording will capture the presenter’s audio, video, and computer screen. Student video and audio will be recorded if and when you unmute your audio and share your video during the recorded sessions. These recordings will not be shared with or accessible to the public without prior written consent. \n\n\nStudent Central\nKey information for students across OHSU’s Schools of Dentistry, Medicine, Nursing, the OHSU-PSU School of Public Health and the College of Pharmacy. Student Central helps you find out more about student services, resources, policies and technology."
  },
  {
    "objectID": "instructors.html",
    "href": "instructors.html",
    "title": "Instructors",
    "section": "",
    "text": "Email: wakim@ohsu.edu\nOffice: VPT 622A\n\nPronouns: she/her/hers\nYou are welcome to address me as Nicky (pronounced “nik-EE”), Dr. Wakim (pronounced “wah-KEEM”), Dr. W, Dr. Nicky, Professor, Professor Wakim, or any combination of the prior.\nBest method to contact: Office hours or email for general course questions or E-mail/Calendly appointments for private communication.\n\n\n\n\n\n\n\n\n\n\nBrief professor statement: As a professor, my main goal is to instill a growth mindset into my students. Growth mindset means we are NOT stuck in our abilities or knowledge, and that we all can and will grow! This course aims to be as transparent as possible. I want you to understand my motivation for assessments, questions, and lessons. I also want those assessments to be clear, so please ask for clarification whenever needed.\n\n\nWednesdays 3-4pm: in-person following class"
  },
  {
    "objectID": "instructors.html#instructor-nicole-nicky-wakim-phd",
    "href": "instructors.html#instructor-nicole-nicky-wakim-phd",
    "title": "Instructors",
    "section": "",
    "text": "Email: wakim@ohsu.edu\nOffice: VPT 622A\n\nPronouns: she/her/hers\nYou are welcome to address me as Nicky (pronounced “nik-EE”), Dr. Wakim (pronounced “wah-KEEM”), Dr. W, Dr. Nicky, Professor, Professor Wakim, or any combination of the prior.\nBest method to contact: Office hours or email for general course questions or E-mail/Calendly appointments for private communication.\n\n\n\n\n\n\n\n\n\n\nBrief professor statement: As a professor, my main goal is to instill a growth mindset into my students. Growth mindset means we are NOT stuck in our abilities or knowledge, and that we all can and will grow! This course aims to be as transparent as possible. I want you to understand my motivation for assessments, questions, and lessons. I also want those assessments to be clear, so please ask for clarification whenever needed.\n\n\nWednesdays 3-4pm: in-person following class"
  },
  {
    "objectID": "instructors.html#teaching-assistants",
    "href": "instructors.html#teaching-assistants",
    "title": "Instructors",
    "section": "Teaching Assistants",
    "text": "Teaching Assistants\nLiv Ainsworth, Katie Hand, and Miyuki Sun will serve as our TAs for the quarter!! They will have the following office hours and will help answer questions over email.\n\nLiv Ainsworth\n\nOffice hours: Fridays 10-11am\n\nOnline Zoom Link\n\nEmail: ainsworl@ohsu.edu\n\n\n\nKatie Hand\n\nOffice hours: Mondays 4:30-5:30 pm\n\nOnline Webex Link\n\nEmail: handka@ohsu.edu\n\n\n\nMiyuki Sun\n\nOffice hours: Thursdays 10-11am\n\nOnline Webex Link\n\nEmail: sunm@ohsu.edu"
  },
  {
    "objectID": "project.html#labs",
    "href": "project.html#labs",
    "title": "Project Central",
    "section": "Labs",
    "text": "Labs\n\n\n\nLab\nDue Date\nTopics\n\n\n\n\nLab 1\n1/16\nExploring the question\n\n\nLab 2\n2/7\nExploring the data\n\n\nLab 3\n2/21\nA little more data exploration + Fitting and interpreting a model\n\n\nLab 4\n3/7\nBuilding a model\n\n\n\n\nHelp with BMI variable"
  },
  {
    "objectID": "project.html#report",
    "href": "project.html#report",
    "title": "Project Central",
    "section": "Report",
    "text": "Report\nReport Instructions\nDue 3/21/2024 at 11pm\n\nReading and listening sources\nIf you are interested in sources that discuss the social complexities of anti-fat bias, feel free to take a look at the following sources. Please be aware that these resources will discuss anti-fat bias and related histories, including racism and sexism.\n\nArticle: Implicit and explicit anti-fat bias: The role of weight-related attitudes and beliefs\nPodcast: Anti-Fat Bias by Maintenance Phase\nBook: Fearing the Black Body: The Racial Origins of Fat Phobia\n\nMultnomah County Library has unlimited loans for the audiobook\n\nBlog: Dances with Fat\n\nYou can subscribe to Ragen’s weekly newsletter for free\n\n\nIf you have additional sources that you would like to share, please send them to me!"
  },
  {
    "objectID": "data/NHANES_EDA.html",
    "href": "data/NHANES_EDA.html",
    "title": "NHANES",
    "section": "",
    "text": "NHANES\n\nlibrary(NHANES)\nlibrary(skimr)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\ndata(\"NHANES\")\n\n\nskim(NHANES)\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n# 16 Depressed             3327         0.667 FALSE          3 \"Non: 5246, Sev: 1009, Mos: 418\"            \n# 17 SleepTrouble          2228         0.777 FALSE          2 \"No: 5799, Yes: 1973\"                       \n# 18 PhysActive            1674         0.833 FALSE          2 \"Yes: 4649, No: 3677\"          \n\n\nNHANES18 &lt;- NHANES %&gt;% dplyr::filter(Age &gt;= 18)\nNHANES18 %&gt;% tabyl(Depressed, PhysActive)\n\n Depressed   No  Yes\n      None 2297 2949\n   Several  538  471\n      Most  275  143\n      &lt;NA&gt;  423  385\n\nNHANES18 %&gt;% drop_na(Depressed) %&gt;% tabyl(Depressed, PhysActive)\n\n Depressed   No  Yes\n      None 2297 2949\n   Several  538  471\n      Most  275  143\n\nNHANES18Dep &lt;- NHANES18 %&gt;% drop_na(Depressed)\nNHANES18Dep %&gt;% \n  tabyl(Depressed, PhysActive) %&gt;% \n  adorn_totals()\n\n Depressed   No  Yes\n      None 2297 2949\n   Several  538  471\n      Most  275  143\n     Total 3110 3563\n\nchisq_Dep_Phys&lt;- chisq.test(NHANES18Dep$Depressed, NHANES18Dep$PhysActive)\ntidy(chisq_Dep_Phys)\n\n# A tibble: 1 × 4\n  statistic  p.value parameter method                    \n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      96.9 9.26e-22         2 Pearson's Chi-squared test\n\nchisq_Dep_Phys$expected\n\n                     NHANES18Dep$PhysActive\nNHANES18Dep$Depressed        No       Yes\n              None    2444.9363 2801.0637\n              Several  470.2518  538.7482\n              Most     194.8119  223.1881\n\nchisq_Dep_Phys$observed\n\n                     NHANES18Dep$PhysActive\nNHANES18Dep$Depressed   No  Yes\n              None    2297 2949\n              Several  538  471\n              Most     275  143\n\nlibrary(moderndive)\nset.seed(5348)\n# 5347\nNHANES18Dep200 &lt;- NHANES18Dep %&gt;%\n  rep_sample_n(size = 200, reps = 1, replace = FALSE)\n\nNHANES18Dep200 %&gt;% \n  tabyl(Depressed, PhysActive) %&gt;% \n  adorn_totals()\n\n Depressed No Yes\n      None 63  79\n   Several 19  18\n      Most 10  11\n     Total 92 108\n\nchisq_Dep_Phys200&lt;- chisq.test(NHANES18Dep200$Depressed, NHANES18Dep200$PhysActive)\ntidy(chisq_Dep_Phys200)\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                    \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1     0.601   0.740         2 Pearson's Chi-squared test\n\nchisq_Dep_Phys200$expected\n\n                        NHANES18Dep200$PhysActive\nNHANES18Dep200$Depressed    No   Yes\n                 None    65.32 76.68\n                 Several 17.02 19.98\n                 Most     9.66 11.34\n\nchisq_Dep_Phys200$observed\n\n                        NHANES18Dep200$PhysActive\nNHANES18Dep200$Depressed No Yes\n                 None    63  79\n                 Several 19  18\n                 Most    10  11\n\n#------------\nset.seed(5349)\nNHANES18Dep400 &lt;- NHANES18Dep %&gt;%\n  rep_sample_n(size = 400, reps = 1, replace = FALSE)\n\nNHANES18Dep400 %&gt;% \n  tabyl(PhysActive, Depressed) %&gt;% \n  adorn_totals(where = c(\"row\", \"col\")) %&gt;% \n  adorn_title \n\n            Depressed                   \n PhysActive      None Several Most Total\n         No       115      32   27   174\n        Yes       199      26    1   226\n      Total       314      58   28   400\n\nchisq_Dep_Phys400&lt;- chisq.test(NHANES18Dep400$Depressed, NHANES18Dep400$PhysActive)\ntidy(chisq_Dep_Phys400)\n\n# A tibble: 1 × 4\n  statistic       p.value parameter method                    \n      &lt;dbl&gt;         &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      41.2 0.00000000115         2 Pearson's Chi-squared test\n\nchisq_Dep_Phys400$observed\n\n                        NHANES18Dep400$PhysActive\nNHANES18Dep400$Depressed  No Yes\n                 None    115 199\n                 Several  32  26\n                 Most     27   1\n\nchisq_Dep_Phys400$expected\n\n                        NHANES18Dep400$PhysActive\nNHANES18Dep400$Depressed     No    Yes\n                 None    136.59 177.41\n                 Several  25.23  32.77\n                 Most     12.18  15.82\n\nset.seed(5349)\nNHANES18Dep_PAy100 &lt;- NHANES18Dep %&gt;% filter(PhysActive == \"Yes\") %&gt;% \n  rep_sample_n(size = 100, reps = 1, replace = FALSE)\nNHANES18Dep_PAy100 %&gt;% tabyl(Depressed)\n\n Depressed  n percent\n      None 85    0.85\n   Several 12    0.12\n      Most  3    0.03\n\nNHANES18Dep_PAn100 &lt;- NHANES18Dep %&gt;% filter(PhysActive == \"No\") %&gt;% \n  rep_sample_n(size = 100, reps = 1, replace = FALSE)\nNHANES18Dep_PAn100 %&gt;% tabyl(Depressed)\n\n Depressed  n percent\n      None 78    0.78\n   Several 17    0.17\n      Most  5    0.05\n\n(DepPA200_table &lt;- matrix(c(83, 12, 5, 78, 16, 6), nrow = 2, ncol = 3, byrow = T))\n\n     [,1] [,2] [,3]\n[1,]   83   12    5\n[2,]   78   16    6\n\ndimnames(DepPA200_table) &lt;- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several\", \"Most\"))  # column names\nDepPA200_table\n\n     Depression\nPA    None Several Most\n  Yes   83      12    5\n  No    78      16    6\n\nchisq.test(DepPA200_table) \n\n\n    Pearson's Chi-squared test\n\ndata:  DepPA200_table\nX-squared = 0.81762, df = 2, p-value = 0.6644\n\nchisq.test(DepPA200_table)$expected\n\n     Depression\nPA    None Several Most\n  Yes 80.5      14  5.5\n  No  80.5      14  5.5\n\nset.seed(5349)\nNHANES18Dep_PAy50 &lt;- NHANES18Dep %&gt;% filter(PhysActive == \"Yes\") %&gt;% \n  rep_sample_n(size = 50, reps = 1, replace = FALSE)\nNHANES18Dep_PAy50 %&gt;% tabyl(Depressed)\n\n Depressed  n percent\n      None 43    0.86\n   Several  6    0.12\n      Most  1    0.02\n\nNHANES18Dep_PAn50 &lt;- NHANES18Dep %&gt;% filter(PhysActive == \"No\") %&gt;% \n  rep_sample_n(size = 50, reps = 1, replace = FALSE)\nNHANES18Dep_PAn50 %&gt;% tabyl(Depressed)\n\n Depressed  n percent\n      None 30    0.60\n   Several 14    0.28\n      Most  6    0.12"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02_key_info.html#key-dates",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "",
    "text": "Use visualizations and cut off points to flag potentially influential points using residuals, leverage, and Cook’s distance\nHandle influential points and assumption violations by checking data errors, reassessing the model, and making data transformations.\nImplement a model with data transformations and determine if it improves the model fit.\n\n\n\n\nWe have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\nThe residuals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun model1 through augment() (model1 is input)\n\nSo we assigned model1 as the output of the lm() function (model1 is output)\n\nWill give us values about each observation in the context of the fitted regression model\n\ncook’s distance (.cooksd), fitted value (.fitted, \\(\\widehat{Y}_i\\)), leverage (.hat), residual (.resid), standardized residuals (.std.resid)\n\n\n\naug1 &lt;- augment(model1) \nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…\n\n\nRDocumentation on the augment() function.\n\n\n\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nCheck if there is a linear relationship between the mean response (Y) and the explanatory variable (X)\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nCheck that the observations are independent\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\nCheck that the responses (at each level X) are normally distributed\n\nUsually measured through the residuals\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nCheck that the variance (or standard deviation) of the responses is equal for all levels of X\n\nUsually measured through the residuals"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Let’s remind ourselves of the model that we have been working with",
    "text": "Let’s remind ourselves of the model that we have been working with\n\nWe have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Our residuals will help us a lot in our diagnostics!",
    "text": "Our residuals will help us a lot in our diagnostics!\n\n\n \n\nThe residuals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#augment-getting-extra-information-on-the-fitted-model",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#augment-getting-extra-information-on-the-fitted-model",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "augment(): getting extra information on the fitted model",
    "text": "augment(): getting extra information on the fitted model\n\nRun model1 through augment() (model1 is input)\n\nSo we assigned model1 as the output of the lm() function (model1 is output)\n\nWill give us values about each observation in the context of the fitted regression model\n\ncook’s distance (.cooksd), fitted value (.fitted, \\(\\widehat{Y}_i\\)), leverage (.hat), residual (.resid), standardized residuals (.std.resid)\n\n\n\naug1 &lt;- augment(model1) \nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…\n\n\nRDocumentation on the augment() function."
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#revisiting-our-line-assumptions",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#revisiting-our-line-assumptions",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Revisiting our LINE assumptions",
    "text": "Revisiting our LINE assumptions\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nCheck if there is a linear relationship between the mean response (Y) and the explanatory variable (X)\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nCheck that the observations are independent\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\nCheck that the responses (at each level X) are normally distributed\n\nUsually measured through the residuals\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nCheck that the variance (or standard deviation) of the responses is equal for all levels of X\n\nUsually measured through the residuals"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#influential-points",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#influential-points",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Influential points",
    "text": "Influential points\n\n\n\n\nOutliers\n\n\n\nAn observation (\\(X_i, Y_i\\)) whose response \\(Y_i\\) does not follow the general trend of the rest of the data\n\n\n\n \n \n\n\n\n\n\n\n\n\nHigh leverage observations\n\n\n\nAn observation (\\(X_i, Y_i\\)) whose predictor \\(X_i\\) has an extreme value\n\\(X_i\\) can be an extremely high or low value compared to the rest of the observations"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#outliers",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#outliers",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Outliers",
    "text": "Outliers\n\nAn observation (\\(X_i, Y_i\\)) whose response \\(Y_i\\) does not follow the general trend of the rest of the data\nHow do we determine if a point is an outlier?\n\nScatterplot of \\(Y\\) vs. \\(X\\)\nFollowed by evaluation of its residual (and standardized residual)\n\n\n \n\nUse the internally standardized residual (aka studentized residual) to determine if an observation is an outlier"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#poll-everywhere-question-1",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#poll-everywhere-question-1",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#identifying-outliers",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#identifying-outliers",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Identifying outliers",
    "text": "Identifying outliers\n\n\n\n\nInternally standardized residual\n\n\n\\[\nr_i = \\frac{\\widehat\\epsilon_i}{\\sqrt{\\widehat\\sigma^2(1-h_{ii})}}\n\\]\n\n\n\n\nWe flag an observation if the standardized residual is “large”\n\nDifferent sources will define “large” differently\nPennState site uses \\(|r_i| &gt; 3\\)\nautoplot() shows the 3 observations with the highest standardized residuals\nOther sources use \\(|r_i| &gt; 2\\), which is a little more conservative\n\n\n\n\n\n\n \n\nggplot(data = aug1) + \n  geom_histogram(aes(x = .std.resid))"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#countries-that-are-outliers-r_i-2",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#countries-that-are-outliers-r_i-2",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Countries that are outliers (\\(|r_i| > 2\\))",
    "text": "Countries that are outliers (\\(|r_i| &gt; 2\\))\n\nWe can identify the countries that are outliers\n\n\naug1 %&gt;% \n  filter(abs(.std.resid) &gt; 2)\n\n# A tibble: 4 × 10\n  .rownames country     life_expectancy_year…¹ female_literacy_rate…² .std.resid\n  &lt;chr&gt;     &lt;chr&gt;                        &lt;dbl&gt;                  &lt;dbl&gt;      &lt;dbl&gt;\n1 33        Central Af…                   48                     44.2      -2.20\n2 152       South Afri…                   55.8                   92.2      -2.71\n3 161       Swaziland                     48.9                   87.3      -3.65\n4 187       Zimbabwe                      51.9                   80.1      -2.89\n# ℹ abbreviated names: ¹​life_expectancy_years_2011, ²​female_literacy_rate_2011\n# ℹ 5 more variables: .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;, .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;,\n#   .cooksd &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#high-leverage-observations",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#high-leverage-observations",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "High leverage observations",
    "text": "High leverage observations\n\nAn observation (\\(X_i, Y_i\\)) whose response \\(X_i\\) is considered “extreme” compared to the other values of \\(X\\)\n\n \n\nHow do we determine if a point has high leverage?\n\nScatterplot of \\(Y\\) vs. \\(X\\)\nCalculating the leverage of each observation"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#leverage-h_i",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#leverage-h_i",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Leverage \\(h_i\\)",
    "text": "Leverage \\(h_i\\)\n\nValues of leverage are: \\(0 \\leq h_i \\leq 1\\)\nWe flag an observation if the leverage is “high”\n\nDifferent sources will define “high” differently\nSome textbooks use \\(h_i &gt; 4/n\\) where \\(n\\) = sample size\nSome people suggest \\(h_i &gt; 6/n\\)\nPennState site uses \\(h_i &gt; 3p/n\\) where \\(p\\) = number of regression coefficients\n\n\n\naug1 = aug1 %&gt;% relocate(.hat, .after = female_literacy_rate_2011)\naug1 %&gt;% arrange(desc(.hat))\n\n# A tibble: 80 × 10\n   .rownames country        life_expectancy_year…¹ female_literacy_rate…²   .hat\n   &lt;chr&gt;     &lt;chr&gt;                           &lt;dbl&gt;                  &lt;dbl&gt;  &lt;dbl&gt;\n 1 1         Afghanistan                      56.7                   13   0.136 \n 2 104       Mali                             60                     24.6 0.0980\n 3 34        Chad                             57                     25.4 0.0956\n 4 146       Sierra Leone                     55.7                   32.6 0.0757\n 5 62        Gambia                           66                     41.9 0.0540\n 6 70        Guinea-Bissau                    56.2                   42.1 0.0536\n 7 33        Central Afric…                   48                     44.2 0.0493\n 8 118       Nepal                            68.7                   46.7 0.0446\n 9 42        Cote d'Ivoire                    56.9                   47.6 0.0430\n10 169       Togo                             59.6                   48   0.0422\n# ℹ 70 more rows\n# ℹ abbreviated names: ¹​life_expectancy_years_2011, ²​female_literacy_rate_2011\n# ℹ 5 more variables: .std.resid &lt;dbl&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#   .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#countries-with-high-leverage-h_i-4n",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#countries-with-high-leverage-h_i-4n",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Countries with high leverage (\\(h_i > 4/n\\))",
    "text": "Countries with high leverage (\\(h_i &gt; 4/n\\))\n\nWe can look at the countries that have high leverage\n\n\naug1 %&gt;% \n  filter(.hat &gt; 4/80) %&gt;%\n  arrange(desc(.hat))\n\n# A tibble: 6 × 10\n  .rownames country       life_expectancy_years_…¹ female_literacy_rate…²   .hat\n  &lt;chr&gt;     &lt;chr&gt;                            &lt;dbl&gt;                  &lt;dbl&gt;  &lt;dbl&gt;\n1 1         Afghanistan                       56.7                   13   0.136 \n2 104       Mali                              60                     24.6 0.0980\n3 34        Chad                              57                     25.4 0.0956\n4 146       Sierra Leone                      55.7                   32.6 0.0757\n5 62        Gambia                            66                     41.9 0.0540\n6 70        Guinea-Bissau                     56.2                   42.1 0.0536\n# ℹ abbreviated names: ¹​life_expectancy_years_2011, ²​female_literacy_rate_2011\n# ℹ 5 more variables: .std.resid &lt;dbl&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#   .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#poll-everywhere-question-2",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#poll-everywhere-question-2",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#countries-with-high-leverage-h_i-4n-1",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#countries-with-high-leverage-h_i-4n-1",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Countries with high leverage (\\(h_i > 4/n\\))",
    "text": "Countries with high leverage (\\(h_i &gt; 4/n\\))\nLabel only countries with large leverage:\n\nggplot(aug1, aes(x = female_literacy_rate_2011, y = life_expectancy_years_2011,\n                 label = country)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  geom_text(aes(label = ifelse(.hat &gt; 0.05, as.character(country), ''))) +\n  geom_vline(xintercept = mean(aug1$female_literacy_rate_2011), color = \"grey\") +\n  geom_hline(yintercept = mean(aug1$life_expectancy_years_2011), color = \"grey\")"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#what-does-the-model-look-like-without-the-high-leverage-points",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#what-does-the-model-look-like-without-the-high-leverage-points",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "What does the model look like without the high leverage points?",
    "text": "What does the model look like without the high leverage points?\nSensitivity analysis removing countries with high leverage\n\nmodel1_lowlev &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                    data = aug1_lowlev)\ntidy(model1_lowlev) %&gt;% gt() %&gt;% # Without high-leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n49.563\n3.888\n12.746\n0.000\n    female_literacy_rate_2011\n0.247\n0.044\n5.562\n0.000\n  \n  \n  \n\n\n\ntidy(model1) %&gt;% gt() %&gt;% # With high leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n    female_literacy_rate_2011\n0.232\n0.031\n7.377\n0.000"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#cooks-distance",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#cooks-distance",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Cook’s distance",
    "text": "Cook’s distance\n\nMeasures the overall influence of an observation\n\n \n\nAttempts to measure how much influence a single observation has over the fitted model\n\nMeasures how all fitted values change when the \\(ith\\) observation is removed from the model\nCombines leverage and outlier information"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#identifying-points-with-high-cooks-distance",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#identifying-points-with-high-cooks-distance",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Identifying points with high Cook’s distance",
    "text": "Identifying points with high Cook’s distance\n\n\nThe Cook’s distance for the \\(i^{th}\\) observation is\n\\[d_i = \\frac{h_i}{2(1-h_i)} \\cdot r_i^2\\] where \\(h_i\\) is the leverage and \\(r_i\\) is the studentized residual\n\n\nAnother rule for Cook’s distance that is not strict:\n\nInvestigate observations that have \\(d_i &gt; 1\\)\n\nCook’s distance values are already in the augment tibble: .cooksd\n\n\n\n\naug1 = aug1 %&gt;% relocate(.cooksd, .after = female_literacy_rate_2011)\naug1 %&gt;% arrange(desc(.cooksd))\n\n# A tibble: 80 × 10\n   .rownames country       life_expectancy_year…¹ female_literacy_rate…² .cooksd\n   &lt;chr&gt;     &lt;chr&gt;                          &lt;dbl&gt;                  &lt;dbl&gt;   &lt;dbl&gt;\n 1 33        Central Afri…                   48                     44.2  0.126 \n 2 161       Swaziland                       48.9                   87.3  0.0903\n 3 152       South Africa                    55.8                   92.2  0.0577\n 4 187       Zimbabwe                        51.9                   80.1  0.0531\n 5 114       Morocco                         73.8                   57.6  0.0350\n 6 118       Nepal                           68.7                   46.7  0.0311\n 7 14        Bangladesh                      71                     53.4  0.0280\n 8 23        Botswana                        58.9                   85.6  0.0249\n 9 54        Equatorial G…                   61.4                   91.1  0.0231\n10 62        Gambia                          66                     41.9  0.0228\n# ℹ 70 more rows\n# ℹ abbreviated names: ¹​life_expectancy_years_2011, ²​female_literacy_rate_2011\n# ℹ 5 more variables: .hat &lt;dbl&gt;, .std.resid &lt;dbl&gt;, .fitted &lt;dbl&gt;,\n#   .resid &lt;dbl&gt;, .sigma &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#plotting-cooks-distance",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#plotting-cooks-distance",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Plotting Cook’s Distance",
    "text": "Plotting Cook’s Distance\n\n# plot(model) shows figures similar to autoplot()\n# adds on Cook's distance though\nplot(model1, which = 4)"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#model-without-those-4-points-qq-plot-residual-plot",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#model-without-those-4-points-qq-plot-residual-plot",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Model without those 4 points: QQ Plot, Residual plot",
    "text": "Model without those 4 points: QQ Plot, Residual plot\n\nmodel1_lowcd &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                    data = aug1_lowcd)\ntidy(model1_lowcd) %&gt;% gt() %&gt;% # Without high-leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n52.388\n2.078\n25.208\n0.000\n    female_literacy_rate_2011\n0.226\n0.024\n9.208\n0.000\n  \n  \n  \n\n\n\ntidy(model1) %&gt;% gt() %&gt;% # With high leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n    female_literacy_rate_2011\n0.232\n0.031\n7.377\n0.000"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#model-without-those-4-points-qq-plot-residual-plot-1",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#model-without-those-4-points-qq-plot-residual-plot-1",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Model without those 4 points: QQ Plot, Residual plot",
    "text": "Model without those 4 points: QQ Plot, Residual plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI am okay with this!\n\nAnd don’t forget: we may want more variables in our model!\nYou do not need to produce plots with the influential points taken out"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#summary-of-how-we-identify-influential-points",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#summary-of-how-we-identify-influential-points",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Summary of how we identify influential points",
    "text": "Summary of how we identify influential points\n\nUse scatterplot of \\(Y\\) vs. \\(X\\) to see if any points fall outside of range we expect\nUse standardized residuals, leverage, and Cook’s distance to further identify those points\nLook at the models run with and without the identified points to check for drastic changes\n\nLook at QQ plot and residuals to see if assumptions hold without those points\nLook at coefficient estimates to see if they change in sign and large magnitude\n\n\n \n\nNext: how to handle? It’s a little wishy washy"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#how-do-we-deal-with-influential-points",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#how-do-we-deal-with-influential-points",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "How do we deal with influential points?",
    "text": "How do we deal with influential points?\n\nIt’s always weird to be using numbers to help you diagnose an issue, but the issue kinda gets unresolved\nIf an observation is influential, we can check data errors:\n\nWas there a data entry or collection problem?\nIf you have reason to believe that the observation does not hold within the population (or gives you cause to redefine your population)\n\nIf an observation is influential, we can check our model:\n\nDid you leave out any important predictors?\nShould you consider adding some interaction terms?\nIs there any nonlinearity that needs to be modeled?\n\nBasically, deleting an observation should be justified outside of the numbers!\n\nIf it’s an honest data point, then it’s giving us important information!\n\nA really well thought out explanation from StackExchange"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#when-we-have-detected-problems-in-our-model",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#when-we-have-detected-problems-in-our-model",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "When we have detected problems in our model…",
    "text": "When we have detected problems in our model…\n\nWe have talked about influential points\nWe have talked about identifying issues with our LINE assumptions\n\nWhat are our options once we have identified issues in our linear regression model?\n\nSee if we need to add predictors to our model\n\nNicky’s thought for our life expectancy example\n\nTry a transformation if there is an issue with linearity or normality\nTry a transformation if there is unequal variance\nTry a weighted least squares approach if unequal variance (might be lesson at end of course)\nTry a robust estimation procedure if we have a lot of outlier issues (outside scope of class)"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#transformations",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#transformations",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Transformations",
    "text": "Transformations\n\nWhen we have issues with our LINE (mostly linearity, normality, or equality of variance) assumptions\n\nWe can use transformations to improve the fit of the model\n\nTransformations can…\n\nMake the relationship more linear\nMake the residuals more normal\n“Stabilize” the variance so that it is more constant\nIt can also bring in or reduce outliers\n\nWe can transform the dependent (\\(Y\\)) variable of the independent (\\(X\\)) variable\n\nUsually we want to try transforming the \\(X\\) first\n\n\n \n\nRequires trial and error!!\nMajor drawback: interpreting the model becomes harder!"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#common-transformations",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#common-transformations",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Common transformations",
    "text": "Common transformations\n\nTukey’s transformation (power) ladder\n\nUse R’s gladder() command from the describedata package\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower p\n-3\n-2\n-1\n-1/2\n0\n1/2\n1\n2\n3\n\n\n\n\n\n\\(\\frac{1}{x^3}\\)\n\\(\\frac{1}{x^2}\\)\n\\(\\frac{1}{x}\\)\n\\(\\frac{1}{\\sqrt{x}}\\)\n\\(\\log(x)\\)\n\\(\\sqrt{x}\\)\n\\(x\\)\n\\(x^2\\)\n\\(x^3\\)\n\n\n\n\n\n\nHow to use the power ladder for the general distribution shape\n\nIf data are skewed left, we need to compress smaller values towards the rest of the data\n\nGo “up” ladder to transformations with power &gt; 1\n\nIf data are skewed right, we need to compress larger values towards the rest of the data\n\nGo “down” ladder to transformations with power &lt; 1\n\n\n\n\n\nHow to use the power ladder for heteroscedasticity\n\nIf higher \\(X\\) values have more spread\n\nCompress larger values towards the rest of the data\nGo “down” ladder to transformations with power &lt; 1\n\nIf lower \\(X\\) values have more spread\n\nCompress smaller values towards the rest of the data\nGo “up” ladder to transformations with power &gt; 1"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#poll-everywhere-question-3",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#poll-everywhere-question-3",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#transform-dependent-variable",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#transform-dependent-variable",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Transform dependent variable?",
    "text": "Transform dependent variable?\n\nggplot(gapm, aes(x = life_expectancy_years_2011)) +\n  geom_histogram()"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#gladder-of-life-expectancy",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#gladder-of-life-expectancy",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "gladder() of life expectancy",
    "text": "gladder() of life expectancy\n\ngladder(gapm$life_expectancy_years_2011)"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#ladder-of-life-expectancy",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#ladder-of-life-expectancy",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "ladder() of life expectancy",
    "text": "ladder() of life expectancy\n\n\n\nladder() output tests various transformations of the data for normality\nShapiro-Wilkes test is used to assess for normality\n\n\\(H_0\\): data are from a normal population\n\\(H_A\\): data are NOT from a normal population\n\n\n\n\nladder(gapm$life_expectancy_years_2011) %&gt;% \n  gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      Transformation\n      statistic\n      p.value\n    \n  \n  \n    cubic\n0.963\n0.000\n    square\n0.956\n0.000\n    identity\n0.944\n0.000\n    sqrt\n0.935\n0.000\n    log\n0.924\n0.000\n    1/sqrt\n0.911\n0.000\n    inverse\n0.896\n0.000\n    1/square\n0.860\n0.000\n    1/cubic\n0.815\n0.000"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#transform-independent-variable",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#transform-independent-variable",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Transform independent variable?",
    "text": "Transform independent variable?\n\nggplot(gapm, aes(x = female_literacy_rate_2011)) +\n  geom_histogram()"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#gladder-of-female-literacy-rate",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#gladder-of-female-literacy-rate",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "gladder() of female literacy rate",
    "text": "gladder() of female literacy rate\n\ngladder(gapm$female_literacy_rate_2011)"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#ladder-of-female-literacy-rate",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#ladder-of-female-literacy-rate",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "ladder() of female literacy rate",
    "text": "ladder() of female literacy rate\n\n\n\nladder() output tests various transformations of the data for normality\nShapiro-Wilkes test is used to assess for normality\n\n\\(H_0\\): data are from a normal population\n\\(H_A\\): data are NOT from a normal population\n\n\n\n\nladder(gapm$female_literacy_rate_2011) %&gt;% \n  gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      Transformation\n      statistic\n      p.value\n    \n  \n  \n    cubic\n0.850\n0.000\n    square\n0.830\n0.000\n    identity\n0.792\n0.000\n    sqrt\n0.755\n0.000\n    log\n0.693\n0.000\n    1/sqrt\n0.599\n0.000\n    inverse\n0.479\n0.000\n    1/square\n0.264\n0.000\n    1/cubic\n0.159\n0.000"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#tips",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#tips",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Tips",
    "text": "Tips\n\nRecall, assessing our LINE assumptions are not on \\(Y\\) alone!!\n\nWe can use gladder() to get a sense of what our transformations will do to the data, but we need to check with our residuals again!!\n\nTransformations usually work better if all values are positive (or negative)\nIf observation has a 0, then we cannot perform certain transformations\nLog function only defined for positive values\n\nWe might take the \\(log(X+1)\\) if \\(X\\) includes a 0 value\n\nWhen we make cubic or sqaure transformations, we MUST include the original \\(X\\)\n\nWe do not do this for \\(Y\\) though"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#add-quadratic-and-cubic-transformations-to-dataset",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#add-quadratic-and-cubic-transformations-to-dataset",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Add quadratic and cubic transformations to dataset",
    "text": "Add quadratic and cubic transformations to dataset\n\nHelpful to make a new variable with the transformation in your dataset\n\n\ngapm &lt;- gapm %&gt;% \n  mutate(LE_2 = life_expectancy_years_2011^2,\n         LE_3 = life_expectancy_years_2011^3,\n         FLR_2 = female_literacy_rate_2011^2,\n         FLR_3 = female_literacy_rate_2011^3)\n\nglimpse(gapm)\n\nRows: 188\nColumns: 8\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andor…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9, 76.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.9, 99.5,…\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"…\n$ LE_2                       &lt;dbl&gt; 3214.89, 5882.89, 5882.89, 6822.76, 3708.81…\n$ LE_3                       &lt;dbl&gt; 182284.3, 451217.7, 451217.7, 563560.0, 225…\n$ FLR_2                      &lt;dbl&gt; 169.00, 9158.49, NA, NA, 3433.96, 9880.36, …\n$ FLR_3                      &lt;dbl&gt; 2197.0, 876467.5, NA, NA, 201230.1, 982107.…"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#we-are-going-to-compare-a-few-different-models-with-transformations",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#we-are-going-to-compare-a-few-different-models-with-transformations",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "We are going to compare a few different models with transformations",
    "text": "We are going to compare a few different models with transformations\nWe are going to call life expectancy \\(LE\\) and female literacy rate \\(FLR\\)\n\nModel 1: \\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\nModel 2: \\(LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\nModel 3: \\(LE^3 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\nModel 4: \\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\epsilon\\)\nModel 5: \\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon\\)\nModel 6: \\(LE^3 = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon\\)"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#poll-everywhere-question-4",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#poll-everywhere-question-4",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#compare-scatterplots-does-linearity-improve",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#compare-scatterplots-does-linearity-improve",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Compare Scatterplots: does linearity improve?",
    "text": "Compare Scatterplots: does linearity improve?"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#run-models-with-transformations-examples",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#run-models-with-transformations-examples",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Run models with transformations: examples",
    "text": "Run models with transformations: examples\nModel 2: \\(LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\nmodel2 &lt;- lm(LE_2 ~ female_literacy_rate_2011,\n             data = gapm)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n2,401.272\n352.070\n6.820\n0.000\n    female_literacy_rate_2011\n31.174\n4.166\n7.484\n0.000\n  \n  \n  \n\n\n\n\nModel 6: \\(LE^3 = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon\\)\n\nmodel6 &lt;- lm(LE_3 ~ \n               female_literacy_rate_2011 + FLR_2 + FLR_3,\n             data = gapm)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n67,691.796\n149,056.945\n0.454\n0.651\n    female_literacy_rate_2011\n8,092.133\n8,473.154\n0.955\n0.343\n    FLR_2\n−128.596\n147.876\n−0.870\n0.387\n    FLR_3\n0.840\n0.794\n1.059\n0.293"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#normal-q-q-plots-comparison",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#normal-q-q-plots-comparison",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Normal Q-Q plots comparison",
    "text": "Normal Q-Q plots comparison"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#residual-plots-comparison",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#residual-plots-comparison",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Residual plots comparison",
    "text": "Residual plots comparison"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#summary-of-transformations",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#summary-of-transformations",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Summary of transformations",
    "text": "Summary of transformations\n\nIf the model without the transformation is blatantly violating a LINE assumption\n\nThen a transformation is a good idea\n\nIf the model without a transformation is not following the LINE assumptions very well, but is mostly okay\n\nThen try to avoid a transformation\nThink about what predictors might need to be added\nEspecially if you keep seeing the same points as influential\n\nIf interpretability is important in your final work, then transformations are not a great solution"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#models-comparison",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#models-comparison",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Models comparison",
    "text": "Models comparison\n\n# library(gtsummary) for tbl_regression() and tbl_merge()\n\ntbl_model1 &lt;- tbl_regression(model1)\n\ntbl_model2 &lt;- tbl_regression(model2)\n\ntbl_model3 &lt;- tbl_regression(model3)\n\ntbl_model4 &lt;- tbl_regression(model4)\n\ntbl_model5 &lt;- tbl_regression(model5)\n\ntbl_model6 &lt;- tbl_regression(model6)\n\n# Compare models 1-3\ntbl_merge(\n  tbls = list(tbl_model1, tbl_model2, tbl_model3),\n  tab_spanner = c(\"Model 1: y=LE\", \"Model 2: y=LE^2\", \"Model 3: y=LE^3\")\n  )\n\n\n\n\n\n  \n    \n      Characteristic\n      \n        Model 1: y=LE\n      \n      \n        Model 2: y=LE^2\n      \n      \n        Model 3: y=LE^3\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    female_literacy_rate_2011\n0.23\n0.17, 0.29\n&lt;0.001\n31\n23, 39\n&lt;0.001\n3,166\n2,327, 4,006\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n# Compare models 4-6\ntbl_merge(\n  tbls = list(tbl_model4, tbl_model5, tbl_model6),\n  tab_spanner = c(\"Model 4: y=LE\", \"Model 5: y=LE\", \"Model 6: y=LE^3\")\n  )\n\n\n\n\n\n  \n    \n      Characteristic\n      \n        Model 4: y=LE\n      \n      \n        Model 5: y=LE\n      \n      \n        Model 6: y=LE^3\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    female_literacy_rate_2011\n0.02\n-0.38, 0.42\n&gt;0.9\n0.65\n-0.61, 1.9\n0.3\n8,092\n-8,784, 24,968\n0.3\n    FLR_2\n0.00\n0.00, 0.00\n0.3\n-0.01\n-0.03, 0.01\n0.4\n-129\n-423, 166\n0.4\n    FLR_3\n\n\n\n0.00\n0.00, 0.00\n0.3\n0.84\n-0.74, 2.4\n0.3\n  \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#other-fit-statistics-comparison",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#other-fit-statistics-comparison",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Other fit statistics comparison",
    "text": "Other fit statistics comparison\n\nglance(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4109366\n0.4033845\n6.142157\n54.4136\n1.501286e-10\n1\n-257.7164\n521.4329\n528.579\n2942.635\n78\n80\n  \n  \n  \n\n\n\nglance(model2) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4179234\n0.4104609\n812.8336\n56.00298\n9.352191e-11\n1\n-648.5445\n1303.089\n1310.235\n51534476\n78\n80\n  \n  \n  \n\n\n\nglance(model3) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4196986\n0.4122588\n82263.89\n56.41291\n8.285324e-11\n1\n-1017.917\n2041.835\n2048.981\n527853141587\n78\n80\n  \n  \n  \n\n\n\nglance(model4) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4195991\n0.4045238\n6.13629\n27.83346\n8.008115e-10\n2\n-257.1239\n522.2477\n531.7758\n2899.362\n77\n80\n  \n  \n  \n\n\n\nglance(model5) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4278828\n0.4052993\n6.132293\n18.94664\n2.844144e-09\n3\n-256.5488\n523.0977\n535.0078\n2857.981\n76\n80\n  \n  \n  \n\n\n\nglance(model6) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4414424\n0.419394\n81763.02\n20.02158\n1.160111e-09\n3\n-1016.39\n2042.78\n2054.69\n508074577758\n76\n80"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#example-chapter-5-problem-9",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#example-chapter-5-problem-9",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Example: Chapter 5 Problem 9",
    "text": "Example: Chapter 5 Problem 9\n\nIn an experiment designed to describe the dose–response curve for vitamin K, individual rats were depleted of their vitamin K reserves and then fed dried liver for 4 days at different dosage levels.\nThe response of each rat was measured as the concentration of a clotting agent needed to clot a sample of its blood in 3 minutes.\nThe results of the experiment on 12 rats are given in the following table; values are expressed in common logarithms for both dose and response.\n\nNote: by “common logarithm” the authors mean a base 10 logarithm\n\n\n\nQuestion: why did they choose a log-log transformation?\n\n\nrats &lt;- read_excel(\"data/CH05Q09.xls\")\nglimpse(rats)\n\nRows: 12\nColumns: 3\n$ RAT     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ LOGCONC &lt;dbl&gt; 2.65, 2.25, 2.26, 1.95, 1.72, 1.60, 1.55, 1.32, 1.13, 1.07, 0.…\n$ LOGDOSE &lt;dbl&gt; 0.18, 0.33, 0.42, 0.54, 0.65, 0.75, 0.83, 0.92, 1.01, 1.04, 1.…\n\nloglog_plot &lt;- ggplot(rats, aes(x = LOGDOSE, y = LOGCONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Transformed variables\")\nloglog_plot"
  },
  {
    "objectID": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#reference-all-run-models",
    "href": "lessons/07_SLR_Diag_02/07_SLR_Diag_02.html#reference-all-run-models",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Reference: all run models",
    "text": "Reference: all run models\n\n\nModel 2: \\(LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\nmodel2 &lt;- lm(LE_2 ~ female_literacy_rate_2011,\n             data = gapm)\n\ntidy(model2) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n2401.27207\n352.069818\n6.820443\n1.726640e-09\n    female_literacy_rate_2011\n31.17351\n4.165624\n7.483514\n9.352191e-11\n  \n  \n  \n\n\n\n\nModel 3: \\(LE^3 \\sim FLR\\)\n\nmodel3 &lt;- lm(LE_3 ~ female_literacy_rate_2011,\n             data = gapm)\n\ntidy(model3) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n95453.189\n35631.6898\n2.678885\n9.005716e-03\n    female_literacy_rate_2011\n3166.481\n421.5875\n7.510853\n8.285324e-11\n  \n  \n  \n\n\n\n\nModel 4: \\(LE \\sim FLR + FLR^2\\)\n\nmodel4 &lt;- lm(life_expectancy_years_2011 ~ \n               female_literacy_rate_2011 + FLR_2,\n             data = gapm)\n\ntidy(model4) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n57.030875456\n6.282845592\n9.07723652\n8.512585e-14\n    female_literacy_rate_2011\n0.019348795\n0.201021963\n0.09625215\n9.235704e-01\n    FLR_2\n0.001578649\n0.001472592\n1.07202008\n2.870595e-01\n  \n  \n  \n\n\n\n\n\nModel 5: \\(LE \\sim FLR + FLR^2 + FLR^3\\)\n\nmodel5 &lt;- lm(life_expectancy_years_2011 ~ \n               female_literacy_rate_2011 + FLR_2 + FLR_3,\n             data = gapm)\n\ntidy(model5) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n4.732796e+01\n1.117939e+01\n4.2335001\n6.373341e-05\n    female_literacy_rate_2011\n6.517986e-01\n6.354934e-01\n1.0256576\n3.083065e-01\n    FLR_2\n-9.952763e-03\n1.109080e-02\n-0.8973895\n3.723451e-01\n    FLR_3\n6.245016e-05\n5.953283e-05\n1.0490038\n2.975008e-01\n  \n  \n  \n\n\n\n\nModel 6: \\(LE^3 \\sim FLR + FLR^2 + FLR^3\\)\n\nmodel6 &lt;- lm(LE_3 ~ \n               female_literacy_rate_2011 + FLR_2 + FLR_3,\n             data = gapm)\n\ntidy(model6) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n67691.7963283\n1.490569e+05\n0.4541338\n0.6510268\n    female_literacy_rate_2011\n8092.1325988\n8.473154e+03\n0.9550320\n0.3425895\n    FLR_2\n-128.5960879\n1.478757e+02\n-0.8696230\n0.3872447\n    FLR_3\n0.8404736\n7.937625e-01\n1.0588477\n0.2930229\n  \n  \n  \n\n\n\n\n\n\n\n\nLesson 7: SLR 5"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/01_Review_key_info.html#key-dates",
    "href": "lessons/14_Purposeful_selection/01_Review_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01_key_info.html#key-dates",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html",
    "title": "SLR: Inference and Prediction",
    "section": "",
    "text": "Estimate the variance of the residuals\nUsing a hypothesis test, determine if there is enough evidence that population slope \\(\\beta_1\\) is not 0 (applies to \\(\\beta_0\\) as well)\nCalculate and report the estimate and confidence interval for the population slope \\(\\beta_1\\) (applies to \\(\\beta_0\\) as well)\nCalculate and report the estimate and confidence interval for the expected/mean response given \\(X\\)\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)\n\n\n\n\n\n\n\n\n\nWe fit Gapminder data with female literacy rate as our independent variable and life expectancy as our dependent variable\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~\n               female_literacy_rate_2011,\n                 data = gapm)\n# Get regression table:\ntidy(model1) %&gt;% gt() %&gt;% \n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]\n\n\n\n\n\nRows: 188 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (2): life_expectancy_years_2011, female_literacy_rate_2011\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n \nThe (population) regression model is denoted by:\n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable with a…\n\nNormal distribution with mean 0 and constant variance \\(\\sigma^2\\)\ni.e. \\(\\epsilon \\sim N(0, \\sigma^2)\\)"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#lets-revisit-the-regression-analysis-process",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#lets-revisit-the-regression-analysis-process",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Let’s revisit the regression analysis process",
    "text": "Let’s revisit the regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#lets-remind-ourselves-of-the-model-that-we-fit-last-lesson",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#lets-remind-ourselves-of-the-model-that-we-fit-last-lesson",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Let’s remind ourselves of the model that we fit last lesson",
    "text": "Let’s remind ourselves of the model that we fit last lesson\n\nWe fit Gapminder data with female literacy rate as our independent variable and life expectancy as our dependent variable\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\nmodel1 &lt;- gapm %&gt;% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\n# Get regression table:\ntidy(model1) %&gt;% gt() %&gt;% \n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    FemaleLiteracyRate\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#fitted-line-is-derived-from-the-population-slr-model",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#fitted-line-is-derived-from-the-population-slr-model",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Fitted line is derived from the population SLR model",
    "text": "Fitted line is derived from the population SLR model\n \nThe (population) regression model is denoted by:\n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable with a…\n\nNormal distribution with mean 0 and constant variance \\(\\sigma^2\\)\ni.e. \\(\\epsilon \\sim N(0, \\sigma^2)\\)"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#widehatsigma2-needed-ingredient-for-inference",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#widehatsigma2-needed-ingredient-for-inference",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "\\(\\widehat\\sigma^2\\): Needed ingredient for inference",
    "text": "\\(\\widehat\\sigma^2\\): Needed ingredient for inference\n\nRecall our population model residuals are distributed by \\(\\epsilon \\sim N(0, \\sigma^2)\\)\n\nAnd our estimated residuals are \\(\\widehat\\epsilon \\sim N(0, \\widehat\\sigma^2)\\)\n\nHence, the variance of the errors (residuals) is estimated by \\(\\widehat{\\sigma}^2\\)\n\n\n\\[\\widehat{\\sigma}^2 = S_{y|x}^2= \\frac{1}{n-2}\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 =\\frac{1}{n-2}SSE = MSE\\]"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#widehatsigma2-i-hope-r-can-calculate-that-for-me",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#widehatsigma2-i-hope-r-can-calculate-that-for-me",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "\\(\\widehat\\sigma^2\\): I hope R can calculate that for me…",
    "text": "\\(\\widehat\\sigma^2\\): I hope R can calculate that for me…\n\nThe standard deviation \\(\\widehat{\\sigma}\\) is given in the R output as the Residual standard error\n\n\\(4^{th}\\) line from the bottom in the summary() output of the model:\n\n\n\n(m1_sum = summary(model1))\n\n\nCall:\nlm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        50.92790    2.66041  19.143  &lt; 2e-16 ***\nFemaleLiteracyRate  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\nm1_sum$sigma\n\n[1] 6.142157\n\n# number of observations (pairs of data) used to run the model\nnobs(model1) \n\n[1] 80"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#widehatsigma2-to-sse",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#widehatsigma2-to-sse",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "\\(\\widehat\\sigma^2\\) to SSE",
    "text": "\\(\\widehat\\sigma^2\\) to SSE\n\nRecall how we minimized the SSE to find our line of best fit\nSSE and \\(\\widehat\\sigma^2\\) are closely related:\n\n\\[\\begin{aligned}\n\\widehat{\\sigma}^2 & = \\frac{1}{n-2}SSE\\\\\n6.142^2 & = \\frac{1}{80-2}SSE\\\\\nSSE & = 78 \\cdot 6.142^2 = 2942.48\n\\end{aligned}\\]\n\n2942.48 is the smallest sums of squares of all possible regression lines through the data"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#do-we-trust-our-estimate-widehatbeta_1",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#do-we-trust-our-estimate-widehatbeta_1",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Do we trust our estimate \\(\\widehat\\beta_1\\)?",
    "text": "Do we trust our estimate \\(\\widehat\\beta_1\\)?\n\nSo far, we have shown that we think the estimate is 0.232\n\n \n\n\\(\\widehat\\beta_1\\) (coefficient estimate) uses our sample data to estimate the population parameter \\(\\beta_1\\)\n\n \n\nInference helps us figure out mathematically how much we trust our best-fit line\n\n \n\nAre we certain that the relationship between \\(X\\) and \\(Y\\) that we estimated reflects the true, underlying relationship?"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-2",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-2",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#inference-for-the-population-slope-hypothesis-test-and-ci",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#inference-for-the-population-slope-hypothesis-test-and-ci",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Inference for the population slope: hypothesis test and CI",
    "text": "Inference for the population slope: hypothesis test and CI\n\n\n\n\nPopulation model\n\n\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma^2)\\)\n\\(\\sigma^2\\) is the variance of the residuals\n\n\n\n\nSample best-fit (least-squares) line\n\n\n\\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X \\]\nNote: Some sources use \\(b\\) instead of \\(\\widehat{\\beta}\\)\n\n\n\n\n\n \nWe have two options for inference:\n\nConduct the hypothesis test\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote: R reports p-values for 2-sided tests\n\nConstruct a 95% confidence interval for the population slope \\(\\beta_1\\)"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#steps-in-hypothesis-testing",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#steps-in-hypothesis-testing",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Steps in hypothesis testing",
    "text": "Steps in hypothesis testing"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#general-steps-for-hypothesis-test-for-population-slope-beta_1-t-test",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#general-steps-for-hypothesis-test-for-population-slope-beta_1-t-test",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "General steps for hypothesis test for population slope \\(\\beta_1\\) (t-test)",
    "text": "General steps for hypothesis test for population slope \\(\\beta_1\\) (t-test)\n\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions (checked in our Model Evaluation step)\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nOften, we are curious if the coefficient is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(t\\), and follows a Student’s t-distribution.\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic for \\(\\widehat\\beta_1\\) is\n\\[t = \\frac{ \\widehat\\beta_1 - \\beta_1}{ \\text{SE}_{\\widehat\\beta_1}} = \\frac{ \\widehat\\beta_1}{ \\text{SE}_{\\widehat\\beta_1}}\\]\nwhen we assume \\(H_0: \\beta_1 = 0\\) is true.\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(2\\cdot P(T &gt; t)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at the \\(100\\alpha\\%\\) significiance level. There is (sufficient/insufficient) evidence that there is significant association between (\\(Y\\)) and (\\(X\\)) (p-value = \\(P(T &gt; t)\\))."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#standard-error-of-fitted-slope-widehatbeta_1",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#standard-error-of-fitted-slope-widehatbeta_1",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Standard error of fitted slope \\(\\widehat\\beta_1\\)",
    "text": "Standard error of fitted slope \\(\\widehat\\beta_1\\)\n\n   \n\n\n\\[\\text{SE}_{\\widehat\\beta_1} = \\frac{s_{\\textrm{residuals}}}{s_x\\sqrt{n-1}}\\]\n\n\\(\\text{SE}_{\\widehat\\beta_1}\\) is the variability of the statistic \\(\\widehat\\beta_1\\)\n\n\n   \n\n\n\n\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\n\n\n\n\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\n\n\n\\(n\\) is the sample size, or the number of (complete) pairs of points"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#calculating-standard-error-for-widehatbeta_1-12",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#calculating-standard-error-for-widehatbeta_1-12",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Calculating standard error for \\(\\widehat\\beta_1\\) (1/2)",
    "text": "Calculating standard error for \\(\\widehat\\beta_1\\) (1/2)\n\n\nOption 1: Calculate using the formula\n\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# standard deviation of the residuals (Residual standard error in summary() output)\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n# standard deviation of x's\n(s_x &lt;- sd(gapm$FemaleLiteracyRate, na.rm=T))\n\n[1] 21.95371\n\n# number of pairs of complete observations\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(se_b1 &lt;- s_resid/(s_x * sqrt(n-1))) # compare to SE in regression output\n\n[1] 0.03147744"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#calculating-standard-error-for-widehatbeta_1-22",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#calculating-standard-error-for-widehatbeta_1-22",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Calculating standard error for \\(\\widehat\\beta_1\\) (2/2)",
    "text": "Calculating standard error for \\(\\widehat\\beta_1\\) (2/2)\n\n\nOption 2: Use regression table\n\n\n# recall model1_b1 is regression table restricted to b1 row\nmodel1_b1 &lt;-tidy(model1) %&gt;% filter(term == \"FemaleLiteracyRate\")\nmodel1_b1 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 45) %&gt;% fmt_number(decimals = 4)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n0.2322\n0.0315\n7.3766\n0.0000"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#some-important-notes",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#some-important-notes",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Some important notes",
    "text": "Some important notes\n\nToday we are discussing the hypothesis test for a single coefficient\n\n \n\nThe test statistic for a single coefficient follows a Student’s t-distribution\n \n\nIt can also follow an F-distribution, but we will discuss this more with multiple linear regression and multi-level categorical covariates\n\n\n \n\nSingle coefficient testing can be done on any coefficient, but it is most useful for continuous covariates or binary covariates\n \n\nThis is because testing the single coefficient will still tell us something about the overall relationship between the covariate and the outcome\n\n \n\nWe will talk more about this with multiple linear regression and multi-level categorical covariates"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-3",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-3",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-14",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-14",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (1/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (1/4)\n\nSteps 1-4 are setting up our hypothesis test: not much change from the general steps\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions (checked in our Model Evaluation step)\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nWe are testing if the slope is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(t\\), and follows a Student’s t-distribution."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-24",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-24",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/4)\n\n\n\nCompute the value of the test statistic\n\n\n\n\nOption 1: Calculate the test statistic using the values in the regression table\n\n\n# recall model1_b1 is regression table restricted to b1 row\nmodel1_b1 &lt;-tidy(model1) %&gt;% filter(term == \"FemaleLiteracyRate\")\nmodel1_b1 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n(TestStat_b1 &lt;- model1_b1$estimate / model1_b1$std.error)\n\n[1] 7.376557\n\n\n\nOption 2: Get the test statistic value (\\(t^*\\)) from R\n\n\nmodel1_b1 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n0.23\n0.03\n7.38\n0.00"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-34",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-34",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (3/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (3/4)\n\n\n\nCalculate the p-value\n\n\n\n\nThe \\(p\\)-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true\nWe know the probability distribution of the test statistic (the null distribution) assuming \\(H_0\\) is true\nStatistical theory tells us that the test statistic \\(t\\) can be modeled by a \\(t\\)-distribution with \\(df = n-2\\).\n\nWe had 80 countries’ data, so \\(n=80\\)\n\nOption 1: Use pt() and our calculated test statistic\n\n\n(pv = 2*pt(TestStat_b1, df=80-2, lower.tail=F))\n\n[1] 1.501286e-10\n\n\n\nOption 2: Use the regression table output\n\n\nmodel1_b1 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n0.2321951\n0.03147744\n7.376557\n1.501286e-10"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-44",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-44",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (4/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (4/4)\n\n\n\nWrite conclusion for the hypothesis test\n\n\n\nWe reject the null hypothesis that the slope is 0 at the \\(5\\%\\) significance level. There is sufficient evidence that there is significant association between female life expectancy and female literacy rates (p-value &lt; 0.0001)."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#note-on-hypothesis-testing-using-r",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#note-on-hypothesis-testing-using-r",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Note on hypothesis testing using R",
    "text": "Note on hypothesis testing using R\n\nWe can basically skip Step 5 if we are using the “Option 2” route\n\n \n\nIn our assignments: if you use Option 2, Step 5 is optional\n\nUnless I specifically ask for the test statistic!!"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-14",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-14",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (1/4)",
    "text": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (1/4)\n\nSteps 1-4 are setting up our hypothesis test: not much change from the general steps\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions (checked in our Model Evaluation step)\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nWe are testing if the intercept is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_0 = 0\\\\\n\\text{vs. } H_A&: \\beta_0 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThis is the same as the slope. The test statistic is \\(t\\), and follows a Student’s t-distribution."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-24",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-24",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (2/4)",
    "text": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (2/4)\n\n\n\nCompute the value of the test statistic\n\n\n\n\nOption 1: Calculate the test statistic using the values in the regression table\n\n\n# recall model1_b1 is regression table restricted to b1 row\nmodel1_b0 &lt;-tidy(model1) %&gt;% filter(term == \"(Intercept)\")\nmodel1_b0 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n  \n  \n  \n\n\n\n(TestStat_b0 &lt;- model1_b0$estimate / model1_b0$std.error)\n\n[1] 19.1429\n\n\n\nOption 2: Get the test statistic value (\\(t^*\\)) from R\n\n\nmodel1_b0 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-34",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-34",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (3/4)",
    "text": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (3/4)\n\n\n\nCalculate the p-value\n\n\n\n \n\nOption 1: Use pt() and our calculated test statistic\n\n\n(pv = 2*pt(TestStat_b0, df=80-2, lower.tail=F))\n\n[1] 3.325312e-31\n\n\n \n\nOption 2: Use the regression table output\n\n\nmodel1_b0 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9279\n2.660407\n19.1429\n3.325312e-31"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-44",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-44",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (4/4)",
    "text": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (4/4)\n\n\n\nWrite conclusion for the hypothesis test\n\n\n\nWe reject the null hypothesis that the intercept is 0 at the \\(5\\%\\) significance level. There is sufficient evidence that the intercept for the association between average female life expectancy and female literacy rates is different from 0 (p-value &lt; 0.0001).\n   \n\nNote: if we fail to reject \\(H_0\\), then we could decide to remove the intercept from the model to force the regression line to go through the origin (0,0) if it makes sense to do so for the application."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#inference-for-the-population-slope-hypothesis-test-and-ci-1",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#inference-for-the-population-slope-hypothesis-test-and-ci-1",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Inference for the population slope: hypothesis test and CI",
    "text": "Inference for the population slope: hypothesis test and CI\n\n\n\n\nPopulation model\n\n\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma^2)\\)\n\\(\\sigma^2\\) is the variance of the residuals\n\n\n\n\nSample best-fit (least-squares) line\n\n\n\\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X \\]\nNote: Some sources use \\(b\\) instead of \\(\\widehat{\\beta}\\)\n\n\n\n\n\n \nWe have two options for inference:\n\nConduct the hypothesis test\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote: R reports p-values for 2-sided tests\n\nConstruct a 95% confidence interval for the population slope \\(\\beta_1\\)"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#confidence-interval-for-population-slope-beta_1",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#confidence-interval-for-population-slope-beta_1",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Confidence interval for population slope \\(\\beta_1\\)",
    "text": "Confidence interval for population slope \\(\\beta_1\\)\nRecall the general CI formula:\n\\[\\widehat{\\beta}_1 \\pm t_{\\alpha, n-2}^* \\cdot SE_{\\widehat{\\beta}_1}\\]\nTo construct the confidence interval, we need to:\n\nSet our \\(\\alpha\\)-level\nFind \\(\\widehat\\beta_1\\)\nCalculate the \\(t_{n-2}^*\\)\nCalculate \\(SE_{\\widehat{\\beta}_1}\\)"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#calculate-ci-for-population-slope-beta_1-12",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#calculate-ci-for-population-slope-beta_1-12",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Calculate CI for population slope \\(\\beta_1\\) (1/2)",
    "text": "Calculate CI for population slope \\(\\beta_1\\) (1/2)\n\n\n\\[\\widehat{\\beta}_1  \\pm t^*\\cdot SE_{\\beta_1}\\]\n\nwhere \\(t^*\\) is the \\(t\\)-distribution critical value with \\(df = n -2\\).\n\n\n\nOption 1: Calculate using each value\n\nSave values needed for CI:\n\nb1 &lt;- model1_b1$estimate\nSE_b1 &lt;- model1_b1$std.error\n\n\nnobs(model1) # sample size n\n\n[1] 80\n\n(tstar &lt;- qt(.975, df = 80-2))\n\n[1] 1.990847\n\n\nUse formula to calculate each bound\n\n(CI_LB &lt;- b1 - tstar*SE_b1)\n\n[1] 0.1695284\n\n(CI_UB &lt;- b1 + tstar*SE_b1)\n\n[1] 0.2948619"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#calculate-ci-for-population-slope-beta_1-22",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#calculate-ci-for-population-slope-beta_1-22",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Calculate CI for population slope \\(\\beta_1\\) (2/2)",
    "text": "Calculate CI for population slope \\(\\beta_1\\) (2/2)\n\n\n\\[\\widehat{\\beta}_1  \\pm t^*\\cdot SE_{\\beta_1}\\]\n\nwhere \\(t^*\\) is the \\(t\\)-distribution critical value with \\(df = n -2\\).\n\n\n\nOption 2: Use the regression table\n\n\ntidy(model1, conf.int = T) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n45.631\n56.224\n    FemaleLiteracyRate\n0.232\n0.031\n7.377\n0.000\n0.170\n0.295"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#reporting-the-coefficient-estimate-of-the-population-slope",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#reporting-the-coefficient-estimate-of-the-population-slope",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Reporting the coefficient estimate of the population slope",
    "text": "Reporting the coefficient estimate of the population slope\n\nWhen we report our results to someone else, we don’t usually show them our full hypothesis test\n\nIn an informal setting, someone may want to see it\n\nTypically, we report the estimate with the confidence interval\n\nFrom the confidence interval, your audience can also deduce the results of a hypothesis test\n\nOnce we found our CI, we often just write the interpretation of the coefficient estimate:\n\n\n\nGeneral statement for population slope inference\n\n\nFor every increase of 1 unit in the \\(X\\)-variable, there is an expected average increase of \\(\\widehat\\beta_1\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\nIn our example: For every 1% increase in female literacy rate, the average life expectancy is expected to increase, on average, 0.232 years (95% CI: 0.170, 0.295)."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-4",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-4",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#for-reference-quick-ci-for-beta_0",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#for-reference-quick-ci-for-beta_0",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "For reference: quick CI for \\(\\beta_0\\)",
    "text": "For reference: quick CI for \\(\\beta_0\\)\n\nCalculate CI for population intercept \\(\\beta_0\\): \\(\\widehat{\\beta}_0 \\pm t^*\\cdot SE_{\\beta_0}\\)\n\nwhere \\(t^*\\) is the \\(t\\)-distribution critical value with \\(df = n -2\\)\n\nUse the regression table\n\n\ntidy(model1, conf.int = T) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n45.631\n56.224\n    FemaleLiteracyRate\n0.232\n0.031\n7.377\n0.000\n0.170\n0.295\n  \n  \n  \n\n\n\n\n\n\nGeneral statement for population intercept inference\n\n\nThe expected outcome for the \\(Y\\)-variable is (\\(\\widehat\\beta_0\\)) when the \\(X\\)-variable is 0 (95% CI: LB, UB).\n\n\n\nFor example: The expected/average life expectancy is 50.9 years when the female literacy rate is 0 (95% CI: 45.63, 56.22)."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#finding-a-mean-response-given-a-value-of-our-independent-variable",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#finding-a-mean-response-given-a-value-of-our-independent-variable",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Finding a mean response given a value of our independent variable",
    "text": "Finding a mean response given a value of our independent variable\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n    FemaleLiteracyRate\n0.232\n0.031\n7.377\n0.000\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]\n\nWhat is the expected/predicted life expectancy for a country with female literacy rate 60%?\n\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot 60 = 64.82\\]\n\n(y_60 &lt;- 50.9 + 0.232*60)\n\n[1] 64.82\n\n\n\nHow do we interpret the expected value?\n\nWe sometimes call this “predicted” value, since we can technically use a literacy rate that is not in our sample\n\nHow variable is it?"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#mean-responseprediction-with-regression-line",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#mean-responseprediction-with-regression-line",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Mean response/prediction with regression line",
    "text": "Mean response/prediction with regression line\n\n\nRecall the population model:\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma^2)\\)\n\n\nWhen we take the expected value, at a given value \\(X^*\\), the average expected response at \\(X^*\\) is:\n\n\\[\\widehat{E}[Y|X^*] = \\widehat\\beta_0 + \\widehat\\beta_1 X^*\\]\n\n\n\n\n\n\n\n\n\nThese are the points on the regression line\nThe mean responses have variability, and we can calculate a CI for it, for every value of \\(X^*\\)"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#ci-for-population-mean-response-eyx-or-mu_yx",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#ci-for-population-mean-response-eyx-or-mu_yx",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "CI for population mean response (\\(E[Y|X^*]\\) or \\(\\mu_{Y|X^*})\\)",
    "text": "CI for population mean response (\\(E[Y|X^*]\\) or \\(\\mu_{Y|X^*})\\)\n\\[\\widehat{E}[Y|X^*] \\pm t_{n-2}^* \\cdot SE_{\\widehat{E}[Y|X^*]}\\]\n\\[SE_{\\widehat{E}[Y|X^*]} = s_{\\text{residuals}} \\sqrt{\\frac{1}{n} + \\frac{(X^* - \\overline{X})^2}{(n-1)s_X^2}}\\]\n\n\\(\\widehat{E}[Y|X^*]\\) is the predicted value at the specified point \\(X^*\\) of the explanatory variable\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\\(\\overline{X}\\) is the sample mean of the explanatory variable \\(x\\)\n\\(s_X\\) is the sample sd of the explanatory variable \\(X\\)\n\n\n\nRecall that \\(t_{n-2}^*\\) is calculated using qt() and depends on the confidence level (\\(1-\\alpha\\))"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#example-option-1-ci-for-mean-response-mu_yx",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#example-option-1-ci-for-mean-response-mu_yx",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Example Option 1: CI for mean response \\(\\mu_{Y|X^*}\\)",
    "text": "Example Option 1: CI for mean response \\(\\mu_{Y|X^*}\\)\nFind the 95% CI for the mean life expectancy when the female literacy rate is 60.\n\n\\[\\begin{align}\n\\widehat{E}[Y|X^*] &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E}[Y|X^*]}\\\\\n64.8596 &\\pm 1.990847 \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(X^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 0.9675541\\\\\n64.8596 &\\pm 1.926252\\\\\n(62.93335 &, 66.78586)\n\\end{align}\\]\n\n\n\n\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n\n\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$FemaleLiteracyRate, na.rm=T))\n\n[1] 81.65375\n\n(s_x &lt;- sd(gapm$FemaleLiteracyRate, na.rm=T))\n\n[1] 21.95371\n\n\n\n\n\n(SE_Yx &lt;- s_resid *sqrt(1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] 0.9675541\n\n\n\n\n\n(MOE_Yx &lt;- SE_Yx*tstar)\n\n[1] 1.926252\n\n\n\n\n\n\nY60 - MOE_Yx\n\n[1] 62.93335\n\n\n\n\n\n\nY60 + MOE_Yx\n\n[1] 66.78586"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#example-option-2-ci-for-mean-response-mu_yx",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#example-option-2-ci-for-mean-response-mu_yx",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Example Option 2: CI for mean response \\(\\mu_{Y|X^*}\\)",
    "text": "Example Option 2: CI for mean response \\(\\mu_{Y|X^*}\\)\nFind the 95% CI’s for the mean life expectancy when the female literacy rate is 60 and 80.\n\nUse the base R predict() function\nRequires specification of a newdata “value”\n\nThe newdata value is \\(X^*\\)\nThis has to be in the format of a data frame though\nwith column name identical to the predictor variable in the model\n\n\n\nnewdata &lt;- data.frame(FemaleLiteracyRate = c(60, 80)) \nnewdata\n\n  FemaleLiteracyRate\n1                 60\n2                 80\n\n\n\n\n\npredict(model1, \n        newdata=newdata, \n        interval=\"confidence\")\n\n       fit      lwr      upr\n1 64.85961 62.93335 66.78586\n2 69.50351 68.13244 70.87457\n\n\n\n\n\nInterpretation\n\n\nWe are 95% confident that the average life expectancy for a country with a 60% female literacy rate will be between 62.9 and 66.8 years."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-5",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-5",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#confidence-bands-for-mean-response-mu_yx",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#confidence-bands-for-mean-response-mu_yx",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Confidence bands for mean response \\(\\mu_{Y|X^*}\\)",
    "text": "Confidence bands for mean response \\(\\mu_{Y|X^*}\\)\n\nOften we plot the CI for many values of X, creating confidence bands\nThe confidence bands are what ggplot creates when we set se = TRUE within geom_smooth\nThink about it: for what values of X are the confidence bands (intervals) narrowest?\n\n\ngapm %&gt;%\n  ggplot(aes(x=FemaleLiteracyRate, \n             y=LifeExpectancyYrs)) +\n  geom_point()+\n  geom_smooth(method = lm, se=TRUE)+\n  ggtitle(\"Life expectancy vs. female literacy rate\")"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Width of confidence bands for mean response \\(\\mu_{Y|X^*}\\)",
    "text": "Width of confidence bands for mean response \\(\\mu_{Y|X^*}\\)\n\nFor what values of \\(X^*\\) are the confidence bands (intervals) narrowest? widest?\n\n\\[\\begin{align}\n\\widehat{E}[Y|X^*] &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E}[Y|X^*]}\\\\\n\\widehat{E}[Y|X^*] &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(X^* - \\bar{x})^2}{(n-1)s_x^2}}\n\\end{align}\\]\n\n\n\nLesson 4: SLR 2"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro_key_info.html#key-dates",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html",
    "title": "Lesson 11: Interactions Continued",
    "section": "",
    "text": "Interpret the interaction component of a model with two continuous covariates, and how the main variable’s effect changes.\n\n\n\nWhen there are only two covariates in the model, test whether one is a confounder or effect modifier.\n\n\n\n\n\n\nWe can start by visualizing the relationship between life expectancy and female literacy rate by food supply\nQuestions of interest: Does the effect of female literacy rate on life expectancy differ depending on food supply?\n\nThis is the same as: Is food supply is an effect modifier for female literacy rate? Is food supply an effect modifier of the association between life expectancy and female literacy rate?\n\nLet’s run an interaction model to see!\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel we are fitting:\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\beta_3 FLR^c \\cdot FS^c + \\epsilon\\]\n\n\\(LE\\) as life expectancy\n\\(FLR^c\\) as the centered around the mean female literacy rate (continuous variable)\n\\(FS^c\\) as the centered around the mean food supply (continuous variable)\n\n\n\nCode to center FLR and FS\ngapm_sub = gapm_sub %&gt;% \n  mutate(FLR_c = FemaleLiteracyRate - mean(FemaleLiteracyRate), \n         FS_c = FoodSupplykcPPD - mean(FoodSupplykcPPD))\nmean_FS = mean(gapm_sub$FoodSupplykcPPD) %&gt;% round(digits = 0)\nmean_FLR = mean(gapm_sub$FemaleLiteracyRate) %&gt;% round(digits = 2)\n\n\nIn R:\n\nm_int_fs = lm(LifeExpectancyYrs ~ FLR_c + FS_c + FLR_c*FS_c, data = gapm_sub)\n\nOR\n\nm_int_fs = lm(LifeExpectancyYrs ~ FLR_c*FS_c, data = gapm_sub)\n\n\n\n\n\ntidy_m_fs = tidy(m_int_fs, conf.int=T) \ntidy_m_fs %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 5)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n70.32060\n0.72393\n97.13721\n0.00000\n68.87601\n71.76518\n    FLR_c\n0.15532\n0.03808\n4.07905\n0.00012\n0.07934\n0.23130\n    FS_c\n0.00849\n0.00182\n4.67908\n0.00001\n0.00487\n0.01212\n    FLR_c:FS_c\n−0.00001\n0.00008\n−0.06908\n0.94513\n−0.00016\n0.00015\n  \n  \n  \n\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & 70.32 + 0.16 \\cdot FLR^c + 0.01 \\cdot FS^c - 0.00001 \\cdot FLR^c \\cdot FS^c\n\\end{aligned}\\]\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & 70.32 + 0.16 \\cdot FLR^c + 0.01 \\cdot FS^c - 0.00001 \\cdot FLR^c \\cdot FS^c\n\\end{aligned}\\]\nTo identify different lines, we need to pick example values of Food Supply:\n\n\n\n\nFood Supply of 1812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot (-1000) + \\\\ & \\widehat\\beta_3 FLR^c \\cdot (-1000) \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 - 1000 \\widehat\\beta_2 \\big)+ \\\\ & \\big(\\widehat\\beta_1 - 1000 \\widehat\\beta_3  \\big) FLR^c\n\\end{aligned}\\]\n\n\n\n\n\nFood Supply of 2812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\\\ & \\widehat\\beta_3 FLR^c \\cdot 0 \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 \\big)+ \\\\ & \\big(\\widehat\\beta_1 \\big) FLR^c\n\\end{aligned}\\]\n\n\n\n\n\nFood Supply of 3812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot 1000 + \\\\ & \\widehat\\beta_3 FLR^c \\cdot 1000 \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 + 1000 \\widehat\\beta_2 \\big)+ \\\\ & \\big(\\widehat\\beta_1 + 1000 \\widehat\\beta_3  \\big) FLR^c\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_2 \\cdot FS^c \\bigg] + \\underbrace{\\bigg[\\widehat\\beta_1 + \\widehat\\beta_3 \\cdot FS^c \\bigg]}_\\text{FLR's effect} FLR \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in female literacy rate’s effect, for every one kcal PPD increase in food supply\n\nIn summary, the interaction term can be interpreted as “difference in adjusted female literacy rate effect for every 1 kcal PPD increase in food supply”\nIt will be helpful to test the interaction to round out this interpretation!!\n\n\n\n\n\nWe run an F-test for a single coefficients (\\(\\beta_3\\)) in the below model (see lesson 9)\n\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\beta_3 FLR^c \\cdot FS^c + \\epsilon\\]\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\[\\beta_3=0\\]\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\[\\beta_3\\neq0\\]\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\epsilon\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}\nLE = & \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\\\ & \\beta_3 FLR^c \\cdot FS^c + \\epsilon\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\nFit the reduced and full model\n\n\nm_int_fs_red = lm(LifeExpectancyYrs ~ FLR_c + FS_c, \n                   data = gapm_sub)\nm_int_fs_full = lm(LifeExpectancyYrs ~ FLR_c + FS_c +\n                  FLR_c*FS_c, data = gapm_sub)\n\n\n\nDisplay the ANOVA table with F-statistic and p-value\nanova(m_int_fs_red, m_int_fs_full) %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FLR_c + FS_c\n69.000\n2,005.556\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FLR_c + FS_c + FLR_c * FS_c\n68.000\n2,005.415\n1.000\n0.141\n0.005\n0.945\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and food supply (p = 0.945). Food supply is not an effect modifier of the association between female literacy rate and life expectancy."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#do-we-think-food-supply-is-an-effect-modifier-for-female-literacy-rate",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#do-we-think-food-supply-is-an-effect-modifier-for-female-literacy-rate",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Do we think food supply is an effect modifier for female literacy rate?",
    "text": "Do we think food supply is an effect modifier for female literacy rate?\n\n\n\nWe can start by visualizing the relationship between life expectancy and female literacy rate by food supply\nQuestions of interest: Does the effect of female literacy rate on life expectancy differ depending on food supply?\n\nThis is the same as: Is food supply is an effect modifier for female literacy rate? Is food supply an effect modifier of the association between life expectancy and female literacy rate?\n\nLet’s run an interaction model to see!"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#model-with-interaction-between-two-continuous-variables",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#model-with-interaction-between-two-continuous-variables",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Model with interaction between two continuous variables",
    "text": "Model with interaction between two continuous variables\nModel we are fitting:\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\beta_3 FLR^c \\cdot FS^c + \\epsilon\\]\n\n\\(LE\\) as life expectancy\n\\(FLR^c\\) as the centered around the mean female literacy rate (continuous variable)\n\\(FS^c\\) as the centered around the mean food supply (continuous variable)\n\n\n\nCode to center FLR and FS\ngapm_sub = gapm_sub %&gt;% \n  mutate(FLR_c = FemaleLiteracyRate - mean(FemaleLiteracyRate), \n         FS_c = FoodSupplykcPPD - mean(FoodSupplykcPPD))\nmean_FS = mean(gapm_sub$FoodSupplykcPPD) %&gt;% round(digits = 0)\nmean_FLR = mean(gapm_sub$FemaleLiteracyRate) %&gt;% round(digits = 2)\n\n\nIn R:\n\nm_int_fs = lm(LifeExpectancyYrs ~ FLR_c + FS_c + FLR_c*FS_c, data = gapm_sub)\n\nOR\n\nm_int_fs = lm(LifeExpectancyYrs ~ FLR_c*FS_c, data = gapm_sub)"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#displaying-the-regression-table-and-writing-fitted-regression-equation",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#displaying-the-regression-table-and-writing-fitted-regression-equation",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Displaying the regression table and writing fitted regression equation",
    "text": "Displaying the regression table and writing fitted regression equation\n\ntidy(m_int_wr_inc, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 25) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n60.850\n1.281\n47.488\n0.000\n58.290\n63.410\n    income_levels2Higher income\n2.100\n2.865\n0.733\n0.466\n−3.624\n7.824\n    four_regionsAmericas\n10.800\n3.844\n2.810\n0.007\n3.121\n18.479\n    four_regionsAsia\n7.467\n1.957\n3.815\n0.000\n3.556\n11.377\n    four_regionsEurope\n11.500\n2.865\n4.014\n0.000\n5.776\n17.224\n    income_levels2Higher income:four_regionsAmericas\n2.640\n4.896\n0.539\n0.592\n−7.141\n12.421\n    income_levels2Higher income:four_regionsAsia\n1.543\n3.956\n0.390\n0.698\n−6.360\n9.447\n    income_levels2Higher income:four_regionsEurope\n2.382\n4.020\n0.592\n0.556\n−5.649\n10.412\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\widehat\\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 60.85 + 2.10 \\cdot I(\\text{high income}) + 10.8 \\cdot I(\\text{Americas}) + 7.47\\cdot  I(\\text{Asia}) + 11.50 \\cdot I(\\text{Europe}) + \\\\ & 2.64 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + 1.54 \\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & 2.38 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#comparing-fitted-regression-lines-for-various-food-supply-values",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#comparing-fitted-regression-lines-for-various-food-supply-values",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Comparing fitted regression lines for various food supply values",
    "text": "Comparing fitted regression lines for various food supply values\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & 70.32 + 0.16 \\cdot FLR^c + 0.01 \\cdot FS^c - 0.00001 \\cdot FLR^c \\cdot FS^c\n\\end{aligned}\\]\nTo identify different lines, we need to pick example values of Food Supply:\n\n\n\n\nFood Supply of 1812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot (-1000) + \\\\ & \\widehat\\beta_3 FLR^c \\cdot (-1000) \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 - 1000 \\widehat\\beta_2 \\big)+ \\\\ & \\big(\\widehat\\beta_1 - 1000 \\widehat\\beta_3  \\big) FLR^c\n\\end{aligned}\\]\n\n\n\n\n\nFood Supply of 2812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\\\ & \\widehat\\beta_3 FLR^c \\cdot 0 \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 \\big)+ \\\\ & \\big(\\widehat\\beta_1 \\big) FLR^c\n\\end{aligned}\\]\n\n\n\n\n\nFood Supply of 3812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot 1000 + \\\\ & \\widehat\\beta_3 FLR^c \\cdot 1000 \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 + 1000 \\widehat\\beta_2 \\big)+ \\\\ & \\big(\\widehat\\beta_1 + 1000 \\widehat\\beta_3  \\big) FLR^c\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#interpretation-for-interaction-between-two-continuous-variables",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#interpretation-for-interaction-between-two-continuous-variables",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Interpretation for interaction between two continuous variables",
    "text": "Interpretation for interaction between two continuous variables\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_2 \\cdot FS^c \\bigg] + \\underbrace{\\bigg[\\widehat\\beta_1 + \\widehat\\beta_3 \\cdot FS^c \\bigg]}_\\text{FLR's effect} FLR \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in female literacy rate’s effect, for every one kcal PPD increase in food supply\n\nIn summary, the interaction term can be interpreted as “difference in adjusted female literacy rate effect for every 1 kcal PPD increase in food supply”\nIt will be helpful to test the interaction to round out this interpretation!!"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#test-interaction-between-two-continuous-variables",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#test-interaction-between-two-continuous-variables",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Test interaction between two continuous variables",
    "text": "Test interaction between two continuous variables\n\nWe run an F-test for a single coefficients (\\(\\beta_3\\)) in the below model (see lesson 9)\n\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\beta_3 FLR^c \\cdot FS^c + \\epsilon\\]\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\[\\beta_3=0\\]\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\[\\beta_3\\neq0\\]\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\epsilon\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}\nLE = & \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\\\ & \\beta_3 FLR^c \\cdot FS^c + \\epsilon\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#test-interaction-between-two-continuous-variables-1",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#test-interaction-between-two-continuous-variables-1",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Test interaction between two continuous variables",
    "text": "Test interaction between two continuous variables\n\nFit the reduced and full model\n\n\nm_int_fs_red = lm(LifeExpectancyYrs ~ FLR_c + FS_c, \n                   data = gapm_sub)\nm_int_fs_full = lm(LifeExpectancyYrs ~ FLR_c + FS_c +\n                  FLR_c*FS_c, data = gapm_sub)\n\n\n\nDisplay the ANOVA table with F-statistic and p-value\nanova(m_int_fs_red, m_int_fs_full) %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FLR_c + FS_c\n69.000\n2,005.556\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FLR_c + FS_c + FLR_c * FS_c\n68.000\n2,005.415\n1.000\n0.141\n0.005\n0.945\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and food supply (p = 0.945). Food supply is not an effect modifier of the association between female literacy rate and life expectancy."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#deciding-between-confounder-and-effect-modifier",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#deciding-between-confounder-and-effect-modifier",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Deciding between confounder and effect modifier",
    "text": "Deciding between confounder and effect modifier\n\nThis is more of a model selection question (in coming lectures)\nBut if we had a model with only TWO covariates, we could step through the following process:\n\nTest the interaction (of potential effect modifier): use a partial F-test to test if interaction term(s) explain enough variation compared to model without interaction\n\nRecall that for two continuous covariates, we will test a single coefficient\nFor a binary and continuous covariate, we will test a single coefficient\nFor two binary categorical covariates, we will test a single coefficient\nFor a multi-level categorical covariate (with any other type of covariate), we must test a group of coefficients!!\n\nThen look at the main effect (or potential confounder)\n\nIf interaction already included, then automatically included as main effect (and thus not checked for confounding)\nFor variables that are not included in any interactions:\n\nCheck to see if they are confounders by seeing whether exclusion of the variable changes any of the main effect of the primary explanatory variable by more than 10%"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#reminder-from-lesson-9-general-steps-for-f-test",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#reminder-from-lesson-9-general-steps-for-f-test",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Reminder from Lesson 9: General steps for F-test",
    "text": "Reminder from Lesson 9: General steps for F-test\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1=\\beta_2= \\ldots=\\beta_k=0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=1, 2, \\ldots, k\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}} = \\frac{MSR_{full}}{MSE_{full}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#step-1-testing-the-interaction",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#step-1-testing-the-interaction",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Step 1: Testing the interaction",
    "text": "Step 1: Testing the interaction\n\nWe test with \\(\\alpha = 0.10\\)\nFollow the F-test procedure in Lesson 9 (MLR: Inference/F-test)\n\nThis means we need to follow the 7 steps of the general F-test in previous slide (taken from Lesson 9)\n\nUse the hypothesis tests for the specific variable combo:\n\n\n\n\n\nBinary & continuous variable (Lesson 11, LOB 2)\n\n\nTesting a single coefficient for the interaction term using F-test comparing full model to reduced model\n\n\n\n\n\nMulti-level & continuous variables (Lesson 11, LOB 3)\n\n\nTesting group of coefficients for the interaction terms using F-test comparing full to reduced model\n\n\n\n\n\n\n\n\nBinary & multi-level variable (Lesson 11, LOB 4)\n\n\nTesting group of coefficients for the interaction terms using F-test comparing full to reduced model\n\n\n\n\n\nTwo continuous variables (Lesson 11, LOB 5)\n\n\nTesting a single coefficient for the interaction term using F-test comparing full to reduced model"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#poll-everywhere-questions-2-4",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#poll-everywhere-questions-2-4",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Poll Everywhere Questions 2-4",
    "text": "Poll Everywhere Questions 2-4"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-testing-a-confounder",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-testing-a-confounder",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Step 2: Testing a confounder",
    "text": "Step 2: Testing a confounder\n\nIf interaction already included:\n\nMeaning: F-test showed evidence for alternative/full model\nThen the variable is an effect modifier and we don’t need to consider it as a confounder\nThen automatically included as main effect (and thus not checked for confounding)\n\nFor variables that are not included in any interactions:\n\nCheck to see if they are confounders\nOne way to do this is by seeing whether exclusion of the variable changes any of the main effect of the primary explanatory variable by more than 10%\n\nIf the main effect of the primary explanatory variable changes by less than 10%, then the additional variable is neither an effect modifier nor a confounder\n\nWe leave the variable out of the model"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#testing-for-percent-change-delta-in-a-coefficient",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#testing-for-percent-change-delta-in-a-coefficient",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Testing for percent change ( \\(\\Delta\\%\\)) in a coefficient",
    "text": "Testing for percent change ( \\(\\Delta\\%\\)) in a coefficient\n\nLet’s say we have \\(X_1\\) and \\(X_2\\), and we specifically want to see if \\(X_2\\) is a confounder for \\(X_1\\) (the explanatory variable or variable of interest)\nIf we are only considering \\(X_1\\) and \\(X_2\\), then we need to run the following two models:\n\nFitted model 1 / reduced model (mod1): \\(\\widehat{Y} = \\widehat\\beta_0 + \\widehat\\beta_1X_1\\)\n\nWe call the above \\(\\widehat\\beta_1\\) the reduced model coefficient: \\(\\widehat\\beta_{1, \\text{mod1}}\\) or \\(\\widehat\\beta_{1, \\text{red}}\\)\n\nFitted model 2 / Full model (mod2): \\(\\widehat{Y} = \\widehat\\beta_0 + \\widehat\\beta_1X_1 +\\widehat\\beta_2X_2\\)\n\nWe call this \\(\\widehat\\beta_1\\) the full model coefficient: \\(\\widehat\\beta_{1, \\text{mod2}}\\) or \\(\\widehat\\beta_{1, \\text{full}}\\)\n\n\n\n\n\n\n\n\n\nCalculation for % change in coefficient\n\n\n\\[\n\\Delta\\% = 100\\% \\cdot\\frac{\\widehat\\beta_{1, \\text{mod1}} - \\widehat\\beta_{1, \\text{mod2}}}{\\widehat\\beta_{1, \\text{mod2}}} = 100\\% \\cdot \\frac{\\widehat\\beta_{1, \\text{red}} - \\widehat\\beta_{1, \\text{full}}}{\\widehat\\beta_{1, \\text{full}}}\n\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#is-food-supply-a-confounder-for-female-literacy-rate-13",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#is-food-supply-a-confounder-for-female-literacy-rate-13",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Is food supply a confounder for female literacy rate? (1/3)",
    "text": "Is food supply a confounder for female literacy rate? (1/3)\n\nRun models with and without food supply:\n \n\nModel 1 (reduced): \\(LE = \\beta_0 + \\beta_1 FLR^c + \\epsilon\\)\n\n\nmod1_red = lm(LifeExpectancyYrs ~ FLR_c, data = gapm_sub)\n\n \n\nModel 2 (full): \\(LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\epsilon\\)\n\n\nmod2_full = lm(LifeExpectancyYrs ~ FLR_c + FS_c, data = gapm_sub)\n\n\n \n\nNote that the full model when testing for confounding was the reduced model for testing an interaction\nFull and reduced are always relative qualifiers of the models that we are testing"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#is-food-supply-a-confounder-for-female-literacy-rate-23",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#is-food-supply-a-confounder-for-female-literacy-rate-23",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Is food supply a confounder for female literacy rate? (2/3)",
    "text": "Is food supply a confounder for female literacy rate? (2/3)\n\nRecord the coefficient estimate for centered female literacy rate in both models:\n\n\nModel 1 (reduced):\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n70.29722\n0.72578\n96.85709\n0.00000\n68.84969\n71.74475\n    FLR_c\n0.22990\n0.03219\n7.14139\n0.00000\n0.16570\n0.29411\n  \n  \n  \n\n\n\n\n\nModel 2 (full):\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n70.29722\n0.63537\n110.63985\n0.00000\n69.02969\n71.56475\n    FLR_c\n0.15670\n0.03216\n4.87271\n0.00001\n0.09254\n0.22085\n    FS_c\n0.00848\n0.00179\n4.72646\n0.00001\n0.00490\n0.01206\n  \n  \n  \n\n\n\n\n\nCalculate the percent change:\n\n\\[\n\\Delta\\% = 100\\% \\cdot \\frac{\\widehat\\beta_{1, \\text{mod1}} - \\widehat\\beta_{1, \\text{mod2}}}{\\widehat\\beta_{1, \\text{mod2}}} = 100\\% \\cdot \\frac{0.22990 - 0.15670}{0.15670} = 46.71\\%\n\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#is-food-supply-a-confounder-for-female-literacy-rate-33",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#is-food-supply-a-confounder-for-female-literacy-rate-33",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Is food supply a confounder for female literacy rate? (3/3)",
    "text": "Is food supply a confounder for female literacy rate? (3/3)\nThe percent change in female literacy rate’s coefficient estimate was 46.71%.\nThus, food supply is a confounder of female literacy rate in the association between life expectancy and female literacy rate."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#lets-try-this-out-on-one-of-our-potential-effect-modifiers-or-confounders",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#lets-try-this-out-on-one-of-our-potential-effect-modifiers-or-confounders",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Let’s try this out on one of our potential effect modifiers or confounders",
    "text": "Let’s try this out on one of our potential effect modifiers or confounders\n\n\n\nLook back at income level and world region: is income level an effect modifier, confounder, or has no effect on the association between life expectancy and world region?\nWe can start by visualizing the relationship between life expectancy and world region by income level\nSo we’ll need to revisit the work we did in previous slides on the interaction, then check fo condounding"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#determining-if-income-level-is-an-effect-modifier-confounder-or-neither",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#determining-if-income-level-is-an-effect-modifier-confounder-or-neither",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Determining if income level is an effect modifier, confounder, or neither",
    "text": "Determining if income level is an effect modifier, confounder, or neither\n\nStep 1: Testing the interaction/effect modifier\n\nCompare model with and without interaction using F-test to see if interaction is significant\nModels\n\nModel 1 (red): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\beta_4 I(\\text{high income})+ \\epsilon \\end{aligned}\\)\n\n \n\nModel 2 (full): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\beta_4 I(\\text{high income})+ \\\\ & \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\)\n\n\nStep 2: Testing a confounder (only if not an effect modifier)\n\nCompare model with and without main effect for additional variable (income level) using F-test to see if additional variable (income level) is a confounder\nModels\n\nModel 1 (reduced): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\epsilon \\end{aligned}\\)\nModel 2 (full): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\beta_4 I(\\text{high income})+ \\epsilon \\end{aligned}\\)"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#step-1-results-from-lesson-11-lob-4",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#step-1-results-from-lesson-11-lob-4",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Step 1: Results from Lesson 11 LOB 4",
    "text": "Step 1: Results from Lesson 11 LOB 4\n\nFit the reduced and full model\n\n\nm_int_wr_inc_red = lm(LifeExpectancyYrs ~ income_levels2 + four_regions, \n                   data = gapm_sub)\nm_int_wr_inc_full = lm(LifeExpectancyYrs ~ income_levels2 + four_regions +\n                          income_levels2*four_regions, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ income_levels2 + four_regions\n67.000\n1,693.242\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ income_levels2 + four_regions + income_levels2 * four_regions\n64.000\n1,681.304\n3.000\n11.938\n0.151\n0.928\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between world region and income level (p = 0.928).\nThus, income level is not an effect modifier of world region. However, we can continue to test if income level is a confounder."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-see-if-income-is-a-confounder",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-see-if-income-is-a-confounder",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Step 2: See if income is a confounder",
    "text": "Step 2: See if income is a confounder\n\nFit the reduced and full model for testing the confounder\n\n \n\nModel 1 (reduced): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\epsilon \\end{aligned}\\)\n\n\nmod1_wr_inc_red = lm(LifeExpectancyYrs ~ four_regions, \n                   data = gapm_sub)\n\n \n\nModel 2 (full): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\beta_4 I(\\text{high income})+ \\epsilon \\end{aligned}\\)\n\n\nmod1_wr_inc_full = lm(LifeExpectancyYrs ~ four_regions + income_levels2, \n                   data = gapm_sub)"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-see-if-income-is-a-confounder-1",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-see-if-income-is-a-confounder-1",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Step 2: See if income is a confounder",
    "text": "Step 2: See if income is a confounder\n\nRecord the coefficient estimate for centered female literacy rate in both models:\nModel 1 (reduced):\\(\\begin{aligned}\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{Americas}) + \\widehat\\beta_2 I(\\text{Asia}) + \\widehat\\beta_3 I(\\text{Europe}) \\end{aligned}\\)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n61.27000\n1.16508\n52.58870\n0.00000\n58.94512\n63.59488\n    four_regionsAmericas\n14.33000\n1.90257\n7.53193\n0.00000\n10.53349\n18.12651\n    four_regionsAsia\n8.11824\n1.71883\n4.72313\n0.00001\n4.68837\n11.54810\n    four_regionsEurope\n14.78217\n1.59304\n9.27924\n0.00000\n11.60332\n17.96103\n  \n  \n  \n\n\n\n\n\nModel 2 (full): \\(\\begin{aligned}\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{Americas}) + \\widehat\\beta_2 I(\\text{Asia}) + \\widehat\\beta_3 I(\\text{Europe}) + \\widehat\\beta_4 I(\\text{high income})\\end{aligned}\\)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n60.54716\n1.16190\n52.11048\n0.00000\n58.22800\n62.86632\n    four_regionsAmericas\n12.04102\n2.05816\n5.85038\n0.00000\n7.93292\n16.14912\n    four_regionsAsia\n7.77808\n1.66414\n4.67394\n0.00001\n4.45645\n11.09971\n    four_regionsEurope\n12.51938\n1.79139\n6.98864\n0.00000\n8.94375\n16.09501\n    income_levels2Higher income\n3.61419\n1.46967\n2.45917\n0.01651\n0.68070\n6.54767"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-see-if-income-is-a-confounder-2",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-see-if-income-is-a-confounder-2",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Step 2: See if income is a confounder",
    "text": "Step 2: See if income is a confounder\n\nCalculate the percent change for \\(\\widehat\\beta_1\\): \\[\n\\Delta\\%  = 100\\% \\cdot \\frac{\\widehat\\beta_{1, \\text{mod1}} - \\widehat\\beta_{1, \\text{mod2}}}{\\widehat\\beta_{1, \\text{mod2}}} = 100\\% \\cdot \\frac{14.33000 - 12.04102}{12.04102} = 19.01\n\\]\nCalculate the percent change for \\(\\widehat\\beta_2\\): \\[\n\\Delta\\%  = 100\\% \\cdot \\frac{\\widehat\\beta_{2, \\text{mod1}} - \\widehat\\beta_{2, \\text{mod2}}}{\\widehat\\beta_{2, \\text{mod2}}} = 100\\% \\cdot \\frac{8.11824 - 7.77808}{7.77808} = 4.37\n\\]\nCalculate the percent change for \\(\\widehat\\beta_3\\): \\[\n\\Delta\\%  = 100\\% \\cdot \\frac{\\widehat\\beta_{3, \\text{mod1}} - \\widehat\\beta_{3, \\text{mod2}}}{\\widehat\\beta_{3, \\text{mod2}}} = 100\\% \\cdot \\frac{14.78217 - 12.51938}{12.51938} = 18.07\n\\]\nNote that two of these % changes are greater than 10%, and one is less than 10%…"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-see-if-income-is-a-confounder-3",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#step-2-see-if-income-is-a-confounder-3",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Step 2: See if income is a confounder",
    "text": "Step 2: See if income is a confounder\n\n\n\nThere is no set rule when we have more than one estimated coefficient that we examine for confoundeing\nIn this, I would consider\n\nThe majority of coefficients (2/3 coefficients) changes more than 10%\nThe change in coefficients for all three are in the same direction\nThe plot of life expectancy vs world region by income level have a shift in mean life expectancy from lower to higher income level\n\nThus, I would conclude that income level is a confounder, so we would leave income level’s main effect in the model"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#if-you-want-extra-practice",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#if-you-want-extra-practice",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "If you want extra practice",
    "text": "If you want extra practice\n\nTry out this procedure to determine if a variable is an effect modifier or confounder or nothing on the other interactions we tested out in Lesson 11"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#general-interpretation-of-the-interaction-term-reference",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#general-interpretation-of-the-interaction-term-reference",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "General interpretation of the interaction term (reference)",
    "text": "General interpretation of the interaction term (reference)\n\\(E[Y\\mid X_{1},X_{2} ]=\\beta_0 + \\underbrace{(\\beta_1+\\beta_3X_{2}) }_\\text{$X_{1}$'s effect} X_{1}+ \\underbrace{\\beta_2X_{2}}_\\text{$X_{2}$ held constant}\\)\n\\({\\color{white}{E[Y\\mid X_{1},X_{2} ]}}=\\beta_0 + \\underbrace{(\\beta_2+\\beta_3X_{1}) }_\\text{$X_{2}$'s effect}X_{2} + \\underbrace{\\beta_1X_{1}}_\\text{$X_{1}$ held constant}\\)\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in \\(X_{1}\\)’s effect, per unit increase in \\(X_{2}\\);\n\n\\(\\beta_3\\) = mean change in \\(X_{2}\\)’s effect, per unit increase in \\(X_{1}\\);\nwhere the “\\(X_{1}\\) effect” equals the change in \\(E[Y]\\) per unit increase in \\(X_{1}\\) with \\(X_{2}\\) held constant, i.e. “adjusted \\(X_{1}\\) effect”\n\nIn summary, the interaction term can be interpreted as “difference in adjusted \\(X_1\\) (or \\(X_2\\)) effect per unit increase in \\(X_2\\) (or \\(X_1\\))”"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#a-glimpse-at-how-interactions-might-be-incorporated-into-model-selection",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#a-glimpse-at-how-interactions-might-be-incorporated-into-model-selection",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "A glimpse at how interactions might be incorporated into model selection",
    "text": "A glimpse at how interactions might be incorporated into model selection\n\nIdentify outcome (Y) and primary explanatory (X) variables\nDecide which other variables might be important and could be potential confounders. Add these to the model.\n\nThis is often done by indentifying variables that previous research deemed important, or researchers believe could be important\nFrom a statistical perspective, we often include variables that are significantly associated with the outcome (in their respective SLR)\n\n(Optional step) Test 3 way interactions\n\nThis makes our model incredibly hard to interpret. Our class will not cover this!!\nWe will skip to testing 2 way interactions\n\nTest 2 way interactions\n\nWhen testing a 2 way interaction, make sure the full and reduced models contain the main effects\nFirst test all the 2 way interactions together using a partial F-test (with \\(alpha = 0.10\\))\n\nIf this test not significant, do not test 2-way interactions individually\nIf partial F-test is significant, then test each of the 2-way interactions\n\n\nRemaining main effects - to include of not to include?\n\nFor variables that are included in any interactions, they will be automatically included as main effects and thus not checked for confounding\nFor variables that are not included in any interactions:\n\nCheck to see if they are confounders by seeing whether exclusion of the variable(s) changes any of the coefficient of the primary explanatory variable (including interactions) X by more than 10%\n\nIf any of X’s coefficients change when removing the potential confounder, then keep it in the model\n\n\n\n\n\n\nLesson 12: Interactions 2"
  },
  {
    "objectID": "lessons/11_Interactions_01/01_Review_key_info.html#key-dates",
    "href": "lessons/11_Interactions_01/01_Review_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/13_Model_selection/01_Review_key_info.html#key-dates",
    "href": "lessons/13_Model_selection/01_Review_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html",
    "href": "lessons/13_Model_selection/13_Model_selection.html",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Understand the motivation for model selection, including bias-variance trade off and alignment of research goals (association vs. prediction)\nExplain the general process or idea behind different model selection techniques\nRecognize common model fit statistics and understand what they measure"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#why-cant-i-just-throw-in-all-the-variables-into-my-model",
    "href": "lessons/13_Model_selection/13_Model_selection.html#why-cant-i-just-throw-in-all-the-variables-into-my-model",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Why can’t I just throw in all the variables into my model?",
    "text": "Why can’t I just throw in all the variables into my model?\n\nFirst, let’s think about the number of observations in our dataset\nFor example: In the Gapminder dataset, I can use an indicator for each country\n\nRemember that each country is an observation\nSo we have a perfectly fit model - a covariate for each observation\nBut we cannot generalize this to any other countries\nAnd we haven’t identified any meaningful relationships between life expectancy and other measured characteristics\n\nMore covariates in the model is not always better\n\nOverfitting the data limits our generalizability and prevents us from answering research questions"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#model-complexity-vs.-parsimony",
    "href": "lessons/13_Model_selection/13_Model_selection.html#model-complexity-vs.-parsimony",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Model Complexity vs. Parsimony",
    "text": "Model Complexity vs. Parsimony\n\n\nSuppose we have \\(p = 30\\) covariates (in the true model) and n = 50 observations. We could consider the following two alternatives:\n\nWe could fit a model using all of the covariates.\n\nIn this case, \\(\\widehat\\beta\\) is unbiased for \\(\\beta\\) (in a linear model fit using OLS). But \\(\\widehat\\beta\\) has very high variance.\n\nWe could fit a model using only the five strongest covariates.\n\nIn this case, \\(\\widehat\\beta\\) will be biased for \\(\\beta\\), but it will have lower variance (compared to the estimate including all covariates)\n\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#bias-variance-trade-off",
    "href": "lessons/13_Model_selection/13_Model_selection.html#bias-variance-trade-off",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Bias-variance trade off",
    "text": "Bias-variance trade off\n\n\n\nRecall mean square error is a function of SSE (sum of squared residuals)\n\\[\nMSE = \\dfrac{1}{n} \\sum_{i=1}^{n} \\big(Y_i - \\widehat{Y}_i \\big)^2\n\\]\nMSE can also be written as a function of the bias and variance\n\\[\nMSE = \\text{bias}\\big(\\widehat\\beta\\big)^2 + \\text{variance}\\big(\\widehat\\beta\\big)\n\\]\nFor the same data:\n\nMore covariates in model: less bias, more variance\nLess covariates in model: more bias, less variance\n\nOut goal: find a model with just the right amount of covariates to balance bias and vairance\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#some-important-definitions",
    "href": "lessons/13_Model_selection/13_Model_selection.html#some-important-definitions",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Some important definitions",
    "text": "Some important definitions\n\nModel selection: picking the “best” model from a set of possible models\n\nModels will have the same outcome, but typically differ by the covariates that are included, their transformations, and their interactions\n\n\n \n\nModel selection strategies: a process or framework that helps us pick our “best” model\n\nThese strategies often differ by the approach and criteria used to the determine the “best” model\n\n\n \n\nOverfitting: result of fitting a model so closely to our particular sample data that it cannot be generalized to other samples (or the population)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#model-selection-basics-slide-adjusted-from-jodi-lapidus",
    "href": "lessons/13_Model_selection/13_Model_selection.html#model-selection-basics-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Model Selection basics (slide adjusted from Jodi Lapidus)",
    "text": "Model Selection basics (slide adjusted from Jodi Lapidus)\n\n“Because models always fall far short of the complex reality under study, there are no best or optimal strategies for modeling.”\n\nFrom: Statistical Foundations for Model-Based Adjustments\n\nNot all statistical texts provide practical advice on model development\n\nA lot of resources include methods/code to compare models, but does not include much advice re: selecting which model to ultimately use.\nOther texts are sparse on details or incorporate simplistic approaches\n\nModel development strategy should align with research goals\n\nPrediction vs. Estimating Association\nStrategy may depend on study design and data set size"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#the-goals-of-association-vs.-prediction",
    "href": "lessons/13_Model_selection/13_Model_selection.html#the-goals-of-association-vs.-prediction",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "The goals of association vs. prediction",
    "text": "The goals of association vs. prediction\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nGoal: Understand one variable’s (or a group of variable’s) effect on the response after adjusting for other factors\nMainly interpret the coefficient of the variable that is the focus of the study\n\nInterpreting the coefficients of the other variables is not important, but can help bring context\n\nAny variables not selected for the final model have still been adjusted for, since they had a chance to be in the model\nExample: How is body mass of a penguin associated with flipper length?\n\n\n\n\n\n\nPrediction\n\n\n\nGoal: to calculate the most precise prediction of the response variable\nInterpreting coefficients is not important\nChoose only the variables that are strong predictors of the response variable\n\nExcluding irrelevant variables can help reduce widths of the prediction intervals\n\nExample: What is the flipper length of a penguin with body mass of 3000 g (and all its other characteristics)?"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#model-building-for-association-vs.-prediction",
    "href": "lessons/13_Model_selection/13_Model_selection.html#model-building-for-association-vs.-prediction",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Model building for association vs. prediction",
    "text": "Model building for association vs. prediction\nMore information on the two analysis goals:\n\nIf you ever get the chance, check out Dr. Kristin Sainani’s series on Statistics"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#poll-everywhere-question-1",
    "href": "lessons/13_Model_selection/13_Model_selection.html#poll-everywhere-question-1",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#model-selection-strategies-for-continuous-outcomes",
    "href": "lessons/13_Model_selection/13_Model_selection.html#model-selection-strategies-for-continuous-outcomes",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Model selection strategies for continuous outcomes",
    "text": "Model selection strategies for continuous outcomes\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nSelection of potential models is tied more with the research context with some incorporation of prediction scores\nPre-specification of multivariable model\nPurposeful model selection\n\n“Risk factor modeling”\n\nChange in Estimate (CIE) approaches\n\nWill learn in Survival Analysis (BSTA 514)\n\n\n\n\n\n\n\nPrediction\n\n\n\nSelection of potential models is fully dependent on prediction scores\nAutomated strategies\n\nStepwise selection (forward/backward)\n\nYou’ll see these a lot, but they’re not really good methods\n\nBest subset\nRegularization techniques (LASSO, Ridge, Elastic net)\n\n\n\n\n\n\n\nFor categorical outcomes, there are more prediction model selection strategies (will learn more in BSTA 513)\n\nExamples: Decision trees, Random forest, Neural networks, K-means"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#pre-specification-of-multivariable-model-slide-adjusted-from-jodi-lapidus",
    "href": "lessons/13_Model_selection/13_Model_selection.html#pre-specification-of-multivariable-model-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Pre-specification of multivariable model (slide adjusted from Jodi Lapidus)",
    "text": "Pre-specification of multivariable model (slide adjusted from Jodi Lapidus)\n\nIn a clinical trial, we often have to write and finalize a statistical analysis plan (SAP) before the trial starts\nIf we wish to compare treatment effects adjusted for covariates, all covariates typically specified in advance\n\nExample: Comparing effectiveness of 3-drug vs. 2-drug regimen for delaying AIDS onset or death. Covariates such as severity of HIV infection at baseline would have been specified in advance.\nVariables such as study site, as well as any randomization stratification variables are common covariates.\n\nIn these cases, only a limited number of multivariable models are fit and reported\n\nDo not perform all the model building steps outlined in Hosmer and Lemeshow texts"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#purposeful-model-selection-slide-adjusted-from-jodi-lapidus",
    "href": "lessons/13_Model_selection/13_Model_selection.html#purposeful-model-selection-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Purposeful model selection (slide adjusted from Jodi Lapidus)",
    "text": "Purposeful model selection (slide adjusted from Jodi Lapidus)\n\nCan use this type of model selection for any type of regression\nCareful, well-thought out variable selection process\n\nConsiders both confounding and interaction, as well as checking model assumptions, fit, etc.\n\nOften a reasonable strategy, especially in epidemiology and more exploratory clinical studies\n\nHowever, not always appropriate!\nE.g. clinical trials with model specified in advance. (pre-specified model)\n\n\n \n\nThis is the selection process that we will focus on in this class!"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#change-in-estimate-cie-approach-slide-adjusted-from-jodi-lapidus",
    "href": "lessons/13_Model_selection/13_Model_selection.html#change-in-estimate-cie-approach-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Change in estimate (CIE) approach (slide adjusted from Jodi Lapidus)",
    "text": "Change in estimate (CIE) approach (slide adjusted from Jodi Lapidus)\n\nCIE strategies select covariates on the basis of how much their control changes exposure effect estimates\n\nObserved change is presumed to measure confounding by the covariate.\n\nWhat estimate?\n\nH/L text suggest using coefficients from the model\nWe typically use the coefficient estimate from the explanatory variable that we are most interested in\n\nWhat magnitude change is ”important”?\n\nH/L text suggest 10%\n\nOne must choose an effect measure to judge change importance, where “importance” needs to be evaluated along a contextually meaningful scale\nAccurate assessment of confounding may require examining changes from removing entire sets of covariates\n\nAdd in or eliminate candidate confounders one at time?\nAdd in or eliminate candidate confounders in sets?"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#stepwise-selection-slide-adjusted-from-adrianna-westbrook",
    "href": "lessons/13_Model_selection/13_Model_selection.html#stepwise-selection-slide-adjusted-from-adrianna-westbrook",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Stepwise selection (slide adjusted from Adrianna Westbrook)",
    "text": "Stepwise selection (slide adjusted from Adrianna Westbrook)\n\nThis is an incredibly common approach that statisticians use, often because it is an older and more recognized method\n\nBUT IT IS ALSO ONE OF THE WORST MODEL SELECTION STRATEGIES!!\n\nMajor disadvantages to stepwise selection:\n\nProne to overfitting\nBiased estimates\nCements the wrong idea that we are looking for our “most significant” covariates\n\nPredictors/covariates are added or removed one at time if they are below a certain threshold (usually p-value below 0.10 to 0.20)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#stepwise-selection-two-common-approaches",
    "href": "lessons/13_Model_selection/13_Model_selection.html#stepwise-selection-two-common-approaches",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Stepwise selection: two common approaches",
    "text": "Stepwise selection: two common approaches\n\nI will introduce two of the approaches so that you understand the general process if a collaborator ever mentions stepwise selection\nForward selection:\n\nFor \\(p\\) cvariates potential covariates, run all simple linear regressions:\n\n\\(Y= \\beta_0 + \\beta_1 X_1 + \\epsilon\\) through \\(Y= \\beta_0 + \\beta_1 X_{p} + \\epsilon\\)\nInclude the \\(X_i\\) with the lowest p-value (assuming it is below the threshold)\n\nNow run \\(Y= \\beta_0 + \\beta_1 X_i + \\beta_2 X_1 + \\epsilon\\) through \\(Y= \\beta_0 + \\beta_1 X_i + \\beta_2 X_{p} + \\epsilon\\) and enter the next \\(X_j\\) with the lowest p-value\nContinue process until no more predictors come back with a p-value below the threshold\n\nBackward selection:\n\nStart with a full model (\\(Y= \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p + \\epsilon\\)) and remove predictor with the highest p-value (assuming it is above the threshold)\nRepeatedly remove the variable with the highest p-value until all remaining variables meet the stopping criteria (are below the threshold)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#best-subset-slide-adjusted-from-adrianna-westbrook",
    "href": "lessons/13_Model_selection/13_Model_selection.html#best-subset-slide-adjusted-from-adrianna-westbrook",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Best subset (slide adjusted from Adrianna Westbrook)",
    "text": "Best subset (slide adjusted from Adrianna Westbrook)\n\nI don’t see this approach very often\nQuite literally making subsets of the data and using the “best” one\nGeneral steps:\n\nRun every possible model fitting 1 to all possible \\(p\\) predictors/covariates\nYou can limit number of potential predictors\n\\(2^p\\) = total number of models where \\(p\\) = number of predictors\nYou will get the best fitting model within each category (i.e., 1 predictor model, 2 predictor model,…, \\(p\\) predictor model)\nThen have to find the best fitting model between the best models from each category\n\nMajor disadvantages to best subset:\n\nDoes not account for interactions\nNeeds to run a lot of models (takes A LOT of time)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#regularization-techniques",
    "href": "lessons/13_Model_selection/13_Model_selection.html#regularization-techniques",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Regularization techniques",
    "text": "Regularization techniques\n\nRegularization techniques (LASSO, ridge, elastic net) adds a penalization that shrinks (or regularizes) coefficients down to reduce overfitting\n\n\n\n\n\n\n\n\n\n\n\nLASSO (Least About Shrinkage and Selection Operator)\nRidge\nElastic Net\n\n\nPenalization\nL-1 Norm, uses absolute value\nL-2 Norm, uses squared value\nBest of both worlds, L-1 and L-2 used\n\n\nPro’s\nReduces overfitting, will shrink coefficient to zero\nReduces overfitting, handles collinearity, can handle k&gt;n\nReduces overfitting, handles collinearity, handles k&gt;n, shrinks coefficients to zero\n\n\nCon’s\nCannot handle k&gt;n, doesn’t handle multicollinearity well\nDoes not shrink coefficients to zero, difficult to interpret\nMore difficult for R to do than the other two (but not really that bad)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#poll-everywhere-question-2",
    "href": "lessons/13_Model_selection/13_Model_selection.html#poll-everywhere-question-2",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#introduction-to-model-fit-statistics",
    "href": "lessons/13_Model_selection/13_Model_selection.html#introduction-to-model-fit-statistics",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Introduction to model fit statistics",
    "text": "Introduction to model fit statistics\n\nSo far we have compared models using the F-test\nThe F-test is a great way to compare models that are nested\n\nBasically, this means that the “full” model contains all the covariates that the “reduced” model contains\nThe full model will have additional covariates, but the covariates in the reduced is a subset of the covariates in the full\n\nWhat if we want to compare models that are not nested?\n\nThere is a special group of fit statistics that can help us compare models\nNote: these are sometimes used in the model building process (within one strategy)\n\nHelpful if we want to compare selected models across strategies\nHelpful if we have a few “final” models with different covariates that we want to compare"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#common-model-fit-statistics",
    "href": "lessons/13_Model_selection/13_Model_selection.html#common-model-fit-statistics",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Common model fit statistics",
    "text": "Common model fit statistics\n\nThe following model fit statistics combine information about the SSE, the number of parameters in the model, and the sample size\nFor Mallow’s Cp, AIC, and BIC: smaller values indicate better model fit!\nFor Adjusted R-squared: larger values indicate better model fit!\n\n\n\n\nFit statistic\nEquation\nR code\n\n\n\n\nR-squared / Adjusted R-squared\n\\(Adj. R^2 = 1 - \\frac{SSE/(n-p-1)}{SSY/(n-1)}\\)\nWithin summary(model_name)\n\n\nMallow’s \\(C_p\\)\n\\(C_p = \\Bigg[ \\dfrac{\\widehat\\sigma^2_p}{\\widehat\\sigma^2_{max}} - 1 \\Bigg](n-p) + p\\)\nols_mallows_cp()\n\n\nAkaike information criterion (AIC)\n\\(AIC = n\\log(SSE) - n \\log(n) + 2(p+1)\\)\nAIC(model_name)\n\n\nBayesian information criterion (BIC)\n\\(BIC = n\\log(SSE) - n\\log(n) + log(n)\\cdot(p+1)\\)\nBIC(model_name)\n\n\n\n \n\nWe don’t need to know the exact formulas for them!"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection.html#common-model-fit-statistics-1",
    "href": "lessons/13_Model_selection/13_Model_selection.html#common-model-fit-statistics-1",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Common model fit statistics",
    "text": "Common model fit statistics\n\nThere is no hypothesis testing for these fit statistics\n\nOnly helpful if you are comparing models\nWorks for nested and non-nested models\n\nCommon to report all or some of them\nAll of the fit statistics will not necessarily reach a consensus about the best fitting model\n\nEach weigh SSE, number of parameters, and number of observations differently\n\n\n\n\nhttps://www.researchgate.net/figure/Model-Fit-Statistics_tbl1_308844501\nLesson 13: Model selection"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/01_Review_key_info.html#key-dates",
    "href": "lessons/15_MLR_Diagnostics/01_Review_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html",
    "href": "lessons/01_Review/01_Review.html",
    "title": "Lesson 1: Review",
    "section": "",
    "text": "In 511, we talked about categorical and continuous outcomes (dependent variables)\n \nWe also talked about their relationship with 1-2 continuous or categorical exposure (independent variables or predictor)\n \nWe had many good ways to assess the relationship between an outcome and exposure:\n \n\n\n\n\n\n\n\n\n\n\nContinuous Outcome\nCategorical Outcome\n\n\nContinuous Exposure\nCorrelation, simple linear regression\n??\n\n\nCategorical Exposure\nt-tests, paired t-tests, 2 sample t-tests, ANOVA\nproportion t-test, Chi-squared goodness of fit test, Fisher’s Exact test, Chi-squared test of independence, etc."
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#what-did-we-learn-in-511",
    "href": "lessons/01_Review/01_Review.html#what-did-we-learn-in-511",
    "title": "Lesson 1: Review",
    "section": "What did we learn in 511?",
    "text": "What did we learn in 511?\n\nYou set up a really important foundation\n\nIncluding distributions, mathematical definitions, hypothesis testing, and more!\n\n \nTests and statistical approaches learned are incredibly helpful!\n \nWhile you had to learn a lot of different tests and approaches for each combination of categorical/continuous exposure with categorical/continuous outcome\n\nThose tests cannot handle more complicated data\n\n \nWhat happens when other variables influence the relationship between your exposure and outcome?\n\nDo we just ignore them?"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#what-did-we-learn-in-511-1",
    "href": "lessons/01_Review/01_Review.html#what-did-we-learn-in-511-1",
    "title": "Lesson 1: Review",
    "section": "What did we learn in 511?",
    "text": "What did we learn in 511?\n\nYou set up a really important foundation\n\nIncluding distributions, mathematical definitions, hypothesis testing, and more!\n\n \nTests and statistical approaches learned are incredibly helpful!\n \nWhile you had to learn a lot of different tests and approaches for each combination of categorical/continuous exposure with categorical/continuous outcome\n\nThose tests cannot handle more complicated data\n\n \nWhat happens when other variables influence the relationship between your exposure and outcome?\n\nDo we just ignore them?"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#what-will-we-learn-in-this-class",
    "href": "lessons/01_Review/01_Review.html#what-will-we-learn-in-this-class",
    "title": "Lesson 1: Review",
    "section": "What will we learn in this class?",
    "text": "What will we learn in this class?\n\nWe will be building towards models that can handle many variables!\n \n\nRegression is the building block for modeling multivariable relationships\n\n \nIn Linear Models we will build, interpret, and evaluate linear regression models"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#process-of-regression-data-analysis",
    "href": "lessons/01_Review/01_Review.html#process-of-regression-data-analysis",
    "title": "Lesson 1: Review",
    "section": "Process of regression data analysis",
    "text": "Process of regression data analysis\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#main-sections-of-the-course",
    "href": "lessons/01_Review/01_Review.html#main-sections-of-the-course",
    "title": "Lesson 1: Review",
    "section": "Main sections of the course",
    "text": "Main sections of the course\n\nReview\nSimple Linear Regression\n\nModel evaluation and Model use\n\nIntro to MLR: estimation and testing\n\nModel use\n\nDiving into our predictors: categorical variables, interactions between variable\n\nModel fitting\n\nKey ingredients: model evaluation, diagnostics, selection, and building\n\nModel evaluation and Model selection"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#main-sections-of-the-course-1",
    "href": "lessons/01_Review/01_Review.html#main-sections-of-the-course-1",
    "title": "Lesson 1: Review",
    "section": "Main sections of the course",
    "text": "Main sections of the course\n\n\nReview\n\n\n\nIntro to SLR: estimation and testing\n\nModel fitting\n\nIntro to MLR: estimation and testing\n\nModel fitting\n\nDiving into our predictors: categorical variables, interactions between variable\n\nModel fitting\n\nKey ingredients: model evaluation, diagnostics, selection, and building\n\nModel evaluation and Model selection"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#before-we-begin",
    "href": "lessons/01_Review/01_Review.html#before-we-begin",
    "title": "Lesson 1: Review",
    "section": "Before we begin",
    "text": "Before we begin\n\nFeel free to visit my or Meike’s Introducation to Biostatistics\nMeike’s BSTA 511 page\nNicky’s BSTA 525 page"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#learning-objectives",
    "href": "lessons/01_Review/01_Review.html#learning-objectives",
    "title": "Lesson 1: Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify important descriptive statistics and visualize data from a continuous variable\nIdentify important distributions that will be used in 512/612\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\nUse our previous tools in 511 to conduct a hypothesis test\nDefine error rates and power"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#learning-objectives-1",
    "href": "lessons/01_Review/01_Review.html#learning-objectives-1",
    "title": "Lesson 1: Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\n\nIdentify important descriptive statistics and visualize data from a continuous variable\n\n\n\nIdentify important distributions that will be used in 512/612\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\nUse our previous tools in 511 to conduct a hypothesis test\nDefine error rates and power"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#some-basic-statistics-talk",
    "href": "lessons/01_Review/01_Review.html#some-basic-statistics-talk",
    "title": "Lesson 1: Review",
    "section": "Some Basic Statistics “Talk”",
    "text": "Some Basic Statistics “Talk”\n\n\n\nRandom variable \\(Y\\)\n\nSample \\(Y_i, i=1,\\dots, n\\)\n\nSummation:\n\\(\\sum_{i=1}^n Y_i =Y_1 + Y_2 + \\ldots + Y_n\\)\nProduct:\n\\(\\prod_{i=1}^n Y_i = Y_1 \\times Y_2 \\times \\ldots \\times Y_n\\)"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#descriptive-statistics-continuous-variables",
    "href": "lessons/01_Review/01_Review.html#descriptive-statistics-continuous-variables",
    "title": "Lesson 1: Review",
    "section": "Descriptive Statistics: continuous variables",
    "text": "Descriptive Statistics: continuous variables\n\n\nMeasures of central tendency\n\nSample mean\n\\[\n\\overline{x} = \\dfrac{x_1+x_2+...+x_n}{n}=\\dfrac{\\sum_{i=1}^nx_i}{n}\n\\]\nMedian\n\n\nMeasures of variability (or dispersion)\n\nSample variance\n\nAverage of the squared deviations from the sample mean\n\nSample standard deviation\n\\[\n\\begin{aligned}\ns = & \\sqrt{\\dfrac{(x_1-\\overline{x})^2+(x_2-\\overline{x})^2+...+(x_n-\\overline{x})^2}{n-1}} \\\\\n= & \\sqrt{\\dfrac{\\sum_{i=1}^n(x_i-\\overline{x})^2}{n-1}}\n\\end{aligned}\n\\]\nIQR\n\nRange from 1st to 3rd quartile"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#descriptive-statistics-continuous-variables-r-code",
    "href": "lessons/01_Review/01_Review.html#descriptive-statistics-continuous-variables-r-code",
    "title": "Lesson 1: Review",
    "section": "Descriptive Statistics: continuous variables (R code)",
    "text": "Descriptive Statistics: continuous variables (R code)\n\n\nMeasures of central tendency\n\nSample mean\n\nmean( sample )\n\nMedian\n\nmedian( sample )\n\n\n\nMeasures of variability (or dispersion)\n\nSample variance\n\nvar( sample )\n\nSample standard deviation\n\nsd( sample )\n\nIQR\n\nIQR( sample )\n\n\n\n\n\nOr all together!!\n\n\ndds.discr %&gt;% get_summary_stats(age)\n\n# A tibble: 1 × 13\n  variable     n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 age       1000     0    95     18    12    26    14  10.4  22.8  18.5 0.584\n# ℹ 1 more variable: ci &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#data-visualization",
    "href": "lessons/01_Review/01_Review.html#data-visualization",
    "title": "Lesson 1: Review",
    "section": "Data visualization",
    "text": "Data visualization\n\nUsing the library ggplot2 to visualize data\nWe will load the package:\n\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#histogram-using-ggplot2",
    "href": "lessons/01_Review/01_Review.html#histogram-using-ggplot2",
    "title": "Lesson 1: Review",
    "section": "Histogram using ggplot2",
    "text": "Histogram using ggplot2\nWe can make a basic graph for a continuous variable:\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_histogram(data = dds.discr, \n       aes(x = age))\n\n\n\n\n\n\n\n\n\n\nSome more information on histograms using ggplot2"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#spruced-up-histogram-using-ggplot2",
    "href": "lessons/01_Review/01_Review.html#spruced-up-histogram-using-ggplot2",
    "title": "Lesson 1: Review",
    "section": "Spruced up histogram using ggplot2",
    "text": "Spruced up histogram using ggplot2\nWe can make a more formal, presentable graph:\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram() +\n  theme(text = element_text(size=20)) +\n  labs(x = \"Age\", \n       y = \"Count\", \n       title = \"Distribution of Age in Sample\")\n\n\nI would like you to turn in homework, labs, and project reports with graphs like these."
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#other-basic-plots-from-ggplot2",
    "href": "lessons/01_Review/01_Review.html#other-basic-plots-from-ggplot2",
    "title": "Lesson 1: Review",
    "section": "Other basic plots from ggplot2",
    "text": "Other basic plots from ggplot2\nWe can also make a density and boxplot for the continuous variable with ggplot2\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_density()\n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_boxplot()"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#learning-objectives-2",
    "href": "lessons/01_Review/01_Review.html#learning-objectives-2",
    "title": "Lesson 1: Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify important descriptive statistics and visualize data from a continuous variable\n\n\n\nIdentify important distributions that will be used in 512/612\n\n\n\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\nUse our previous tools in 511 to conduct a hypothesis test\nDefine error rates and power"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#distributions-that-will-be-used-in-this-class",
    "href": "lessons/01_Review/01_Review.html#distributions-that-will-be-used-in-this-class",
    "title": "Lesson 1: Review",
    "section": "Distributions that will be used in this class",
    "text": "Distributions that will be used in this class\n\nNormal distribution\nChi-square distribution\nStudent’s t distribution\nF distribution"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#normal-distribution",
    "href": "lessons/01_Review/01_Review.html#normal-distribution",
    "title": "Lesson 1: Review",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\n\n\nWhere did we see this?\n\nBasically everywhere! Think Central Limit Theorem\n\n\n \n\nNotation: \\(Y\\sim N(\\mu,\\sigma^2)\\)\nArguably the most important distribution in statistics\nIf we know \\(E(Y)=\\mu\\), \\(Var(Y)=\\sigma^2\\) then\n\n2/3 of \\(Y\\)’s distribution lies within 1 \\(\\sigma\\) of \\(\\mu\\)\n95% is within \\(\\mu\\pm 2\\sigma\\)\n\\(&gt;99\\)% lies within \\(\\mu\\pm 3\\sigma\\)\n\n\n \n\nLinear combinations of Normal’s are Normal\ne.g., \\((aY+b)\\sim \\mbox{N}(a\\mu+b,\\;a^2\\sigma^2)\\)\nStandard normal: \\(Z=\\frac{Y-\\mu}{\\sigma} \\sim \\mbox{N}(0,1)\\)"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#chi-squared-distribution-models-sampling-variance",
    "href": "lessons/01_Review/01_Review.html#chi-squared-distribution-models-sampling-variance",
    "title": "Lesson 1: Review",
    "section": "Chi-squared distribution: models sampling variance",
    "text": "Chi-squared distribution: models sampling variance\n\n\n\nNotation: \\(X \\sim \\chi^2_{df}\\) OR \\(X \\sim \\chi^2_{\\nu}\\)\n\nDegrees of freedom (df): \\(df=n-1\\)\n\\(X\\) takes on only positive values\n\nIf \\(Z_i\\sim \\mbox{N}(0,1)\\), then \\(Z_i^2\\sim \\chi^2_1\\)\n\nA standard normal distribution squared is the Chi squared distribution with df of 1.\n\nUsed in hypothesis testing and CI’s for variance or standard deviation\n\nSample variance (and SD) is random and thus can be modeled by a probability distribution: Chi-sqaured\n\nChi-squared distribution used to model the ratio of the sample variance \\(s^2\\) to population variance \\(\\sigma^2\\):\n\n\\(\\dfrac{(n-1)s^2}{\\sigma^2}\\sim \\chi^2_{n-1}\\)"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#students-t-distribution",
    "href": "lessons/01_Review/01_Review.html#students-t-distribution",
    "title": "Lesson 1: Review",
    "section": "Student’s t Distribution",
    "text": "Student’s t Distribution\n\n\n\nWhere did we see this?\n\nInference of means: single sample, paired, two independent samples\n\n\n \n\nNotation: \\(T \\sim t_{df}\\) OR \\(T \\sim t_{n-1}\\)\n\nDegrees of freedom (df): \\(df=n-1\\)\n\\(T = \\dfrac{\\overline{x} - \\mu_x}{\\dfrac{s}{\\sqrt{n}}}\\sim t_{n-1}\\)\n\n\n \n\nIn linear modeling, used for inference on individual regression parameters\n\nThink: our estimated coefficients ( \\(\\hat{\\beta_{}}\\) )"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#f-distribution",
    "href": "lessons/01_Review/01_Review.html#f-distribution",
    "title": "Lesson 1: Review",
    "section": "F-Distribution",
    "text": "F-Distribution\n\n\n\nWhere did we see this?\n\nInference for 2+ means: ANOVA test\n\nModel ratio of sample variances (and is a ratio of Chi-squared RVs)\nIf \\(X_1^2\\sim \\chi^2_{df1}\\) and \\(X_2^2\\sim \\chi^2_{df2}\\), where \\(X_1^2\\perp X_2^2\\), then:\n\n\\[\\dfrac{X_1^2/df1}{X_2^2/df2} \\sim F_{df1,df2}\\]\n\nImportant relationship with \\(t\\) distribution: \\(T^2 \\sim F_{1,\\nu}\\)\n\nThe square of a t-distribution with \\(df=\\nu\\)\nis an F-distribution with numerator df (\\(df_1 = 1\\)) and denominator df (\\(df_2 = \\nu\\))"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#f-distribution-1",
    "href": "lessons/01_Review/01_Review.html#f-distribution-1",
    "title": "Lesson 1: Review",
    "section": "F-Distribution",
    "text": "F-Distribution\n\n\n\nModel ratio of sample variances\n\nRatio of variances is important for hypothesis testing of regression models\n\nIf \\(X_1^2\\sim \\chi^2_{df1}\\) and \\(X_2^2\\sim \\chi^2_{df2}\\), where \\(X_1^2\\perp X_2^2\\), then:\n\n\\[\\dfrac{X_1^2/df1}{X_2^2/df2} \\sim F_{df1,df2}\\] - only takes on positive values\n\nImportant relationship with \\(t\\) distribution: \\(T^2 \\sim F_{1,\\nu}\\)\n\nThe square of a t-distribution with \\(df=\\nu\\)\nis an F-distribution with numerator df (\\(df_1 = 1\\)) and denominator df (\\(df_2 = \\nu\\))\n\n\n\n\n\n\nIs there a relationship between our chi-squared and F-distribution?\n\n\nRecall, \\(\\dfrac{(n-1)s^2}{\\sigma^2}\\sim \\chi^2_{n-1}\\).\nThe F-distribution for a ratio of variances between two models is: \\(F = \\dfrac{s_1^2\\sigma^2_2}{s_2^2\\sigma^2_1} \\sim F_{n_1-1, n_2-1}\\)"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#r-code-for-probability-distributions",
    "href": "lessons/01_Review/01_Review.html#r-code-for-probability-distributions",
    "title": "Lesson 1: Review",
    "section": "R code for probability distributions",
    "text": "R code for probability distributions\n\n\nHere is a site with the various probability distributions and their R code.\n\nIt also includes practice with R code to see what each function outputs"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#learning-objectives-3",
    "href": "lessons/01_Review/01_Review.html#learning-objectives-3",
    "title": "Lesson 1: Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify important descriptive statistics and visualize data from a continuous variable\nIdentify important distributions that will be used in 512/612\n\n\n\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\n\n\n\nUse our previous tools in 511 to conduct a hypothesis test\nDefine error rates and power"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#confidence-interval-for-one-mean",
    "href": "lessons/01_Review/01_Review.html#confidence-interval-for-one-mean",
    "title": "Lesson 1: Review",
    "section": "Confidence interval for one mean",
    "text": "Confidence interval for one mean\n\n\nThe confidence interval for population mean \\(\\mu\\):\n\\[\n\\overline{x} \\pm t^{*}\\dfrac{s}{\\sqrt{n}}\n\\]\n\nwhere \\(t^*\\) is the critical value for the 95% (or other percent) corresponding to the t-distribution and dependent on \\(df=n-1\\)\n\n\n\nWe can use R to find the critical t-value, \\(t^*\\)\n\n\nFor example the critical value for the 95% CI with \\(n=10\\) subjects is…\n\nqt(0.975, df=9)\n\n[1] 2.262157\n\n\n\nRecall, that as the \\(df\\) increases, the t-distribution converges towards the Normal distribution"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#confidence-interval-for-one-mean-1",
    "href": "lessons/01_Review/01_Review.html#confidence-interval-for-one-mean-1",
    "title": "Lesson 1: Review",
    "section": "Confidence interval for one mean",
    "text": "Confidence interval for one mean\n\n\nThe confidence interval for population mean \\(\\mu\\):\n\\[\n\\overline{x} \\pm t^{*}\\dfrac{s}{\\sqrt{n}}\n\\]\n\nwhere \\(t^*\\) is the critical value for the 95% (or other percent) corresponding to the t-distribution and dependent on \\(df=n-1\\)\n\n\n\nWe can use R to find the critical t-value, \\(t^*\\)\n\n\nFor example the critical value for the 95% CI with \\(n=10\\) subjects is…\n\nqt(0.975, df=9)\n\n[1] 2.262157\n\n\n\nRecall, that as the \\(df\\) increases, the t-distribution converges towards the Normal distribution\n\n\n\n\nWe can also use t.test in R to calculate the confidence interval if we have a dataset.\n\nt.test(dds.discr$age)\n\n\n    One Sample t-test\n\ndata:  dds.discr$age\nt = 39.053, df = 999, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.65434 23.94566\nsample estimates:\nmean of x \n     22.8"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#confidence-interval-for-two-independent-means",
    "href": "lessons/01_Review/01_Review.html#confidence-interval-for-two-independent-means",
    "title": "Lesson 1: Review",
    "section": "Confidence interval for two independent means",
    "text": "Confidence interval for two independent means\n\n\nThe confidence interval for difference in independent population means, \\(\\mu_1\\) and \\(\\mu_2\\):\n\\[\n\\overline{x}_1 - \\overline{x}_2 \\pm t^{*}\\sqrt{\\dfrac{s_1^2}{n_1} + \\dfrac{s_2^2}{n_2}}\n\\]\n\nwhere \\(t^*\\) is the critical value for the 95% (or other percent) corresponding to the t-distribution and dependent on \\(df=n_1 + n_2 -2\\)\n\n\n\n\n\n \n\nPlease check out my notes on this if you’d like: https://nwakim.github.io/F24_EPI_525/schedule.html\n\nIt’s under Lesson 13"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#heres-a-decent-source-for-other-r-code-for-tests-in-511",
    "href": "lessons/01_Review/01_Review.html#heres-a-decent-source-for-other-r-code-for-tests-in-511",
    "title": "Lesson 1: Review",
    "section": "Here’s a decent source for other R code for tests in 511",
    "text": "Here’s a decent source for other R code for tests in 511\nWebsite from UCLA"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#learning-objectives-4",
    "href": "lessons/01_Review/01_Review.html#learning-objectives-4",
    "title": "Lesson 1: Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify important descriptive statistics and visualize data from a continuous variable\nIdentify important distributions that will be used in 512/612\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\n\n\n\nUse our previous tools in 511 to conduct a hypothesis test\n\n\n\nDefine error rates and power"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#steps-in-hypothesis-testing",
    "href": "lessons/01_Review/01_Review.html#steps-in-hypothesis-testing",
    "title": "Lesson 1: Review",
    "section": "Steps in hypothesis testing",
    "text": "Steps in hypothesis testing"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#example-one-sample-t-test",
    "href": "lessons/01_Review/01_Review.html#example-one-sample-t-test",
    "title": "Lesson 1: Review",
    "section": "Example: one sample t-test",
    "text": "Example: one sample t-test\n\nBodyTemps = read.csv(\"data/BodyTemperatures.csv\")\n\nggplot(data = BodyTemps, \n       aes(x = Temperature)) +\n  geom_histogram() +\n  theme(text = element_text(size=20)) +\n  labs(x = \"Temperature\", y = \"Count\", \n       title = \"Distribution of Body Temperature in Sample\") +\n  geom_vline(aes(xintercept = mean(BodyTemps$Temperature, na.rm = T)), \n             color = \"red\", linewidth = 2)"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#example-one-sample-t-test-using-p-value-approach",
    "href": "lessons/01_Review/01_Review.html#example-one-sample-t-test-using-p-value-approach",
    "title": "Lesson 1: Review",
    "section": "Example: one sample t-test using p-value approach",
    "text": "Example: one sample t-test using p-value approach\nWe want to see what the mean population body temperature is.\n\nState the null and alternative hypotheses:\n\n\n\n\n\n\n\n\\(H_0: \\mu = 98.6\\)\n\\(H_0\\): The population mean body temperature is 98.6 degrees F\n\n\n\\(H_A: \\mu \\neq 98.6\\)\n\\(H_A\\): The population mean body temperature is not 98.6 degrees F\n\n\n\nThe significance level is \\(\\alpha = 0.05\\)\nThe test statistic, \\(t_{\\overline{x}}\\) follows a student’s t-distribution with \\(df = n-1 = 129\\)\nThe test statistic is: \\(t_{\\overline{x}} = \\dfrac{\\overline{x}-\\mu_0}{\\dfrac{s}{\\sqrt{n}}}\\) and with the data: \\(t_{\\overline{x}} = \\dfrac{98.25-98.6}{\\dfrac{0.73}{\\sqrt{130}}} = -5.45\\)\nCalculate the p-value: \\(p-value = P(t \\leq -5.45) + P(t \\geq 5.45)\\)\n\n2*pt(-5.4548, df = 130-1, lower.tail=T)\n\n[1] 2.410889e-07\n\n\nConclusion: We reject the null hypothesis. There is sufficient evidence that the (population) mean body temperature after is different from 98.6 degree ( \\(p-value &lt; 0.001\\))."
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#example-one-sample-t-test-using-critical-values-approach",
    "href": "lessons/01_Review/01_Review.html#example-one-sample-t-test-using-critical-values-approach",
    "title": "Lesson 1: Review",
    "section": "Example: one sample t-test using critical values approach",
    "text": "Example: one sample t-test using critical values approach\nWe want to see what the mean population body temperature is.\n\nState the null and alternative hypotheses:\n\n\n\n\n\n\n\n\\(H_0: \\mu = 98.6\\)\n\\(H_0\\): The population mean body temperature is 98.6 degrees F\n\n\n\\(H_A: \\mu \\neq 98.6\\)\n\\(H_A\\): The population mean body temperature is not 98.6 degrees F\n\n\n\nThe significance level is \\(\\alpha = 0.05\\)\nThe test statistic, \\(t_{\\overline{x}}\\) follows a student’s t-distribution with \\(df = n-1 = 129\\)\nDecision rule (critical value): For \\(\\alpha=0.05\\) , \\(2*P(t \\geq t^*) = 0.05\\)\n\nqt(0.05/2, df = 130-1, lower.tail=F)\n\n[1] 1.978524\n\n\nThe test statistic is: \\(t_{\\overline{x}} = \\dfrac{\\overline{x}-\\mu_0}{\\dfrac{s}{\\sqrt{n}}}\\) and with the data: \\(t_{\\overline{x}} = \\dfrac{98.25-98.6}{\\dfrac{0.73}{\\sqrt{130}}} = -5.45\\)\nConclusion: We reject the null hypothesis. There is sufficient evidence that the (population) mean body temperature after is different from 98.6 degree ( 95% CI: \\(98.12, 98.38\\))."
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#how-did-we-get-the-95-ci",
    "href": "lessons/01_Review/01_Review.html#how-did-we-get-the-95-ci",
    "title": "Lesson 1: Review",
    "section": "How did we get the 95% CI?",
    "text": "How did we get the 95% CI?\n\nThe t.test function can help us answer this, and give us the needed information for both approaches.\n\n\nBodyTemps = read.csv(\"data/BodyTemperatures.csv\")\n\nt.test(x = BodyTemps$Temperature, \n       # alternative = \"two-sided\", \n       mu = 98.6)\n\n\n    One Sample t-test\n\ndata:  BodyTemps$Temperature\nt = -5.4548, df = 129, p-value = 2.411e-07\nalternative hypothesis: true mean is not equal to 98.6\n95 percent confidence interval:\n 98.12200 98.37646\nsample estimates:\nmean of x \n 98.24923"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#learning-objectives-5",
    "href": "lessons/01_Review/01_Review.html#learning-objectives-5",
    "title": "Lesson 1: Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify important descriptive statistics and visualize data from a continuous variable\nIdentify important distributions that will be used in 512/612\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\nUse our previous tools in 511 to conduct a hypothesis test\n\n\n\nDefine error rates and power"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#outcomes-of-our-hypothesis-test",
    "href": "lessons/01_Review/01_Review.html#outcomes-of-our-hypothesis-test",
    "title": "Lesson 1: Review",
    "section": "Outcomes of our hypothesis test",
    "text": "Outcomes of our hypothesis test"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#prabilities-of-outcomes",
    "href": "lessons/01_Review/01_Review.html#prabilities-of-outcomes",
    "title": "Lesson 1: Review",
    "section": "Prabilities of outcomes",
    "text": "Prabilities of outcomes\n\nType 1 error is \\(\\alpha\\)\n\nThe probability that we falsely reject the null hypothesis (but the null is true!!)\n\nPower is \\(1-\\beta\\)\n\nThe probability of correctly rejecting the null hypothesis"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#what-i-think-is-the-most-intuitive-way-to-look-at-it",
    "href": "lessons/01_Review/01_Review.html#what-i-think-is-the-most-intuitive-way-to-look-at-it",
    "title": "Lesson 1: Review",
    "section": "What I think is the most intuitive way to look at it",
    "text": "What I think is the most intuitive way to look at it"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html",
    "href": "lessons/00_Welcome/00_Welcome.html",
    "title": "Welcome to BSTA 512/612!",
    "section": "",
    "text": "Call me “Nicky,” “Dr. W,” “Professor Wakim,” or any combo!\n\nIf you are comfortable with it, I prefer Nicky\n\nAssistant Professor of Biostatistics\n \nOriginally from DC area (Virginia side!)\nTwo kitties\nVolleyball, biking, pickleball\nBut also sleeping, TV, and reading\nJust started taking a couple classes at PCC (French, ceramics, yoga)\nSlowly regrowing my plant collection after moving from Michigan\n\n\n\n\n Video"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#nicky-wakim-sheher",
    "href": "lessons/00_Welcome/00_Welcome.html#nicky-wakim-sheher",
    "title": "Welcome to BSTA 512/612!",
    "section": "Nicky Wakim (she/her)",
    "text": "Nicky Wakim (she/her)\n\n\n\nCall me “Nicky,” “Dr. W,” “Professor Wakim,” or any combo!\n\nIf you are comfortable with it, I prefer Nicky\n\nAssistant Professor of Biostatistics\n \nOriginally from DC area (Virginia side!)\nTwo kitties\nVolleyball, biking, pickleball\nBut also sleeping, TV, and reading\nTaking ceramics at PCC\nI am currently moving!\n\n\n\n\n Video"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#some-important-tasks",
    "href": "lessons/00_Welcome/00_Welcome.html#some-important-tasks",
    "title": "Welcome to BSTA 512/612!",
    "section": "Some important tasks",
    "text": "Some important tasks\n\nStar the class website: https://nwakim.github.io/BSTA_512_W25/\nComplete the WhenIsGood for office hours\nComplete Homework 0 by this Thursday at 11pm!\nHighly suggest that you make an appointment with a learning specialist through Student Academic Support Services!"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website",
    "href": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website",
    "title": "Welcome to BSTA 512/612!",
    "section": "Let’s visit the website",
    "text": "Let’s visit the website\n\nHomepage\n\nGitHub\n\nSyllabus\nSchedule\n\nWeeks, class info, quizzes, homeworks, projects\n\nSearch"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#lets-go-through-the-syllabus",
    "href": "lessons/00_Welcome/00_Welcome.html#lets-go-through-the-syllabus",
    "title": "Welcome to BSTA 512/612!",
    "section": "Let’s go through the syllabus!",
    "text": "Let’s go through the syllabus!\nSyllabus page\n\nCourse Description\nClass assessments and breakdown\n\n\n\nWelcome"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR_key_info.html#key-dates",
    "href": "lessons/03_SLR/03_SLR_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates\n\nLab 1 due this Thursday at 11pm"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR_shiny_app.html",
    "href": "lessons/03_SLR/03_SLR_shiny_app.html",
    "title": "Untitled",
    "section": "",
    "text": "Loading required package: ggplot2\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks plotly::filter(), stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nLife Expectancy Simple Linear Regression\nLet’s try to find a model fit with the lowest score!\nClick two distinct points of the plot to draw a line. Try to minimize the score (at the top of the plot) using different lines. To create a new line just click on the plot more than twice. In this activity, the number we get is our “score.” In statistics, this score is actually called the sum of squared errors (SSE). Were you able to get the best-fit line presented in class?"
  },
  {
    "objectID": "lessons/18_In_class_activities/12_In_class_activities.html#example-1-palmer-penguins",
    "href": "lessons/18_In_class_activities/12_In_class_activities.html#example-1-palmer-penguins",
    "title": "Lesson 12: In-class exercise!!",
    "section": "Example 1: Palmer Penguins",
    "text": "Example 1: Palmer Penguins\n\nRevisit the Palmer Penguins dataset that we say in HW 4"
  },
  {
    "objectID": "lessons/18_In_class_activities/12_In_class_activities.html#lets-take-a-look-at-the-variables",
    "href": "lessons/18_In_class_activities/12_In_class_activities.html#lets-take-a-look-at-the-variables",
    "title": "Lesson 12: In-class exercise!!",
    "section": "Let’s take a look at the variables",
    "text": "Let’s take a look at the variables\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…"
  },
  {
    "objectID": "lessons/18_In_class_activities/12_In_class_activities.html#questions-we-can-ask-and-answer-so-far",
    "href": "lessons/18_In_class_activities/12_In_class_activities.html#questions-we-can-ask-and-answer-so-far",
    "title": "Lesson 12: In-class exercise!!",
    "section": "Questions we can ask and answer so far…",
    "text": "Questions we can ask and answer so far…\n\nUsing SLR, does each variable predict flipper length significantly?"
  },
  {
    "objectID": "lessons/18_In_class_activities/12_In_class_activities.html#example",
    "href": "lessons/18_In_class_activities/12_In_class_activities.html#example",
    "title": "Lesson 12: In-class exercise!!",
    "section": "Example",
    "text": "Example\n\nThe following example comes from this textbook\n\n\nSuppose that we are conducting an observational study of adults to assess whether physical activity level (PAL) is associated with systolic blood pressure (SBP), accounting (i.e., controlling) for AGE and SAB (sex assigned at birth). The extraneous variable here is AGE and SAB, while the explanatory variable (variable of interest) is PAL. We need to determine whether we can ignore AGE and/or SAB in our analysis and still correctly assess the PAL–SBP association."
  },
  {
    "objectID": "lessons/18_In_class_activities/12_In_class_activities.html#explore-the-data-first-thing-to-do",
    "href": "lessons/18_In_class_activities/12_In_class_activities.html#explore-the-data-first-thing-to-do",
    "title": "Lesson 12: In-class exercise!!",
    "section": "Explore the data: First thing to do",
    "text": "Explore the data: First thing to do\n\n\nCode\nggplot(SBP) + geom_histogram(aes(x = SBP))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCode\nggplot(SBP) + geom_histogram(aes(x = PAL))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCode\nggplot(SBP) + geom_histogram(aes(x = AGE))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html",
    "title": "MLR: Inference / F-test",
    "section": "",
    "text": "Interpret MLR (population) coefficient estimates with additional variable in model\nUnderstand the use of the general F-test and interpret what it measures.\nUnderstand the context of the Overall F-test, conduct the needed hypothesis test, and interpret the results.\nUnderstand the context of the single covariate F-test, conduct the needed hypothesis test, and interpret the results.\nUnderstand the context of the group of covariates F-test, conduct the needed hypothesis test, and interpret the results.\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#lets-map-that-to-our-regression-analysis-process",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#lets-map-that-to-our-regression-analysis-process",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Let’s map that to our regression analysis process",
    "text": "Let’s map that to our regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#interpreting-the-estimated-population-coefficients",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#interpreting-the-estimated-population-coefficients",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Interpreting the estimated population coefficients",
    "text": "Interpreting the estimated population coefficients\n\nFor a population model: \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\epsilon\\]\n\nWhere \\(X_1\\) and \\(X_2\\) are continuous variables\nNo need to specify \\(Y\\) because it required to be continuous in linear regression\n\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB)."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#getting-these-interpretations-from-our-regression-table",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#getting-these-interpretations-from-our-regression-table",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Getting these interpretations from our regression table",
    "text": "Getting these interpretations from our regression table\nWe fit the regression model in R and printed the regression table:\n\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model: \\(\\widehat{\\text{LE}} = 33.595 + 0.157 \\text{ FLR} + 0.008 \\text{ FS}\\)\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 0 kcal PPD (95% CI: 24.674, 41.517).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012)."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#lets-just-examine-the-general-interpretation-vs.-the-example",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#lets-just-examine-the-general-interpretation-vs.-the-example",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Let’s just examine the general interpretation vs. the example",
    "text": "Let’s just examine the general interpretation vs. the example\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 0 kcal PPD (95% CI: 24.674, 41.517).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012)."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#what-we-need-in-our-interpretations-of-coefficients-reference",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#what-we-need-in-our-interpretations-of-coefficients-reference",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "What we need in our interpretations of coefficients (reference)",
    "text": "What we need in our interpretations of coefficients (reference)\n\n\n\nUnits of Y\nUnits of X\nDiscussing intercept: Mean or average or expected before Y\nDiscussing coefficient for continuous covariate: Mean or average or expected before difference, increase, or decrease\n\nOR: Mean or average or expected before Y\nOnly need before difference or Y!!\n\nConfidence interval\n\n\n\nIf other covariates in the model\n\nDiscussing intercept: Must state that variables are equal to 0\n\nor at their centered value if centered!\n\nDiscussing coefficient for covariate: Must state “adjusting for all other variables”, “Controlling for all other variables”, or “Holding all other variables constant”\n\nIf only one other variable in the model, then replace “all other variables” with the single variable name"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#we-must-revisit-our-dear-friend-the-f-test",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#we-must-revisit-our-dear-friend-the-f-test",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "We must revisit our dear friend, the F-test!",
    "text": "We must revisit our dear friend, the F-test!\n\nhttps://www.writerswrite.co.za/foreshadowing/"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#remember-from-lesson-5-f-test-vs.-t-test-for-the-population-slope",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#remember-from-lesson-5-f-test-vs.-t-test-for-the-population-slope",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Remember from Lesson 5: F-test vs. t-test for the population slope",
    "text": "Remember from Lesson 5: F-test vs. t-test for the population slope\nThe square of a \\(t\\)-distribution with \\(df = \\nu\\) is an \\(F\\)-distribution with \\(df = 1, \\nu\\)\n\\[T_{\\nu}^2 \\sim F_{1,\\nu}\\]\n\nWe can use either F-test or t-test to run the following hypothesis test:\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote that the F-test does not support one-sided alternative tests, but the t-test does!"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#remember-from-lesson-5-planting-a-seed-about-the-f-test",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#remember-from-lesson-5-planting-a-seed-about-the-f-test",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Remember from Lesson 5: Planting a seed about the F-test",
    "text": "Remember from Lesson 5: Planting a seed about the F-test\nWe can think about the hypothesis test for the slope…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_1\\neq0\\)\n\n\n\n\nin a slightly different way…\n\n\n\n\n\n\nNull model (\\(\\beta_1=0\\))\n\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\nSmaller (reduced) model\n\n\n\n\n\n\nAlternative model (\\(\\beta_1\\neq0\\))\n\n\n\n\\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)\nLarger (full) model\n\n\n\n\n\n\nIn multiple linear regression, we can start using this framework to test multiple coefficient parameters at once\n\nDecide whether or not to reject the smaller reduced model in favor of the larger full model\nCannot do this with the t-test!"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#we-can-extend-this",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#we-can-extend-this",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "We can extend this!!",
    "text": "We can extend this!!\nWe can create a hypothesis test for more than one coefficient at a time…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=\\beta_2=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_1\\neq0\\) and/or \\(\\beta_2\\neq0\\)\n\n\n\n\nin a slightly different way…\n\n\n\n\n\n\nNull model\n\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\nSmaller (reduced) model\n\n\n\n\n\n\nAlternative* model\n\n\n\n\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\)\nLarger (full) model\n\n\n\n\n\n*This is not quite the alternative, but if we reject the null, then this is the model we move forward with"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#poll-everywhere-question-1",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#poll-everywhere-question-1",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#building-a-very-important-toolkit-three-types-of-tests",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#building-a-very-important-toolkit-three-types-of-tests",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Building a very important toolkit: three types of tests",
    "text": "Building a very important toolkit: three types of tests\n\n\nOverall test\n\n\nDoes at least one of the covariates/predictors contribute significantly to the prediction of Y?\n\n\n\n\nTest for addition of a single variable (covariate subset test)\n\n\nDoes the addition of one particular covariate add significantly to the prediction of Y achieved by other covariates already present in the model?\n\n\n\n\nTest for addition of group of variables (covariate subset test)\n\n\nDoes the addition of some group of covariates add significantly to the prediction of Y achieved by other covariates already present in the model?"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#variation-explained-vs.-unexplained",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#variation-explained-vs.-unexplained",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Variation: Explained vs. Unexplained",
    "text": "Variation: Explained vs. Unexplained\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 &= \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY &= SSR + SSE\n\\end{aligned}\\]\n\n\\(Y_i - \\overline{Y}\\) = the deviation of \\(Y_i\\) around the mean \\(\\overline{Y}\\)\n\n(the total amount deviation unexplained at \\(X_{i1},\\ldots,X_{ik}\\) ).\n\n\\(\\widehat{Y}_i- \\overline{Y}\\) = the deviation of the fitted value \\(\\widehat{Y}_i\\) around the mean \\(\\overline{Y}\\)\n\n(the amount deviation explained by the regression at \\(X_{i1},\\ldots,X_{ik}\\) ).\n\n\\(Y_i - \\widehat{Y}_i\\) = the deviation of the observation \\(Y\\) around the fitted regression line\n\n(the amount deviation unexplained by the regression at \\(X_{i1},\\ldots,X_{ik}\\) )"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#another-way-to-think-of-ssy-ssr-and-sse",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#another-way-to-think-of-ssy-ssr-and-sse",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Another way to think of SSY, SSR, and SSE",
    "text": "Another way to think of SSY, SSR, and SSE\n\nLet’s create a data frame of each component within the SS’s\n\nDifference in SSY: \\(Y_i - \\overline{Y}\\)\nDifference in SSR: \\(\\widehat{Y}_i- \\overline{Y}\\)\nDifference in SSE: \\(Y_i - \\widehat{Y}_i\\)\n\nUsing our simple linear regression model as an example:\n\n\nslr1 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate, data = gapm_sub)\naug_slr1 = augment(slr1)\nSS_df = gapm_sub %&gt;% select(LifeExpectancyYrs) %&gt;%\n  mutate(SSY_diff = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         y_fit = aug_slr1$.fitted, \n         SSR_diff = y_fit - mean(LifeExpectancyYrs), \n         SSE_diff = aug_slr1$.resid)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#plot-the-components-of-each-sum-of-squares",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#plot-the-components-of-each-sum-of-squares",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Plot the components of each sum of squares",
    "text": "Plot the components of each sum of squares\n\nSSY_plot = ggplot(SS_df, aes(SSY_diff)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) \nSSR_plot = ggplot(SS_df, aes(SSR_diff)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) \nSSE_plot = ggplot(SS_df, aes(SSE_diff)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) \ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = 64.64\\]\n\\[SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = 27.24\\]\n\\[SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = 37.39\\]"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#when-running-a-f-test-for-linear-models",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#when-running-a-f-test-for-linear-models",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "When running a F-test for linear models…",
    "text": "When running a F-test for linear models…\n\nWe need to define a larger, full model (more parameters)\nWe need to define a smaller, reduced model (fewer parameters)\nUse the F-statistic to decide whether or not we reject the smaller model\n\nThe F-statistic compares the SSE of each model to determine if the full model explains a significant amount of additional variance\n\n\n\n\n \n\\[\nF = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\n\\]\n\n\n\\(SSE(R) \\geq SSE(F)\\)\nNumerator measures difference in unexplained variation between the models\n\nBig difference = added parameters greatly reduce the unexplained variation (increase explained variation)\nSmaller difference = added parameters don’t reduce the unexplained variation\n\nTake ratio of difference to the unexplained variation in the full model"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#poll-everywhere-question-2",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#poll-everywhere-question-2",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#we-will-keep-working-with-the-mlr-model-from-last-class",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#we-will-keep-working-with-the-mlr-model-from-last-class",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "We will keep working with the MLR model from last class",
    "text": "We will keep working with the MLR model from last class\nNew population model for example:\n\\[\\text{Life expectancy} = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_2 \\text{Food supply} + \\epsilon\\]\n\n# Fit regression model:\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\ntidy(mr1, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{Life expectancy}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{Female literacy rate} + \\widehat{\\beta}_2 \\text{Food supply} \\\\\n\\widehat{\\text{Life expectancy}} &= 33.595 + 0.157\\ \\text{Female literacy rate}\n+ 0.008\\ \\text{Food supply}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#overall-f-test",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#overall-f-test",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Overall F-test",
    "text": "Overall F-test\nDoes at least one of the covariates/predictors contribute significantly to the prediction of Y?\n\nFor a general population MLR model, \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\nWe can create a hypothesis test for all the covariate coefficients…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=\\beta_2= \\ldots=\\beta_k=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\nAt least one \\(\\beta_j\\neq0\\) (for \\(j=1, 2, \\ldots, k\\))\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k + \\epsilon\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#overall-f-test-general-steps-for-hypothesis-test",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#overall-f-test-general-steps-for-hypothesis-test",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Overall F-test: general steps for hypothesis test",
    "text": "Overall F-test: general steps for hypothesis test\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1=\\beta_2= \\ldots=\\beta_k=0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=1, 2, \\ldots, k\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}} = \\frac{MSR_{full}}{MSE_{full}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject if: \\(P(F_{k, n-k-1} &gt; F) &lt; \\alpha\\)\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that at least one predictor’s coefficient is not 0 (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#overall-f-test-a-word-on-the-conclusion",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#overall-f-test-a-word-on-the-conclusion",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Overall F-test: a word on the conclusion",
    "text": "Overall F-test: a word on the conclusion\n\nIf \\(H_0\\) is rejected, we conclude there is sufficient evidence that at least one predictor’s coefficient is different from zero.\nSame as: at least one independent variable contributes significantly to the prediction of \\(Y\\)\n\n \n\nIf \\(H_0\\) is not rejected, we conclude there is insufficient evidence that at least one predictor’s coefficient is different from zero.\nSame as: Not enough evidence that at least one independent variable contributes significantly to the prediction of \\(Y\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#lets-think-about-our-mlr-example-for-life-expectancy",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#lets-think-about-our-mlr-example-for-life-expectancy",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Let’s think about our MLR example for life expectancy",
    "text": "Let’s think about our MLR example for life expectancy\nOur proposed population model\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\ \\text{FS}\n\\end{aligned}\\]\nOur main question for the Overall F-test: Is the regression model containing female literacy rate and food supply useful in estimating countries’ life expectancy?\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(LE = \\beta_0 + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\nmod_red1 = lm(LifeExpectancyYrs ~ 1, data = gapm_sub)\naug_red1  = augment(mod_red1)\n\nmod_full1 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD,\n               data = gapm_sub)\naug_full1  = augment(mod_full1)\n\nSS_df2 = gapm_sub %&gt;% select(LifeExpectancyYrs) %&gt;%\n  mutate(SSY_diff_r1 = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         SSR_diff_r1 = aug_red1$.fitted - mean(LifeExpectancyYrs), \n         SSE_diff_r1 = aug_red1$.resid, \n         SSY_diff_f1 = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         SSR_diff_f1 = aug_full1$.fitted - mean(LifeExpectancyYrs), \n         SSE_diff_f1 = aug_full1$.resid)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-1",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-1",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\n\nReduced / null model \\[LE = \\beta_0 + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 0\\]\n \n\\[SSE = 64.64\\]\n\n\n\nFull / Alternative model \\[LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 36.39\\]\n \n\\[SSE = 28.25\\]"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#poll-everywhere-question-3",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#poll-everywhere-question-3",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "So let’s step through our hypothesis test (1/3)",
    "text": "So let’s step through our hypothesis test (1/3)\n\n\n\nMet underlying LINE assumptions\n\n\n\n \n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1=\\beta_2=0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_1\\neq0 \\text{ or } \\beta_2\\neq0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k =2\\) and denominator \\(df=n-k-1 = 72 - 2-1=69\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "So let’s step through our hypothesis test (2/3)",
    "text": "So let’s step through our hypothesis test (2/3)\n\n\n\nCompute the value of the test statistic / 6. Calculate the p-value\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}=44.443\\] OR use ANOVA table:\n\nanova(mod_red1, mod_full1) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ 1\n71.000\n4,589.119\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD\n69.000\n2,005.556\n2.000\n2,583.563\n44.443\n0.000"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "So let’s step through our hypothesis test (3/3)",
    "text": "So let’s step through our hypothesis test (3/3)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n \nWe reject the null hypothesis at the 5% significance level. There is sufficient evidence that either countries’ female literacy rate or the food supply (or both) contributes significantly to the prediction of life expectancy (p-value &lt; 0.001)."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#covariate-subset-test-single-variable",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#covariate-subset-test-single-variable",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Covariate subset test: Single variable",
    "text": "Covariate subset test: Single variable\nDoes the addition of one particular covariate of interest add significantly to the prediction of Y achieved by other covariates already present in the model?\n\nFor a general population MLR model, \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\beta_j X_j +\\ldots + \\beta_k X_k + \\epsilon\\]\n\nWe can create a hypothesis test for a single \\(j\\) covariate coefficient (where \\(j\\) can be any value \\(1, 2, \\ldots, k\\))…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_j=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_j\\neq0\\)\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(\\begin{aligned}Y = &\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_j X_j +\\\\ &\\ldots + \\beta_k X_k + \\epsilon \\end{aligned}\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#single-covariate-f-test-general-steps-for-hypothesis-test-reference",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#single-covariate-f-test-general-steps-for-hypothesis-test-reference",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Single covariate F-test: general steps for hypothesis test (reference)",
    "text": "Single covariate F-test: general steps for hypothesis test (reference)\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_j=0\\\\\n\\text{vs. } H_A&: \\beta_j\\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that predictor/covariate \\(j\\) significantly improves the prediction of Y, given all the other covariates are in the model (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#lets-think-about-our-mlr-example-for-life-expectancy-1",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#lets-think-about-our-mlr-example-for-life-expectancy-1",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Let’s think about our MLR example for life expectancy",
    "text": "Let’s think about our MLR example for life expectancy\nOur proposed population model\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\ \\text{FS}\n\\end{aligned}\\]\nOur main question for the single covariate subset F-test: Is the regression model containing food supply improve the estimation of countries’ life expectancy, given female literacy rate is already in the model?\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-2",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-2",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\n\nReduced / null model \\[LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 27.24\\]\n \n\\[SSE = 37.39\\]\n\n\n\nFull / Alternative model \\[LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 36.39\\]\n \n\\[SSE = 28.25\\]"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#poll-everywhere-question-4",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#poll-everywhere-question-4",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13-1",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13-1",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "So let’s step through our hypothesis test (1/3)",
    "text": "So let’s step through our hypothesis test (1/3)\n\n\n\nMet underlying LINE assumptions\n\n\n\n \n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_2=0\\\\\n\\text{vs. } H_A&: \\beta_2\\neq0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k =2\\) and denominator \\(df=n-k-1 = 72 - 2-1=69\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23-1",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23-1",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "So let’s step through our hypothesis test (2/3)",
    "text": "So let’s step through our hypothesis test (2/3)\n\n\n\nCompute the value of the test statistic / 6. Calculate the p-value\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\] ANOVA table:\n\nanova(mod_red2, mod_full2) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate\n70.000\n2,654.875\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD\n69.000\n2,005.556\n1.000\n649.319\n22.339\n0.000"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33-1",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33-1",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "So let’s step through our hypothesis test (3/3)",
    "text": "So let’s step through our hypothesis test (3/3)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n \nWe reject the null hypothesis at the 5% significance level. There is sufficient evidence that countries’ food supply contributes significantly to the prediction of life expectancy, given that female literacy rate is already in the model (p-value &lt; 0.001)."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#covariate-subset-test-group-of-variables",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#covariate-subset-test-group-of-variables",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Covariate subset test: group of variables",
    "text": "Covariate subset test: group of variables\nDoes the addition of some group of covariates of interest add significantly to the prediction of Y obtained through other independent variables already present in the model?\n\nFor a general population MLR model, \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\nWe can create a hypothesis test for a group of covariate coefficients (subset of many)… For example…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=\\beta_3 =0\\) (this can be any coefficients)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\nAt least one \\(\\beta_j\\neq0\\) (for \\(j=2,3\\))\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(Y = \\beta_0 + \\beta_2 X_2 + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X + \\beta_3 X_3+\\epsilon\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#covariate-subset-f-test-general-steps-for-hypothesis-test-reference",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#covariate-subset-f-test-general-steps-for-hypothesis-test-reference",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Covariate subset F-test: general steps for hypothesis test (reference)",
    "text": "Covariate subset F-test: general steps for hypothesis test (reference)\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\nFor example:\n\\[\\begin{align}\nH_0 &: \\beta_1 = \\beta_3 = 0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=1,3\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that predictors/covariates \\(2,3\\) significantly improve the prediction of Y, given all the other covariates are in the model (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#we-need-to-slightly-alter-our-mlr-example-for-life-expectancy",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#we-need-to-slightly-alter-our-mlr-example-for-life-expectancy",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "We need to slightly alter our MLR example for life expectancy",
    "text": "We need to slightly alter our MLR example for life expectancy\nOur proposed population model to include water source percent (WS):\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\beta_3 WS + \\epsilon\\]\n\nWe don’t have a fitted multiple regression model for this yet!\n\nOur main question for the group covariate subset F-test: Is the regression model containing food supply and water source percent improve the estimation of countries’ life expectancy, given percent female literacy rate is already in the model?\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\beta_3 WS + \\epsilon\\)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-3",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-3",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\n\nReduced / null model \\[LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 27.24\\]\n \n\\[SSE = 37.39\\]\n\n\n\nFull / Alternative model \\[LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\beta_3 WS + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 43.26\\]\n \n\\[SSE = 21.38\\]"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13-2",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13-2",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "So let’s step through our hypothesis test (1/3)",
    "text": "So let’s step through our hypothesis test (1/3)\n\n\n\nMet underlying LINE assumptions\n\n\n\n \n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_2=\\beta_3=0\\\\\n\\text{vs. } H_A&: \\beta_2\\neq0 \\text{ and/or } \\beta_3\\neq0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k =2\\) and denominator \\(df=n-k-1 = 72 - 2-1=69\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23-2",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23-2",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "So let’s step through our hypothesis test (2/3)",
    "text": "So let’s step through our hypothesis test (2/3)\n\n\n\nCompute the value of the test statistic / 6. Calculate the p-value\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\] ANOVA table:\n\nanova(mod_red3, mod_full3) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate\n70.000\n2,654.875\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD + WaterSourcePrct\n68.000\n1,517.916\n2.000\n1,136.959\n25.467\n0.000"
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33-2",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33-2",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "So let’s step through our hypothesis test (3/3)",
    "text": "So let’s step through our hypothesis test (3/3)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n \nWe reject the null hypothesis at the 5% significance level. There is sufficient evidence that countries’ food supply or water source (or both) contribute significantly to the prediction of life expectancy, given that female literacy rate is already in the model (p-value &lt; 0.001)."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf.html#other-ways-to-word-the-hypothesis-tests-reference",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf.html#other-ways-to-word-the-hypothesis-tests-reference",
    "title": "Lesson 9: MLR: Inference + F-test",
    "section": "Other ways to word the hypothesis tests (reference)",
    "text": "Other ways to word the hypothesis tests (reference)\n\nSingle covariate subset F-test\n\n\\(H_0:\\) \\(X^*\\) does not significantly improve the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_p\\) are already in the model\n\\(H_A:\\) \\(X^*\\) significantly improves the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_p\\) are already in the model\n\nGroup covariate subset F-test\n\n\\(H_0:\\) The addition of the \\(s\\) variables \\(X_1^*, X_2^*, \\ldots, X_s^*\\) does not significantly improve the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_q\\) are already in the model\n\\(H_A:\\) The addition of the \\(s\\) variables \\(X_1^*, X_2^*, \\ldots, X_s^*\\) significantly improves the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_q\\) are already in the model\n\n\n\n\nLesson 9: MLR 2"
  },
  {
    "objectID": "homework/HW_05.html",
    "href": "homework/HW_05.html",
    "title": "Homework 5",
    "section": "",
    "text": "Caution\n\n\n\nThis homework is not ready to be worked on!! Nicky needs to update it."
  },
  {
    "objectID": "homework/HW_05.html#directions",
    "href": "homework/HW_05.html#directions",
    "title": "Homework 5",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the HW3 datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as your HW3 .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\nFor each question, make sure to include all code and resulting output in the html file to support your answers\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nWrite all answers in complete sentences as if communicating the results to a collaborator.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your .qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_05.html#question-1",
    "href": "homework/HW_05.html#question-1",
    "title": "Homework 5",
    "section": "Question 1",
    "text": "Question 1\nWe are going to revisit the Palmer Penguins dataset from Homework 4. Choosing what to test, interpretations of coefficients, F-test conclusions, and interactions\nFor this problem we will be using the penguins dataset from the palmerpenguins R package. We will look at the association between flipper length of penguins (measured in mm) and specific species of penguins.\nDescription from help file:\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\n\nMore info about the data.\n\n# first install the palmerpenguins package\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\ndata(penguins)\n\n# run the command below to learn more about the variables in the penguins dataset\n# ?penguins\n\n\nPart a\nMake a plot of flipper length (outcome) and body mass (explanatory variable). Discuss what you see in the plot.\n\n\nPart b\nWrite the simple linear regression model that we will fit for the association between body mass and flipper length. If you use any short hand, please write it out. For example: Let \\(BD\\) represent bill depth.\n\n\nPart c\nRun the simple linear regression model for the association between body mass and flipper length. Display the regression table output.\n\n\nPart d\nInterpret the coefficient for body mass. Note that as we move forward with a multivariate model, we will refer to this is estimate at the the crude or unadjusted coefficient estimate.\n\n\nPart e\nDiscuss how centering body mass might help with interpretability. Then, center body mass around the mean, run the model again, and display the regression table. Does the intercept and/or slope change from Part c?\n\n\nPart f\nMake a plot of flipper length (outcome) and body mass (explanatory variable) by bill depth. Discuss what you see in the plot. (Hint: bill depth will be the color in the plot.)\n\n\nPart g\nMake a plot of flipper length (outcome) and body mass (explanatory variable) by penguin species. Discuss what you see in the plot and relate it back to the plot in Part f.\n\n\nPart h\nUsing only body mass and bill depth as covariates, write out the model that we would fit including the main effects of body mass and bill depth and their interaction (this is a mathematical equation). How many coefficients are tested when we test for a significant interaction?\n\n\n\n\n\n\nNote\n\n\n\nBoth covariates should be represented as centered. For the rest of the homework, we will use the centered body mass and bill depth.\n\n\n\n\nPart i\nCenter bill depth.\n\n\nPart j\nUsing only body mass and bill depth as covariates, test if bill depth is an effect modifier.\n\n\nPart k\nUsing only body mass and species as covariates, write out the model that we would fit including the main effects of body mass and species and their interaction. How many coefficients are tested when we test for a significant interaction?\nHint: Homework 4 can help guide us with the species’ categories.\n\n\nPart l\nUsing only body mass and species as covariates, test if species is an effect modifier.\n\n\nPart m\nUsing the results in the above parts, we will move forward with the following model:\n\\[\\begin{aligned}\nFL = & \\beta_0 + \\beta_1 BM^c + \\beta_2 BD^c +  \\beta_3 I(\\textrm{Chinstrap}) + \\beta_4 I(\\textrm{Gentoo}) +  \\\\ & \\beta_5 BM^c \\cdot I(\\textrm{Chinstrap}) + \\beta_6 BM^c \\cdot I(\\textrm{Gentoo}) + \\epsilon\n\\end{aligned}\\]\nRun the above model and display the regression table output.\nPlease note that this is not exactly the best method for selecting a model. I just wanted to step us through a similar thought process.\n\n\nPart n\nInterpret each coefficient in the model above. There should be 7 total interpretations."
  },
  {
    "objectID": "homework/HW_00.html",
    "href": "homework/HW_00.html",
    "title": "Homework 0",
    "section": "",
    "text": "Caution\n\n\n\nHomework is ready! 1/6/25"
  },
  {
    "objectID": "homework/HW_00.html#directions",
    "href": "homework/HW_00.html#directions",
    "title": "Homework 0",
    "section": "Directions",
    "text": "Directions\nThis homework must be turned into Sakai. I want to make sure we are all familiar with the process of downloading a .qmd file, editing it, and resubmitting an .html and .qmd file.\nHere are the instructions for downloading and submitting your homework:\n\nGo to this github site to download the homework’s .qmd file.\nWhen you reach the site, it should look like this (from my 2023 class):\nClick the “Download raw file” icon () to download the .qmd file. This will likely download the file into your “Downloads” folder. It is up to you to move the file into the appropriate folder.\nPlease rename you homework as Lastname_Firstinitial_HW0.qmd . This will help organize the homeworks when the TAs grade them.\nPlease also update the following line under subtitle: \"BSTA 512/612\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\nEdit the document with your explanations and code. If you are writing out an answer or calculating by hand, you can take a picture of your work and embed it within the .qmd file. The pictures will be viewable on the .html file.\nPlease upload your homework to this Sakai assignment. Upload both your .qmd code file and the rendered .html file.\n\nThese instructions will not appear on every homework. You may come back to this page for reference.\n\nPurpose\nThis homework is meant to introduce yourself to me. I’d like you to share some information over Sakai.\n\n\nGrading\nGrading will be done as a check/no check for turning in your work. If you are stressed about time, please turn in whatever you have completed."
  },
  {
    "objectID": "homework/HW_00.html#questions",
    "href": "homework/HW_00.html#questions",
    "title": "Homework 0",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nPlease upload a picture of yourself to Sakai. Make sure your face is visible in this picture. It will help me and the TAs identify you.\nPlease follow the below steps to add a picture of yourself in Sakai:\n\nClick on the top right circle on your Sakai page. If you do not have a picture already, your initials will be showing.\nClick “Profile.”\nOnce in your profile, hover over the square then click “Change picture.”\nUpload a picture file of yourself.\n\nPlease include the picture you used below. This will make sure we are all able to insert a picture within Quarto. It will also make sure I can see the photo within the .html file.\n\n\nQuestion 2\nProvide a pronunciation of your name: Please follow the below steps to add an audio and written pronunciation of your name in Sakai:\n\nClick on the top right circle on your Sakai page. If you do not have a picture already, your initials will be showing.\nClick “Profile.”\nHover over the section “Name Pronunciation and Pronouns.” Click the edit button that should appear in the top right corner of the section.\nAdd a recording of your name pronunciation.\nOPTIONAL You may also edit the phonetic pronunciation of your name. I realize that doing this is may require a lot of time researching phonetics.\nOPTIONAL If you are comfortable sharing your pronouns, please change these as well.\n\n\n\nQuestion 3\nPlease complete the following whenisgood poll so that we can schedule office hours. Please use a unique identifier (does not have to be your name), so that I can make sure each student can attend at least one office hour.\n\n\nQuestion 4\nCompletion of this question is only necessary if you have accommodations. If you have any learning accommodations, please email me about your needs. I should receive a direct email from the Office of Student Access, but it is important that we discuss how accommodations will translate to our class.\n\n\nQuestion 5\nGo to the OneDrive folder and request access.\n\n\nQuestion 6\nVote on your preferred attendance policy here!\n\n\nQuestion 7\nIf you are submitting a late homework or informing me about an extension, who should be included in the email?\n(See Syllabus for answer)"
  },
  {
    "objectID": "homework/HW_03.html",
    "href": "homework/HW_03.html",
    "title": "Homework 3",
    "section": "",
    "text": "Caution\n\n\n\nThis homework is ready! 2/6/25"
  },
  {
    "objectID": "homework/HW_03.html#directions",
    "href": "homework/HW_03.html#directions",
    "title": "Homework 3",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the HW3 datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as your HW3 .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\nFor each question, make sure to include all code and resulting output in the html file to support your answers\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nWrite all answers in complete sentences as if communicating the results to a collaborator.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your .qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_03.html#questions",
    "href": "homework/HW_03.html#questions",
    "title": "Homework 3",
    "section": "Questions",
    "text": "Questions\n\nMidterm feedback!!\nPlease complete the midterm feedback along with this homework!!\n\n\nQuestion 1\nThis question and data are adapted from this textbook.\nIn an experiment designed to describe the dose–response curve for vitamin K, individual rats were depleted of their vitamin K reserves and then fed dried liver for 4 days at different dosage levels. The response of each rat was measured as the concentration of a clotting agent needed to clot a sample of its blood in 3 minutes. The results of the experiment on 12 rats are given in the following table; values are expressed in common logarithms for both dose and response.\n\nclot = read_excel(here(\"./data/CH05Q09.xls\"))\nclot %&gt;% gt() %&gt;%\n  cols_label(RAT = md(\"**Rat**\"),\n             LOGCONC = md(\"**Log10 Concentration (Y)**\"),\n             LOGDOSE = md(\"**Log10 Dose (X)**\"))\n\n\n\n\n\n  \n    \n      Rat\n      Log10 Concentration (Y)\n      Log10 Dose (X)\n    \n  \n  \n    1\n2.65\n0.18\n    2\n2.25\n0.33\n    3\n2.26\n0.42\n    4\n1.95\n0.54\n    5\n1.72\n0.65\n    6\n1.60\n0.75\n    7\n1.55\n0.83\n    8\n1.32\n0.92\n    9\n1.13\n1.01\n    10\n1.07\n1.04\n    11\n0.95\n1.09\n    12\n0.88\n1.15\n  \n  \n  \n\n\n\n\nUse the log-transformed values as given in the dataset.\nUse the following scatterplot to build your answers off of:\n\n\n\n\n\n\n\n\n\n\nPart a\nFit a linear regression model to the data and add the regression line to the plot.\n\n\nPart b\nUse R to create the ANOVA table for the regression described in the exercise.\n\n\nPart c\nUsing the F-test, determine whether there is an association between the log10 concentration and log10 dose.\n\n\n\n\n\n\nNote\n\n\n\nMake sure to include all needed steps for an F-test. Calculating the F test statistic (step 5) is not needed if you use the ANOVA table. Make sure your conclusion connects back the research context.\n\n\n\n\nPart d\nRewrite your hypothesis test in Part c to show the null and alternative models that we are testing. Did we reject the smaller (reduced) model?\n\n\n\n\n\n\nNote\n\n\n\nYou do not need to go through the hypothesis test process again. A quick statement on rejecting or not is okay.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you prefer to write out the models by hand, remember that you can take a picture of your work and insert it into this document. HW0 can be a good reference for how we’ve done this before.\n\n\n\n\n\nQuestion 2\nWe will continue to work with the study and dataset from Question 2 above.\n\nPart a\nFind the correlation coefficient between the two variables. Is the value consistent with your description of the relationship in Question 2? Why or why not?\n\n\nPart b\nCalculate the coefficient of determination using linear regression summary output. Can we also calculate the coefficient of determination from the ANOVA in Question 2?\n\n\nPart c\nGive an interpretation of the coefficient of determination in the context of the study.\n\n\n\nQuestion 3\nA high respiratory rate is a potential diagnostic indicator of respiratory infection in children. To judge whether a respiratory rate is “high” however, a physician must have a clear picture of the distribution of normal rates. To this end, Italian researchers measured the respiratory rates (in breaths/minute) of 618 children between the ages of 15 days and 3 years (measured in months).\nThe data and problem framing came from the Sleuth3 package. Please make sure to run the following code to load the data. You can directly access the dataset ex0824 from the package. I have included a new assignment of the data to q1_data if you would like to use that.\n\nif(!require(Sleuth3)) { install.packages(\"Sleuth3\"); library(Sleuth3) }\nq1_data = ex0824 \n\n\nPart a\nCreate a scatterplot of the dependent and independent variables with both the best-fit line and a smoothed curve through the points. Describe the relationship between the dependent and independent variables, and also comment on whether you think it is reasonable to use a linear regression to model the relationship.\n\n\nPart b\nWrite out the population regression model for the simple linear regression model. Please leave the variables untransformed for now.\n\n\nPart c\nFit the regression model, display the regression table, and write out the fitted regression line.\n\n\nPart d\nAssess the normality of the model’s fitted residuals by creating a histogram, density plot, and boxplot of the residuals to visually inspect the distribution of the residuals, and describe any deviations from normality.\n\n\nPart e\nAssess the normality of the model’s fitted residuals by creating QQ plot of the residuals.\nBonus work, but not required: Compare the QQ plot to 4 such plots simulated from normal data, and discuss why or why not the residuals could have come from a normal distribution.\n\n\nPart f\nTest the normality of the model’s fitted residuals and comment on whether the test’s conclusion is consistent with your visual inspection or not. Make sure to include the hypotheses, needed R code, and a conclusion to the test based on the p-value (as shown in these slides).\n\n\nPart g\nCreate a residual plot using ggplot and the residuals. Discuss what this means in the context of our model assumptions.\n\n\nPart h\nDetermine whether there are any observations with high leverage. Please use the cutoff, \\(h_i &gt; 6/n\\). If there are observations with high leverage, print the observations and state how many high leverage points there are.\n\n\nPart i\nPrint the 10 observations with highest Cook’s distance. If there are observations with high Cook’s distance (\\(d_i &gt;1\\)), state how many observations have high Cook’s distance.\n\n\nPart j\nCreate a histogram for rate. Describe its distribution shapes.\n\n\nPart k\nUsing the above histogram, and Tukey’s ladder of transformations, discuss the range of transformations that will be appropriate for Rate. Explain your reasoning.\nThen use gladder() to decide on two possible transformations. Explain your reasoning.\nNote: questions below will ask about model fit with the transformations. For now, just explain why you chose the two that you did.\n\n\nPart l\nAdd the two rate transformations you chose above to the dataset. You do not need to print any output, just make sure the code is visible.\n\n\nPart m\nCreate scatterplots using two transformed rates and age. Discuss if either transformation potentially improves the model fit. Explain why or why not. Note: including lines will help!\n\n\nPart n\nUsing one of the transoformed outcomes, fit the regression model, display the regression table, and write out the fitted regression line.\n\n\nPart o\nAssess the normality of the model’s fitted residuals by creating QQ plot of the residuals. Does the transformation improve the QQ plot?\n\n\nPart p\nCreate a residual plot using ggplot and the residuals. Discuss what this means in the context of our model assumptions. Does the transformation improve our model assumptions?\n\n\nPart q\nBetween the model with the untransformed outcome and the transformed outcome, which would you recommend using for analysis? (Hint: there are pros and cons to both models)\n\n\n\nQuestion 4\nThis question uses the same dataset as HW 2, question 1.\nThis question is based on data collected as part of an observational study of patients who suffered from stroke.\nDataset: The main goal was to study various psychological factors: optimism, fatalism, depression, spirituality, and their relationship with stroke severity and other health outcomes among the study participants. Data were collected using questionnaires during a baseline interview and also medical chart review. More information about this study can be found in the article Fatalism, optimism, spirituality, depressive symptoms and stroke outcome: a population based analysis.\nThe dataset that you will work with is called completedata.sas7bdat. The two variables we are interested in are:\n\nCovariate 1: Fatalism (larger values indicate that the individual feels less control of their life)\n\nPotential scores range from 8 to 40\n\nCovariate 2: Optimism (larger values indicate that the individual feels higher levels of optimism)\n\nPotential scores range from 6 to 24\n\nCovariate 3: Spirituality (larger values indicate that the individual has more belief in a higher power)\n\nPotential scores range from 2 to 8\n\nOutcome: Depression (larger values imply increased depression)\n\nPotential scores range from 0 to 27\n\n\nFor our homework purposes we will assume each variable is continuous.\n\ndep_df = read_sas(here(\"./data/completedata.sas7bdat\"))\n\n\nPart a\nFit the regression model with all the covariates (Fatalism, Optimism, Spirituality), display the regression table, and write out the fitted regression line.\n\n\nPart b\nInterpret each coefficient (\\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\)).\nDoes the intercept make sense for the range of values that each covariate can take? Explain.\n\n\nPart c\nRecall in Homework 2, we ran a simple linear regression model for Depression vs. Fatalism with the following interpretation for the coefficient: For every 1 point higher fatalism score, there is an expected difference of 0.25 points higher depression score (95%CI: 0.17, 0.32).\nDoes the addition of Optimism and Spirituality change our coefficient estimate for Fatalism? (No need for an official hypothesis test here. I just want us to note some differences.)\n\n\nPart d\nFrom the fitted regression model, calculate the regression line when Optimism score is 10 and Spirituality score is 6."
  },
  {
    "objectID": "labs/Lab_03_work.html",
    "href": "labs/Lab_03_work.html",
    "title": "Lab 3 Instructions",
    "section": "",
    "text": "IMPORTANT TO READ\n\n\n\n\nPlease do not delete the rubric from your .qmd file. I will use it to circle the grades!\nThere is an intructions file and a file for you to edit and turn in. Please only work in the latter file!!"
  },
  {
    "objectID": "labs/Lab_03_work.html#directions",
    "href": "labs/Lab_03_work.html#directions",
    "title": "Lab 3 Instructions",
    "section": "1 Directions",
    "text": "1 Directions\nPlease turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here.\n\n\n\n\n\n\nCaution\n\n\n\nThis is the instructions file. The link above will take you to the editing file where you can add your work and turn it in!! Please do not remove anything from the editing file!!\n\n\n\n1.1 Purpose\nThe main purpose of this lab is to perform some quality control on our data, recode some of the multi-selection categorical variables, continue data exploration, and start analyzing the main relationship of our research question.\n\n\n1.2 Grading\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\n1.2.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning (like Section 2.4 and explanations in Section 2.5)"
  },
  {
    "objectID": "labs/Lab_03_work.html#lab-activities",
    "href": "labs/Lab_03_work.html#lab-activities",
    "title": "Lab 3 Instructions",
    "section": "2 Lab activities",
    "text": "2 Lab activities\nBefore starting this lab, you should go back to Lab 2, save a new .rda file that contains all the new variables from that Lab. Then you can load it here!\n\n2.1 Restate your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\nHow is implicit anti-fat bias, as measured by the IAT score, associated with “insert main independent variable here”?\n\n\n2.2 Quality Control\nThere are a few more issues with the data that we need to look into. First, there is another coding for NA values in the race variable: -999. We will need to filter out these observations.\nWe will also need to look at individuals who have potentially answered the survey questions untruthfully. We cannot catch everything, but a good place to start is by looking at individuals who have done more than one of the following:\n\nselected the earliest or latest possible birth year\nselected the lowest or highest possible education\nselected all gender identities (for those using gender identity)\nselected all races (for those using multiple selection race)\nselected the lowest or highest weight (for those looking at BMI)\nselected the lowest or highest height (for those looking at BMI)\n\nI want to take a second to mention that any of the above selections, and combinations of the above selections, are valid. However, we should start to flag the possibility that someone has not gone through the survey properly if we notice that most or all of the respondent’s answers are the first answer choice, last answer choice, or selected all options. Additionally, not all of these carry the same importance in discerning validity. For example, a recorded age of 111 years old is the most striking to me. When paired with other selections that are the maximum or minimum (or first or last) option, then I will record it for future investigation. If this observation looks to be an outlier or high leverage point in our analysis, that is when I’ll decide to remove it.\n\n\n\n\n\n\nTasks\n\n\n\n\nFilter out observations with a value of -999 in the race variable.\nGlimpse at the observations that may indicate a respondent who has not properly completed the survey portion. This will require filtering for specific answer choices. Please see examples of filter() on it’s documentation page.\n\n\n\n\n\n2.3 Working with multi-selection variables\nIn the list of variables that we may choose to work with (in Lab 2), there are two that allowed respondents to select multiple categories. The two variables are genderIdentitiy and raceombmulti. If you did not choose these variables to work with, you may skip this section.\nIf you chose one or both of these variables, then we need to make new variables that correspond to indicators for each possible selection in the respective variable.\nLet’s start with the grepl function. For this function, we can input one of our column names and a value, then it will output, for each row, if the value is in the column. For example, in genderIdentity an individual may identify as a “Trans female/Trans woman” and “Gender queer/Gender nonconforming.” In our dataset in R, this would show as [4,5] in genderIdentity. If we want to create two separate indicators for anyone who identifies as “Trans female/Trans woman” then I need to look for the value 4 in the column genderIdentity. I will run a separate indicator to find individuals who identify as “Gender queer/Gender nonconforming.” Here is an example code of how I would use grepl to do this:\n\niat_prep_new = iat_prep_old %&gt;%\n  mutate(ind_tf_tw = grepl(4, genderIdentity), \n         ind_gq_gnc = grepl(5, genderIdentity))\n\nYou will need to extend this to all other gender identities.\nFor race, raceombmulti is also the follow up question to raceomb_002. So our indicators need to reflect both variables. In this case, we need to use grepl on both columns at once. For example, if I want to create an indicator for individuals who identify as American Indian/Alaskan Native then I need to find individuals who identify as American Indian/Alaskan Native only and individuals who identify as American Indian/Alaskan Native in addition to another race. For example, my code might look like:\n\niat_prep_new = iat_prep_old %&gt;%\n  mutate(ind_AIAN = grepl(1, raceomb_002) | grepl(1, raceombmulti))\n\nI suggest only searching for 1-7 in both raceomb_002 and raceombmulti. Note that if raceomb_002 = 8 , then individuals identified as “multiracial” and will select values in raceombmulti.\n\n\n\n\n\n\nTask\n\n\n\nIf you are using genderIdentity or raceombmulti, create indicator variables for each possible selection.\n\n\n\n\n2.4 Thinking about potential confounders and effect modifiers\nBefore we explore more of the data, I want us to take a second to think through potential confounders and effect modifiers from the covariates that we selected in Lab 2. For some of the covariates, we were asked to explain why we chose them. Now I want you to consider how each could alter the relationship between IAT score and your variable of interest (from your research question). For each covariate, explain how it might or might not change the relationship. For example, if our variable of interest is fat group identity, then we may consider that self-perception of size is a confounder since it could be linked with fat group identity and potentially be associated with IAT score.\nFor multi-level, unordered categorical covariates, you might consider if a specific category has an impact. For example, we might consider creating an indicator for white, non-Hispanic/Lantinx respondents since the history of fatphobia is tied with white-centered colonization and white supremacy (Redpath, 2023). Thus IAT scores might look different for White respondents vs. minority respondents (those who answered American Indian/Alaskan Natives; East Asian; South Asian; Native Hawaiian or other Pacific Islander; Black or African American; or Hispanic or Latino). Alternatively, we may not want to center our analysis on whiteness. The same fatphobic history involving white supremacy was particularly targetting Black people. So perhaps we want make an indicator for Black or African American respondents. Another option is leaving race as is - we may have enough data to handle the inclusion of all groups!\nThe purpose of this section is to make sure we are thinking about the relationships between variables in our analysis. I do not want us to make any decisions based solely on the data. I want any changes or manipulations in our variables to be motivated by research-backed evidence.\nFinally, for this project, we are most interested in the relationship we identified in our research question! Other variables are supporting this question, and improving that model fit so that we get as close to the true relationship in our research question as possible!\n\n\n\n\n\n\nTask\n\n\n\nFor each variable, consider how each could alter the relationship between IAT score and your variable of interest (from your research question). For each covariate, explain how it might or might not change the relationship.\n\n\n\n\n2.5 Continuing data exploration\nIn this section, we are going to further explore the variables that we might be adjusting for in our model (potential covariates outside the variable or interest in our research question).\n\n2.5.1 Bivariate exploration\nWe want to look at all other relationships between IAT score and each covariate (outside of the research question variable). Some of you have already made these plots in Lab 2, so you can simply refine them and display them here. There are a few questions that I want you to consider:\n\nFor categorical variables, is there an inherent order? Does the ordered values follow an approximately linear relationship? Are the categories “evenly spaced”? For example, education categories are not necessarily evenly spaced.\nAgain for categorical variables, is there a natural place to divide the categories up? For example, in education, it might be helpful to control for the fact that students in college might be asked to complete this test as an assignment. Thus, we might make an indicator for individuals in college vs. not. This decision can be informed by our plot, but it should not be driven by our plot!!\n\n\n\n\n\n\n\nTask\n\n\n\nFor each variable outside of your research question, create the appropriate plot to visualize the relationship between IAT score and the variable. Comment if there is an obvious trend or not.\n\n\n\n\n2.5.2 Multivariate exploration\nNow we want to extend our plots for Lab 2 where we looked at the outcome (IAT score) and our main variable of interest (as identified by our research question). Here, we will run the same plot, but include another variable. This will help us visualize potential confounders or effect modifiers. Note that if you made indicator variables (for race, gender identity or any other variable), then you should have a plot for each indicator variable.\nYou will need to really think about what kind of plot will best displays these relationships! IAT score is continuous, and many of your variable of interest is categorical. You may consider side-by-side boxplots where the color is the additional variable. You might also consider a jitter plot or only plotting the means. Remember you’re goal for plotting is to get a sense of the relationship only from the plot! Your audience should not have to work hard to understand what the plot is communicating. For example, I wanted to look at IAT score, internalization of societal standards, and race. I might make my plot like such:\n\n\nCode to contruct multivariate plot\nggplot(iat_prep2, aes(x = important_001, y=IAT_score, color = as.factor(raceomb_002_f))) +\n  # geom_jitter(size = 2, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 3, shape = 18) +\n  stat_summary(fun = mean, geom=\"line\") +\n  scale_x_discrete(limits = levels(iat_prep2$important_001_f)) +\n  labs(x = \"Importance of weight to sense of self \\n (Internalization of societal standards)\", \n       y = \"IAT score\",\n       title = \"Mean IAT scores for importance of weight to sense of self by race\", \n       color = \"Race\") +\n  theme(axis.text.x = element_text(angle = 45, size = 8, hjust = 1))\n\n\n\n\n\n\n\nCode to contruct multivariate plot\nggplot(iat_prep2, aes(x = important_001, y=IAT_score, color = as.factor(raceomb_002_f))) +\n  # geom_jitter(size = 2, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 3, shape = 18) +\n  stat_summary(fun = mean, geom=\"line\") +\n  scale_x_discrete(limits = levels(iat_prep2$important_001_f), labels = function(x) str_wrap(x, width=10)) +\n  labs(x = \"Importance of weight to sense of self \\n (Internalization of societal standards)\", \n       y = \"IAT score\",\n       title = \"Mean IAT scores for importance of weight to sense of self by race\", \n       color = \"Race\")\n\n\n\n\n\nNote that the above plot is specific for these variables!! Other variables may require a different type of visua\nlization!! Also note that I originally had geom_jitter() in my plot, which would make the plot really hard to understand!! Try uncommenting it to see what I mean by “hard to understand.” Also, think about why I connected the mean IAT scores across the different levels of internalization. I had a hard time connecting specific race’s points to identify a trend. Again, try commenting out stat_summary(fun = mean, geom=\"line\") to see what I mean.\n\n\n\n\n\n\nNote\n\n\n\nAn aside: You may see that collapsing groups might wash out differences. If we make an indicator for Black of African American respondents, as we mentioned above, then including White respondents with other minority groups may wash out their association with IAT score and wrongly lead us to a model that says identifying as Black or African American has no association with IAT, where we clearly see that Black or African American respondents have a unique trajectory for IAT scores.\n\n\nPlease make sure that you have made the needed changes to your plot in Lab 2. I noticed many unordered groups in plots where there should be an inherent order and unreadable axes because the text was not tilted. Please see discussion on Slack for what some students did to achieve these plots.\nHere are a few sources that might help you get started with the visualizations:\n\nIntro to R\nModern Data Visualization with R\n\n\n\n\n\n\n\nTask\n\n\n\nFor at least 3 variables outside of your research question, create the appropriate plot to visualize the relationship between IAT score, your main variable (in research question), and the variable outside your research question. Comment whether you can determine anything from the plot or not. If you can, is there any indication that the variable is a confounder or effect modifier?\n\n\n\n\n\n2.6 Fit a simple linear regression\nAs a starting point, it is good to fit a simple linear regression for our primary research question. This is often called the “crude” association. It just means that we are not adjusting for any other variables, and establishing the “starting point” for our analysis. It is likely that the results of the regression will change as we add other variables in the model.\n\n\n\n\n\n\nTask\n\n\n\nRun a simple linear regression model for the relationship in your primary research question. Print the regression table. Interpret the results and comment on the initial trend you see.\n\n\n\n\n\n\n\n\nBonus Task\n\n\n\nThis is not required in Lab 3. However, if you want to run a multiple linear regression model with one other variable that you plotted in Multivariate Exploration, then you should try it! Do the results align with your ideas in Section 2.4 and/or the visualization you saw in Section 2.5?"
  },
  {
    "objectID": "labs/Lab_03_work.html#bibliography",
    "href": "labs/Lab_03_work.html#bibliography",
    "title": "Lab 3 Instructions",
    "section": "3 Bibliography",
    "text": "3 Bibliography\nRedpath, F. (2023). Abolish the Body Mass Index: A Historical and Current Analysis of the Traumatizing Nature of the BMI. Tapestries:  Interwoven Voices of Local and Global Identities, 12(1). https://digitalcommons.macalester.edu/tapestries/vol12/iss1/12"
  },
  {
    "objectID": "labs/Lab_03.html",
    "href": "labs/Lab_03.html",
    "title": "Lab 3",
    "section": "",
    "text": "# PLEASE DO NOT REMOVE THIS CODE CHUNK!!!\n### ADD YOUR LIBRARIES HERE!!! ####\n\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(here)"
  },
  {
    "objectID": "labs/Lab_03.html#directions",
    "href": "labs/Lab_03.html#directions",
    "title": "Lab 3",
    "section": "1 Directions",
    "text": "1 Directions\nYou can download the .qmd file for this lab here.\nThis is your editing file. Please do not remove anything from this editing file!! You will only add your code and work to this file.\n\n1.1 Grading\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\n1.1.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning (like Section 2.4 and explanations in Section 2.5)"
  },
  {
    "objectID": "labs/Lab_03.html#lab-activities",
    "href": "labs/Lab_03.html#lab-activities",
    "title": "Lab 3",
    "section": "1 Lab activities",
    "text": "1 Lab activities\nBefore starting this lab, you should go back to Lab 2, save a new .rda file that contains all the new variables from that Lab. Then you can load it here!\n\n1.1 Restate your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\n\n\n1.2 Quality Control\n\n\n\n\n\n\nTasks\n\n\n\n\nFilter out observations with a value of -999 in the race variable.\nGlimpse at the observations that may indicate a respondent who has not properly completed the survey portion. This will require filtering for specific answer choices. Please see examples of filter() on it’s documentation page.\n\n\n\n\n\n1.3 Working with multi-selection variables\n\n\n\n\n\n\nTask\n\n\n\nIf you are using genderIdentity or raceombmulti, create indicator variables for each possible selection.\n\n\n\n\n1.4 Thinking about potential confounders and effect modifiers\n\n\n\n\n\n\nTask\n\n\n\nFor each variable, consider how each could alter the relationship between IAT score and your variable of interest (from your research question). For each covariate, explain how it might or might not change the relationship.\n\n\n\n\n1.5 Continuing data exploration\n\n1.5.1 Bivariate exploration\n\n\n\n\n\n\nTask\n\n\n\nFor each variable outside of your research question, create the appropriate plot to visualize the relationship between IAT score and the variable. Comment if there is an obvious trend or not.\n\n\n\n\n1.5.2 Multivariate exploration\n\n\n\n\n\n\nTask\n\n\n\nFor at least 3 variables outside of your research question, create the appropriate plot to visualize the relationship between IAT score, your main variable (in research question), and the variable outside your research question. Comment whether you can determine anything from the plot or not. If you can, is there any indication that the variable is a confounder or effect modifier?\n\n\n\n\n\n1.6 Fit a simple linear regression\n\n\n\n\n\n\nTask\n\n\n\nRun a simple linear regression model for the relationship in your primary research question. Print the regression table. Interpret the results and comment on the initial trend you see.\n\n\n\n\n\n\n\n\nBonus Task\n\n\n\nThis is not required in Lab 3. However, if you want to run a multiple linear regression model with one other variable that you plotted in Multivariate Exploration, then you should try it! Do the results align with your ideas in Section 2.4 and/or the visualization you saw in Section 2.5?"
  },
  {
    "objectID": "labs/Lab_02.html",
    "href": "labs/Lab_02.html",
    "title": "Lab 2",
    "section": "",
    "text": "Task Summary\n\n\n\nDownload the 2021 data and codebook from the archives and store in accessible folder.\n\n\n\n\n\n\n\n\n\n\n\nTask Summary\n\n\n\nRead csv, save as rda, load rda, glimpse at data.\nHow many rows and columns are in the dataset? Do you think we will need all these variables for our analysis?\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nDescribe our target population. Keep your description to 3-5 characteristics, not including our restriction on the US population.\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nFrom the 8 attitudes and beliefs, please select 3 that you think will be the most important variables related to your research question. In 1-2 lines, briefly explain why you chose each variable. This can be informal and bulleted.\n\n\n\n\n\n\n\n\nTask\n\n\n\nUsing R, select your identified variables from your dataset.\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nIdentify and list the variables that are coded numerically and correspond to categories. Create a new variable for the categorical/factor version of the variable. It is up to you to check that your code ran properly!! If you are using multi-choice categorical variables (might include race, gender), then do not convert the variable yet!\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nFrom the codebook, find the variables that we can use to construct the test date. Then use make_date() to create the test date.\n\n\n\n\n\n\n\n\nTask\n\n\n\nFollowing the above example, create an age variable that measures the years between individuals’ birth and test date. Then remove the variables used to make age.\n\n\n\n\n\nThis is optional!!\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nMake a new dataset with only complete cases. Save this dataset in your project folder.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nPlease plot a histogram of the IAT scores. What do you notice about the outcome?\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nUsing ggplot or tables, visualize your variables. Get a sense of each variable’s distribution. Do you notice anything out of the ordinary?\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nTake a look at the scatterplot, violin, or box plot of IAT score and your variable of interest. Use R and ggplot to make this plot. If your variable of interest is categorical, then make sure to use a violin or boxplot.\n\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nPlease restate the research question that you proposed in Lab 1. Please make sure it is only one question, one sentence long. What are your thoughts on the research question now that we looked at the data? Feel free to change it now that we’ve looked at the data. If you change your question, make sure 4.2 reflects the new research question."
  },
  {
    "objectID": "labs/Lab_02.html#directions",
    "href": "labs/Lab_02.html#directions",
    "title": "Lab 2",
    "section": "Directions",
    "text": "Directions\nPlease turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here.\nThe rest of this lab’s instructions are embedded into the lab activities.\n\nPurpose\nThe main purpose of this lab is to introduce our dataset, codebook, and variables. We will continue to think about the context of our research question, but our main focus is to become familiar with the data.\n\n\nGrading\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\nRubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nSome tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning (like target population, choosing variables, revisiting research question)"
  },
  {
    "objectID": "labs/Lab_02.html#lab-activities",
    "href": "labs/Lab_02.html#lab-activities",
    "title": "Lab 2",
    "section": "",
    "text": "Task Summary\n\n\n\nDownload the 2021 data and codebook from the archives and store in accessible folder.\n\n\n\n\n\n\n\n\n\n\n\nTask Summary\n\n\n\nRead csv, save as rda, load rda, glimpse at data.\nHow many rows and columns are in the dataset? Do you think we will need all these variables for our analysis?\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nDescribe our target population. Keep your description to 3-5 characteristics, not including our restriction on the US population.\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nFrom the 8 attitudes and beliefs, please select 3 that you think will be the most important variables related to your research question. In 1-2 lines, briefly explain why you chose each variable. This can be informal and bulleted.\n\n\n\n\n\n\n\n\nTask\n\n\n\nUsing R, select your identified variables from your dataset.\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nIdentify and list the variables that are coded numerically and correspond to categories. Create a new variable for the categorical/factor version of the variable. It is up to you to check that your code ran properly!! If you are using multi-choice categorical variables (might include race, gender), then do not convert the variable yet!\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nFrom the codebook, find the variables that we can use to construct the test date. Then use make_date() to create the test date.\n\n\n\n\n\n\n\n\nTask\n\n\n\nFollowing the above example, create an age variable that measures the years between individuals’ birth and test date. Then remove the variables used to make age.\n\n\n\n\n\nThis is optional!!\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nMake a new dataset with only complete cases. Save this dataset in your project folder.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nPlease plot a histogram of the IAT scores. What do you notice about the outcome?\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nUsing ggplot or tables, visualize your variables. Get a sense of each variable’s distribution. Do you notice anything out of the ordinary?\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nTake a look at the scatterplot, violin, or box plot of IAT score and your variable of interest. Use R and ggplot to make this plot. If your variable of interest is categorical, then make sure to use a violin or boxplot.\n\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nPlease restate the research question that you proposed in Lab 1. Please make sure it is only one question, one sentence long. What are your thoughts on the research question now that we looked at the data? Feel free to change it now that we’ve looked at the data. If you change your question, make sure 4.2 reflects the new research question."
  },
  {
    "objectID": "labs/Lab_01.html",
    "href": "labs/Lab_01.html",
    "title": "Lab 1",
    "section": "",
    "text": "Task\n\n\n\nAnswer the questions from the instructions.\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nIf you decide to listen, feel free to share a quote that most impacted you.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nIn 5-10 bullet points, write down some of your ideas on the study design that you may want to mention.\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nTake the Weight IAT test.\n\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nWrite your research question\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nDisplay your working directory using the here package and here() function.\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nWrite your introduction"
  },
  {
    "objectID": "labs/Lab_01.html#directions",
    "href": "labs/Lab_01.html#directions",
    "title": "Lab 1",
    "section": "",
    "text": "Please turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy. We can work out another way for you to turn in the labs.\nYou can download the .qmd file for this lab here.\n\n\nThis lab will serve as an introduction to our quarter long project.\nThere will be no analysis in this lab. Instead, we are building our knowledge around the research question and setting up our folder.\n\n\n\nEach lab will follow the rubric on the Project page. Since this lab does not include coding nor analysis, this portion of the rubric is excluded."
  },
  {
    "objectID": "labs/Lab_01.html#lab-activities",
    "href": "labs/Lab_01.html#lab-activities",
    "title": "Lab 1",
    "section": "",
    "text": "Task\n\n\n\nAnswer the questions from the instructions.\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nIf you decide to listen, feel free to share a quote that most impacted you.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nIn 5-10 bullet points, write down some of your ideas on the study design that you may want to mention.\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nTake the Weight IAT test.\n\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nWrite your research question\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nDisplay your working directory using the here package and here() function.\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nWrite your introduction"
  },
  {
    "objectID": "labs/Lab_04_instructions.html",
    "href": "labs/Lab_04_instructions.html",
    "title": "Lab 4 Instructions",
    "section": "",
    "text": "Caution\n\n\n\nNeeds to be worked on!"
  },
  {
    "objectID": "labs/Lab_04_instructions.html#directions",
    "href": "labs/Lab_04_instructions.html#directions",
    "title": "Lab 4 Instructions",
    "section": "1 Directions",
    "text": "1 Directions\nPlease turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here.\n\n\n\n\n\n\nCaution\n\n\n\nThis is the instructions file. The link above will take you to the editing file where you can add your work and turn it in!! Please do not remove anything from the editing file!!\n\n\n\n1.1 Purpose\nThe main purpose of this lab is to perform model selection, identify one or more potential final models, and start our interpretation of our main relationship.\n\n\n1.2 Grading\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades."
  },
  {
    "objectID": "labs/Lab_04_instructions.html#lab-activities",
    "href": "labs/Lab_04_instructions.html#lab-activities",
    "title": "Lab 4 Instructions",
    "section": "2 Lab activities",
    "text": "2 Lab activities\nBefore starting this lab, you should go back to Lab 2, save a new .rda file that contains all the new variables from that Lab. Then you can load it here!\n\n2.1 Restate your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\nHow is implicit anti-fat bias, as measured by the IAT score, associated with “insert main independent variable here”?\n\n\n2.2 Step 1: Simple linear regressions / analysis\nWe have done most of this step through visualizations in Lab 2 and 3. Now, we will quickly run a simple linear regression model for each covariate against the IAT score (outcome). Remember, the goal of this is to see if each covariate explains enough variation of the outcome, IAT score. You should have at least 9 simple linear regression models and their results. Results include the F-statistic and p-value from the test if each covariate explains enough variation of the outcome. Please revisit the slides from Lesson 5 (SLR: More inference + Evaluation) for more help with this test.\n\n\n\n\n\n\nVERY IMPORTANT FOR VARIABLES WE ORDERED USING FACTOR!!\n\n\n\nI asked that you order variables to make plots more interpretable. However, for the lm(), R reads the ordered variables in an unexpected way. For these variables to run correctly in R, we need to unorder the variables. We can also set a reference level that makes sense.\nFor example, I may want to unorder my variable iam_001 and set the reference to Neither underweight nor overweight. I can do this with:\n\niat_2021_new = iat_2021_old %&gt;% \n  mutate(iam_unordered = factor( iam_ordered, ordered = FALSE ) %&gt;% \n           relevel( ref = \"Neither underweight nor overweight\"))\n\n\n\nRecall, we mentioned 3 options to running and outputting the results of\n\nWe can run lm() for each covariate in separate lines of code, and use something like summary() or anova() to look at the results of each. (More time consuming to write, but less complicated coding)\nWe can use lapply() to run lm() and display the anova() on each covariate in one line of code. (Less time consuming to write, but more complicated coding, and more prone to errors that may not be apparent from output)\nWe can use sapply() to run lm(), anova(), and display the p-value for each covariate in one line of code. (Less time consuming to write, but more complicated coding, more prone to errors that may not be apparent from output, and no sense of what’s going on in the regression)\n\nPlease take a note for yourself if your dataset contains the original numeric versions of variables that we created factors for. I am not saying that you should take them out. They might be useful if our sample is not big enough to handle all the categorical covariates that we’ve included, but I think our sample is large enough.\n\n\n\n\n\n\nTasks\n\n\n\n\nRun a simple linear regression model for each covariate against the IAT score (outcome).\nDisplay results from the test if each covariate explains enough variation of the outcome. This may be from three options in the instructions: summary()/anova() only, lapply(), or sapply()\n\nInterpretation of the results will be in the next step.\n\n\n\n\n2.3 Step 2: Preliminary variable selection\nUsing the previous p-values from the F-test on each covariate’s SLR, decide which covariates will be included in the initial model. Recall the decision rule: we keep covariates that explain enough variation using p-value &lt; 0.25. Note that because our sample size is so large, the p-values might be really small. For now, that’s okay, but this means we may want to alter our Step 3 a little bit.\nOnce you have decided on the covariates, run the model and display the regression table.\n\n\n\n\n\n\nTasks\n\n\n\n\nDecide which covariates will be included in the initial model and list them.\nRun the initial model and display the regression table.\n\nNo need to write out the model, but you may in addition to the list.\n\n\n\n\n2.4 Step 3: Assess change in coefficient\nNow that all the selected variables are in one initial model, we can start considering the effect of each variable (outside of our main research question).\nRemember our general rule: We can remove a variable if (1) p-value &gt; 0.05 for the F-test to include or exclude the variable and (2) change in coefficient (\\(\\Delta\\%\\)) of our explanatory variable is &lt; 10%. Please remember that the p-values for the F-test for a multi-level categorical variable must be calculated by creating a reduced and full model.\nIt might be helpful to copy your list of covariates here and make note of the ones that you are removing. It was hard for me to keep track of all the variables when our dataset contains sooo many categorical covariates, and the regression table is so long.\nSince our sample size is quite large, most (if not all) of the F-tests will conclude that the variable should be kept in the model. At this point, I advise that you turn to some common sense and the change in coefficients.\n\nFor common sense, you may notice that some of your covariates are essentially measuring the same thing. If there is clinical relevance to having both in the model, then keep them in, but if not, you will have to decide which is more interpretable/relevant/aligned with your research question. For example, if you chose variables involving attitudes and beliefs that are measuring similar things, then you might exclude one. There are measurements like “I am …” with relative weight groups and “Compared to most…” with relative weight groups. These two might capture a lot of the same information, so we may chose one. (Additionally, this might create issues with multicollinearity, which we will discuss on the last day, so just keep that in mind!) Another example is if you used gender identity, this might be a good time to throw out sex assigned at birth. Remember, my reasoning for using SAB was that (1) lab work has been extensive and I wanted to give you an option to avoid multi-selection variables, and (2) it might capture some of the differences around fat attitudes tied with gender. If you included gender identity in your work, then sex assigned at birth could be superfluous.\nFor change in coefficients, focus on the variable of your research question. Does the removal of variables change the coefficients for your explanatory variable? Remember what we discussed with change in coefficients when our explanatory variable is a multi-level categorical variable (Lesson 11.2 Interactions continued slides 26-28). You may find these changes small, which tracks with a lot of our plots in Lab 3. Nothing seemed to have such a big effect on IAT score, and as a consequence it’s hard to see big changes for a potential confounder.\n\nNote that I put common sense first. The change in coefficients may not be very large, and may lead you to think we don’t need a lot of the variables in our model. However, I would let common sense override the change in coefficients if your reasoning is well justified.\npsst… There might be some code in Step 4 that might help you get started in this step.\n\n\n\n\n\n\nTasks\n\n\n\nRemove variables from the initial model based on your common sense, change in coefficient, and/or p-values of the F-tests.\nYou do NOT need to show all your work here. You just need to include:\n\nA brief explanation of what variables were dropped and why (a sentence per variable), and\nAn example of your process with one variable is enough (including code that you ran)\n\n\n\n\n\n2.5 Step 4: Assess scale for continuous variables\nThere is one variable in our model (unless you removed it) that is continuous: age. We need to assess the scale for age. In this step we will have ZERO delivarables. To save you time, I will walk you through my thought process, and why I determined age is fine as is. If you still want to try something else out with age, then you can!\nFirst, we can start with a scatterplot of IAT score and age. Your plot may look a little different than mine.\n\nggplot(data = iat, aes(x = age, y = IAT_score)) +\n  geom_point(size = 0.8) + geom_smooth() + xlim(0, 111)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nIn the above scatterplot, it looks like the relationship is mostly linear (and increasing) until we get to approximately 90 years old. At that point IAT score decreases with age. Let’s say we 100% believe there is suddenly more responders around 110 years old than 90-100 year olds. I’m already skeptical of this since we did a quality control in Lab 3. We’ll play it out because it’s not worth making judgement calls on what we consider “admissable” data.\nWe could do quantiles, splines, or polynomials, but those approaches will either make more categorical variables or make the relationship between age and IAT score harder to interpret. We have a pretty linear relationship up until the higher ages!\nI wanted to investigate the linearity a little more so I created an indicator for individuals who are 100 years or older:\n\niat1 = iat %&gt;% mutate(ind_age_100 = ifelse(age &gt; 100, \"TRUE\", \"FALSE\"))\n\nNow I can see if the linearity differs between the two groups of ages:\n\nggplot(iat1, aes(x = age, y = IAT_score, color = ind_age_100)) +\n  geom_point(size = 0.8) + geom_smooth(method = \"lm\")\n\n\n\n\nI am happy to see that both groups’ IAT score are increasing with age. It actually looks like my indicator might be a confounder… In that case, we only need to include the indicator in the model so that the relationship between age and IAT is adjusted for the indicator. I can test to see if the indicator is a big enough confoudner using the change in coefficient of age and my explanatory variable.\nHere’s the model without the indicator:\n\nprelim_model = lm(IAT_score ~ iam_unordered + identfat + comptomost + \n                  ind_m + ind_f + ind_tmm + ind_twf + ind_gqnc + \n                    ind_other +\n                  race + \n                  ethn +\n                  edu_14_f +\n                  age, data = iat1)\n\nAnd we’ll take a look at the coefficients for the model:\n\nprelim_model$coefficients[c(2:6, 46)] # by using c(2:6, 46) I am telling R to \n\n      iam_unorderedVery underweight iam_unorderedModerately underweight \n                       -0.061151601                        -0.015513320 \n  iam_unorderedSlightly underweight    iam_unorderedSlightly overweight \n                        0.006954580                        -0.023369363 \n iam_unorderedModerately overweight                                 age \n                       -0.056237038                         0.003857134 \n\n                                      # only print certain variables' coefficients\n\nThen we can run the model with the indicator, then look at the coefficients:\n\nprelim_model2 = lm(IAT_score ~ iam_unordered + identfat + comptomost + \n                  ind_m + ind_f + ind_tmm + ind_twf + ind_gqnc + ind_other +\n                  race + \n                  ethn +\n                  edu_14_f +\n                  age + ind_age_100, \n                 data = iat1)\nprelim_model2$coefficients[c(2:6, 46)] # by using c(2:6, 46) I am telling R to \n\n      iam_unorderedVery underweight iam_unorderedModerately underweight \n                       -0.060385515                        -0.015352043 \n  iam_unorderedSlightly underweight    iam_unorderedSlightly overweight \n                        0.006817642                        -0.023677757 \n iam_unorderedModerately overweight                                 age \n                       -0.056762301                         0.003918822 \n\n                                       # only print certain variables' coefficients\n\nWe can check the % change in the coefficients between the models.\nRecall, \\[\n\\Delta\\% = 100\\% \\cdot \\frac{\\widehat\\beta_{FLR, full} - \\widehat\\beta_{FLR, red}}{\\widehat\\beta_{FLR, full}}\n\\] Here’s how I quickly do it with the coefficients:\n\n100 * ( prelim_model2$coefficients[c(2:6, 46)] - prelim_model$coefficients[c(2:6, 46)] ) /\n  prelim_model2$coefficients[c(2:6, 46)]\n\n      iam_unorderedVery underweight iam_unorderedModerately underweight \n                         -1.2686585                          -1.0505274 \n  iam_unorderedSlightly underweight    iam_unorderedSlightly overweight \n                         -2.0085770                           1.3024639 \n iam_unorderedModerately overweight                                 age \n                          0.9253735                           1.5741387 \n\n\nBased on %’s above, it doesn’t look like the indicator makes much of a difference in my model. It is likely because there are only 29 individuals over the age of 100 and 201,031 individuals under the age of 100 (In my dataset). Those 29 individuals will not have a big impact on the linear relationship between age and IAT, even though the first smoothed scatterplot made it look like it does.\nTo bring this point home, I can plot age and IAT with and without the individuals that are 100 years or older. Let me know if you find a better way to overlay these plots! (I have been a little stressed on time, and couldn’t find a quick answer.)\n\nggplot(iat1, aes(x = age, y = IAT_score)) +\n  geom_point() + geom_smooth(method = \"lm\") + xlim(0, 111) +\n  labs(title = \"With individuals 100 years or older\")\n\n\n\nggplot(iat1 %&gt;% filter(age &lt; 100), aes(x = age, y = IAT_score)) +\n  geom_point() + geom_smooth(method = \"lm\") + xlim(0, 111) +\n  labs(title = \"Without individuals 100 years or older\")\n\n\n\n\nI see no difference. Thus, I think it’s okay to leave age as is!!\n\n\n\n\n\n\nTasks\n\n\n\nNo tasks here! If you want to try out what I did above, you can!\n\n\n\n\n2.6 Step 5: Check for interactions\nNow we’re going to check if there are any interactions. I will walk you through a streamlined way to check for interactions between your explanatory variable and all the other variables in the model.\nFirst, I want you to revisit your work in Lab 3. Remind yourself of the variables that you identified as possible effect modifiers.\nAs you check for interactions, don’t forget to make your decisions based on your discussion/hypotheses in Lab 3. Always prioritize investigation of interactions that are justified clinically before investigating interactions only based on statistical significance.\n\n1vars = names(model.frame(prelim_model))[-1]\n\n.env &lt;- environment()\n2interactions &lt;- combn(vars, 2, function(x) paste(x, collapse=\" * \")) %&gt;%\n3    grep(., pattern = \"iam_unordered\", value = T)\n\n\n1\n\nCreate a vector of the variable names that are in your preliminary model. Note I use [-1] to remove IAT_score from my list. Please make sure to change prelim_model to the name of your model at this point.\n\n2\n\nHere we are just combining all our covariates into interactions that R can understand. This makes it so we don’t have to write it all ourselves.\n\n3\n\nMake sure to change the pattern = \"iam_unordered\" to be pattern = to your explanatory variable.\n\n\n\n\nNow that we’ve created the set up for all the possible interactions, we can run them through the lm() function and see the summary of the models. In the following code I use the lappy() function to fit an individual model for the main effects + each interaction listed in interactions.\n\n\n\n\n\n\nNote\n\n\n\nPlease note that this code takes a while to run. Once you run it and take note of the results, you can comment out or add #| eval: false to prevent it from running every time you render. You don’t need to show the results for this in your submitted work, but I want to see the code, and read about your decisions about from results.\n\n\n\nsummary = lapply(interactions,\n             function(int) summary(lm(reformulate(c(vars, int), \"IAT_score\", env=.env),\n                                      data = iat)))\nsummary\n\nYou can alse go straight to using the anova() function to compare the preliminary model.\n\nanova_res = lapply(interactions,\n             function(int) anova(lm(reformulate(c(vars, int), \"IAT_score\", env=.env),\n                                      data = iat),\n1                                 prelim_model))\nanova_res[1]\ng = anova_res[[1]]\ng$F\n\n\n1\n\nYou will to change this name for the preliminary model if you called it something different.\n\n\n\n\n[[1]]\nAnalysis of Variance Table\n\nModel 1: IAT_score ~ iam_unordered + identfat + comptomost + ind_m + ind_f + \n    ind_tmm + ind_twf + ind_gqnc + ind_other + race + ethn + \n    edu_14_f + age + iam_unordered * identfat\nModel 2: IAT_score ~ iam_unordered + identfat + comptomost + ind_m + ind_f + \n    ind_tmm + ind_twf + ind_gqnc + ind_other + race + ethn + \n    edu_14_f + age\n  Res.Df   RSS  Df Sum of Sq     F    Pr(&gt;F)    \n1 200990 31337                                  \n2 201014 31346 -24   -9.4223 2.518 5.558e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n[1]       NA 2.518048\n\n\n\n\n\n\n\n\nTasks\n\n\n\nUsing your discussion in Lab 3 and the results from the F-test on interactions:\n\nCreate a list of the interactions that you will include in your model.\nRun the preliminary final model that includes the main effects and interactions.\n\n\n\n\n\n2.7 Step 6: Assess model fit\nAt this point we may want to compare different models. While Steps 1-5 have been directing us towards a single model, you may have been interested in other models along the way. Maybe there were some interactions that you thought were interesting, but didn’t think of before. Maybe you would like to combine different groups for categorical variables.\nIf you are completely happy with your model, then you don’t have to do this step.\nYou might create a table like such:\n\nsum = summary(prelim_model)\nmodel_fit_stats = data.frame(Model = \"Preliminary main effects model\", Adjusted_R_sq = sum$adj.r.squared, AIC = AIC(prelim_model), BIC = BIC(prelim_model))\n\nmodel_fit_stats\n\n                           Model Adjusted_R_sq      AIC      BIC\n1 Preliminary main effects model    0.04511326 197006.6 197486.5\n\n\n\n\n\n\n\n\nTasks\n\n\n\nOptional: Create a table that displays some fo the model fit statistics to compare preliminary final models.\n\n\n\n\n2.8 Create a forest plot of your coefficient estimates\nIt’s often helpful to have a visualization of coefficient estimates. Forest plots are a nice way to show all the values together. Below I have started a forest plot using my prelim_model. You can make the plot with your final model.\nI used the plot_model() function to make the plot, and here’s a site that discusses some of it’s capabilities. The below plot is just a starting point!! You’ll need to clean up the variables, title, etc.\nYou may use another function to make the plots. I chose this one since it can handle the model as input.\n\nplot_model(prelim_model, show.values = TRUE, value.offset = 0.5) + ylim(-0.25, 0.25)\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\nHere are some other packages for forest plots:\n\nhttps://cran.r-project.org/web/packages/forestploter/vignettes/forestploter-intro.html\nhttps://larmarange.github.io/ggstats/articles/ggcoef_model.html\n\n\n\n\n\n\n\nTasks\n\n\n\nCreate a forest plot to visualize the coefficient estimates."
  },
  {
    "objectID": "labs/Lab_04.html",
    "href": "labs/Lab_04.html",
    "title": "Lab 4",
    "section": "",
    "text": "Before starting this lab, you should go back to Lab 2, save a new .rda file that contains all the new variables from that Lab. Then you can load it here!\n\n\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\nRun a simple linear regression model for each covariate against the IAT score (outcome).\nDisplay results from the test if each covariate explains enough variation of the outcome. This may be from three options in the instructions: summary()/anova() only, lapply(), or sapply()\n\nInterpretation of the results will be in the next step.\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\nDecide which covariates will be included in the initial model and list them.\nRun the initial model and display the regression table.\n\nNo need to write out the model, but you may in addition to the list.\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\nRemove variables from the initial model based on your common sense, change in coefficient, and/or p-values of the F-tests.\nYou do NOT need to show all your work here. You just need to include:\n\nA brief explanation of what variables were dropped and why (a sentence per variable), and\nAn example of your process with one variable is enough (including code that you ran)\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\nNo tasks here! If you want to try out what I did above, you can!\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\nUsing your discussion in Lab 3 and the results from the F-test on interactions:\n\nCreate a list of the interactions that you will include in your model.\nRun the preliminary final model that includes the main effects and interactions.\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\nOptional: Create a table that displays some fo the model fit statistics to compare preliminary final models.\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\nCreate a forest plot to visualize the coefficient estimates."
  },
  {
    "objectID": "labs/Lab_04.html#directions",
    "href": "labs/Lab_04.html#directions",
    "title": "Lab 4",
    "section": "",
    "text": "Please turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here.\n\n\n\n\n\n\nCaution\n\n\n\nThis is the instructions file. The link above will take you to the editing file where you can add your work and turn it in!! Please do not remove anything from the editing file!!\n\n\n\n\nThe main purpose of this lab is to perform model selection, identify one or more potential final models, and start our interpretation of our main relationship.\n\n\n\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning"
  },
  {
    "objectID": "labs/Lab_04.html#lab-activities",
    "href": "labs/Lab_04.html#lab-activities",
    "title": "Lab 4",
    "section": "",
    "text": "Before starting this lab, you should go back to Lab 2, save a new .rda file that contains all the new variables from that Lab. Then you can load it here!\n\n\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\nRun a simple linear regression model for each covariate against the IAT score (outcome).\nDisplay results from the test if each covariate explains enough variation of the outcome. This may be from three options in the instructions: summary()/anova() only, lapply(), or sapply()\n\nInterpretation of the results will be in the next step.\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\n\nDecide which covariates will be included in the initial model and list them.\nRun the initial model and display the regression table.\n\nNo need to write out the model, but you may in addition to the list.\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\nRemove variables from the initial model based on your common sense, change in coefficient, and/or p-values of the F-tests.\nYou do NOT need to show all your work here. You just need to include:\n\nA brief explanation of what variables were dropped and why (a sentence per variable), and\nAn example of your process with one variable is enough (including code that you ran)\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\nNo tasks here! If you want to try out what I did above, you can!\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\nUsing your discussion in Lab 3 and the results from the F-test on interactions:\n\nCreate a list of the interactions that you will include in your model.\nRun the preliminary final model that includes the main effects and interactions.\n\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\nOptional: Create a table that displays some fo the model fit statistics to compare preliminary final models.\n\n\n\n\n\n\n\n\n\n\n\nTasks\n\n\n\nCreate a forest plot to visualize the coefficient estimates."
  },
  {
    "objectID": "labs/BMI_help.html",
    "href": "labs/BMI_help.html",
    "title": "BMI Variable Help",
    "section": "",
    "text": "Link to github page for qmd file\n\nLoading the needed packages:\n\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(here)\nif(!require(lubridate)) { install.packages(\"lubridate\"); library(lubridate) }\n\n\n\nLoading my IAT dataset (as it’s Rda file):\n\nload(file = here(\"../TA_files/Project/data/IAT_data.rda\"))\n\n\n\nSelecting the variables that I want to look at:\n\niat_prep = iat_2021_raw %&gt;% \n  select(IAT_score = D_biep.Thin_Good_all, \n         att7, iam_001, identfat_001, \n         myweight_002, myheight_002,\n         identthin_001, controlother_001, \n         controlyou_001, mostpref_001,\n         important_001, \n         birthmonth, birthyear, month, year, \n         raceomb_002, raceombmulti, ethnicityomb, \n         edu, edu_14, \n         genderIdentity, \n         birthSex)\n\n\n\nSelf-reported BMI\nI started investigating the BMI because I was curious how the paper [@elran-barak2018] used it and just wanted to check reproducibility. There are a few issues with the self-reported BMI that immediately stuck out:\n\nComponents of BMI (weight and height) were self-reported\n\nPeople told they are underweight often add pounds (REFERENCE)\nPeople told they are overweight often subtract pounds (REFERENCE)\n\nRaw data from weight and height are categorical. This is according to the codebook associated with this dataset. Please find your codebook file named Weight_IAT_public_2021_codebook.csv . You can find the value names for myweight_002 and myheight_002.\nFor example, in the weight variable,\n\nmost categories identify a lower limit to the weight in the group. One example group is weight is greater than or equal to 200 pounds and less than 205 pounds (labelled as “200 lb :: 91 kg”).\nthe first category for weight is “below 50lb:: 23kg” with 258 observations\nthe last category for weight is “above 440lb:: above 200kg” with 295 observations\n\nWhile the 5 groups of weight leading up the last category have 33, 28, 34, 20, and 89 observations, respectively.\n\nMy intention here is not the question anyone’s weight, but keep in mind that surveys sometimes have people selecting the first or last option because they are not taking the survey seriously\n\n\n\nMy exact steps\n\nI wanted to get a table of the counts within each weight group. I used the gt package to make a table of what I thought was a categorical variable. It looks like R interprets the numbered categories as numbers.\n\niat_prep %&gt;%\n  dplyr::select(myweight_002) %&gt;%\n  tbl_summary()\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 465,8861\n    \n  \n  \n    myweight_002\n23 (18, 29)\n        Unknown\n141,326\n  \n\n  \n    \n      1 Median (Q1, Q3)\n    \n  \n\n\n\n\nI will first check the class of the variable to make sure R is doing what I think it’s doing.\n\nclass(iat_prep$myweight_002)\n\n[1] \"integer\"\n\n\nSo R is interpreting the values as integers. I will need to make them categories to view them through gt commands.\nLet’s make it a category:\n\niat_prep2 = iat_prep %&gt;% \n  mutate(myweight = as.factor(myweight_002))\n\nNow we make the table:\n\niat_prep2 %&gt;%\n  dplyr::select(myweight) %&gt;%\n  tbl_summary()\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 465,8861\n    \n  \n  \n    myweight\n\n        1\n258 (&lt;0.1%)\n        2\n257 (&lt;0.1%)\n        3\n329 (0.1%)\n        4\n363 (0.1%)\n        5\n379 (0.1%)\n        6\n329 (0.1%)\n        7\n327 (0.1%)\n        8\n360 (0.1%)\n        9\n589 (0.2%)\n        10\n1,002 (0.3%)\n        11\n2,180 (0.7%)\n        12\n3,766 (1.2%)\n        13\n6,175 (1.9%)\n        14\n9,038 (2.8%)\n        15\n12,068 (3.7%)\n        16\n15,598 (4.8%)\n        17\n16,007 (4.9%)\n        18\n17,518 (5.4%)\n        19\n19,093 (5.9%)\n        20\n17,794 (5.5%)\n        21\n15,599 (4.8%)\n        22\n16,636 (5.1%)\n        23\n14,854 (4.6%)\n        24\n14,643 (4.5%)\n        25\n13,510 (4.2%)\n        26\n12,778 (3.9%)\n        27\n12,243 (3.8%)\n        28\n11,498 (3.5%)\n        29\n9,414 (2.9%)\n        30\n9,099 (2.8%)\n        31\n7,274 (2.2%)\n        32\n8,775 (2.7%)\n        33\n4,691 (1.4%)\n        34\n5,411 (1.7%)\n        35\n4,595 (1.4%)\n        36\n5,659 (1.7%)\n        37\n3,494 (1.1%)\n        38\n3,938 (1.2%)\n        39\n2,489 (0.8%)\n        40\n2,932 (0.9%)\n        41\n1,941 (0.6%)\n        42\n3,197 (1.0%)\n        43\n1,244 (0.4%)\n        44\n1,794 (0.6%)\n        45\n1,442 (0.4%)\n        46\n1,322 (0.4%)\n        47\n1,251 (0.4%)\n        48\n1,238 (0.4%)\n        49\n900 (0.3%)\n        50\n800 (0.2%)\n        51\n651 (0.2%)\n        52\n1,152 (0.4%)\n        53\n347 (0.1%)\n        54\n436 (0.1%)\n        55\n346 (0.1%)\n        56\n409 (0.1%)\n        57\n295 (&lt;0.1%)\n        58\n384 (0.1%)\n        59\n165 (&lt;0.1%)\n        60\n202 (&lt;0.1%)\n        61\n126 (&lt;0.1%)\n        62\n342 (0.1%)\n        63\n92 (&lt;0.1%)\n        64\n154 (&lt;0.1%)\n        65\n129 (&lt;0.1%)\n        66\n113 (&lt;0.1%)\n        67\n139 (&lt;0.1%)\n        68\n85 (&lt;0.1%)\n        69\n85 (&lt;0.1%)\n        70\n55 (&lt;0.1%)\n        71\n65 (&lt;0.1%)\n        72\n120 (&lt;0.1%)\n        73\n26 (&lt;0.1%)\n        74\n26 (&lt;0.1%)\n        75\n26 (&lt;0.1%)\n        76\n33 (&lt;0.1%)\n        77\n28 (&lt;0.1%)\n        78\n34 (&lt;0.1%)\n        79\n20 (&lt;0.1%)\n        80\n89 (&lt;0.1%)\n        81\n295 (&lt;0.1%)\n        Unknown\n141,326\n  \n\n  \n    \n      1 n (%)\n    \n  \n\n\n\n\nThe table is really long, so a histogram would work much better to visualize how many observations are in each category:\n\nggplot(data = iat_prep, aes(x = myweight_002)) + \n  geom_histogram() +\n  geom_vline(aes(xintercept = mean(iat_prep$myweight_002, \n                                   na.rm = T)), \n             color = \"red\", linewidth = 2)\n\nWarning: Use of `iat_prep$myweight_002` is discouraged.\nℹ Use `myweight_002` instead.\n\n\nWarning: Removed 141326 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\nWe need to convert the heights and weights to their cm and kg respectively. Since I only have a number category, I’ve gone into the codebook to find what each numbered category represents. If you put 8, you are 43 inches tall; 16:51 in; and 32:67in. Now I can use a line to see if I can create an equation to convert these values.\n\\[\n\\begin{align}\nin & = m\\times cat+b \\\\\n43 &= m \\times 8 + b \\\\\nb & = 43-8m \\\\\n\\\\\n51 &= 16m + b \\\\\n51 &= 16m + (43-8m) \\\\\nm &=1 \\\\\nb&=43-8m = 43-8=35 \\\\\n\\end{align}\n\\]\nThen we double check with third set of points:\n\\[\n\\begin{align}\n67 & = 1 \\times 32 + 35 \\\\\n67 & = 67 \\\\\n\\end{align}\n\\]\n\niat_prep$myheight_in = 1*iat_prep$myheight_002 + 35\n\nThen we need to convert height to meters since BMI is in \\(kg/m^2\\).\n\niat_prep$myheight_m = 0.0254*iat_prep$myheight_in\n\nOkay, now we need to do something similar for weight. Three more points to find the conversion: 10:90lb; 20:140lb; and 30: 190lb.\n\\[\n\\begin{align}\nlb & = m\\times cat+b \\\\\n90 &= m \\times 10 + b \\\\\nb & = 90-10m \\\\\n\\\\\n140 &= 20m + b \\\\\n140 &= 20m + (90-10m) \\\\\nm &=5 \\\\\nb&=90-10m = 90-50=40 \\\\\n\\end{align}\n\\]\nThen we double check with third set of points:\n\\[\n\\begin{align}\n190 & = 5 \\times 30 + 40 \\\\\n190 & = 190 \\\\\n\\end{align}\n\\]\n\niat_prep$myweight_lb = 5*iat_prep$myweight_002 + 40\n\nThen we need to convert height to meters since BMI is in \\(kg/m^2\\).\n\niat_prep$myweight_kg = 0.453592*iat_prep$myweight_lb\n\n\niat_prep$bmi = iat_prep$myweight_kg/(iat_prep$myheight_m)^2\n\n\nggplot(data = iat_prep, aes(x = bmi)) + \n  geom_histogram(binwidth = 1) +\n  geom_vline(aes(xintercept = mean(bmi, \n                                   na.rm = T)), \n             color = \"red\", linewidth = 2)\n\nWarning: Removed 142470 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nFrom histogram, looks like there are a couple observations at BMIs greater than 200. Let’s double check that.\n\nsummary(iat_prep$bmi)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   4.28   20.81   23.71   25.48   28.21  241.41  142470 \n\n\nOkay, so we now know the max is 241.41. I want to see the observations that have BMIs this large. I’ll take a look at their other values to see if there are any other issues.\n\niat_prep_bmi = iat_prep %&gt;% filter(bmi &gt; 200)\nhead(iat_prep_bmi, 10)\n\n     IAT_score att7 iam_001 identfat_001 myweight_002 myheight_002\n1  -0.61544208    4       1            5           81            2\n2   0.54476890    1       6            2           80            1\n3   0.70458996    1       2            3           80            2\n4   0.28206698    6       7            1           81            1\n5   0.33790313    5       7            3           81            2\n6   1.23311171    1       7            4           80            2\n7  -0.02357343    7       1            1           81            2\n8           NA    7       1            1           81            2\n9   0.33704837    4       1            1           81            2\n10 -0.47687442    4       4           NA           81            1\n   identthin_001 controlother_001 controlyou_001 mostpref_001 important_001\n1              1                5              5            4             5\n2              3                4              4            3             1\n3              3                3              2            4             5\n4              5                1              1            6             5\n5              4                3              2            1             4\n6              1                5              1            1             2\n7              5                1              1            2             5\n8              5                1              5            7             5\n9              5                1              1            4             4\n10            NA               NA              1           NA            NA\n   birthmonth birthyear month year raceomb_002    raceombmulti ethnicityomb edu\n1          12      1910     1 2021           8 [1,2,3,4,5,6,7]            1  12\n2          12      2009     1 2021           4                            1   1\n3          10      1916     1 2021           4                            1   1\n4          11      1910     2 2021          NA                            3   1\n5           4      2007     2 2021           6                            3  NA\n6           5      2001     2 2021           6                            2   5\n7           2      1980     2 2021           8 [1,2,3,4,5,6,7]            1   9\n8          NA        NA     2 2021          NA                           NA  NA\n9           5      1976     2 2021           5                            2   4\n10          9        NA     2 2021        -999                           NA  NA\n   edu_14 genderIdentity birthSex myheight_in myheight_m myweight_lb\n1      12            [2]        2          37     0.9398         445\n2       1            [1]        1          36     0.9144         440\n3       1            [1]        1          37     0.9398         440\n4       1            [6]        2          36     0.9144         445\n5      NA            [1]        1          37     0.9398         445\n6       5            [1]        1          37     0.9398         440\n7       9  [1,2,3,4,5,6]        2          37     0.9398         445\n8      NA                      NA          37     0.9398         445\n9       4            [1]        1          37     0.9398         445\n10     NA            [2]        2          36     0.9144         445\n   myweight_kg      bmi\n1     201.8484 228.5359\n2     199.5805 238.6963\n3     199.5805 225.9681\n4     201.8484 241.4087\n5     201.8484 228.5359\n6     199.5805 225.9681\n7     201.8484 228.5359\n8     201.8484 228.5359\n9     201.8484 228.5359\n10    201.8484 241.4087\n\n\nLooking at the subset of individuals with BMIs greater than 200, I am reminded that there is some serious quality control that needs to be done to this dataset. Other variable observations indicate that some of these rows are individuals who did not accurately fill out their survey. Right now, we keep them in our dataset, but we will need to examine them for outliers."
  },
  {
    "objectID": "labs/Project_template.html#statistical-methods",
    "href": "labs/Project_template.html#statistical-methods",
    "title": "Project Template: Title here",
    "section": "Statistical Methods",
    "text": "Statistical Methods"
  },
  {
    "objectID": "labs/Project_template.html#results",
    "href": "labs/Project_template.html#results",
    "title": "Project Template: Title here",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "labs/Project_template.html#discussion",
    "href": "labs/Project_template.html#discussion",
    "title": "Project Template: Title here",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "labs/Project_template.html#conclusion",
    "href": "labs/Project_template.html#conclusion",
    "title": "Project Template: Title here",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "labs/Project_template.html#references",
    "href": "labs/Project_template.html#references",
    "title": "Project Template: Title here",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "labs/Lab_02_work.html",
    "href": "labs/Lab_02_work.html",
    "title": "Lab 2 Work",
    "section": "",
    "text": "IMPORTANT TO READ\n\n\n\n\nPlease do not delete the rubric from your .qmd file. I will use it to circle the grades!\nPlease delete everything"
  },
  {
    "objectID": "labs/Lab_02_work.html#directions",
    "href": "labs/Lab_02_work.html#directions",
    "title": "Lab 2 Work",
    "section": "Directions",
    "text": "Directions\nPlease turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here.\nThe rest of this lab’s instructions are embedded into the lab activities.\n\nPurpose\nThe main purpose of this lab is to introduce our dataset, codebook, and variables. We will continue to think about the context of our research question, but our main focus is to become familiar with the data.\n\n\nGrading\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\nRubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nSome tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning (like target population, choosing variables, revisiting research question)"
  },
  {
    "objectID": "labs/Lab_02_work.html#lab-activities",
    "href": "labs/Lab_02_work.html#lab-activities",
    "title": "Lab 2 Work",
    "section": "Lab activities",
    "text": "Lab activities\n\n1. Access and download the data\nThis serves as good practice for accessing data that is online or needs to be downloaded from a collaborator.\nData can be accessed here. Under “Weight IAT 2004-2022” there are several drop down menus:\n\nI opened the first “Datasets & Codebooks,” then selected “OSF Storage (United States).” Once selected, the “Download as zip” option pops up in the top right part of the Files section.\n\nWe will be working with the Weight_IAT.public.2021.csv dataset. Please locate the zip file called Weight IAT.public.2021-CSV.zip . T0 download, you need to click the row of the zip file, but you can’t click the name of the zip file. If a link opens, then you clicked the name. If the row is highlighted blue and clickable “Download” and “View” buttons appear on the top right, then you selected it correctly! (See below image for what it should look like.)\n\nThen click the “Download” button to download! Note that the name does not have an underscore between “Weight” and “IAT.” I like to have my datasets named without spaces, so I will replace the space with an underscore.\nFor the codebook, perform the same process for the file named: Weight_IAT_public_2021_codebook.xlsx\nYou will need to unzip the actual data.\nMove the data to a folder that you can easily access as you work from this document. I like to have a folder named data to house my data.\n\n\n\n\n\n\nTask Summary\n\n\n\nDownload the 2021 data and codebook from the archives and store in accessible folder.\n\n\n\n\n2. Load data and needed packages\nFirst, load the packages that you will need in the remainder of this lab. You can add to this as you need to. At the top of your R code chunk, you can add the following option to repress the messages from the loading packages:\n\n\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(here)\nif(!require(lubridate)) { install.packages(\"lubridate\"); library(lubridate) }\n\nUsing R, load the data (csv file) into this document. Note that this is a csv file that we can load with basic R packages. Name your dataset something that feels intuitive to you and will distinguish it from other datasets that you work with.\n\n\n[1] \"/Users/wakim/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/Teaching/Classes/W25_BSTA_512_612/BSTA_512_W25_site\"\n\n\nLoading the csv file every time you render will take a long time. One way to speed this up is by saving the data as an rda file (R data file). Change the following R code to save the rda file. You will also need to remove the #| eval: false at the top of the code chunk once you have corrected the code. If you are confused on the syntax, don’t forget that you can use ?save for more information.\n\nsave(&lt;whatever you called the read csv file&gt;, file = \"Where you would like to save the file with its name\")\n\nCheck that you have an rda file where you saved it. Now use load() with the file path to load the rda data here.\n\nload(file = \"Where you would like to save the file with its name\")\n\nAt this point, if you think you loaded the file correctly, add #| eval: false to the code chunk where you loaded the csv file and back to the chunk where you saved the rda file.\nTake a glimpse at the data to make sure you loaded it correctly.\nHow many rows and columns are in the dataset? Do you think we will need all these variables for our analysis?\n\n\n\n\n\n\nTask Summary\n\n\n\nRead csv, save as rda, load rda, glimpse at data.\nHow many rows and columns are in the dataset? Do you think we will need all these variables for our analysis?\n\n\n\n\n3. Data wrangling\nAs you go through this process, it is important that you look at the codebook for more information on each variable.\n\n3.1 What’s our target population?\nAs many of you mentioned in Lab 1, individuals taking the IAT test are not necessarily representative of the world population. I want you to articulate the target population that you think our analysis can give information about. To what population can we generalize our analysis results? We can get very specific with this population, but try to restrict your population to 3-5 characteristics.\nAfter you articulate the population, I want to add one more restriction to our population: US residency. The sample includes individuals residing in many different countries. Since we are discussing attitudes and beliefs that is inherently connected to society and culture, I think it is important that we restrict our analysis and discussion to a country that we have some social experience in. Thus, let’s restrict our data to the US only by filtering the variable countryres to category 1 (corresponding to the US).\n\n\n\n\n\n\nTask\n\n\n\nDescribe our target population. Keep your description to 3-5 characteristics, not including our restriction on the US population.\n\n\n\n\n3.2 Restrict your analysis to 1 outcome and 9 possible covariates/predictors\nWe are going to restrict our analysis to the single outcome, IAT score, which is named D_biep.Thin_Good_all. You can rename this variable.\nWe will also restrict our analysis to the following 9 potential variables so our work is a little more manageable.\n\n\n\n\n\n\nTask\n\n\n\nFrom the following 7 attitudes and beliefs, please select 3 that you think will be the most important variables related to your research question. In 1-2 lines, briefly explain why you chose each variable. This can be informal and bulleted.\n\n\n(Make sure you chose the variable that is part of your research question!)\n\nSelf-perception of weight (iam_001)\nFat group identity (identfat_001 )\nThin group identity (identthen_001 )\nControllability of weight of others (controlother_001)\nControllability of weight of yourself (controlyou_001)\nAwareness of societal standards (mostpref_001 )\nInternalization of societal standards (important_001)\n\nWe will start our data exploration with the following 4 demographic variables:\n\nAge (we need to construct)\nRace (raceomb_002 or raceombmulti)\nEthnicity\nSex assigned at birth (birthSex)\n\nPlease pick 2 additional variables to include in your analysis:\n\nEducation (edu)\nGender (genderIdentity)\nSelf-reported BMI (through self-reported height and weight)\nPolitical identity\nReligion\n\nI have chosen these variables for a mixture of reasons. For example, I have left out variables about residence and occupation because those variables have hundreds of categories that would be overwhelming in linear regression. For the 4 required demographic variables, I chose age because I really want us to get practice with a continuous variable. I chose race and ethnicity because of the intertwined history of racism and anti-fat bias in Western countries (including the U.S. where most participants reside).\n\n\n\n\n\n\nA note of the available variables on race\n\n\n\nThe dataset has two separate race variables. One has mutually exclusive categories (raceomb_002) and the other allows participants to make multiple selections (raceombmulti). The former (raceomb_002) allows one participant to identify with only one race category.\nImportant lesson from We All Count about using a multiple selection race question. We can try out all these options!\n\n\nFinally, I chose sex assigned at birth because adults in 2021 in the US were likely raised in a society where your sex assigned at birth impacted the gender stereotypes that you were raised in, which could impact exposure to diet culture. This in addition to the many medical conditions associated with one’s sex assigned at birth that may affect weight. The reason why I am leaving gender as an optional variable is because the question on gender allows participants to chose multiple options. The binary sex assigned at birth will make our analysis a little easier from a statistics stand point. Unfortunately, we need to balance achievable learning objectives and the most appropriate variable. Since I have required race as a variable and has a multi-level option, I do not want to overload our analysis with another multi-level variable. Sex assigned at birth will not create more work for you (that is outside of the course objectives) while capturing medical conditions and some of the societal impact of diet culture. This is certainly a limitation in our analysis that we should address in our discussion. I do encourage you to look into gender if the binary sex assigned at birth does not feel right for you. I am happy to help!\n\n\n\n\n\n\nA word on self-reported BMI\n\n\n\nThis variable is rooted in racism and anti-fat bias. The American Medical Association made a few press releases on policies using BMI as a measure, with alternative measures (frankly, just other measures of fatness to use as a diagnostic tool instead of checking true indicators of health). However, I can think of a couple examples where BMI might help us understand some context in this research, so I have left it as an option. Although still self-reported, it might be interesting to see how BMI (which is the closest measurement available in this dataset to an “objective” measure of fatness) is related to individuals’ attitudes and beliefs. I am not saying there is anything to the relationship, but it might be worth checking out if you are interested.\nI will also say, in this dataset, there are MANY issues constructing the variable for BMI from height and weight. If you do not feel strongly about including it, I would suggest you avoid the variable self-reported BMI. It is not worth bringing in a racist and anti-fat variable into the dataset if you do not have a specific use for it. If you do plan to use it, please come to me for help as early as possible!\n\n\nIf you would like to investigate a variable outside the list, please let me know by emailing or chatting with me.\n\n\n\n\n\n\nTask\n\n\n\nUsing R, select your identified variables from your dataset. Your new dataset should have 10 columns for the 10 variables.\n\n\n\n\n [1] \" \"             \"[2]\"           \"[1]\"           \"[3]\"          \n [5] \"[1,3]\"         \"[5]\"           \"[1,5,6]\"       \"[2,5]\"        \n [9] \"[1,2]\"         \"[1,5]\"         \"[4]\"           \"[6]\"          \n[13] \"[1,2,3,4,5,6]\" \"[5,6]\"         \"[1,6]\"         \"[3,5]\"        \n[17] \"[4,5]\"         \"[2,6]\"         \"[2,4]\"         \"[2,5,6]\"      \n[21] \"[1,4]\"         \"[2,3]\"         \"[3,4,5,6]\"     \"[1,2,3,4]\"    \n[25] \"[1,3,5,6]\"     \"[1,3,5]\"       \"[2,3,4]\"       \"[3,5,6]\"      \n[29] \"[2,4,6]\"       \"[1,2,5]\"       \"[3,6]\"         \"[1,2,6]\"      \n[33] \"[1,4,6]\"       \"[2,3,5]\"       \"[1,3,4,6]\"     \"[1,2,3,4,5]\"  \n[37] \"[4,6]\"         \"[2,4,5]\"       \"[1,2,4]\"       \"[1,3,6]\"      \n[41] \"[1,2,3,4,6]\"   \"[3,4,5]\"       \"[1,2,3]\"       \"[1,3,4,5,6]\"  \n[45] \"[4,5,6]\"       \"[1,2,3,5]\"     \"[1,4,5]\"       \"[3,4]\"        \n[49] \"[2,3,4,5]\"     \"[1,4,5,6]\"     \"[1,2,5,6]\"     \"[1,3,4]\"      \n\n\n\n\n3.3 Manipulating variables that are coded as numeric variables\nMany variables in this dataset are coded as numeric values, but have specific categories linking up to the numbers. Using mutate() and cases() similar to our Data Management lesson, please create a new categorical variable with the specified categories from the codebook. Make sure that you create a variable with a new name! Since some of these variables are ordered categories, we will investigate if it’s appropriate to use the numeric or categorical version of the variable.\n\n\n\n\n\n\nExample of how I would create new variable for self-perception of weight (iam_001):\n\n\n\nBy looking at the codebook, I see that respondents answer the following question: “Currently, I am:”\n\n“Very underweight”\n“Moderately underweight”\n“Slightly underweight”\n“Neither underweight nor underweight”\n“Slightly overweight”\n“Moderately overweight”\n“Very overweight”\n\nIf I look at the data as is, I see that the variable is numeric.\n\niat_2021 %&gt;%\n  dplyr::select(iam_001) %&gt;%\n  tbl_summary()\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 465,8861\n    \n  \n  \n    iam_001\n\n        1\n2,023 (0.6%)\n        2\n7,902 (2.4%)\n        3\n24,399 (7.3%)\n        4\n148,081 (44%)\n        5\n88,566 (27%)\n        6\n43,090 (13%)\n        7\n18,978 (5.7%)\n        Unknown\n132,847\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\nAgain, I want to create a varaible with the answers instead of numbers, so I will change transform the variable to include the text:\n\niat_2021 = iat_2021 %&gt;%\n  mutate(iam_001_f = case_match(iam_001,\n                             7 ~ \"Very overweight\",\n                             6 ~ \"Moderately overweight\",\n                             5 ~ \"Slightly overweight\",\n                             4 ~ \"Neither underweight nor underweight\",\n                             3 ~ \"Slightly underweight\",\n                             2 ~ \"Moderately underweight\",\n                             1 ~ \"Very underweight\",\n                             .default = NA # to add NA if unknown\n                             ))\n\nI have called the new variable iam_001_f to indicate that the variable is not in factor form. You can also call it something like iam_001_cat to indicate the categorical form.\n\n\n\n\n\n\n\n\nTask\n\n\n\nIdentify and list the variables that are coded numerically and correspond to categories. Create a new variable for the categorical/factor version of the variable. It is up to you to check that your code ran properly!! If you are using multi-choice categorical variables (might include race, gender), then do not convert the variable yet!\n\n\n\n\n3.4 Creating age from birth date and test date\nThis dataset does not have an available “age” variable. However, we have enough information to determine each individual’s age from the test date and their self-reported birth date. We can use the lubridate package to configure the age. First, we need to use make_date() to construct the birth date and test date. Below, I have implemented make_date() to make the birth date.\n\n\n\n\n\n\nTask\n\n\n\nFrom the codebook, find the variables that we can use to construct the test date. Then use make_date() to create the test date.\n\n\n\niat_2021 = iat_2021 %&gt;%\n  mutate(birthdate = make_date(month = birthmonth, year = birthyear), \n         testdate = make_date(month = month, year = year))\n\nOnce the two dates are created, we can use further use lubridate to calculate the age in years. This code is a little complicated, so here is an example of how I have created age:\n\niat_2021 = iat_2021 %&gt;%\n  mutate(age = interval(start = birthdate, end = testdate) %&gt;%\n          as.period() %&gt;% year()) %&gt;%\n  select(-birthmonth, -birthyear, -year, -month, \n         -testdate, -birthdate)\n\nNote that the name of my dataset is iat_2021 and I feed it into mutate(). Within mutate(), I assigned age to the interval between the name of my birth date (birthdate) and the name of my test date (testdate). I need to convert the interval to a period of time (as.period()), then to a measurement of years (year()).\n\n\n\n\n\n\nTask\n\n\n\nFollowing the above example, create an age variable that measures the years between individuals’ birth and test date. Then remove the variables used to make age.\n\n\n\n\n3.5 If you chose BMI, create the variable\nRaw data from weight and height are categorical. This is according to the codebook associated with this dataset. Please find your codebook file named Weight_IAT_public_2021_codebook.csv . You can find the value names for myweight_002 and myheight_002.\n\nFor example, in the weight variable,\n\nmost categories identify a lower limit to the weight in the group. One example group is weight is greater than or equal to 200 pounds and less than 205 pounds (labelled as “200 lb :: 91 kg”).\nthe first category for weight is “below 50lb:: 23kg” with 258 observations\nthe last category for weight is “above 440lb:: above 200kg” with 295 observations\n\nWhile the 5 groups of weight leading up the last category have 33, 28, 34, 20, and 89 observations, respectively.\n\n\n\nI will post an extra resource outlining some of my work on the BMI variable.\n\n\n3.6 Make a new dataset with only complete cases\nHandling missing data is outside the scope of our class. There are many techniques to handling missing data, but we will use complete case analysis. This means we will only use observations that have information for every variable we chose. The function drop_na() will give you the complete cases. You can feed your dataset into the function and assign it as a new dataframe.\nFor example:\n\nnew_df = old_df %&gt;% drop_na()\n\niat_2021 = iat_2021 %&gt;%\n    mutate(iam_001_f = case_match(iam_001,\n                                  7 ~ \"Very overweight\",\n                                  6 ~ \"Moderately overweight\",\n                                  5 ~ \"Slightly overweight\",\n                                  4 ~ \"Neither underweight nor underweight\",\n                                  3 ~ \"Slightly underweight\",\n                                  2 ~ \"Moderately underweight\",\n                                  1 ~ \"Very underweight\",\n                                  .default = NA) %&gt;% \n             factor(levels = c(\"Very underweight\", \n                               \"Moderately underweight\", \n                               \"Slightly underweight\", \n                               \"Neither underweight nor underweight\", \n                               \"Slightly overweight\", \n                               \"Moderately overweight\", \n                               \"Very overweight\")))\n\n\n\n\n4. Some exploratory data analysis\n\n4.1 Peek at your outcome\nThis serves as a check to make sure we are all looking at the correct outcome: IAT score.\n\n\n\n\n\n\nTask\n\n\n\nPlease plot a histogram of the IAT scores. What do you notice about the outcome?\n\n\n\n\n4.2 Univariate exploratory data analysis\n\n\n\n\n\n\nTask\n\n\n\nUsing ggplot or tables, visualize your variables. Get a sense of each variable’s distribution. Do you notice anything out of the ordinary?\n\n\n\n\n4.3 Bivariate exploratory data analysis\n\n\n\n\n\n\nTask\n\n\n\nTake a look at the scatterplot, violin, or box plot of IAT score and your variable of interest. Use R and ggplot to make this plot. If your variable of interest is categorical, then make sure to use a violin or boxplot.\n\n\n\n\n\n5. Revisit your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate the research question that you proposed in Lab 1. What are your thoughts on the research question now that we looked at the data? Feel free to change it now that we’ve looked at the data. If you change your question, make sure 4.2 reflects the new research question.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn research, we typically do NOT change our research question after looking at the data! Researchers typically form their questions from other research and their expertise. We may not have expertise in this field and we have not been studying implicit bias, so I want to be a little more flexible with our analysis."
  },
  {
    "objectID": "labs/Project_report_instructions.html",
    "href": "labs/Project_report_instructions.html",
    "title": "Project Report Instructions",
    "section": "",
    "text": "Important\n\n\n\nInstructions and rubric are completely done! (3/15/2024)"
  },
  {
    "objectID": "labs/Project_report_instructions.html#directions",
    "href": "labs/Project_report_instructions.html#directions",
    "title": "Project Report Instructions",
    "section": "1 Directions",
    "text": "1 Directions\n\n\n\n\n\n\nProject template\n\n\n\nYou may use this project template to get started on the report. It is your responsibility to meet the formatting guidelines below!!\nDO NOT USE SITE PAGE (“Project Report Instructions”, current page) as your template!!\n\n\n\n1.1 Purpose\nProject reports serve as a great way to communicate the knowledge learned in a statistics class and connect it to context within research. It is important that we can take a step back from the numbers and analysis to see what questions linear regression can help us answer.\n\n\n1.2 Formatting guide\n\nThe report will be written in Quarto. Turn in both the qmd and html files\n\nNo code should appear in the html document\n\nThis means all R code chunks should have #| echo: false\nThis also means warnings and messages should be turned off\n\n\nThe report should be 10 - 14 paragraphs long\nTables and figures should NOT have variable names as they appear in the data frame\n\nVariable names should be understood by a reader\nVariable names should be written in full words\nInclude a title or caption for all figures\nFigure and tables appear on same page or close to same page where they are first referenced\nTables and figures are an appropriate size in the html - Nicky is able to read all words in figures and tables\n\nWriting, spelling, and grammar should be admissable\n\nThis means I can generally follow your thought/what you are trying to communicate\nSome spelling and grammar mistakes are allowed\n\nI will not take off points if there are a few sprinkled in\nIf every or close to every sentence has mistakes, then I will take off\n\n\nSectioning of the report\n\nMain sections that were required: Introduction, Statistical Methods, Results, Discussion, Conclusion, and References\nOther sections that might help group specific methods or results\n\nTitle information at the top of the html\n\nThis includes the title itself, your name, and the date\n\n\n\n\n\n\n\n\nThe project report is a separate file from the labs\n\n\n\nYou can save tables and figures from labs or separate files, then load them in the report\n\nSave R objects in analyses file:\n\nSuppose you named the Table 1 as table1\nsave(table1, file = \"table1.Rdata\")\n\nLoad R objects in report file: load(file = \"table1.Rdata\")\n\n\n\n\n\n1.3 Examples of reports\nThe following are examples of reports from BSTA 513 with the feedback that I gave them.\nPlease note that 513 uses a different type of outcome than our class. These examples are meant to help guide you with the formatting and some appropriate content.\nAlso note that these were converted to PDFs so I could write in feedback. Some of the tables and figure sizes were distorted. They need to be legible in the html.\n\nReport 1 with my feedback\nReport 2 with my feedback\n\nThe above reports have code showing in their html. Remember that I am asking you to hide all code, warnings, and messages.\n\n\n1.4 Grading\nThe project report is out of 36 points. Note that the Statistical Methods and Results sections are graded on an 8-point scale, while all other components are graded on a 4-point scale.\n\n1.4.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences with very few grammatical or spelling errors. With little editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences with some (around 2 per section) grammatical or spelling errors. With some editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences, but have many grammatical or spelling errors. With major editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences, but are very hard to follow due to grammar mistakes.\nLab not submitted on Sakai (or by email if late) with .html file. Report is not written with complete sentences. With major editing, the report can be distributed.\n\n\nFigures and work\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables look professional, are easily interpreted by the reader, and easily convey the intended message.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. For the most part, figures and tables look professional, are easily interpreted by the reader, and easily convey the intended message. A few mistakes in the figures are made.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables look semi-professional, are not so easily interpreted by the reader, and convey the intended message but after some work by the reader. Some mistakes in the figures are made.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables do not look professional, are not easily interpreted by the reader, and/or do not convey the intended message. Many mistakes in the figures are made.\nRequested output is not displayed, Missing one or more figures.\n\n\nIntroduction\nProvides a good background for the research question, includes motivation for the question, and references previous research that justifies this analysis.\nProvides a decent background for the research question and includes motivation for the question. Previous research is mentioned, but feels disconnected to the current analysis.\nProvides a decent background for the research question and includes motivation for the question. Previous research is mentioned, but feels disconnected to the current analysis.\nDoes not provide a background that connects to the research question. Motivation and previous research are not mentioned.\nNo introduction included.\n\n\nMethods (8 points)\nDescribes statistical methods concisely and highlights pertinent information to the reader (listed Sections below). Demonstrates proper analyses were performed.\nDescribes statistical methods and highlights pertinent information to the reader (listed Sections below). Details were omitted or added that were not needed to explain the overarching methods. Demonstrates proper analyses were performed.\nDescribes statistical methods and highlights pertinent information to the reader (listed Sections below). Details were omitted or added that were not needed to explain the overarching methods. Some incorrect analyses included in the description.\nDescribes statistical methods, but lacks clarity. Demonstrates a lack of understanding about the overall process of regression analysis. Incorrect analyses included in the description.\nNo methods included.\n\n\nResults (8 points)\nCorrectly interprets coefficients for the explanatory variable and identifies any other interesting trends. Highlights pertinent results to the reader (listed Sections below).\nCorrectly interprets coefficients, but does correctly incorporate the interaction (if in the model). Highlights pertinent results to the reader (listed Sections below).\nIncorrectly interprets coefficients. Highlights pertinent results to the reader (listed Sections below).\nIncorrectly interprets coefficients.Omits pertinent results to the reader (listed Sections below).\nNo results included.\n\n\nDiscussion\nThoroughly and concisely discusses limitations and considerations of the results, and their consequences.\nDiscusses limitations and considerations of the results and their consequences, but misses some big considerations.\nDiscusses limitations and considerations of the results, but does not discuss the consequences.\nDiscusses limitations and considerations of the results, but misses many considerations and does not discuss consequences.\nNo discussion included.\n\n\nConclusion and References\nFor the conclusion, main research question is answered and statistical caveats described to non-technical person. References are mostly cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is answered and statistical caveats described to non-technical person. References are sometimes cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is somewhat answered (but focus is not on the research question) and statistical caveats described to non-technical person. References are sometimes cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is somewhat answered (but not the focus at all) and statistical caveats are not described. References are not cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is not answered. Or references are not included at all.\n\n\n\n\nIn formatting, an example of a report with little editing needed is one that has zero to some grammar or spelling mistakes, no code chunks showing, and no output warnings nor messages showing.\nProfessional figures mean\n\nI can read the words and numbers in the html\n\nVariable names are converted from the data frame version to readable text\nFor example: iam_001 does not show up on axes, instead something like: Response to \"Currently, I am...\"\n\nColors are only used if conveying information\nIntended message of the figure is easily understood\n\nIf you are trying to show a trend of mean IAT vs. an ordered categorical variable, then the variable is ordered on the x-axis\n\n\nFor the references\n\nI will not be overly critical about the formatting\nBy consistency, I mean that you if you are citing things like (Last Name, Year) it doesn’t suddenly change to number citations.\nIf you would like to use Quarto’s citation tool, you can! I actually pair it with Zotero and it works beautifully! (But I would not embark on this if you haven’t used Zotero before)"
  },
  {
    "objectID": "labs/Project_report_instructions.html#sections",
    "href": "labs/Project_report_instructions.html#sections",
    "title": "Project Report Instructions",
    "section": "2 Sections",
    "text": "2 Sections\n\n2.1 Title\n\nPurpose: Create an identifiable name for your research project that includes the main research question’s variables and gives some context to the analysis or results\n\n\n\n2.2 Introduction\n\nLength: 1-2 paragraphs\nPurpose: Introduce the research question and why it is important to study\nThis section is non-technical.\n\nBy reading just the introduction and conclusion, someone without a technical background should have an idea of what they study was about, why it is important, and what the main results are\n\nYou may start with the introduction written in Lab 1, but you should edit it and make sure it flows into your report well!\nShould contain some references\n\n\n\n2.3 Statistical Methods\n\nLength: 3-5 paragraphs\nPurpose: Describe the analyses that were conducted and methods used to select variables and check diagnostics\nImportant to keep in mind: methods typically describe your approach and process, not the results of that process\n\nFor example: I might say “We investigated the linearity of each continuous covariate visually. If continuous variables were not linear, then we divided the variable into categories using existing guidelines from &lt;insert reference here&gt; or creating quartiles.”\n\nIn the methods section, I would NOT say: “We investigated the linearity of each continuous covariate visually. We found that age was not linearly related to IAT scores. Thus, we categorized age into the following groups: ___, ____, ____, ____, and ____.”\n\nThe last two sentences about age would be more appropriate in the Results section\n\n\n\nSome important methods to discuss (You may divide these into your sections, not necessarily with these names)\n\nGeneral approach to the dataset\n\n3-5 sentences\nDid you need to do any quality control?\nMissing data: we performed complete case analysis\n\n1 sentence\nCan be included in the Exploratory data analysis section\n\n\nVariables and variable creation\n\nThis includes a description of analyses for Table 1 and what statistics were used to summarize the variables\n\nMore on creation of Table 1, not discussing the results of Table 1\n\nIncludes (not required)\n\nIndicators for gender identity or race\nCreating BMI\nCategorizing a continuous variable (even if performed in model selection)\nUsing scoring for an ordered categorical variable (that is not your explanatory variable)\n\n1-2 sentences per variable\n\nModel building: we performed purposeful selection\n\n3-5 sentences\nIncludes\n\nDescribe purposeful selection: combining existing literature, clinical significance, and analysis\nHow did you build the model? Describe the process\nDid you consider confounders and effect modifiers?\n\n\nModel diagnostics\n\n2-5 sentences\nIncludes\n\nProcess of investigating model diagnostics\nBy the time you build the model, LINE assumptions should be met\nIf assumptions were not met, what process did you use to fix it?\n\n\n\n\n\n\n2.4 Results\n\nLength: ~3 paragraphs\nPurpose: Relay the results from our sample’s analysis typically focusing on the numbers and interpretations\nSome important results to discuss (also could be sections)\n\nSample data set statistics (Table 1)\n\n3-5 sentences\nInclude a brief description of the sample’s characteristics\nTable 1 should be referenced and appear here!\n\nFinal model\n\n1-2 sentences\nDescribe final model (or models if comparing a few)\n\nWhat variables were included in your final model?\nWhat interactions with your explanatory variable did you include?\n\n\nInterpret the model coefficients in the context of the research question\n\n1-2 paragraphs\nInterpreting the explanatory variable’s relationship with IAT score is the most important thing to report!!\n\nWhen doing this, make sure you account for ALL interactions: If your explanatory variable has multiple interactions and you are trying to interpret one, then what does that mean about the other variables involved in the other interactions? If this is confusing, please make an appointment with me!!\n\n\nResults of model diagnostics if there is anything worth noting\n\nTables & figures\n\nThe following are required tables or figures\n\nTable 1 summarizing participant characteristics both overall and stratified by your primary independent variable\nTable or figure with regression results\n\nCan be a forest plot\nIf you have A LOT of coefficient estimates, the forest plot may not work well!\n\n\n1-3 figures that you think are helpful in understanding the results, for example\n\nDAG explaining connection between variables (if you did this)\nTable or figure to compare model fit statistics (if you did this)\nTable or figure for unadjusted relationship between outcome and explanatory variables\n\n\n\n\n\n2.5 Discussion\n\nLength: 2-3 paragraphs\nPurpose: Discuss the results and give them context outside of the sample and its analysis\nSome important things to include\n\nInclude a paragraph on the limitations of the results\n\nYou don’t need to hit all the limitations, but think about the big ones (generalizability? independence of samples? large sample size vs. clinical significance? the way we handled variables?)\n\nAfter limitations, discuss the positive parts of the results\n\nWhat can we do with these results? What impact can it have?\n\nAny overarching trends that are worth noting? (Giebel et al. 2024)\n\nShould contain some references\n\n\n\n2.6 Conclusion\n\nLength: 1 short paragraph (more like ~3 sentences)\nPurpose: Describe the main conclusions to a non-technical audience\n\n\n\n2.7 References\n\nInclude your references here!\nYou introduction should have references, especially when discussing the social science behind the analysis\nYou must reference the IAT data source!!"
  },
  {
    "objectID": "homework/HW_02.html",
    "href": "homework/HW_02.html",
    "title": "Homework 2",
    "section": "",
    "text": "Caution\n\n\n\nHomework is ready! 1/21/25"
  },
  {
    "objectID": "homework/HW_02.html#directions",
    "href": "homework/HW_02.html#directions",
    "title": "Homework 2",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the HW1 datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as your HW1 .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\nFor each question, make sure to include all code and resulting output in the html file to support your answers\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nWrite all answers in complete sentences as if communicating the results to a collaborator.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your .qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_02.html#question-1",
    "href": "homework/HW_02.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\nThis homework assignment is based on data collected as part of an observational study of patients who suffered from stroke.\nDataset: The main goal was to study various psychological factors: optimism, fatalism, depression, spirituality, and their relationship with stroke severity and other health outcomes among the study participants. Data were collected using questionnaires during a baseline interview and also medical chart review. More information about this study can be found in the article Fatalism, optimism, spirituality, depressive symptoms and stroke outcome: a population based analysis.\nThe dataset that you will work with is called completedata.sas7bdat. It is SIMILAR but does not exactly match the data in the article. It contains information on complete cases (i.e. excludes participants who had missing data on one or more variables of interest) who suffered a stroke. The two variables we are interested in are:\n\nCovariate: Fatalism (larger values indicate that the individual feels less control of their life)\n\nScores range from 8 to 40\n\nOutcome: Depression (larger values imply increased depression)\n\nScores range from 0 to 27\n\n\nFor our homework purposes we will assume they are continuous.\n\nfatal_dep = read_sas(here(\"./data/completedata.sas7bdat\"))\n\n\nPart a\nPlot the data, with title and axis labels, for Depression (y-axis) vs. Fatalism (x-axis). Comment on what you see.\n\n\nPart b\nFit a linear regression model to estimate the association between the predictor Fatalism and the outcome Depression.\nInterpret the slope and intercept. Does the intercept make sense?\n\n\n\n\n\n\nNote\n\n\n\nMake sure to include the confidence interval. Whenever asked to interpret coefficients, you must include confidence intervals. Also, the “units” for fatalism and depression are scores.\n\n\n\n\nPart c\nIn your dataset, make a new variable FatalismC, equal to Fatalism centered at its median (C is for centered).\n\\[\n\\text{FatalismC} = \\text{Fatalism} - \\text{median of Fatalism}\n\\]\nThis is one way of centering a variable, and can be used when the intercept estimate does not make sense. (Hint: the mutate() function will work well here!)\nPlot the data, with title and axis labels, for Depression (y-axis) vs. FatalismC (x-axis).\n\n\nPart d\nRe-run the regression from Part b using this new variable for FatalismC. Interpret the new slope and intercept. Which of the following are the same as Part b: intercept, slope?\n\n\n\n\n\n\nNote\n\n\n\nMake sure to include the confidence interval. Whenever asked to interpret coefficients, you must include confidence intervals. Also, the “units” for the centered fatalism is still the score.\n\n\n\n\nPart e\nFrom the above interpretations, what would be the equivalent conclusion from a hypothesis test for the association between Depression and Fatalism?\n\n\n\n\n\n\nNote\n\n\n\nYou do not need to go through the whole process for the hypothesis test. You only need to state whether it is rejected or not and site the confidence interval as evidence."
  },
  {
    "objectID": "homework/HW_02.html#question-2",
    "href": "homework/HW_02.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nThis question and data are adapted from this textbook.\nIn an experiment designed to describe the dose–response curve for vitamin K, individual rats were depleted of their vitamin K reserves and then fed dried liver for 4 days at different dosage levels. The response of each rat was measured as the concentration of a clotting agent needed to clot a sample of its blood in 3 minutes. The results of the experiment on 12 rats are given in the following table; values are expressed in common logarithms for both dose and response.\n\nclot = read_excel(here(\"./data/CH05Q09.xls\"))\nclot %&gt;% gt() %&gt;%\n  cols_label(RAT = md(\"**Rat**\"),\n             LOGCONC = md(\"**Log10 Concentration (Y)**\"),\n             LOGDOSE = md(\"**Log10 Dose (X)**\"))\n\n\n\n\n\n  \n    \n      Rat\n      Log10 Concentration (Y)\n      Log10 Dose (X)\n    \n  \n  \n    1\n2.65\n0.18\n    2\n2.25\n0.33\n    3\n2.26\n0.42\n    4\n1.95\n0.54\n    5\n1.72\n0.65\n    6\n1.60\n0.75\n    7\n1.55\n0.83\n    8\n1.32\n0.92\n    9\n1.13\n1.01\n    10\n1.07\n1.04\n    11\n0.95\n1.09\n    12\n0.88\n1.15\n  \n  \n  \n\n\n\n\nUse the log-transformed values as given in the dataset.\nUse the following scatterplot to build your answers off of:\n\n\n\n\n\n\n\n\n\n\nPart a\nFit a linear regression model to the data and add the regression line to the plot.\n\n\nPart b\nUse R to create the ANOVA table for the regression described in the exercise.\n\n\nPart c\nUsing the F-test, determine whether there is an association between the log10 concentration and log10 dose.\n\n\n\n\n\n\nNote\n\n\n\nMake sure to include all needed steps for an F-test. Calculating the F test statistic (step 5) is not needed if you use the ANOVA table. Make sure your conclusion connects back the research context.\n\n\n\n\nPart d\nRewrite your hypothesis test in Part c to show the null and alternative models that we are testing. Did we reject the smaller (reduced) model?\n\n\n\n\n\n\nNote\n\n\n\nYou do not need to go through the hypothesis test process again. A quick statement on rejecting or not is okay.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you prefer to write out the models by hand, remember that you can take a picture of your work and insert it into this document. HW0 can be a good reference for how we’ve done this before."
  },
  {
    "objectID": "homework/HW_02.html#question-3",
    "href": "homework/HW_02.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\nWe will continue to work with the study and dataset from Question 2 above.\n\nPart a\nFind the correlation coefficient between the two variables. Is the value consistent with your description of the relationship in Question 2? Why or why not?\n\n\nPart b\nCalculate the coefficient of determination using linear regression summary output. Can we also calculate the coefficient of determination from the ANOVA in Question 2?\n\n\nPart c\nGive an interpretation of the coefficient of determination in the context of the study."
  },
  {
    "objectID": "homework/HW_01.html",
    "href": "homework/HW_01.html",
    "title": "Homework 1",
    "section": "",
    "text": "Caution\n\n\n\nHomework is ready! 1/9/25"
  },
  {
    "objectID": "homework/HW_01.html#directions",
    "href": "homework/HW_01.html#directions",
    "title": "Homework 1",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the HW1 datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as your HW1 .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\nFor each question, make sure to include all code and resulting output in the html file to support your answers\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nWrite all answers in complete sentences as if communicating the results to a collaborator.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your .qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_01.html#questions",
    "href": "homework/HW_01.html#questions",
    "title": "Homework 1",
    "section": "Questions",
    "text": "Questions\nThe following questions were adapted from this textbook.\n\nQuestion 1\nPlease use R code to determine the following answers. (adapted from problem 3.3 in Applied Regression Analysis and Other Multivariable Methods)\n\n\n\n\n\n\nType ?pnorm in the console to get some information on a potentially helpful function.\n\n\n\n\nPart a\nFrom a normal distribution with mean 4 and standard deviation 6, what is \\(P(X&gt;2)\\)?\n\n\nPart b\nFrom a normal distribution with mean 4 and standard deviation 6, for what value (in place of ??) would \\(P(X&gt;??) = 0.1\\)?\n\n\n\nQuestion 2\nSuppose that the height (\\(H\\)) of assigned-male-at-birth (AMAB) patients registered at a clinic has the normal distribution with mean 70 inches and variance 4. (adapted from problem 3.11 in Applied Regression Analysis and Other Multivariable Methods)\n\nPart a\nFor a random sample of patients of size \\(n = 25\\), the expression \\(P(\\bar{H} &lt; 65)\\), in which \\(\\bar{H}\\) denotes the sample mean height, is equivalent to saying \\(P(Z &lt; ?)\\)\n\n\n\n\n\n\n\\(Z\\) is a standard normal random variable.\n\n\n\n\n\nPart b\nUsing the pnorm function, show that the probability expressions in Part a are equal.\n\n\nPart c\nFind an interval \\((a, b)\\) such that \\(P(a&lt; \\bar{H} &lt;b) = 0.80\\) for the same random sample in Part a.\n\n\n\nQuestion 3\nTest the null hypothesis that the true population average height is the same for two independent groups from one hospital versus the alternative hypothesis that these two population averages are different, using the following data:\n\nGroup 1: [69.25, 72.80, 68.73, 72.01, 70.36, 71.49, 72.73]\nGroup 2: [67.54, 68.51, 71.84, 70.59, 71.52, 71.50]\n\nYou may assume that the populations from which the data come are each normally distributed, with equal population variances. What conclusion should be drawn, with \\(\\alpha = 0.05\\)?\n\n\n\n\n\n\nPlease attempt this problem using R. Take a look at the information for the t.test function. You will need to set x, y, alternative, and var.equal=T. You can use the below groups coded in R.\n\n\n\n\ngrp1 = c(69.25, 72.80, 68.73, 72.01, 70.36, 71.49, 72.73)\ngrp2 = c(67.54, 68.51, 71.84, 70.59, 71.52, 71.50)\n\n\n\nQuestion 4\nThe choice of an alternative hypothesis (\\(H_A\\) or \\(H_1\\)) should depend primarily on (choose all that apply). Explain your reasoning.\n\nthe data obtained from the study.\nwhat the investigator is interested in determining.\nthe critical region.\nthe significance level.\nthe power of the test.\n\n\n\nQuestion 5\nVisit this site on dplyr.\nFor one of the functions that we have not discussed in class, please use it on the dds.discr dataset. Please write in words what you would like to perform, then write the code.\nNote: the dds.discr dataset is an .Rda file. Instead of using read_csv() or read_excel(), you can use load().\n\n\nQuestion 6\nThe accompanying data CH05Q01 gives the dry weights (Y) of 11 chick embryos ranging in age from 6 to 16 days (X ). Also given in the data are the values of the common logarithms of the weights (Z).\n\nLoad the dataset using the readxl package.\n\nThis readxl package was installed as a part of the tidyverse, however it does not get loaded when you load the tidyverse package and thus you need to do that separately.\nUse the command read_excel(), as shown below\n\n\n\nlibrary(readxl)\n# you might need to update the location of the data file\n# you can choose whatever name you like for the tibble when loading it into R's workspace \nch05q01 &lt;- read_excel(\"./data/CH05Q01.xls\")\n\n\nPart a\nCreate two scatterplots in R using ggplot:\n\nBetween age (X) and dry weight (Y)\nBetween age (X) and log10 dry weight (Z)\n\nObserve the following two scatter diagrams. Describe the relationships between age (X) and dry weight (Y) and between age and log10 dry weight (Z).\n\n\nPart b\nState the population simple linear regression models for these two regressions: Y regressed on X and Z regressed on X.\n\n\n\n\n\n\nThis is asking for the regression models BEFORE you find the values of the coefficients.\n\n\n\n\n\nPart c\nDetermine the least-squares estimates of each of the regression lines in part (b).\n\n\n\n\n\n\nNow get the regression coefficients using R and plug them into the regression models from (b). You can get the coefficients from the R output!\n\n\n\n\n\nPart d\nUsing ggplot, create the respective best-fit lines on your plots. Which of the two regression lines has the better fit? Based on your answers to parts (a)–(c), is it more appropriate to run a linear regression of Y on X or of Z on X? Explain.\n\n\nPart e (to be covered on Monday 1/22)\nFor the regression that you chose as being more appropriate in part (d), find 95% confidence intervals for the true slope and intercept. Interpret each interval with regard to the null hypothesis that the true value is 0.\n\n\n\n\n\n\nYou can get the CI’s from the R output\n\n\n\n\n\nPart f\nFor the regression that you chose as being more appropriate in part (d), add 95% confidence and prediction bands. Using your sketch, find and interpret an approximate 95% confidence interval for the mean response of an 8-day-old chick."
  },
  {
    "objectID": "homework/HW_01.html#question-7",
    "href": "homework/HW_01.html#question-7",
    "title": "Homework 1",
    "section": "Question 7",
    "text": "Question 7\nGo to the R Documentation of the lm() function. Please answer the following questions about the function arguments and output.\n\nPart a\nIf you wanted to perform weighted least squares in R, what argument would you need to change?\n\n\nPart b\nWhat is the default method in lm()? Can it be used to solve OLS? (Hint: check out the Wiki page for the method)\n\n\nPart c\nIn linear regression, a singular fit means that the estimated standard deviation of your residuals is very close to zero. In the lm() function, will you get an error if you have a singular fit?"
  },
  {
    "objectID": "homework/HW_01.html#question-8",
    "href": "homework/HW_01.html#question-8",
    "title": "Homework 1",
    "section": "Question 8",
    "text": "Question 8\nQuick True/False question: Is ordinary least squares the only way to find the best fit line for linear regression?\nExplain your reasoning."
  },
  {
    "objectID": "homework/HW_04.html",
    "href": "homework/HW_04.html",
    "title": "Homework 4",
    "section": "",
    "text": "Caution\n\n\n\nHomework 4 is updated (Nicky 2/19/25)"
  },
  {
    "objectID": "homework/HW_04.html#directions",
    "href": "homework/HW_04.html#directions",
    "title": "Homework 4",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the HW3 datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as your HW3 .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\nFor each question, make sure to include all code and resulting output in the html file to support your answers\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nWrite all answers in complete sentences as if communicating the results to a collaborator.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your .qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_04.html#questions",
    "href": "homework/HW_04.html#questions",
    "title": "Homework 4",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nThis question uses the same dataset as HW 2, question 1 and HW 3, question 4.\nThis question is based on data collected as part of an observational study of patients who suffered from stroke.\nDataset: The main goal was to study various psychological factors: optimism, fatalism, depression, spirituality, and their relationship with stroke severity and other health outcomes among the study participants. Data were collected using questionnaires during a baseline interview and also medical chart review. More information about this study can be found in the article Fatalism, optimism, spirituality, depressive symptoms and stroke outcome: a population based analysis.\nThe dataset that you will work with is called completedata.sas7bdat. The two variables we are interested in are:\n\nCovariate 1: Fatalism (larger values indicate that the individual feels less control of their life)\n\nPotential scores range from 8 to 40\n\nCovariate 2: Optimism (larger values indicate that the individual feels higher levels of optimism)\n\nPotential scores range from 6 to 24\n\nCovariate 3: Spirituality (larger values indicate that the individual has more belief in a higher power)\n\nPotential scores range from 2 to 8\n\nOutcome: Depression (larger values imply increased depression)\n\nPotential scores range from 0 to 27\n\n\nFor our homework purposes we will assume each variable is continuous.\n\ndep_df = read_sas(here(\"./data/completedata.sas7bdat\"))\n\n\nPart a\nFit the regression model with all the covariates (Fatalism, Optimism, Spirituality), display the regression table, and write out the fitted regression line.\n\n\nPart e\nDoes at least one of the covariates contribute significantly to the prediction of Depression? (Note: this is an overall test. Please follow the hypothesis test steps. To complete step 4-6, simply output your ANOVA table.)\n\n\nPart f\nDoes the addition of Spirituality add significantly to the prediction of Depression achieved by Fatalism and Optimism?\n\n\nPart g\nDoes the addition of Spirituality and Optimism add significantly to the prediction of Depression achieved by Fatalism?\n\n\n\nQuestion 2\nA team of nutrition experts investigated the influence of protein content in diet on the relationship between age (explanatory variable) and height (outcome, in centimeters) for children. Using the dataset, CH12Q03.xls, answer the following questions.\nThis question was adapted from this textbook.\n\nPart a\nUsing R, make a variable that is a factor for Diet. Make sure to check what values the original variable for Diet can take. How many indicator functions do you need to represent the categorical variable Diet (protein-rich vs. protein-poor)?\n\n\nPart b\nAt a level of significance \\(\\alpha = 0.10\\), test whether protein diet modifies the effect of age on height. Justify your answer (e.g., perform a hypothesis test for the interaction between diet and age).\nNote: recall that we model an effect modifier with an interaction.\n\n\nPart c\nIs it possible that diet is a confounder? Note: this will depend on your results from Part b.\n\n\nPart d\nWrite the fitted regression equation for our model in Part b. Write the respective regression lines for each specific diet group: protein rich and protein poor. Interpret the slope of each regression line (include the 95% CI here).\n\n\n\nQuestion 3\nAn experiment was conducted regarding a quantitative analysis of factors found in high-density lipoprotein (HDL) in a sample of human blood serum. Three variables thought to be predictive of, or associated with, HDL measurement (Y) were the total cholesterol (X1) and total triglyceride (X2) concentrations in the sample, plus the presence or absence of a certain sticky component of the serum called sinking pre-beta or SPB (X3), coded as 0 if absent and 1 if present. Using the dataset, CH09Q05.xls, answer the following questions.\n\nPart a\nUse \\(\\alpha= 0.05\\), test whether if there is a crude association between HDL measurement and total cholesterol. Note: testing for a crude association means we fit a simple linear regression model and see if the association is significant.\n\n\nPart b\nSometimes simple linear regression leads us to believe that there is no association between two variables, but missing interaction might be obscuring the association. Use \\(\\alpha= 0.1\\) to test whether total triglyceride is an effect modifier of the association between HDL and total cholesterol. Make sure to include a concluding statement.\nNote: Since the data frame has the variables named as \\(Y\\), \\(X1\\), and \\(X2\\), you may use those in the regression equations, but when you are making a conclusion, please use the specific names of the variables to identify each. For example, \\(Y\\) is actually HDL.\n\n\nPart c\nTest whether total triglyceride is a confounder by comparing the model in Part a to a model that includes total triglyceride. Make sure to include a concluding statement and interpret your coefficients."
  },
  {
    "objectID": "lessons/09_MLR_Inf/09_MLR_Inf_key_info.html#key-dates",
    "href": "lessons/09_MLR_Inf/09_MLR_Inf_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html",
    "title": "Lesson 10: Categorical Covariates",
    "section": "",
    "text": "Understand why we need a new way to code categorical variables compared to continuous variables\nWrite the regression equation for a categorical variable using reference cell coding\nCalculate and interpret coefficients for reference cell coding\nChange the reference level in a categorical variable for reference cell coding\nCreate new variables and interpret coefficient for ordinal / scoring coding\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#lets-map-that-to-our-regression-analysis-process",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#lets-map-that-to-our-regression-analysis-process",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Let’s map that to our regression analysis process",
    "text": "Let’s map that to our regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#still-looking-at-gapminder-life-expectancy-data",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#still-looking-at-gapminder-life-expectancy-data",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Still looking at Gapminder Life Expectancy data",
    "text": "Still looking at Gapminder Life Expectancy data\n\nWe will look at life expectancy vs. these world regions\nGapminder uses four world regions\n\nAfrica\nThe Americas\nAsia\nEurope\n\n\n \n\nNote: I am calling the expected life expectancy \\(\\widehat{LE}\\)\n\nPreviously, I have written \\(\\widehat{\\text{life expectancy}}\\)"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#linear-regression-with-a-categorical-covariate",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#linear-regression-with-a-categorical-covariate",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Linear regression with a categorical covariate",
    "text": "Linear regression with a categorical covariate\n\n\nBad option for visualization:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_point() +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nGood option for visualization:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_jitter(size = 1, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\",\n       caption = \"Diamonds = region averages\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nUsed geom_jitter()\n\n\nGood option for visualization:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_boxplot() +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#linear-regression-with-a-categorical-covariate-1",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#linear-regression-with-a-categorical-covariate-1",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Linear regression with a categorical covariate",
    "text": "Linear regression with a categorical covariate\n\n\n\nWhen using a categorical covariate/predictor (that is not ordered),\n\nWe do NOT, technically, find a best-fit line\n\nInstead we model the means of the outcome\n\nFor the different levels of the categorical variable\n\nIn 511, we used Kruskal-Wallis test and our ANOVA table to test if groups means were statistically different from one another\nWe can do this using linear models AND we can include other variables in the model"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#there-are-different-ways-to-code-categorical-variables",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#there-are-different-ways-to-code-categorical-variables",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "There are different ways to code categorical variables",
    "text": "There are different ways to code categorical variables\n\nReference cell coding (sometimes called dummy coding)\n\nCompares each level of a variable to the omitted (reference) level\n\nEffect coding (sometimes called sum coding or deviation coding)\n\nCompares deviations from the grand mean\n\nOrdinal encoding (sometimes called scoring)\n\nCategories have a natural, even spaced ordering\n\n\nIf you want to learn more about these and other coding schemes:\n\nCoding Systems for Categorical Variables in Regression Analysis\nCategorical Data Encoding Techniques\nCoding Schemes for Categorical Variables"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#building-the-regression-equation-problem-with-a-single-coefficient",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#building-the-regression-equation-problem-with-a-single-coefficient",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Building the regression equation: problem with a single coefficient",
    "text": "Building the regression equation: problem with a single coefficient\n\n\nPreviously: simple linear regression\n\nOutcome \\(Y\\) = numerical variable\nPredictor \\(X\\) = numerical variable\n\nThe regression (best-fit) line is: \\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X \\]\n\nNew: what if the explanatory variable is categorical?\nNaively, we could write: \\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\)\nOr, with our variables: \\[\\widehat{\\textrm{LE}} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot \\textrm{WR} \\]\n\nBut what does \\(\\textrm{WR}\\) (world regions) mean in this equation?\n\nWhat values can it take? How do we represent each region?\n\n\n \n\nNote: the above is WRONG"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#building-the-regression-equation-how-do-we-map-categories-to-means",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#building-the-regression-equation-how-do-we-map-categories-to-means",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Building the regression equation: how do we map categories to means?",
    "text": "Building the regression equation: how do we map categories to means?\n\nIf we only have world region in our model and want to map it to an expected life expectancy…\n\n\n\n\nWe want to create a function that can map each region to life expectancy\n\nIf in Africa: \\(\\widehat{LE} = 61.32\\) years\nIf in the Americas: \\(\\widehat{LE} = 74.64\\) years\nIf in Asia: \\(\\widehat{LE} = 71.70\\) years\nIf in Europe: \\(\\widehat{LE} = 77.61\\) years\n\n\n \n\nCan we make one equation for \\(\\widehat{LE}\\) by putting the “if” statements within the equation?"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#building-the-regression-equation-indicator-functions",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#building-the-regression-equation-indicator-functions",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Building the regression equation: Indicator functions",
    "text": "Building the regression equation: Indicator functions\n\nIn order to represent each region in the equation, we need to introduce a new function:\n\nIndicator function:\n\n\\[I(X = x) \\text{ or } I(x) =\n\\left\\{\n\\begin{array}{@{}ll@{}}\n1, & \\text{if}\\ X = x \\\\\n  0, & \\text{else}\n\\end{array}\\right. \\]\n\nThis basically a binary yes/no if \\(X\\) is a specific value \\(x\\)\n\nFor example, if we want to identify a country as being in the Americas region, we can make:\n\\[I(WR = \\text{Americas}) \\text{ or }I(\\text{Americas}) =\n\\left\\{\n\\begin{array}{@{}ll@{}}\n1, & \\text{if}\\ WR = \\text{Americas} \\\\\n  0, & \\text{else}\n\\end{array}\\right. \\]"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#poll-everywhere-question-1",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#poll-everywhere-question-1",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#building-the-regression-equation-indicators-in-our-equation",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#building-the-regression-equation-indicators-in-our-equation",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Building the regression equation: Indicators in our equation",
    "text": "Building the regression equation: Indicators in our equation\n\n\n\\[\\begin{aligned}\n\\widehat{\\textrm{LE}} = & 61.32 \\cdot I(\\text{Africa}) + 74.64 \\cdot I(\\text{Americas}) + \\\\ &71.7 \\cdot I(\\text{Asia}) + 77.61 \\cdot I(\\text{Europe})\n\\end{aligned}\\]\n\nHowever, a linear regression equation still requires an intercept!\n\nSo one of our regions need to become our “reference” group\nWe’ll use Africa as our reference\nThat means we need to adjust all the numbers\n\n\n\\[\\begin{aligned}\n\\widehat{\\textrm{LE}} = & 61.32 + 13.32 \\cdot I(\\text{Americas}) + \\\\ &10.38 \\cdot I(\\text{Asia}) + 16.29 \\cdot I(\\text{Europe}) \\\\\n\\widehat{\\textrm{LE}} = & \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) + \\\\ & \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#viewing-the-regression-equation-another-way",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#viewing-the-regression-equation-another-way",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Viewing the regression equation another way",
    "text": "Viewing the regression equation another way\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) + \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\\]\n\n\n\n\n\n\n\n\n\n\nWorld region\nRegression equation for WR\nAverage Life Expectancy for WR\n\n\n\n\nAfrica\n\\(\\begin{aligned} \\widehat{\\textrm{LE}} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 0 + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 \\end{aligned}\\)\n\\(\\widehat{\\textrm{LE}} = \\widehat\\beta_0\\)\n\n\nAmericas\n\\(\\begin{aligned} \\widehat{\\textrm{LE}} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 1+ \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 \\end{aligned}\\)\n\\(\\widehat{\\textrm{LE}} = \\widehat\\beta_0 + \\widehat\\beta_1\\)\n\n\nAsia\n\\(\\begin{aligned} \\widehat{\\textrm{LE}} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 0 + \\\\ & \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 \\cdot 0 \\end{aligned}\\)\n\\(\\widehat{\\textrm{LE}} = \\widehat\\beta_0 + \\widehat\\beta_2\\)\n\n\nEurope\n\\(\\begin{aligned} \\widehat{\\textrm{LE}} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 0 + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 1 \\end{aligned}\\)\n\\(\\widehat{\\textrm{LE}} = \\widehat\\beta_0 + \\widehat\\beta_3\\)"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#interpretation-of-regression-equation-coefficients",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#interpretation-of-regression-equation-coefficients",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Interpretation of regression equation coefficients",
    "text": "Interpretation of regression equation coefficients\n\nRemember: expected, mean, and average are interchangeable\n\n\n\n\n\n\n\n\nCoefficient\nInterpretation\n\n\n\n\n\\(\\widehat{\\beta}_0\\)\nExpected/mean/average life expectancy of Africa\n\n\n\\(\\widehat{\\beta}_1\\)\nDifference in mean life expectancy of the Americas and Africa -OR-\nMean difference in life expectancy of the Americas and Africa\n\n\n\\(\\widehat{\\beta}_2\\)\nDifference in mean life expectancy between Asia and Africa -OR-\nMean difference in life expectancy between Asia and Africa\n\n\n\\(\\widehat{\\beta}_3\\)\nDifference in mean life expectancy between Europe and Africa -OR-\nMean difference in life expectancy between Europe and Africa"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#poll-everywhere-question-2",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#poll-everywhere-question-2",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#regression-table-with-lm-function",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#regression-table-with-lm-function",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Regression table with lm() function",
    "text": "Regression table with lm() function\n\nmodel1 &lt;- lm(LifeExpectancyYrs ~ four_regions, data = gapm2)\ntidy(model1, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 38) %&gt;% \n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n61.32\n0.76\n80.26\n0.00\n59.81\n62.83\n    four_regionsAmericas\n13.32\n1.23\n10.83\n0.00\n10.89\n15.74\n    four_regionsAsia\n10.38\n1.08\n9.61\n0.00\n8.25\n12.51\n    four_regionsEurope\n16.29\n1.13\n14.37\n0.00\n14.05\n18.52\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{LE}} = 61.32 + 13.32 \\cdot I(\\text{Americas}) + 10.38 \\cdot I(\\text{Asia}) + 16.29 \\cdot I(\\text{Europe})\\]\n\nWhich world region did R choose as the reference level?\nHow you would calculate the mean life expectancies of world regions using only the results from the regression table?"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#bringing-in-the-numbersunits95-ci",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#bringing-in-the-numbersunits95-ci",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Bringing in the numbers/units/95% CI",
    "text": "Bringing in the numbers/units/95% CI\n\n\n\n\n\n\n\nCoefficient\nInterpretation\n\n\n\n\n\\(\\widehat{\\beta}_0\\)\nAverage life expectancy of countries in Africa is 61.32 years (95% CI: 59.81, 62.83).\n\n\n\\(\\widehat{\\beta}_1\\)\nThe difference in mean life expectancy between countries in the Americas and Africa is 13.32 (95% CI: 10.89, 15.74).\n\n\n\\(\\widehat{\\beta}_2\\)\nThe difference in mean life expectancy between countries in the Americas and Africa is 10.38 (95% CI: 8.25, 12.51).\n\n\n\\(\\widehat{\\beta}_3\\)\nThe difference in mean life expectancy between countries in Europe and Africa is 18.52 (95% CI: 14.05, 18.52).\n\n\n\n \n\nDon’t forget that we can use the confidence intervals to assess whether the mean difference with Africa is significant or not"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#we-can-also-use-r-to-report-each-regions-average-life-expectancy",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#we-can-also-use-r-to-report-each-regions-average-life-expectancy",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "We can also use R to report each region’s average life expectancy",
    "text": "We can also use R to report each region’s average life expectancy\nFind the 95% CI’s for the mean life expectancy for the Americas, Asia, and Europe\n\nUse the base R predict() function (see Lesson 4 for more info)\nRequires specification of a newdata “value”\n\n\nnewdata &lt;- data.frame(four_regions = c(\"Africa\", \"Americas\", \"Asia\", \"Europe\")) \n\n\n\n\n(pred = predict(model1, \n                newdata=newdata, \n                interval=\"confidence\"))\n\n       fit      lwr      upr\n1 61.32037 59.81287 62.82787\n2 74.63824 72.73841 76.53806\n3 71.70185 70.19435 73.20935\n4 77.60889 75.95751 79.26027\n\n\n\n\n\nInterpretations\n\n\n\nThe average life expectancy for countries in the Americas is 74.64 years (95% CI: 72.74, 76.54).\nThe average life expectancy for countries in Asia is 71.7 years (95% CI: 70.19, 73.21).\nThe average life expectancy for countries in Europe is 77.61 years (95% CI: 75.96, 79.26)."
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#another-way-to-look-at-coefficient-values",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#another-way-to-look-at-coefficient-values",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Another way to look at coefficient values",
    "text": "Another way to look at coefficient values\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) +  \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\\]\n\n\nCode\n# means of each level of `four_regions`\ngapm2_ave &lt;- gapm2 %&gt;% \n  group_by(four_regions) %&gt;% \n  summarise(\n    life_ave = mean(LifeExpectancyYrs))\n\n# mean of `africa`\nmean_africa &lt;- gapm2_ave %&gt;% \n  filter(four_regions == \"Africa\") %&gt;% \n  pull(life_ave)\n\n# differences in means between levels of `four_regions` and `africa`\ngapm2_ave_diff &lt;- gapm2_ave %&gt;% \n  mutate(`Difference with Africa` = life_ave - mean_africa) %&gt;%\n  rename(`World regions` = four_regions, \n         `Average life expectancy` = life_ave)\n\n# At the beginning of the Rmd we loaded knitr, which is where the kable command is from\n# library(knitr)\ngapm2_ave_diff %&gt;% kable(\n  digits = 1,\n  format = \"markdown\"\n  ) \n\n\n\n\n\nWorld regions\nAverage life expectancy\nDifference with Africa\n\n\n\n\nAfrica\n61.3\n0.0\n\n\nAmericas\n74.6\n13.3\n\n\nAsia\n71.7\n10.4\n\n\nEurope\n77.6\n16.3\n\n\n\n\n\n\\[\\widehat{\\textrm{LE}} = 61.32 + 13.32 \\cdot I(\\text{Americas}) + 10.38 \\cdot I(\\text{Asia}) + 16.29 \\cdot I(\\text{Europe})\\]"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#reference-levels",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#reference-levels",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Reference levels",
    "text": "Reference levels\nWhy is Africa not one of the variables in the regression equation?\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) +  \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\\]\n\nCategorical variables have to have at least 2 levels. If they have 2 levels, we call them binary\n\n \n\nWe choose one level as our reference level to which all other levels of the categorical variable are compared\n\nThe levels \\(\\text{Americas}, \\text{Asia}, \\text{Europe}\\) are compared to the level \\(\\text{Africa}\\)\n\n\n \n\nThe intercept of the regression equation is the mean of the outcome restricted to the reference level\n\nRecall that the intercept is the mean life expectancy of Africa, which was our reference level\n\n\n \n\nIf the categorical variable has \\(r\\) levels, then we need \\(r-1\\) variables/coefficients to model it!"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#we-can-change-the-reference-level-to-europe-12",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#we-can-change-the-reference-level-to-europe-12",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "We can change the reference level to Europe (1/2)",
    "text": "We can change the reference level to Europe (1/2)\n\nSuppose we want to compare the mean life expectancies of world regions to the \\(\\text{Europe}\\) level instead of \\(\\text{Africa}\\)\nBelow is the estimated regression equation for when \\(Africa\\) is the reference level\n\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) +  \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\\]\n\nUpdate the variables to make \\(Europe\\) the reference level:\n\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Africa}) +  \\widehat\\beta_2 \\cdot I(\\text{Americas}) + \\widehat\\beta_3 \\cdot I(\\text{Asia})\\]"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#we-can-change-the-reference-level-to-europe-22",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#we-can-change-the-reference-level-to-europe-22",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "We can change the reference level to Europe (2/2)",
    "text": "We can change the reference level to Europe (2/2)\n\nNow update the coefficients of the regression equation using the output below.\n\n\n\n\n\n\nWorld regions\nAverage life expectancy\nDifference with Europe\n\n\n\n\nAfrica\n61.32\n-16.29\n\n\nAmericas\n74.64\n-2.97\n\n\nAsia\n71.70\n-5.91\n\n\nEurope\n77.61\n0.00\n\n\n\n\n\n\\[\\widehat{\\textrm{LE}} = 77.61 -16.29 \\cdot I(\\text{Africa}) -2.97 \\cdot I(\\text{Americas}) -5.91 \\cdot I(\\text{Asia})\\]"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#r-change-reference-level-to-europe-12",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#r-change-reference-level-to-europe-12",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "R: Change reference level to europe (1/2)",
    "text": "R: Change reference level to europe (1/2)\n\nfour_regions data type was originally a character - check this with str()\n\n\nstr(gapm$four_regions) \n\n chr [1:195] \"asia\" \"europe\" \"africa\" \"europe\" \"africa\" \"americas\" ...\n\n\n\nIn order to change the reference level, we need to convert it to data type factor\n\nI also did this at the beginning to capitalize each region\n\n\n\ngapm_ex = gapm %&gt;% \n mutate(\n   four_regions = factor(four_regions, \n                         levels = c(\"africa\", \"americas\", \"asia\", \"europe\"), \n                         labels = c(\"Africa\", \"Americas\", \"Asia\", \"Europe\"))\n   )\nstr(gapm_ex$four_regions) \n\n Factor w/ 4 levels \"Africa\",\"Americas\",..: 3 4 1 4 1 2 2 4 3 4 ...\n\nlevels(gapm_ex$four_regions) # order of factor levels\n\n[1] \"Africa\"   \"Americas\" \"Asia\"     \"Europe\""
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#r-change-reference-level-to-europe-22",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#r-change-reference-level-to-europe-22",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "R: Change reference level to Europe (2/2)",
    "text": "R: Change reference level to Europe (2/2)\n\nNow change the order of the factor levels\nCode below uses fct_relevel() from the forcats package that gets loaded as a part of the tidyverse\nAny levels not mentioned will be left in their existing order, after the explicitly mentioned levels.\n\n\ngapm2 &lt;- gapm2 %&gt;% \n  mutate(\n    four_regions = fct_relevel(four_regions, \"Europe\")\n    )\n\n\nCheck the order:\n\n\nlevels(gapm2$four_regions)\n\n[1] \"Europe\"   \"Africa\"   \"Americas\" \"Asia\""
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#r-run-model-with-europe-as-the-reference-level",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#r-run-model-with-europe-as-the-reference-level",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "R: Run model with Europe as the reference level",
    "text": "R: Run model with Europe as the reference level\n\nlevels(gapm2$four_regions)\n\n[1] \"Europe\"   \"Africa\"   \"Americas\" \"Asia\"    \n\nmodel2 &lt;- lm(LifeExpectancyYrs ~ four_regions, data = gapm2)\ntidy(model2) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% \n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n77.61\n0.84\n92.72\n0.00\n    four_regionsAfrica\n−16.29\n1.13\n−14.37\n0.00\n    four_regionsAmericas\n−2.97\n1.28\n−2.33\n0.02\n    four_regionsAsia\n−5.91\n1.13\n−5.21\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\\widehat{\\textrm{LE}} &=  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Africa}) +  \\widehat\\beta_2 \\cdot I(\\text{Americas}) + \\widehat\\beta_3 \\cdot I(\\text{Asia}) \\\\ \\widehat{\\textrm{LE}} &= 77.61 -16.29 \\cdot I(\\text{Africa}) -2.97 \\cdot I(\\text{Americas}) -5.91 \\cdot I(\\text{Asia}) \\end{aligned}\\]"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#fitted-values-residuals",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#fitted-values-residuals",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Fitted values & residuals",
    "text": "Fitted values & residuals\n\n\nSimilar to as before:\n\nObserved values \\(Y\\) are the values in the dataset\nFitted values \\(\\widehat{Y}\\) are the values that fall on the best-fit line for a specific value of x are the means of the outcome stratified by the categorical predictor’s levels\nResiduals (\\(\\widehat{\\epsilon} = Y - \\widehat{Y}\\)) are the differences between the two"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#fitted-values-are-the-same-as-the-means",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#fitted-values-are-the-same-as-the-means",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Fitted values are the same as the means",
    "text": "Fitted values are the same as the means\n\nm1_aug &lt;- augment(model1)\n\nggplot(m1_aug, aes(x = four_regions, y = .fitted)) + geom_point() +\n  theme(axis.text = element_text(size = 22), axis.title = element_text(size = 22))"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#residual-plots-now-the-spread-within-each-region",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#residual-plots-now-the-spread-within-each-region",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Residual plots (now the spread within each region)",
    "text": "Residual plots (now the spread within each region)\n\nggplot(m1_aug, aes(x=.resid)) + geom_histogram() + \n  theme(axis.text = element_text(size = 22), title = element_text(size = 22))"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#poll-everywhere-question-3",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#poll-everywhere-question-3",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#lets-look-at-life-expectancy-vs.-four-income-levels",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#lets-look-at-life-expectancy-vs.-four-income-levels",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Let’s look at life expectancy vs. four income levels",
    "text": "Let’s look at life expectancy vs. four income levels\n\nGapminder discusses individual income levels\n\n \n\nIncome levels for a country is based on average GDP per capita, and grouped into:\n\nLow income\nLower middle income\nUpper middle income\nHigh income"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#visualizing-the-ordinal-variable-income-levels",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#visualizing-the-ordinal-variable-income-levels",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Visualizing the ordinal variable, income levels",
    "text": "Visualizing the ordinal variable, income levels\n\n\n\n\n\n\n\n\n\n\n\n\nA few changes needed:\n\nPut the income levels in order\n\n\ngapm2 = gapm2 %&gt;%\n mutate(income_levels = factor(income_levels, \n            ordered = T, \n            levels = c(\"Low income\", \n            \"Lower middle income\", \n            \"Upper middle income\", \n            \"High income\")))\n\n\nMake the income levels readable\n\nHow to Rotate Axis Labels in ggplot2?"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#much-better-visualizing-the-ordinal-variable-income-levels",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#much-better-visualizing-the-ordinal-variable-income-levels",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Much better: Visualizing the ordinal variable, income levels",
    "text": "Much better: Visualizing the ordinal variable, income levels\n\n\n\nggplot(gapm2, aes(x = income_levels, y = LifeExpectancyYrs)) +\n  geom_jitter(size = 1, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  labs(x = \"Income levels\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. income levels\",\n       caption = \"Diamonds = Income level averages\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20), \n        axis.text.x=element_text(angle = 20, vjust = 1, hjust=1))"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#how-can-we-code-this-variable",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#how-can-we-code-this-variable",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "How can we code this variable?",
    "text": "How can we code this variable?\nWe have two options:\n\n\n\n\nTreat the levels as nominal, and use reference cell coding\n\n\n\nLike we did with world regions\nThis option will not break the linearity assumption\nFor \\(g\\) categories of the variable, we will have \\(g-1\\) coefficients to estimate\n\n\n\n\n\n\nUse the ordinal values to score the levels and treat as a numerical variable\n\n\n\nEven if a variable is inherently ordered, we need to check that linearity holds if categories are represented as numbers\nThis way of coding preserves more power in the model (less coefficients to estimate means more power)\nOnly one coefficient to estimate"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#some-important-considerations-when-scoring-ordinal-variables",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#some-important-considerations-when-scoring-ordinal-variables",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Some important considerations when scoring ordinal variables",
    "text": "Some important considerations when scoring ordinal variables\n\nEven if a variable is inherently ordered, we need to check that linearity holds if categories are represented as numbers (more in next lessons)\n\nLinearity is an assumption of linear regression: that the relationship between X and Y is linear\n\n\n \n\nAssumes differences between adjacent groups are equal\n\nIncome levels are pre-set groups by Gapminder\nMight be hard to interpret “every 1-level increase in income level”\n\n\n \n\nIs the variable part of the main relationship that you are investigating? (even if linearity holds)\n\nIf yes, consider leaving as reference cell coding unless the interpretation makes sense\nIf no, and just needed as an adjustment in your model, then power benefit of scoring might be worth it!"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#check-that-linearity-holds-for-income-levels",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#check-that-linearity-holds-for-income-levels",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Check that linearity holds for income levels",
    "text": "Check that linearity holds for income levels\n\n\n\nUsing visual assessment, linearity holds for our income levels (more in next lessons)\nWe can use the ordinal encoding for income levels"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#poll-everywhere-question-4",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#poll-everywhere-question-4",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#ordinal-coding-scoring",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#ordinal-coding-scoring",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Ordinal coding / Scoring",
    "text": "Ordinal coding / Scoring\n\nMap each income level to a number\nUsually start at 1\n\n\n\n\nIncome Level\nScore\n\n\n\n\nLow income\n1\n\n\nLower middle income\n2\n\n\nUpper middle income\n3\n\n\nHigh income\n4\n\n\n\n\ngapm2 = gapm2 %&gt;%\n  mutate(income_num = as.numeric(income_levels))\nstr(gapm2$income_num)\n\n num [1:187] 1 3 3 4 2 4 3 2 4 4 ..."
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#run-the-model-with-the-scored-income",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#run-the-model-with-the-scored-income",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Run the model with the scored income",
    "text": "Run the model with the scored income\n\nmod_inc2 = lm(LifeExpectancyYrs ~ income_num, data = gapm2)\ntidy(mod_inc2) %&gt;% gt() %&gt;% tab_options(table.font.size = 37) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n54.01\n1.06\n51.03\n0.00\n    income_num\n6.25\n0.37\n16.91\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{\\textrm{LE}} &=  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot \\text{Income level} \\\\\n\\widehat{\\textrm{LE}} &=  54.01 + 6.25 \\cdot \\text{Income level}\n\\end{aligned}\\]\n\nKeep in mind: We cannot calculate the expected outcome outside of the scoring values\n\nFor example, we cannot find the mean life expectancy for an income level of 1.5"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#interpreting-the-model",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#interpreting-the-model",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Interpreting the model",
    "text": "Interpreting the model\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n54.01\n1.06\n51.03\n0.00\n51.92\n56.10\n    income_num\n6.25\n0.37\n16.91\n0.00\n5.52\n6.98\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{LE}} =  54.01 + 6.25 \\cdot \\text{Income level}\\]\n\nInterpreting the intercept: At an income level of 0, mean life expectancy is 54.01 (95% CI: 51.92, 56.10).\n\nNote: this does not make sense because there is no income level of 0!\n\nInterpreting the coefficient for income: For every 1-level increase in income level, mean life expectancy increases 6.25 years (95% CI: 5.52, 6.98)."
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#what-if-life-expectancy-vs.-income-level-looked-like-this",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#what-if-life-expectancy-vs.-income-level-looked-like-this",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "What if life expectancy vs. income level looked like this?",
    "text": "What if life expectancy vs. income level looked like this?\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo longer maintaining the linearity assumption\nNeed to use reference cell coding\n\n \n\nWe would fit the following model: \\[\\begin{aligned}\n\\textrm{LE} = & \\beta_0 + \\beta_1 \\cdot I(\\text{Lower middle income}) + \\\\ & \\beta_2 \\cdot I(\\text{Upper middle income}) + \\\\ & \\beta_3 \\cdot I(\\text{High income}) + \\epsilon\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#if-time",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#if-time",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "If time…",
    "text": "If time…\nLet’s walk through categorical variables that have multiple selections\n\nSo each group is not mutually exclusive\nWe could make an indicator for each category, but individuals could be a part of multiple categories\n\n \n\nAlso, thinking about income levels - can we combine two groups to make one??\n\n\n\nLesson 10: Categorical Covariates"
  },
  {
    "objectID": "lessons/10_Cat_covariates/01_Review_key_info.html#key-dates",
    "href": "lessons/10_Cat_covariates/01_Review_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html",
    "href": "lessons/03_SLR/03_SLR.html",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "",
    "text": "Identify the aims of your research and see how they align with the intended purpose of simple linear regression\nIdentify the simple linear regression model and define statistics language for key notation\nIllustrate how ordinary least squares (OLS) finds the best model parameter estimates\nSolve the optimal coefficient estimates for simple linear regression using OLS\nApply OLS in R for simple linear regression of real data\n\n\n\n\n\n\n\nRows: 188 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (2): life_expectancy_years_2011, female_literacy_rate_2011\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n \nAverage life expectancy vs. female literacy rate\n \n\nEach point on the plot is for a different country\n\n \n\n\\(X\\) = country’s adult female literacy rate\n\n \n\n\\(Y\\) = country’s average life expectancy (years)\n\n\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]\n\n\n\n\n\n\nggplot(gapm, aes(x = female_literacy_rate_2011,\n                 y = life_expectancy_years_2011)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 3, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nData files\n\nCleaned: lifeexp_femlit_2011.csv\nNeeds cleaning: lifeexp_femlit_water_2011.csv\n\nData were downloaded from Gapminder\n2011 is the most recent year with the most complete data\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same.\nAdult literacy rate is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life.\n\n\n\n\n\nLoad data\n\n\ngapm_original &lt;- read_csv(here::here(\"data\", \"lifeexp_femlit_water_2011.csv\"))\n\nRows: 194 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, water_2011_quart\ndbl (3): life_expectancy_years_2011, female_literacy_rate_2011, water_basic_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nGlimpse of the data\n\n\nglimpse(gapm_original)\n\nRows: 194\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andor…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9, 76.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.9, 99.5,…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 92.6, 100.0, 40.3, 97.0, 99.5, …\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q2\", \"Q4\", \"Q1\", \"Q3\", \"Q4\", \"…\n\n\n\nNote the missing values for our variables of interest\n\n\n\n\n\nGet a sense of the summary statistics\n\n\ngapm_original %&gt;% \n  select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  summary()\n\n life_expectancy_years_2011 female_literacy_rate_2011\n Min.   :47.50              Min.   :13.00            \n 1st Qu.:64.30              1st Qu.:70.97            \n Median :72.70              Median :91.60            \n Mean   :70.66              Mean   :81.65            \n 3rd Qu.:76.90              3rd Qu.:98.03            \n Max.   :82.90              Max.   :99.80            \n NA's   :7                  NA's   :114              \n\n\n\n\n\n\nRemove rows with missing data for life expectancy and female literacy rate\n\n\ngapm &lt;- gapm_original %&gt;% \n  drop_na(life_expectancy_years_2011, female_literacy_rate_2011)\n\nglimpse(gapm)\n\nRows: 80\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\", \"Antigu…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8, 96.7, 9…\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q1\", \"Q3\", \"Q4\", \"Q3\", \"Q3\", \"…\n\n\n\nNo missing values now for our variables of interest\n\n\n\n\n\nAnd no more missing values when we look only at our two variables of interest\n\n\ngapm %&gt;% select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 life_expec…    80    48  81.8   72.4  65.9  75.8  9.95  6.30  69.9  7.95 0.889\n2 female_lit…    80    13  99.8   91.6  71.0  98.0 27.0  11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;\n\n\n\n\nNote\n\n\n\nRemoving the rows with missing data was not needed to run the regression model.\nI did this step since later we will be calculating the standard deviations of the explanatory and response variables for just the values included in the regression model. It’ll be easier to do this if we remove the missing values now."
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#lets-start-with-an-example",
    "href": "lessons/03_SLR/03_SLR.html#lets-start-with-an-example",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Let’s start with an example",
    "text": "Let’s start with an example\n\n\n\n\n\n\n\n\n \nLife expectancy vs. female literacy rate\n \n\nEach point on the plot is for a different country\n\n \n\n\\(X\\) = country’s adult female literacy rate\n\n \n\n\\(Y\\) = country’s life expectancy (years)\n\n\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#reference-how-did-i-code-that",
    "href": "lessons/03_SLR/03_SLR.html#reference-how-did-i-code-that",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Reference: How did I code that?",
    "text": "Reference: How did I code that?\n\n\ngapm %&gt;%\n  ggplot(aes(x = FemaleLiteracyRate,\n             y = LifeExpectancyYrs)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 3, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#dataset-description",
    "href": "lessons/03_SLR/03_SLR.html#dataset-description",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Dataset description",
    "text": "Dataset description\n\nData file: Gapminder_vars_2011.xlsx\nData were downloaded from Gapminder\n2011 is the most recent year with the most complete data\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same.\nAdult literacy rate is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life.\n\n \n\nResearch question: Is there an association between life expectancy and female literacy rates?\n\n\nNational Literacy Trust in England has studied the link between these two variables\n\nPlease note that they clearly state that literacy is linked to life expectancy through many socioeconomic and health factors"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#get-to-know-the-data-12",
    "href": "lessons/03_SLR/03_SLR.html#get-to-know-the-data-12",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Get to know the data (1/2)",
    "text": "Get to know the data (1/2)\n\nLoad data\n\n\nlibrary(readxl)\ngapm1 &lt;- read_xlsx(here(\"data/Gapminder_vars_2011.xlsx\"), na = \"NA\")\n\n\nGlimpse of the data\n\n\nglimpse(gapm1)\n\nRows: 195\nColumns: 18\n$ country                            &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\"…\n$ CO2emissions                       &lt;dbl&gt; 0.412, 1.790, 3.290, 5.870, 1.250, …\n$ ElectricityUsePP                   &lt;dbl&gt; NA, 2210.0, 1120.0, NA, 207.0, NA, …\n$ FoodSupplykcPPD                    &lt;dbl&gt; 2110, 3130, 3220, NA, 2410, 2370, 3…\n$ IncomePP                           &lt;dbl&gt; 1660, 10200, 13000, 42000, 5910, 18…\n$ LifeExpectancyYrs                  &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9,…\n$ FemaleLiteracyRate                 &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.…\n$ population                         &lt;dbl&gt; 2.97e+07, 2.93e+06, 3.68e+07, 8.38e…\n$ WaterSourcePrct                    &lt;dbl&gt; 52.6, 88.1, 92.6, 100.0, 40.3, 97.0…\n$ geo                                &lt;chr&gt; \"afg\", \"alb\", \"dza\", \"and\", \"ago\", …\n$ four_regions                       &lt;chr&gt; \"asia\", \"europe\", \"africa\", \"europe…\n$ eight_regions                      &lt;chr&gt; \"asia_west\", \"europe_east\", \"africa…\n$ six_regions                        &lt;chr&gt; \"south_asia\", \"europe_central_asia\"…\n$ members_oecd_g77                   &lt;chr&gt; \"g77\", \"others\", \"g77\", \"others\", \"…\n$ Latitude                           &lt;dbl&gt; 33.00000, 41.00000, 28.00000, 42.50…\n$ Longitude                          &lt;dbl&gt; 66.00000, 20.00000, 3.00000, 1.5210…\n$ `World bank region`                &lt;chr&gt; \"South Asia\", \"Europe & Central Asi…\n$ `World bank, 4 income groups 2017` &lt;chr&gt; \"Low income\", \"Upper middle income\"…\n\n\n\nNote the missing values for our variables of interest"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#get-to-know-the-data-22",
    "href": "lessons/03_SLR/03_SLR.html#get-to-know-the-data-22",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Get to know the data (2/2)",
    "text": "Get to know the data (2/2)\n\nGet a sense of the summary statistics\n\n\ngapm1 %&gt;% \n  select(LifeExpectancyYrs, \n         FemaleLiteracyRate) %&gt;% \n  summary()\n\n LifeExpectancyYrs FemaleLiteracyRate\n Min.   :47.50     Min.   :13.00     \n 1st Qu.:64.30     1st Qu.:70.97     \n Median :72.70     Median :91.60     \n Mean   :70.66     Mean   :81.65     \n 3rd Qu.:76.90     3rd Qu.:98.03     \n Max.   :82.90     Max.   :99.80     \n NA's   :8         NA's   :115"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#remove-missing-values-12",
    "href": "lessons/03_SLR/03_SLR.html#remove-missing-values-12",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Remove missing values (1/2)",
    "text": "Remove missing values (1/2)\n\nRemove rows with missing data for life expectancy and female literacy rate\n\n\ngapm &lt;- gapm1 %&gt;% drop_na(LifeExpectancyYrs, FemaleLiteracyRate)\nglimpse(gapm)\n\nRows: 80\nColumns: 18\n$ country                            &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\",…\n$ CO2emissions                       &lt;dbl&gt; 0.4120, 1.7900, 1.2500, 5.3600, 4.6…\n$ ElectricityUsePP                   &lt;dbl&gt; NA, 2210.0, 207.0, NA, 2900.0, 1810…\n$ FoodSupplykcPPD                    &lt;dbl&gt; 2110, 3130, 2410, 2370, 3160, 2790,…\n$ IncomePP                           &lt;dbl&gt; 1660, 10200, 5910, 18600, 19600, 70…\n$ LifeExpectancyYrs                  &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8,…\n$ FemaleLiteracyRate                 &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5,…\n$ population                         &lt;dbl&gt; 2.97e+07, 2.93e+06, 2.42e+07, 9.57e…\n$ WaterSourcePrct                    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8,…\n$ geo                                &lt;chr&gt; \"afg\", \"alb\", \"ago\", \"atg\", \"arg\", …\n$ four_regions                       &lt;chr&gt; \"asia\", \"europe\", \"africa\", \"americ…\n$ eight_regions                      &lt;chr&gt; \"asia_west\", \"europe_east\", \"africa…\n$ six_regions                        &lt;chr&gt; \"south_asia\", \"europe_central_asia\"…\n$ members_oecd_g77                   &lt;chr&gt; \"g77\", \"others\", \"g77\", \"g77\", \"g77…\n$ Latitude                           &lt;dbl&gt; 33.00000, 41.00000, -12.50000, 17.0…\n$ Longitude                          &lt;dbl&gt; 66.00000, 20.00000, 18.50000, -61.8…\n$ `World bank region`                &lt;chr&gt; \"South Asia\", \"Europe & Central Asi…\n$ `World bank, 4 income groups 2017` &lt;chr&gt; \"Low income\", \"Upper middle income\"…\n\n\n\nNo missing values now for our variables of interest"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#remove-missing-values-22",
    "href": "lessons/03_SLR/03_SLR.html#remove-missing-values-22",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Remove missing values (2/2)",
    "text": "Remove missing values (2/2)\n\nAnd no more missing values when we look only at our two variables of interest\n\n\ngapm %&gt;%\n  select(LifeExpectancyYrs, \n          FemaleLiteracyRate) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 LifeExpect…    80    48  81.8   72.4  65.9  75.8  9.95  6.30  69.9  7.95 0.889\n2 FemaleLite…    80    13  99.8   91.6  71.0  98.0 27.0  11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;\n\n\n\n\nNote\n\n\n\nRemoving the rows with missing data was not needed to run the regression model.\nI did this step since later we will be calculating the standard deviations of the explanatory and response variables for just the values included in the regression model. It’ll be easier to do this if we remove the missing values now."
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#questions-we-can-ask-with-a-simple-linear-regression-model",
    "href": "lessons/03_SLR/03_SLR.html#questions-we-can-ask-with-a-simple-linear-regression-model",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Questions we can ask with a simple linear regression model",
    "text": "Questions we can ask with a simple linear regression model\n\n\n\n\n\n\n\n\n\nHow do we…\n\ncalculate slope & intercept?\ninterpret slope & intercept?\ndo inference for slope & intercept?\n\nCI, p-value\n\ndo prediction with regression line?\n\nCI for prediction?\n\n\nDoes the model fit the data well?\n\nShould we be using a line to model the data?\n\nShould we add additional variables to the model?\n\nmultiple/multivariable regression\n\n\n\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#association-vs.-prediction",
    "href": "lessons/03_SLR/03_SLR.html#association-vs.-prediction",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Association vs. prediction",
    "text": "Association vs. prediction\n\n\n\n\nAssociation\n\n\n\nWhat is the association between countries’ life expectancy and female literacy rate?\nUse the slope of the line or correlation coefficient\n\n\n\n\n\n\nPrediction\n\n\n\nWhat is the expected life expectancy for a country with a specified female literacy rate?    \n\n\n\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#three-types-of-study-design-there-are-more",
    "href": "lessons/03_SLR/03_SLR.html#three-types-of-study-design-there-are-more",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Three types of study design (there are more)",
    "text": "Three types of study design (there are more)\n\n\n\n\nExperiment\n\n\n\nObservational units are randomly assigned to important predictor levels\n\nRandom assignment controls for confounding variables (age, gender, race, etc.)\n“gold standard” for determining causality\nObservational unit is often at the participant-level\n\n\n\n\n\n\n\nQuasi-experiment\n\n\n\nParticipants are assigned to intervention levels without randomization\nNot common study design\n\n\n\n\n\n\nObservational\n\n\n\nNo randomization or assignment of intervention conditions\nIn general cannot infer causality\n\nHowever, there are casual inference methods…"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#lets-revisit-the-regression-analysis-process",
    "href": "lessons/03_SLR/03_SLR.html#lets-revisit-the-regression-analysis-process",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Let’s revisit the regression analysis process",
    "text": "Let’s revisit the regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-2",
    "href": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-2",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#simple-linear-regression-model",
    "href": "lessons/03_SLR/03_SLR.html#simple-linear-regression-model",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nThe (population) regression model is denoted by:\n \n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n \n\n\nObservable sample data\n\n\\(Y\\) is our dependent variable\n\nAka outcome or response variable\n\n\\(X\\) is our independent variable\n\nAka predictor, regressor, exposure variable\n\n\n\nUnobservable population parameters\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable with a…\n\nNormal distribution with mean 0 and constant variance \\(\\sigma^2\\)\ni.e. \\(\\epsilon \\sim N(0, \\sigma^2)\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#simple-linear-regression-model-another-way-to-view-components",
    "href": "lessons/03_SLR/03_SLR.html#simple-linear-regression-model-another-way-to-view-components",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Simple Linear Regression Model (another way to view components)",
    "text": "Simple Linear Regression Model (another way to view components)\nThe (population) regression model is denoted by:\n \n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n \nComponents\n\n\n\n\\(Y\\)\nresponse, outcome, dependent variable\n\n\n\\(\\beta_0\\)\nintercept\n\n\n\\(\\beta_1\\)\nslope\n\n\n\\(X\\)\npredictor, covariate, independent variable\n\n\n\\(\\epsilon\\)\nresiduals, error term"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#if-the-population-parameters-are-unobservable-how-did-we-get-the-line-for-life-expectancy",
    "href": "lessons/03_SLR/03_SLR.html#if-the-population-parameters-are-unobservable-how-did-we-get-the-line-for-life-expectancy",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "If the population parameters are unobservable, how did we get the line for life expectancy?",
    "text": "If the population parameters are unobservable, how did we get the line for life expectancy?\n\n\n \n\nNote: the population model is the true, underlying model that we are trying to estimate using our sample data\n\nOur goal in simple linear regression is to estimate \\(\\beta_0\\) and \\(\\beta_1\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-3",
    "href": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-3",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#okay-so-how-do-we-estimate-the-regression-line",
    "href": "lessons/03_SLR/03_SLR.html#okay-so-how-do-we-estimate-the-regression-line",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Okay, so how do we estimate the regression line?",
    "text": "Okay, so how do we estimate the regression line?\n \nAt this point, we are going to move over to an R shiny app that I made.\n \nLet’s see if we can eyeball the best-fit line!"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#regression-line-best-fit-line",
    "href": "lessons/03_SLR/03_SLR.html#regression-line-best-fit-line",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Regression line = best-fit line",
    "text": "Regression line = best-fit line\n\n\n\\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X \\]\n\n\\(\\widehat{Y}\\) is the predicted outcome for a specific value of \\(X\\)\n\\(\\widehat{\\beta}_0\\) is the intercept of the best-fit line\n\\(\\widehat{\\beta}_1\\) is the slope of the best-fit line, i.e., the increase in \\(\\widehat{Y}\\) for every increase of one (unit increase) in \\(X\\)\n\nslope = rise over run"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#simple-linear-regression-model-1",
    "href": "lessons/03_SLR/03_SLR.html#simple-linear-regression-model-1",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\n\n\nPopulation regression model\n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n \nComponents\n\n\n\n\\(Y\\)\nresponse, outcome, dependent variable\n\n\n\\(\\beta_0\\)\nintercept\n\n\n\\(\\beta_1\\)\nslope\n\n\n\\(X\\)\npredictor, covariate, independent variable\n\n\n\\(\\epsilon\\)\nresiduals, error term\n\n\n\n\nEstimated regression line\n\n\\[\\widehat{Y} =  \\widehat{\\beta}_0 + \\widehat{\\beta}_1X\\]\n\n \nComponents\n\n\n\n\n\n\n\n\\(\\widehat{Y}\\)\nestimated expected response given predictor \\(X\\)\n\n\n\\(\\widehat{\\beta}_0\\)\nestimated intercept\n\n\n\\(\\widehat{\\beta}_1\\)\nestimated slope\n\n\n\\(X\\)\npredictor, covariate, independent variable"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#we-get-it-nicky-how-do-we-estimate-the-regression-line",
    "href": "lessons/03_SLR/03_SLR.html#we-get-it-nicky-how-do-we-estimate-the-regression-line",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "We get it, Nicky! How do we estimate the regression line?",
    "text": "We get it, Nicky! How do we estimate the regression line?\nFirst let’s take a break!!"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#it-all-starts-with-a-residual",
    "href": "lessons/03_SLR/03_SLR.html#it-all-starts-with-a-residual",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "It all starts with a residual…",
    "text": "It all starts with a residual…\n\n\n\nRecall, one characteristic of our population model was that the residuals, \\(\\epsilon\\), were Normally distributed: \\(\\epsilon \\sim N(0, \\sigma^2)\\)\nIn our population regression model, we had: \\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\nWe can also take the average (expected) value of the population model\nWe take the expected value of both sides and get:\n\n\\[\\begin{aligned}\n        E[Y] & = E[\\beta_0 + \\beta_1X + \\epsilon] \\\\\n        E[Y] & = E[\\beta_0] + E[\\beta_1X] + E[\\epsilon] \\\\\n        E[Y] & = \\beta_0 + \\beta_1X + E[\\epsilon] \\\\\n        E[Y|X] & = \\beta_0 + \\beta_1X \\\\\n\\end{aligned}\\]\n\nWe call \\(E[Y|X]\\) the expected value (or average) of \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#so-now-we-have-two-representations-of-our-population-model",
    "href": "lessons/03_SLR/03_SLR.html#so-now-we-have-two-representations-of-our-population-model",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "So now we have two representations of our population model",
    "text": "So now we have two representations of our population model\n\n\n\n\nWith observed \\(Y\\) values and residuals:\n\n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n\n\n\n\nWith the population expected value of \\(Y\\) given \\(X\\):\n\n\n\\[E[Y|X] = \\beta_0 + \\beta_1X\\]\n\n\n\n\nUsing the two forms of the model, we can figure out a formula for our residuals:\n\\[\\begin{aligned}\nY & = (\\beta_0 + \\beta_1X) + \\epsilon \\\\\nY & = E[Y|X] + \\epsilon \\\\\nY - E[Y|X] & = \\epsilon \\\\\n\\epsilon & = Y - E[Y|X]\n\\end{aligned}\\]\nAnd so we have our true, population model, residuals!\n\nThis is an important fact! For the population model, the residuals: \\(\\epsilon = Y - E[Y|X]\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#back-to-our-estimated-model",
    "href": "lessons/03_SLR/03_SLR.html#back-to-our-estimated-model",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Back to our estimated model",
    "text": "Back to our estimated model\nWe have the same two representations of our estimated/fitted model:\n\n\n\n\nWith observed values:\n\n\n\\[Y =  \\widehat{\\beta}_0 + \\widehat{\\beta}_1X + \\widehat{\\epsilon}\\]\n\n\n\n\n\nWith the estimated expected value of \\(Y\\) given \\(X\\):\n\n\n\\[\\begin{aligned}\n\\widehat{E}[Y|X] & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\widehat{E[Y|X]} & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\widehat{Y} & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\end{aligned}\\]\n\n\n\n\nUsing the two forms of the model, we can figure out a formula for our estimated residuals:\n\\[\\begin{aligned}\nY & = (\\widehat{\\beta}_0 + \\widehat{\\beta}_1X) + \\widehat\\epsilon \\\\\nY & = \\widehat{Y} + \\widehat\\epsilon \\\\\n\\widehat\\epsilon & = Y - \\widehat{Y}\n\\end{aligned}\\]\n\nThis is an important fact! For the estimated/fitted model, the residuals: \\(\\widehat\\epsilon = Y - \\widehat{Y}\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#individual-i-residuals-in-the-estimatedfitted-model",
    "href": "lessons/03_SLR/03_SLR.html#individual-i-residuals-in-the-estimatedfitted-model",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Individual \\(i\\) residuals in the estimated/fitted model",
    "text": "Individual \\(i\\) residuals in the estimated/fitted model\n\n\n\nObserved values for each country \\(i\\): \\(Y_i\\)\n\nValue in the dataset for country \\(i\\)\n\nFitted value for each country \\(i\\): \\(\\widehat{Y}_i\\)\n\nValue that falls on the best-fit line for a specific \\(X_i\\)\nIf two individuals have the same \\(X_i\\), then they have the same \\(\\widehat{Y}_i\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#individual-i-residuals-in-the-estimatedfitted-model-1",
    "href": "lessons/03_SLR/03_SLR.html#individual-i-residuals-in-the-estimatedfitted-model-1",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Individual \\(i\\) residuals in the estimated/fitted model",
    "text": "Individual \\(i\\) residuals in the estimated/fitted model\n\n\n\nObserved values for each individual \\(i\\): \\(Y_i\\)\n\nValue in the dataset for individual \\(i\\)\n\nFitted value for each individual \\(i\\): \\(\\widehat{Y}_i\\)\n\nValue that falls on the best-fit line for a specific \\(X_i\\)\nIf two individuals have the same \\(X_i\\), then they have the same \\(\\widehat{Y}_i\\)\n\n\n\n\nResidual for each individual: \\(\\widehat\\epsilon_i = Y_i - \\widehat{Y}_i\\)\n\nDifference between the observed and fitted value"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-4",
    "href": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-4",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#so-what-do-we-do-with-the-residuals",
    "href": "lessons/03_SLR/03_SLR.html#so-what-do-we-do-with-the-residuals",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "So what do we do with the residuals?",
    "text": "So what do we do with the residuals?\n\nWe want to minimize the residuals\n\nAka minimize the difference between the observed \\(Y\\) value and the estimated expected response given the predictor ( \\(\\widehat{E}[Y|X]\\) )\n\nWe can use ordinary least squares (OLS) to do this in linear regression!\nIdea behind this: reduce the total error between the fitted line and the observed point (error between is called residuals)\n\nVague use of total error: more precisely, we want to reduce the sum of squared errors\nThink back to my R Shiny app!\nWe need to mathematically define this!\n\n\n \n \n\nNote: there are other ways to estimate the best-fit line!!\n\nExample: Maximum likelihood estimation"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#setting-up-for-ordinary-least-squares",
    "href": "lessons/03_SLR/03_SLR.html#setting-up-for-ordinary-least-squares",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Setting up for ordinary least squares",
    "text": "Setting up for ordinary least squares\n\n\n\nSum of Squared Errors (SSE)\n\n\\[ \\begin{aligned}\nSSE & = \\displaystyle\\sum^n_{i=1} \\widehat\\epsilon_i^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{Y}_i)^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - (\\widehat{\\beta}_0+\\widehat{\\beta}_1X_i))^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\n\\(\\widehat\\epsilon_i = Y_i - \\widehat{Y}_i\\)\n\\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1X_i\\)\n\n\n\n\n\n\nThen we want to find the estimated coefficient values that minimize the SSE!"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#steps-to-estimate-coefficients-using-ols",
    "href": "lessons/03_SLR/03_SLR.html#steps-to-estimate-coefficients-using-ols",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Steps to estimate coefficients using OLS",
    "text": "Steps to estimate coefficients using OLS\n\nSet up SSE (previous slide)\nMinimize SSE with respect to coefficient estimates\n\nNeed to solve a system of equations\n\nCompute derivative of SSE wrt \\(\\widehat\\beta_0\\)\nSet derivative of SSE wrt \\(\\widehat\\beta_0 = 0\\)\nCompute derivative of SSE wrt \\(\\widehat\\beta_1\\)\nSet derivative of SSE wrt \\(\\widehat\\beta_1 = 0\\)\nSubstitute \\(\\widehat\\beta_1\\) back into \\(\\widehat\\beta_0\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#minimize-sse-with-respect-to-coefficients",
    "href": "lessons/03_SLR/03_SLR.html#minimize-sse-with-respect-to-coefficients",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "2. Minimize SSE with respect to coefficients",
    "text": "2. Minimize SSE with respect to coefficients\n\nWant to minimize with respect to (wrt) the potential coefficient estimates ( \\(\\widehat\\beta_0\\) and \\(\\widehat\\beta_1\\))\nTake derivative of SSE wrt \\(\\widehat\\beta_0\\) and \\(\\widehat\\beta_1\\) and set equal to zero to find minimum SSE\n\n\\[\n\\dfrac{\\partial SSE}{\\partial \\widehat\\beta_0} = 0 \\text{ and } \\dfrac{\\partial SSE}{\\partial \\widehat\\beta_1} = 0\n\\]\n\nSolve the above system of equations in steps 3-6"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#compute-derivative-of-sse-wrt-widehatbeta_0",
    "href": "lessons/03_SLR/03_SLR.html#compute-derivative-of-sse-wrt-widehatbeta_0",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "3. Compute derivative of SSE wrt \\(\\widehat\\beta_0\\)",
    "text": "3. Compute derivative of SSE wrt \\(\\widehat\\beta_0\\)\n\n\n\\[\nSSE = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n\\]\n\\[\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0}& =\\frac{\\partial\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)^2}{\\partial{\\widehat{\\beta}}_0}=\n\\sum_{i=1}^{n}\\frac{{\\partial\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)}^2}{\\partial{\\widehat{\\beta}}_0} \\\\\n& =\\sum_{i=1}^{n}{2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)\\left(-1\\right)}=\\sum_{i=1}^{n}{-2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)} \\\\\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0} & = -2\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\nDerivative rule: derivative of sum is sum of derivative\nDerivative rule: chain rule"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#set-derivative-of-sse-wrt-widehatbeta_0-0",
    "href": "lessons/03_SLR/03_SLR.html#set-derivative-of-sse-wrt-widehatbeta_0-0",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "4. Set derivative of SSE wrt \\(\\widehat\\beta_0 = 0\\)",
    "text": "4. Set derivative of SSE wrt \\(\\widehat\\beta_0 = 0\\)\n\n\n\\[\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0} & =0 \\\\ -2\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right) & =0 \\\\\n\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right) & =0 \\\\ \\sum_{i=1}^{n}Y_i-n{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\sum_{i=1}^{n}X_i & =0 \\\\\n\\frac{1}{n}\\sum_{i=1}^{n}Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\frac{1}{n}\\sum_{i=1}^{n}X_i & =0 \\\\\n\\overline{Y}-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\overline{X} & =0 \\\\\n{\\widehat{\\beta}}_0 & =\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\n\\(\\overline{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_i\\)\n\\(\\overline{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_i\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#compute-derivative-of-sse-wrt-widehatbeta_1",
    "href": "lessons/03_SLR/03_SLR.html#compute-derivative-of-sse-wrt-widehatbeta_1",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "5. Compute derivative of SSE wrt \\(\\widehat\\beta_1\\)",
    "text": "5. Compute derivative of SSE wrt \\(\\widehat\\beta_1\\)\n\n\n\\[\nSSE = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n\\]\n\\[\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_1}& =\\frac{\\partial\\sum_{i=1}^{n}{(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i)}^2}{\\partial{\\widehat{\\beta}}_1}=\\sum_{i=1}^{n}\\frac{{\\partial(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i)}^2}{\\partial{\\widehat{\\beta}}_1} \\\\\n&=\\sum_{i=1}^{n}{2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)(-X_i)}=\\sum_{i=1}^{n}{-2X_i\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)} \\\\ &=-2\\sum_{i=1}^{n}{X_i\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)}\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\nDerivative rule: derivative of sum is sum of derivative\nDerivative rule: chain rule"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#set-derivative-of-sse-wrt-widehatbeta_1-0",
    "href": "lessons/03_SLR/03_SLR.html#set-derivative-of-sse-wrt-widehatbeta_1-0",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "6. Set derivative of SSE wrt \\(\\widehat\\beta_1 = 0\\)",
    "text": "6. Set derivative of SSE wrt \\(\\widehat\\beta_1 = 0\\)\n\n\n\\[\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_1} & =0 \\\\ \\sum_{i=1}^{n}\\left({X_iY}_i-{\\widehat{\\beta}}_0X_i-{\\widehat{\\beta}}_1{X_i}^2\\right)&=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i{\\widehat{\\beta}}_0}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1}&=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i\\left(\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}\\right)}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1} &=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i\\overline{Y}}+\\sum_{i=1}^{n}{{\\widehat{\\beta}}_1X_i\\overline{X}}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1} &=0 \\\\\n\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}+\\sum_{i=1}^{n}{({\\widehat{\\beta}}_1X_i\\overline{X}}-{X_i}^2{\\widehat{\\beta}}_1) &=0 \\\\\n\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}+{\\widehat{\\beta}}_1\\sum_{i=1}^{n}{X_i(\\overline{X}}-X_i) &=0 \\\\\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\n\\({\\widehat{\\beta}}_0=\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}\\)\n\\(\\overline{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_i\\)\n\\(\\overline{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_i\\)\n\n\n\n         \n \n\n\\[{\\widehat{\\beta}}_1 =\\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})}\\]"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#substitute-widehatbeta_1-back-into-widehatbeta_0",
    "href": "lessons/03_SLR/03_SLR.html#substitute-widehatbeta_1-back-into-widehatbeta_0",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "7. Substitute \\(\\widehat\\beta_1\\) back into \\(\\widehat\\beta_0\\)",
    "text": "7. Substitute \\(\\widehat\\beta_1\\) back into \\(\\widehat\\beta_0\\)\nFinal coefficient estimates for SLR\n\n\n\n\nCoefficient estimate for \\(\\widehat\\beta_1\\)\n\n\n\\[{\\widehat{\\beta}}_1 =\\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})}\\]\n\n\n\n\n\nCoefficient estimate for \\(\\widehat\\beta_0\\)\n\n\n\\[\\begin{aligned}\n{\\widehat{\\beta}}_0 & =\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X} \\\\\n{\\widehat{\\beta}}_0 & = \\overline{Y} - \\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})} \\overline{X} \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-5",
    "href": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-5",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#do-i-need-to-do-all-that-work-every-time",
    "href": "lessons/03_SLR/03_SLR.html#do-i-need-to-do-all-that-work-every-time",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Do I need to do all that work every time??",
    "text": "Do I need to do all that work every time??"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#regression-in-r-lm",
    "href": "lessons/03_SLR/03_SLR.html#regression-in-r-lm",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Regression in R: lm()",
    "text": "Regression in R: lm()\n\nLet’s discuss the syntax of this function\n\n\nmodel1 &lt;- gapm %&gt;% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\n\n \n \nIn the general form:\n\nlm( Y ~ X, data = dataset_name)\ndataset_name %&gt;% lm( formula = Y ~ X )"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#regression-in-r-lm-summary",
    "href": "lessons/03_SLR/03_SLR.html#regression-in-r-lm-summary",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Regression in R: lm() + summary()",
    "text": "Regression in R: lm() + summary()\n\nmodel1 &lt;- gapm %&gt;% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\nsummary(model1)\n\n\nCall:\nlm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        50.92790    2.66041  19.143  &lt; 2e-16 ***\nFemaleLiteracyRate  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#regression-in-r-lm-tidy",
    "href": "lessons/03_SLR/03_SLR.html#regression-in-r-lm-tidy",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Regression in R: lm() + tidy()",
    "text": "Regression in R: lm() + tidy()\n \n\ntidy(model1) %&gt;% \n  gt() %&gt;% \n  tab_options(table.font.size = 45)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    FemaleLiteracyRate\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n \n\nRegression equation for our model (which we saw a looong time ago):\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#how-do-we-interpret-the-coefficients",
    "href": "lessons/03_SLR/03_SLR.html#how-do-we-interpret-the-coefficients",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "How do we interpret the coefficients?",
    "text": "How do we interpret the coefficients?\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]\n\n\nIntercept (\\(\\hat{\\beta}_0\\))\n\nThe expected outcome for the \\(Y\\)-variable when the \\(X\\)-variable (if continuous) is 0\nExample: The expected/average life expectancy is 50.9 years for a country with 0% female literacy.\n\nSlope (\\(\\hat{\\beta}_1\\))\n\nFor every increase of 1 unit in the \\(X\\)-variable (if continuous), there is an expected increase of \\(\\widehat\\beta_1\\) units in the \\(Y\\)-variable.\nWe only say that there is an expected increase and not necessarily a causal increase.\nExample: For every 1 percent increase in the female literacy rate, life expectancy increases, on average, 0.232 years.\n\nCan also say “…average life expectancy increases 0.232…”"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#next-time",
    "href": "lessons/03_SLR/03_SLR.html#next-time",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Next time",
    "text": "Next time\n\nMore on interpreting the estimate coefficients\nInference of our estimated coefficients\nInference of estimated expected \\(Y\\) given \\(X\\)\nPrediction\nHypothesis testing!\n\n\n\nLesson 3: SLR 1"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome_key_info.html#key-dates",
    "href": "lessons/00_Welcome/00_Welcome_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates\n\nHomework 0 due this Thursday at 11pm\n\nLet me know if you have issues with this on Sakai"
  },
  {
    "objectID": "lessons/01_Review/01_Review_key_info.html#key-dates",
    "href": "lessons/01_Review/01_Review_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "",
    "text": "Apply tools from SLR (Lesson 6: SLR Diagnostics) in MLR to evaluate LINE assumptions, including residual plots and QQ-plots\nApply tools involving standardized residuals, leverage, and Cook’s distance from SLR (Lesson 7: SLR Diagnostics 2) in MLR to flag potentially influential points\nUse Variance Inflation Factor (VIF) and it’s general form to detect and correct multicollinearity\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)\n\n\n\n\n\n\n\n\n\n\n\nOur final model contains\n\nFemale Literacy Rate FLR\nCO2 Emissions in quartiles CO2_q\nIncome levels in groups assigned by Gapminder income_levels1\nWorld regions four_regions\nMembership of global and economic groups members_oecd_g77\nFood Supply FoodSupplykcPPD\nClean Water Supply WaterSupplePct\n\n\n\n\n\nDisplay regression table for final model\ntidy(final_model) %&gt;% gt() %&gt;% tab_options(table.font.size = 32) %&gt;%  \n  fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n39.877\n4.889\n8.157\n0.000\n    FemaleLiteracyRate\n−0.073\n0.047\n−1.555\n0.125\n    CO2_q(0.806,2.54]\n1.099\n1.914\n0.574\n0.568\n    CO2_q(2.54,4.66]\n−0.292\n2.419\n−0.121\n0.904\n    CO2_q(4.66,35.2]\n−0.595\n2.524\n−0.236\n0.814\n    income_levels1Lower middle income\n5.441\n2.343\n2.322\n0.024\n    income_levels1Upper middle income\n6.111\n2.954\n2.069\n0.043\n    income_levels1High income\n7.959\n3.277\n2.429\n0.018\n    four_regionsAmericas\n9.003\n2.050\n4.391\n0.000\n    four_regionsAsia\n5.260\n1.637\n3.213\n0.002\n    four_regionsEurope\n6.855\n2.871\n2.387\n0.020\n    WaterSourcePrct\n0.166\n0.066\n2.496\n0.015\n    FoodSupplykcPPD\n0.004\n0.002\n1.825\n0.073\n    members_oecd_g77oecd\n1.119\n2.674\n0.418\n0.677\n    members_oecd_g77others\n1.047\n2.511\n0.417\n0.678\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nPart of the reason why we discussed model diagnostics in SLR was so that we could have accompanying visuals to help us understand\n\n \n\nWith 7 variables in out final model, it is hard to visualize outliers and influential points\n\n \n\nI highly encourage you revisit Lesson 6 and 7 (SLR Diagnostics) to help understand these notes\n\n\n\n\n\nRun final_model through augment() (final_model is input)\n\nSo we assigned final_model as the output of the lm() function\n\nWill give us values about each observation in the context of the fitted regression model\n\ncook’s distance (.cooksd), fitted value (.fitted, \\(\\widehat{Y}_i\\)), leverage (.hat), residual (.resid), standardized residuals (.std.resid)\n\n\n\naug = augment(final_model)\nhead(aug) %&gt;% relocate(.fitted, .resid, .std.resid, .hat, .cooksd, .after = LifeExpectancyYrs)\n\n# A tibble: 6 × 14\n  LifeExpectancyYrs .fitted .resid .std.resid  .hat  .cooksd FemaleLiteracyRate\n              &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;              &lt;dbl&gt;\n1              56.7    61.5 -4.78      -1.43  0.327 0.0663                 13  \n2              76.7    75.3  1.38       0.387 0.227 0.00293                95.7\n3              60.9    58.6  2.30       0.684 0.320 0.0147                 58.6\n4              76.9    74.7  2.21       0.620 0.238 0.00799                99.4\n5              76      76.9 -0.879     -0.233 0.145 0.000614               97.9\n6              73.8    74.6 -0.796     -0.214 0.168 0.000618               99.5\n# ℹ 7 more variables: CO2_q &lt;fct&gt;, income_levels1 &lt;fct&gt;, four_regions &lt;fct&gt;,\n#   WaterSourcePrct &lt;dbl&gt;, FoodSupplykcPPD &lt;dbl&gt;, members_oecd_g77 &lt;chr&gt;,\n#   .sigma &lt;dbl&gt;\n\n\nRDocumentation on the augment() function."
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#regression-analysis-process",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#regression-analysis-process",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Regression analysis process",
    "text": "Regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#lets-remind-ourselves-of-the-final-model",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#lets-remind-ourselves-of-the-final-model",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Let’s remind ourselves of the final model",
    "text": "Let’s remind ourselves of the final model\n\n\n\nOur final model contains\n\nFemale Literacy Rate FLR\nCO2 Emissions in quartiles CO2_q\nIncome levels in groups assigned by Gapminder income_levels1\nWorld regions four_regions\nMembership of global and economic groups members_oecd_g77\nFood Supply FoodSupplykcPPD\nClean Water Supply WaterSupplePct\n\n\n\n\n\nDisplay regression table for final model\ntidy(final_model) %&gt;% gt() %&gt;% tab_options(table.font.size = 32) %&gt;%  \n  fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n39.877\n4.889\n8.157\n0.000\n    FemaleLiteracyRate\n−0.073\n0.047\n−1.555\n0.125\n    CO2_q(0.806,2.54]\n1.099\n1.914\n0.574\n0.568\n    CO2_q(2.54,4.66]\n−0.292\n2.419\n−0.121\n0.904\n    CO2_q(4.66,35.2]\n−0.595\n2.524\n−0.236\n0.814\n    income_levels1Lower middle income\n5.441\n2.343\n2.322\n0.024\n    income_levels1Upper middle income\n6.111\n2.954\n2.069\n0.043\n    income_levels1High income\n7.959\n3.277\n2.429\n0.018\n    four_regionsAmericas\n9.003\n2.050\n4.391\n0.000\n    four_regionsAsia\n5.260\n1.637\n3.213\n0.002\n    four_regionsEurope\n6.855\n2.871\n2.387\n0.020\n    WaterSourcePrct\n0.166\n0.066\n2.496\n0.015\n    FoodSupplykcPPD\n0.004\n0.002\n1.825\n0.073\n    members_oecd_g77oecd\n1.119\n2.674\n0.418\n0.677\n    members_oecd_g77others\n1.047\n2.511\n0.417\n0.678"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#its-a-lot-to-visualize",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#its-a-lot-to-visualize",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "It’s a lot to visualize",
    "text": "It’s a lot to visualize\n\nPart of the reason why we discussed model diagnostics in SLR was so that we could have accompanying visuals to help us understand\n\n \n\nWith 7 variables in out final model, it is hard to visualize outliers and influential points\n\n \n\nI highly encourage you revisit Lesson 6 and 7 (SLR Diagnostics) to help understand these notes"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#remember-our-friend-augment",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#remember-our-friend-augment",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Remember our friend augment()?",
    "text": "Remember our friend augment()?\n\nRun final_model through augment() (final_model is input)\n\nSo we assigned final_model as the output of the lm() function\n\nWill give us values about each observation in the context of the fitted regression model\n\ncook’s distance (.cooksd), fitted value (.fitted, \\(\\widehat{Y}_i\\)), leverage (.hat), residual (.resid), standardized residuals (.std.resid)\n\n\n\naug = augment(final_model)\nhead(aug) %&gt;% relocate(.fitted, .resid, .std.resid, .hat, .cooksd, .after = LifeExpectancyYrs)\n\n# A tibble: 6 × 14\n  LifeExpectancyYrs .fitted .resid .std.resid  .hat  .cooksd FemaleLiteracyRate\n              &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;              &lt;dbl&gt;\n1              56.7    61.5 -4.78      -1.43  0.327 0.0663                 13  \n2              76.7    75.3  1.38       0.387 0.227 0.00293                95.7\n3              60.9    58.6  2.30       0.684 0.320 0.0147                 58.6\n4              76.9    74.7  2.21       0.620 0.238 0.00799                99.4\n5              76      76.9 -0.879     -0.233 0.145 0.000614               97.9\n6              73.8    74.6 -0.796     -0.214 0.168 0.000618               99.5\n# ℹ 7 more variables: CO2_q &lt;fct&gt;, income_levels1 &lt;fct&gt;, four_regions &lt;fct&gt;,\n#   WaterSourcePrct &lt;dbl&gt;, FoodSupplykcPPD &lt;dbl&gt;, members_oecd_g77 &lt;chr&gt;,\n#   .sigma &lt;dbl&gt;\n\n\nRDocumentation on the augment() function."
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#summary-of-the-assumptions-and-their-diagnostic-tool",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#summary-of-the-assumptions-and-their-diagnostic-tool",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Summary of the assumptions and their diagnostic tool",
    "text": "Summary of the assumptions and their diagnostic tool\n\n\n\n\n\n\n\n\nAssumption\nWhat needs to hold?\nDiagnostic tool\n\n\n\n\nLinearity\n\\(\\text{}\\)\n\nRelationship between each \\(X\\) and \\(Y\\) is linear\n\n\nScatterplot of \\(Y\\) vs. \\(X\\)\n\n\\(\\text{}\\)\n\n\nIndependence\n\\(\\text{}\\)\n\nObservations are independent from each other\n\n\nStudy design\n\n\\(\\text{}\\)\n\n\nNormality\n\nResiduals (and thus \\(Y|X_1, X_2, ..., X_p\\))\nare normally distributed\n\n\nQQ plot of residuals\nDistribution of residuals\n\n\n\nEquality of variance\n\nVariance of residuals (and thus \\(Y|X_1, X_2, ..., X_p\\))\nis same across fitted values (homoscedasticity)\n\n\nResidual plot\n\n\\(\\text{}\\)"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#autoplot-to-examine-equality-of-variance-and-normality",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#autoplot-to-examine-equality-of-variance-and-normality",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "autoplot() to examine equality of variance and Normality",
    "text": "autoplot() to examine equality of variance and Normality\n\n\n\nlibrary(ggfortify)\nautoplot(final_model) + theme(text=element_text(size=20))"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#autoplot-to-examine-equality-of-variance-and-normality-1",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#autoplot-to-examine-equality-of-variance-and-normality-1",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "autoplot() to examine equality of variance and Normality",
    "text": "autoplot() to examine equality of variance and Normality\n\n\n\nlibrary(ggfortify)\nautoplot(final_model) + theme(text=element_text(size=20))\n\n\n\n\n\n\n\n\n\nLooks like 3 obs are flagged:\n\n17: Cote d’Ivoire\n59: South Africa\n61: Kingdom of Eswatini (formerly Swaziland in 2011)\n\nWithout them, QQ-plot and residual plot look good\n\nPoints on QQ-plot are close to identity line\nResiduals have pretty consistent spread across fitted values\n\nBut don’t take them out!!!\n\nInstead, discuss what may be missing in our regression model that is not capturing the characteristics of these countries"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#poll-everywhere-question-1",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#poll-everywhere-question-1",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#identifying-outliers",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#identifying-outliers",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Identifying outliers",
    "text": "Identifying outliers\n\n\n\n\nInternally standardized residual\n\n\n\\[\nr_i = \\frac{\\widehat\\epsilon_i}{\\sqrt{\\widehat\\sigma^2(1-h_{ii})}}\n\\]\n\n\n\n\nWe flag an observation if the standardized residual is “large”\n\nDifferent sources will define “large” differently\nPennState site uses \\(|r_i| &gt; 3\\)\nautoplot() shows the 3 observations with the highest standardized residuals\nOther sources use \\(|r_i| &gt; 2\\), which is a little more conservative\n\n\n\n\n\n\n \n\nggplot(data = aug) + \n  geom_histogram(aes(x = .std.resid))"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#countries-that-are-outliers-r_i-2",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#countries-that-are-outliers-r_i-2",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Countries that are outliers (\\(|r_i| > 2\\))",
    "text": "Countries that are outliers (\\(|r_i| &gt; 2\\))\n\nWe can identify the countries that are outliers\n\n\naug %&gt;% relocate(.std.resid, .after = country) %&gt;%\n  filter(abs(.std.resid) &gt; 2) %&gt;% arrange(desc(abs(.std.resid)))\n\n# A tibble: 6 × 15\n  country   .std.resid LifeExpectancyYrs FemaleLiteracyRate CO2_q income_levels1\n  &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt;              &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;         \n1 Swaziland      -2.96              48.9               87.3 (0.8… Lower middle …\n2 South Af…      -2.45              55.8               92.2 (4.6… Upper middle …\n3 Cote d'I…      -2.28              56.9               47.6 [0.0… Lower middle …\n4 Cape Ver…       2.07              72.7               80.3 (0.8… Lower middle …\n5 Sudan           2.05              66.5               63.2 [0.0… Lower middle …\n6 Vanuatu        -2.04              63.2               81.5 [0.0… Lower middle …\n# ℹ 9 more variables: four_regions &lt;fct&gt;, WaterSourcePrct &lt;dbl&gt;,\n#   FoodSupplykcPPD &lt;dbl&gt;, members_oecd_g77 &lt;chr&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#   .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#leverage-h_i",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#leverage-h_i",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Leverage \\(h_i\\)",
    "text": "Leverage \\(h_i\\)\n\nValues of leverage are: \\(0 \\leq h_i \\leq 1\\)\nWe flag an observation if the leverage is “high”\n\nOnly good for SLR: Some textbooks use \\(h_i &gt; 4/n\\) where \\(n\\) = sample size\nOnly good for SLR: Some people suggest \\(h_i &gt; 6/n\\)\nWorks for MLR: \\(h_i &gt; 3p/n\\) where \\(p\\) = number of regression coefficients\n\n\n\naug = aug %&gt;% relocate(.hat, .after = FemaleLiteracyRate)\naug %&gt;% arrange(desc(.hat))\n\n# A tibble: 72 × 15\n   country       LifeExpectancyYrs FemaleLiteracyRate  .hat CO2_q income_levels1\n   &lt;chr&gt;                     &lt;dbl&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;         \n 1 Mexico                     75.8               92.3 0.445 (2.5… Upper middle …\n 2 Tajikistan                 69.9               99.6 0.425 [0.0… Lower middle …\n 3 Bosnia and H…              76.9               96.7 0.367 (4.6… Upper middle …\n 4 Uzbekistan                 69                 99.2 0.363 (2.5… Lower middle …\n 5 Bangladesh                 71                 53.4 0.347 [0.0… Lower middle …\n 6 Afghanistan                56.7               13   0.327 [0.0… Low income    \n 7 Zimbabwe                   51.9               80.1 0.321 [0.0… Low income    \n 8 Angola                     60.9               58.6 0.320 (0.8… Lower middle …\n 9 Myanmar                    67.4               90.4 0.304 [0.0… Lower middle …\n10 Yemen                      67.7               48.5 0.296 (0.8… Lower middle …\n# ℹ 62 more rows\n# ℹ 9 more variables: four_regions &lt;fct&gt;, WaterSourcePrct &lt;dbl&gt;,\n#   FoodSupplykcPPD &lt;dbl&gt;, members_oecd_g77 &lt;chr&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#   .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#countries-with-high-leverage-h_i-3pn",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#countries-with-high-leverage-h_i-3pn",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Countries with high leverage (\\(h_i > 3p/n\\))",
    "text": "Countries with high leverage (\\(h_i &gt; 3p/n\\))\n\nWe can look at the countries that have high leverage: there are NONE\n\n\nn = nrow(gapm2); p = length(final_model$coefficients) - 1\naug %&gt;% \n  filter(.hat &gt; 3*p/n) %&gt;%\n  arrange(desc(.hat))\n\n# A tibble: 0 × 15\n# ℹ 15 variables: country &lt;chr&gt;, LifeExpectancyYrs &lt;dbl&gt;,\n#   FemaleLiteracyRate &lt;dbl&gt;, .hat &lt;dbl&gt;, CO2_q &lt;fct&gt;, income_levels1 &lt;fct&gt;,\n#   four_regions &lt;fct&gt;, WaterSourcePrct &lt;dbl&gt;, FoodSupplykcPPD &lt;dbl&gt;,\n#   members_oecd_g77 &lt;chr&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;, .sigma &lt;dbl&gt;,\n#   .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#identifying-points-with-high-cooks-distance",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#identifying-points-with-high-cooks-distance",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Identifying points with high Cook’s distance",
    "text": "Identifying points with high Cook’s distance\n\n\nThe Cook’s distance for the \\(i^{th}\\) observation is\n\\[d_i = \\frac{h_i}{2(1-h_i)} \\cdot r_i^2\\] where \\(h_i\\) is the leverage and \\(r_i\\) is the studentized residual\n\n\nAnother rule for Cook’s distance that is not strict:\n\nInvestigate observations that have \\(d_i &gt; 1\\)\n\nCook’s distance values are already in the augment tibble: .cooksd\n\n\n\n\nNo countries with high Cook’s distance\n\n\naug = aug %&gt;% relocate(.cooksd, .after = country)\naug %&gt;% arrange(desc(.cooksd)) %&gt;% filter(.cooksd &gt; 1)\n\n# A tibble: 0 × 15\n# ℹ 15 variables: country &lt;chr&gt;, .cooksd &lt;dbl&gt;, LifeExpectancyYrs &lt;dbl&gt;,\n#   FemaleLiteracyRate &lt;dbl&gt;, .hat &lt;dbl&gt;, CO2_q &lt;fct&gt;, income_levels1 &lt;fct&gt;,\n#   four_regions &lt;fct&gt;, WaterSourcePrct &lt;dbl&gt;, FoodSupplykcPPD &lt;dbl&gt;,\n#   members_oecd_g77 &lt;chr&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;, .sigma &lt;dbl&gt;,\n#   .std.resid &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#plotting-cooks-distance",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#plotting-cooks-distance",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Plotting Cook’s Distance",
    "text": "Plotting Cook’s Distance\n\n# plot(model) shows figures similar to autoplot()\n# adds on Cook's distance though\nplot(final_model, which = 4)"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#how-do-we-deal-with-influential-points",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#how-do-we-deal-with-influential-points",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "How do we deal with influential points?",
    "text": "How do we deal with influential points?\n\nIf an observation is influential, we can check data errors:\n\nWas there a data entry or collection problem?\nIf you have reason to believe that the observation does not hold within the population (or gives you cause to redefine your population)\n\nIf an observation is influential, we can check our model:\n\nDid you leave out any important predictors?\nShould you consider adding some interaction terms?\nIs there any nonlinearity that needs to be modeled?\n\nBasically, deleting an observation should be justified outside of the numbers!\n\nIf it’s an honest data point, then it’s giving us important information!\n\nMeans we will need to discuss the limitations of our model\n\nFor example: Think about measurements that might help explain life expectancy that are NOT in our model\n\nA really well thought out explanation from StackExchange"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#poll-everywhere-question-2",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#poll-everywhere-question-2",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#when-we-have-detected-problems-in-our-model",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#when-we-have-detected-problems-in-our-model",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "When we have detected problems in our model…",
    "text": "When we have detected problems in our model…\n\nWe have talked about influential points\nWe have talked about identifying issues with our LINE assumptions\n\n \nWhat are our options once we have identified issues in our linear regression model?\n\nAre we missing a crucial measure in our dataset?\nTry a transformation if there is an issue with linearity or normality\n\nAddressed in model selection\n\nTry a weighted least squares approach if unequal variance (oof, not enough time for us to get to)\nTry a robust estimation procedure if we have a lot of outlier issues (outside scope of class)"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#what-is-multicollinearity-adapted-from-parts-of-stat-501-page",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#what-is-multicollinearity-adapted-from-parts-of-stat-501-page",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "What is multicollinearity? (adapted from parts of STAT 501 page)",
    "text": "What is multicollinearity? (adapted from parts of STAT 501 page)\nSo far, we’ve been ignoring something very important: multicollinearity\n\n\n\n\n\n\nMulticollinearity\n\n\nTwo or more covariates in a multivariable regression model are highly correlated\n\n\n\n\n\n\n\nTypes of multicollinearity\n\nStructural multicollinearity\n\nMathematical artifact caused by creating new covariates from other covariates\nFor example: If we have age, and decide to transform age to include age-squared\n\nThen we have age and age-squared in the model: age-squared is perfectly predicted by age!\n\n\nData-based multicollinearity\n\nResult of a poorly designed experiment, reliance on purely observational data, or the inability to manipulate the system on which the data are collected."
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#poll-everywhere-question-3",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#poll-everywhere-question-3",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#why-is-multicollinearity-a-problem",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#why-is-multicollinearity-a-problem",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Why is multicollinearity a problem?",
    "text": "Why is multicollinearity a problem?\nIn linear regression…\n\nEstimated regression coefficient of any one variable depends on other predictors included in the model\n\nNot necessarily bad, but a big change might be an issue\n\nHypothesis tests for any coefficient may yield different conclusions depending on other predictors included in the model\nMarginal contribution of any one predictor variable in reducing the error sum of squares depends on other predictors included in the model\n\n \nWhen there is multicollinearity in our model:\n\nPrecision of the estimated regression coefficients or correlated covariates decreases a lot\n\nBasically, standard error increases and confidence intervals get wider, which means we’re not as confident in our estimate anymore\nBecause highly correlated covariates are not adding much more information, but are constraining our model more"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#did-you-notice-anything-about-all-the-consequences-of-multicollinearity",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#did-you-notice-anything-about-all-the-consequences-of-multicollinearity",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Did you notice anything about all the consequences of multicollinearity?",
    "text": "Did you notice anything about all the consequences of multicollinearity?\n\nAll consequences relate to estimating a regression coefficient precisely\n\nRecall that precision is linked to analysis goals of association and interpretability\nSee Lesson 12: Model Selection\n\n\n \n\nMulticollinearity is not really an issue when our goal is prediction\n\nHighly correlated covariates/predictors will not hurt our prediction of an outcome"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#how-do-we-detect-multicollinearity",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#how-do-we-detect-multicollinearity",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "How do we detect multicollinearity?",
    "text": "How do we detect multicollinearity?\n\n\n\nVariance inflation factors (VIF): quantifies how much the variance of the estimated coefficient for covariate \\(k\\) increases\n\nIncreases: from SLR with only covariate \\(k\\) to MLR with all other covariates\n\n\n \n\nGeneral rule of thumb\n\n\\(4 &lt; VIF &lt; 10\\): Warrent investigation (but most people aren’t investigating this…)\n\\(VIF &gt; 10\\): Requires correction\n\nInfluencing regression coefficient estimates\n\n\n\n\n\n\n\n\nVIF\n\n\n\\[\nVIF = \\dfrac{1}{1-R_k^2}\n\\]\n\\(R_k^2\\) is the \\(R^2\\)-value obtained by regressing the \\(k^{th}\\) covariate/predictor on the remaining predictors"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#lets-apply-it-to-our-final-model",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#lets-apply-it-to-our-final-model",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Let’s apply it to our final model",
    "text": "Let’s apply it to our final model\n\nNaive way to calculate this:\n\n\nlibrary(rms)\nrms::vif(final_model)\n\n               FemaleLiteracyRate                 CO2_q(0.806,2.54] \n                         4.863139                          2.979224 \n                 CO2_q(2.54,4.66]                  CO2_q(4.66,35.2] \n                         4.758904                          5.180216 \nincome_levels1Lower middle income income_levels1Upper middle income \n                         5.290718                          8.406927 \n        income_levels1High income              four_regionsAmericas \n                         7.293148                          2.531966 \n                 four_regionsAsia                four_regionsEurope \n                         2.096398                          7.771994 \n                  WaterSourcePrct                   FoodSupplykcPPD \n                         4.824266                          3.499250 \n             members_oecd_g77oecd            members_oecd_g77others \n                         2.720955                          5.125196 \n\n\n\nAll \\(VIF &lt; 10\\)\nProblem: multi-level covariates (CO2 Emissions and income level) have different VIF’s even though they should be considered one variable"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#lets-apply-it-to-our-final-model-correctly-12",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#lets-apply-it-to-our-final-model-correctly-12",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Let’s apply it to our final model correctly (1/2)",
    "text": "Let’s apply it to our final model correctly (1/2)\n\nCalculate the GVIF and, more importantly, the \\(GVIF^{1/(2\\cdot df)}\\)\nGVIF is the \\(R^2\\)-value for regressing a covariate’s group indicators on the remaining covariates\n\nCaptures the correlation between covariates better\n\n\\(GVIF^{1/(2\\cdot df)}\\) helps standardize GVIF based on how many levels each categorical covariate has\n\nI’ll refer to this as df-corrected GVIF or standardized GVIF\nIf continuous covariate, \\(GVIF^{1/(2\\cdot df)} = \\sqrt{GVIF}\\)\n\n\n\nlibrary(car)\ncar::vif(final_model)\n\n                        GVIF Df GVIF^(1/(2*Df))\nFemaleLiteracyRate  4.863139  1        2.205253\nCO2_q               8.223951  3        1.420736\nincome_levels1     11.045885  3        1.492336\nfour_regions       13.935918  3        1.551277\nWaterSourcePrct     4.824266  1        2.196421\nFoodSupplykcPPD     3.499250  1        1.870628\nmembers_oecd_g77    7.430919  2        1.651052"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#lets-apply-it-to-our-final-model-correctly-22",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#lets-apply-it-to-our-final-model-correctly-22",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Let’s apply it to our final model correctly (2/2)",
    "text": "Let’s apply it to our final model correctly (2/2)\n\nIf continuous covariate, \\(GVIF^{1/(2\\cdot df)} = \\sqrt{GVIF}\\)\nSo we can square \\(GVIF^{1/(2\\cdot df)}\\) and set VIF rules\nOR: we can correct any \\(GVIF^{1/(2\\cdot df)} &gt; \\sqrt{10} = 3.162\\)\n\n\ncar::vif(final_model)\n\n                        GVIF Df GVIF^(1/(2*Df))\nFemaleLiteracyRate  4.863139  1        2.205253\nCO2_q               8.223951  3        1.420736\nincome_levels1     11.045885  3        1.492336\nfour_regions       13.935918  3        1.551277\nWaterSourcePrct     4.824266  1        2.196421\nFoodSupplykcPPD     3.499250  1        1.870628\nmembers_oecd_g77    7.430919  2        1.651052\n\n\n\nAll of these covariates are okay! No multicollinearity to correct in this dataset!"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#but-what-if-we-do-need-to-make-corrections-for-multicollinearity",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#but-what-if-we-do-need-to-make-corrections-for-multicollinearity",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "But what if we do need to make corrections for multicollinearity?",
    "text": "But what if we do need to make corrections for multicollinearity?\n\nWe have been dealing with data-based multicollinearity in our example\nIf we had issues with multicollinearity, then what are our options?\n\nRemove the variable(s) with large VIF\nUse expert knowledge in the field to decide\n\nIf one variable has a large VIF, then there is usually another one or more variables with large VIFs\n\nBasically, all the covariates that are correlated will have large VIFs\n\nExample: our two largest GVIFs were for world region and income levels\n\nHypothetical: their \\(GVIF^{1/(2\\cdot df)} &gt; 3.162\\)\nRemove one of them\nI’m no expert, but from more of a data equity lens, there’s a lot of generalizations made about world regions\n\nI think relying on the income level of a country might give us more information as well"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#what-about-structural-multicollinearity",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#what-about-structural-multicollinearity",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "What about structural multicollinearity?",
    "text": "What about structural multicollinearity?\n\nStructural multicollinearity\n\nMathematical artifact caused by creating new covariates from other covariates\n\n\n \n\nFor example: If we have age, and decide to transform age to include age-squared\n\nThen we have age and age-squared in the model: age-squared is perfectly predicted by age!\nBy having the untransformed and transformed covariate in the model, they are inherently correlated!\n\n\n \n\nBest practice to reduce the correlation: center you covariate\n\nBy centering age, we no longer have a one-to-one connection between age and age-squared\nIf centered at 40yo: a 35 yo and a 45 yo will both have centered age of 5, and age-squared of 25\n\n\n \n\nCheck out the Penn State site for a work through of an example with VIFs"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#summary-of-multicollinearity",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#summary-of-multicollinearity",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Summary of multicollinearity",
    "text": "Summary of multicollinearity\n\nCorrelated covariates/predictors will hurt our model’s precision and interpretations of coefficients\n\n \n\nWe need to check for multicollinearity by using VIFs or GVIFs\n\n \n\nIf \\(VIF &gt; 10\\) or \\(GVIF^{1/(2\\cdot df)} &gt; 3.162\\), we need to do something about the covariates\n\nData based: remove one the of correlated variables\nStructural based: centering usually fixes it"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#regression-analysis-process-1",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics.html#regression-analysis-process-1",
    "title": "Lesson 15: MLR Model Diagnostics",
    "section": "Regression analysis process",
    "text": "Regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)\n\n\n\n\n\n\n\nLesson 15: MLR Diagnostics"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Understand the motivation for model selection, including bias-variance trade off and alignment of research goals (association vs. prediction)\nExplain the general process or idea behind different model selection techniques\nKnow the formula for and function for various model fit statistics\n\n\n\n\nFirst, let’s think about the number of observations in our dataset\nFor example: In the Gapminder dataset, I can use an indicator for each country\n\nRemember that each country is an observation\nSo we have a perfectly fit model - a covariate for each observation\nBut we cannot generalize this to any other countries\nAnd we haven’t identified any meaningful relationships between life expectancy and other measured characteristics\n\nMore covariates in the model is not always better\n\nOverfitting the data limits our generalizability and prevents us from answering research questions\n\n\n\n\n\n\n\nSuppose we have \\(p = 30\\) covariates (in the true model) and n = 50 observations. We could consider the following two alternatives:\n\nWe could fit a model using all of the covariates.\n\nIn this case, \\(\\widehat\\beta\\) is unbiased for \\(\\beta\\) (in a linear model fit using OLS). But \\(\\widehat\\beta\\) has very high variance.\n\nWe could fit a model using only the five strongest covariates.\n\nIn this case, \\(\\widehat\\beta\\) will be biased for \\(\\beta\\), but it will have lower variance (compared to the estimate including all covariates)\n\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html\n\n\n\n\n\n\n\n\n\n\nRecall mean square error is a function of SSE (sum of squared residuals)\n\\[\nMSE = \\dfrac{1}{n} \\sum_{i=1}^{n} \\big(Y_i - \\widehat{Y}_i \\big)^2\n\\]\nMSE can also be written as a function of the bias and variance\n\\[\nMSE = \\text{bias}\\big(\\widehat\\beta\\big)^2 + \\text{variance}\\big(\\widehat\\beta\\big)\n\\]\nFor the same data:\n\nMore covariates in model: less bias, more variance\nLess covariates in model: more bias, less variance\n\nOut goal: find a model with just the right amount of covariates to balance bias and vairance\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html\n\n\n\n\n\n\n\n\n“Because models always fall far short of the complex reality under study, there are no best or optimal strategies for modeling.”\n\nFrom: Statistical Foundations for Model-Based Adjustments\n\nNot all statistical texts provide practical advice on model development\n\nA lot of resources include methods/code to compare models, but does not include much advice re: selecting which model to ultimately use.\nOther texts are sparse on details or incorporate simplistic approaches\n\nModel development strategy should align with research goals\n\nPrediction vs. Estimating Association\nStrategy may depend on study design and data set size\n\n\n\n\n\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nGoal: Understand one variable’s (or a group of variable’s) effect on the response after adjusting for other factors\nMainly interpret the coefficient of the variable that is the focus of the study\n\nInterpreting the coefficients of the other variables is not important, but can help bring context\n\nAny variables not selected for the final model have still been adjusted for, since they had a chance to be in the model\nExample: How is body mass of a penguin associated with flipper length?\n\n\n\n\n\n\nPrediction\n\n\n\nGoal: to calculate the most precise prediction of the response variable\nInterpreting coefficients is not important\nChoose only the variables that are strong predictors of the response variable\n\nExcluding irrelevant variables can help reduce widths of the prediction intervals\n\nExample: What is the flipper length of a penguin with body mass of 3000 g (and all its other characteristics)?\n\n\n\n\n\n\n\n\nMore information on the two analysis goals:\n\n\n\n\n\nIf you ever get the chance, check out Dr. Kristin Sainani’s series on Statistics\n\n\n\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nSelection of potential models is tied more with the research context with some incorporation of prediction scores\nPre-specification of multivariable model\nPurposeful model selection\n\n“Risk factor modeling”\n\nChange in Estimate (CIE) approaches\n\nWill learn in Survival Analysis (BSTA 514)\n\n\n\n\n\n\n\nPrediction\n\n\n\nSelection of potential models is fully dependent on prediction scores\nAutomated strategies\n\nStepwise selection (forward/backward)\n\nYou’ll see these a lot, but they’re not really good methods\n\nBest subset\nRegularization techniques (LASSO, Ridge, Elastic net)\n\n\n\n\n\n\n\nFor categorical outcomes, there are more prediction model selection strategies (will learn more in BSTA 513)\n\nExamples: Decision trees, Random forest, Neural networks, K-means"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#why-cant-i-just-throw-in-all-the-variables-into-my-model",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#why-cant-i-just-throw-in-all-the-variables-into-my-model",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "First, let’s think about the number of observations in our dataset\nFor example: In the Gapminder dataset, I can use an indicator for each country\n\nRemember that each country is an observation\nSo we have a perfectly fit model - a covariate for each observation\nBut we cannot generalize this to any other countries\nAnd we haven’t identified any meaningful relationships between life expectancy and other measured characteristics\n\nMore covariates in the model is not always better\n\nOverfitting the data limits our generalizability and prevents us from answering research questions"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#model-complexity-vs.-parsimony",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#model-complexity-vs.-parsimony",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Suppose we have \\(p = 30\\) covariates (in the true model) and n = 50 observations. We could consider the following two alternatives:\n\nWe could fit a model using all of the covariates.\n\nIn this case, \\(\\widehat\\beta\\) is unbiased for \\(\\beta\\) (in a linear model fit using OLS). But \\(\\widehat\\beta\\) has very high variance.\n\nWe could fit a model using only the five strongest covariates.\n\nIn this case, \\(\\widehat\\beta\\) will be biased for \\(\\beta\\), but it will have lower variance (compared to the estimate including all covariates)\n\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#bias-variance-trade-off",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#bias-variance-trade-off",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Recall mean square error is a function of SSE (sum of squared residuals)\n\\[\nMSE = \\dfrac{1}{n} \\sum_{i=1}^{n} \\big(Y_i - \\widehat{Y}_i \\big)^2\n\\]\nMSE can also be written as a function of the bias and variance\n\\[\nMSE = \\text{bias}\\big(\\widehat\\beta\\big)^2 + \\text{variance}\\big(\\widehat\\beta\\big)\n\\]\nFor the same data:\n\nMore covariates in model: less bias, more variance\nLess covariates in model: more bias, less variance\n\nOut goal: find a model with just the right amount of covariates to balance bias and vairance\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#model-selection-basics-slide-adjusted-from-jodi-lapidus",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#model-selection-basics-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "“Because models always fall far short of the complex reality under study, there are no best or optimal strategies for modeling.”\n\nFrom: Statistical Foundations for Model-Based Adjustments\n\nNot all statistical texts provide practical advice on model development\n\nA lot of resources include methods/code to compare models, but does not include much advice re: selecting which model to ultimately use.\nOther texts are sparse on details or incorporate simplistic approaches\n\nModel development strategy should align with research goals\n\nPrediction vs. Estimating Association\nStrategy may depend on study design and data set size"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#the-goals-of-association-vs.-prediction",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#the-goals-of-association-vs.-prediction",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Association / Explanatory / One variable’s effect\n\n\n\nGoal: Understand one variable’s (or a group of variable’s) effect on the response after adjusting for other factors\nMainly interpret the coefficient of the variable that is the focus of the study\n\nInterpreting the coefficients of the other variables is not important, but can help bring context\n\nAny variables not selected for the final model have still been adjusted for, since they had a chance to be in the model\nExample: How is body mass of a penguin associated with flipper length?\n\n\n\n\n\n\nPrediction\n\n\n\nGoal: to calculate the most precise prediction of the response variable\nInterpreting coefficients is not important\nChoose only the variables that are strong predictors of the response variable\n\nExcluding irrelevant variables can help reduce widths of the prediction intervals\n\nExample: What is the flipper length of a penguin with body mass of 3000 g (and all its other characteristics)?"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#model-building-for-association-vs.-prediction",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#model-building-for-association-vs.-prediction",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "More information on the two analysis goals:\n\n\n\n\n\nIf you ever get the chance, check out Dr. Kristin Sainani’s series on Statistics"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#model-selection-strategies-for-continuous-outcomes",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#model-selection-strategies-for-continuous-outcomes",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Association / Explanatory / One variable’s effect\n\n\n\nSelection of potential models is tied more with the research context with some incorporation of prediction scores\nPre-specification of multivariable model\nPurposeful model selection\n\n“Risk factor modeling”\n\nChange in Estimate (CIE) approaches\n\nWill learn in Survival Analysis (BSTA 514)\n\n\n\n\n\n\n\nPrediction\n\n\n\nSelection of potential models is fully dependent on prediction scores\nAutomated strategies\n\nStepwise selection (forward/backward)\n\nYou’ll see these a lot, but they’re not really good methods\n\nBest subset\nRegularization techniques (LASSO, Ridge, Elastic net)\n\n\n\n\n\n\n\nFor categorical outcomes, there are more prediction model selection strategies (will learn more in BSTA 513)\n\nExamples: Decision trees, Random forest, Neural networks, K-means"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#pre-specification-of-multivariable-model-slide-adjusted-from-jodi-lapidus",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#pre-specification-of-multivariable-model-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Pre-specification of multivariable model (slide adjusted from Jodi Lapidus)",
    "text": "Pre-specification of multivariable model (slide adjusted from Jodi Lapidus)\n\nIn a clinical trial, we often have to write and finalize a statistical analysis plan (SAP) before the trial starts\nIf we wish to compare treatment effects adjusted for covariates, all covariates typically specified in advance\n\nExample: Comparing effectiveness of 3-drug vs. 2-drug regimen for delaying AIDS onset or death. Covariates such as severity of HIV infection at baseline would have been specified in advance.\nVariables such as study site, as well as any randomization stratification variables are common covariates.\n\nIn these cases, only a limited number of multivariable models are fit and reported\n\nDo not perform all the model building steps outlined in Hosmer and Lemeshow texts"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#purposeful-model-selection-slide-adjusted-from-jodi-lapidus",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#purposeful-model-selection-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Purposeful model selection (slide adjusted from Jodi Lapidus)",
    "text": "Purposeful model selection (slide adjusted from Jodi Lapidus)\n\nCan use this type of model selection for any type of regression\nCareful, well-thought out variable selection process\n\nConsiders both confounding and interaction, as well as checking model assumptions, fit, etc.\n\nOften a reasonable strategy, especially in epidemiology and more exploratory clinical studies\n\nHowever, not always appropriate!\nE.g. clinical trials with model specified in advance. (pre-specified model)\n\n\n \n\nThis is the selection process that we will focus on in this class!"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#change-in-estimate-cie-approach-slide-adjusted-from-jodi-lapidus",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#change-in-estimate-cie-approach-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Change in estimate (CIE) approach (slide adjusted from Jodi Lapidus)",
    "text": "Change in estimate (CIE) approach (slide adjusted from Jodi Lapidus)\n\nCIE strategies select covariates on the basis of how much their control changes exposure effect estimates\n\nObserved change is presumed to measure confounding by the covariate.\n\nWhat estimate?\n\nH/L text suggest using coefficients from the model\nWe typically use the coefficient estimate from the explanatory variable that we are most interested in\n\nWhat magnitude change is ”important”?\n\nH/L text suggest 10%\n\nOne must choose an effect measure to judge change importance, where “importance” needs to be evaluated along a contextually meaningful scale\nAccurate assessment of confounding may require examining changes from removing entire sets of covariates\n\nAdd in or eliminate candidate confounders one at time?\nAdd in or eliminate candidate confounders in sets?"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#stepwise-selection-slide-adjusted-from-adrianna-westbrook",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#stepwise-selection-slide-adjusted-from-adrianna-westbrook",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Stepwise selection (slide adjusted from Adrianna Westbrook)",
    "text": "Stepwise selection (slide adjusted from Adrianna Westbrook)\n\nThis is an incredibly common approach that statisticians use, often because it is an older and more recognized method\n\nBUT IT IS ALSO ONE OF THE WORST MODEL SELECTION STRATEGIES!!\n\nMajor disadvantages to stepwise selection:\n\nProne to overfitting\nBiased estimates\nCements the wrong idea that we are looking for our “most significant” covariates\n\nPredictors/covariates are added or removed one at time if they are below a certain threshold (usually p-value below 0.10 to 0.20)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#stepwise-selection-two-common-approaches",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#stepwise-selection-two-common-approaches",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Stepwise selection: two common approaches",
    "text": "Stepwise selection: two common approaches\n\nI will introduce two of the approaches so that you understand the general process if a collaborator ever mentions stepwise selection\nForward selection:\n\nFor \\(p\\) cvariates potential covariates, run all simple linear regressions:\n\n\\(Y= \\beta_0 + \\beta_1 X_1 + \\epsilon\\) through \\(Y= \\beta_0 + \\beta_1 X_{p} + \\epsilon\\)\nInclude the \\(X_i\\) with the lowest p-value (assuming it is below the threshold)\n\nNow run \\(Y= \\beta_0 + \\beta_1 X_i + \\beta_2 X_1 + \\epsilon\\) through \\(Y= \\beta_0 + \\beta_1 X_i + \\beta_2 X_{p} + \\epsilon\\) and enter the next \\(X_j\\) with the lowest p-value\nContinue process until no more predictors come back with a p-value below the threshold\n\nBackward selection:\n\nStart with a full model (\\(Y= \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p + \\epsilon\\)) and remove predictor with the highest p-value (assuming it is above the threshold)\nRepeatedly remove the variable with the highest p-value until all remaining variables meet the stopping criteria (are below the threshold)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#best-subset-slide-adjusted-from-adrianna-westbrook",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#best-subset-slide-adjusted-from-adrianna-westbrook",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Best subset (slide adjusted from Adrianna Westbrook)",
    "text": "Best subset (slide adjusted from Adrianna Westbrook)\n\nI don’t see this approach very often\nQuite literally making subsets of the data and using the “best” one\nGeneral steps:\n\nRun every possible model fitting 1 to all possible \\(p\\) predictors/covariates\nYou can limit number of potential predictors\n\\(2^p\\) = total number of models where \\(p\\) = number of predictors\nYou will get the best fitting model within each category (i.e., 1 predictor model, 2 predictor model,…, \\(p\\) predictor model)\nThen have to find the best fitting model between the best models from each category\n\nMajor disadvantages to best subset:\n\nDoes not account for interactions\nNeeds to run a lot of models (takes A LOT of time)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#regularization-techniques",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#regularization-techniques",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Regularization techniques",
    "text": "Regularization techniques\n\nRegularization techniques (LASSO, ridge, elastic net) adds a penalization that shrinks (or regularizes) coefficients down to reduce overfitting\n\n\n\n\n\n\n\n\n\n\n\nLASSO (Least About Shrinkage and Selection Operator)\nRidge\nElastic Net\n\n\nPenalization\nL-1 Norm, uses absolute value\nL-2 Norm, uses squared value\nBest of both worlds, L-1 and L-2 used\n\n\nPro’s\nReduces overfitting, will shrink coefficient to zero\nReduces overfitting, handles collinearity, can handle k&gt;n\nReduces overfitting, handles collinearity, handles k&gt;n, shrinks coefficients to zero\n\n\nCon’s\nCannot handle k&gt;n, doesn’t handle multicollinearity well\nDoes not shrink coefficients to zero, difficult to interpret\nMore difficult for R to do than the other two (but not really that bad)"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#introduction-to-model-fit-statistics",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#introduction-to-model-fit-statistics",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Introduction to model fit statistics",
    "text": "Introduction to model fit statistics\n\nSo far we have compared models using the F-test\nThe F-test is a great way to compare models that are nested\n\nBasically, this means that the “full” model contains all the covariates that the “reduced” model contains\nThe full model will have additional covariates, but the covariates in the reduced is a subset of the covariates in the full\n\nWhat if we want to compare models that are not nested?\n\nThere is a special group of fit statistics that can help us compare models\nNote: these are sometimes used in the model building process (within one strategy)\n\nHelpful if we want to compare selected models across strategies\nHelpful if we have a few “final” models with different covariates that we want to compare"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#common-model-fit-statistics",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#common-model-fit-statistics",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Common model fit statistics",
    "text": "Common model fit statistics\n\nThe following model fit statistics combine information about the SSE, the number of parameters in the model, and the sample size\nFor these fit statistics, smaller values indicate better model fit!\n\n\n\n\n\n\n\n\nFit statistic\nEquation\n\n\n\n\nR-squared / Adjusted R-squared\n\\(Adj. R^2 = 1 - \\frac{SSE/(n-p-1)}{SSY/(n-1)}\\)\n\n\nMallow’s \\(C_p\\)\n\\(C_p = \\Bigg[ \\dfrac{\\widehat\\sigma^2_p}{\\widehat\\sigma^2_{max}} - 1 \\Bigg](n-p) + p\\)\n\n\nAkaike information criterion (AIC)\n\\(AIC = n\\log(SSE) - n \\log(n) + 2(p+1)\\)\n\n\nBayesian information criterion (BIC)\n\\(BIC = n\\log(SSE) - n\\log(n) + log(n)\\cdot(p+1)\\)\n\n\n\n \n\nWe don’t need to know the exact formulas for them!"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_maybe.html#common-model-fit-statistics-1",
    "href": "lessons/13_Model_selection/13_Model_selection_maybe.html#common-model-fit-statistics-1",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Common model fit statistics",
    "text": "Common model fit statistics\n\nThere is no hypothesis testing for these fit statistics\n\nOnly helpful if you are comparing models\nWorks for nested and non-nested models\n\nCommon to report all or some of them\nAll of the fit statistics will not necessarily reach a consensus about the best fitting model\n\nEach weigh SSE, number of parameters, and number of observations differently"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html",
    "title": "Lesson 11: Interactions",
    "section": "",
    "text": "Define confounders and effect modifiers, and how they interact with the main relationship we model.\nInterpret the interaction component of a model with a binary categorical covariate and continuous covariate, and how the main variable’s effect changes.\nInterpret the interaction component of a model with a multi-level categorical covariate and continuous covariate, and how the main variable’s effect changes.\nInterpret the interaction component of a model with two categorical covariates, and how the main variable’s effect changes.\n\nNext time:\n\nInterpret the interaction component of a model with two continuous covariates, and how the main variable’s effect changes.\nWhen there are only two covariates in the model, test whether one is a confounder or effect modifier.\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\n\n\n\n\n\n\n\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#lets-map-that-to-our-regression-analysis-process",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#lets-map-that-to-our-regression-analysis-process",
    "title": "Lesson 11: Interactions",
    "section": "Let’s map that to our regression analysis process",
    "text": "Let’s map that to our regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#recall-our-data-and-the-main-relationship",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#recall-our-data-and-the-main-relationship",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Recall our data and the main relationship",
    "text": "Recall our data and the main relationship"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#what-is-a-confounder",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#what-is-a-confounder",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "What is a confounder?",
    "text": "What is a confounder?\n\nA confounding variable, or confounder, is a factor/variable that wholly or partially accounts for the observed effect of the risk factor on the outcome\n\n\n\n\nA confounder must be…\n\nRelated to the outcome Y, but not a consequence of Y\nRelated to the explanatory variable X, but not a consequence of X\n\n\n\n\n\n\n\n\n\n\n\nA classic example: We found an association between ice cream consumption and sunburn!\n\nIf we adjust for a potential confounder, temperature/hot weather, we may see that the association between ice and sunburn is not as large\n\nAnother example: We found an association between socioeconomic status (SES) and lung cancer!\n\nIf we adjust for a potential confounder, exposure to air pollution, we may see that the association between SES and lung cancer decreases"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#including-a-confounder-in-the-model",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#including-a-confounder-in-the-model",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Including a confounder in the model",
    "text": "Including a confounder in the model\n\nIn the following model we have two variables, \\(X_1\\) and \\(X_2\\)\n\n\\[Y= \\beta_0 + \\beta_1X_{1}+ \\beta_2X_{2} + \\epsilon\\]\n\nAnd we assume that every level of the confounder, there is parallel slopes\nNote: to interpret \\(\\beta_1\\), we did not specify any value of \\(X_2\\); only specified that it be held constant\n\nImplicit assumption: effect of \\(X_1\\) is equal across all values of \\(X_2\\)\n\nThe above model assumes that \\(X_{1}\\) and \\(X_{2}\\) do not interact (with respect to their effect on \\(Y\\))\n\nEpidemiology: no “effect modification”\nMeaning the effect of \\(X_{1}\\) is the same regardless of the values of \\(X_{2}\\)\nThis model is often called a “main effects model”"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#where-have-we-modeled-a-confounder-before",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#where-have-we-modeled-a-confounder-before",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Where have we modeled a confounder before?",
    "text": "Where have we modeled a confounder before?\n\n\n\nWe have seen a plot of Life expectancy vs. female literacy rate with different levels of food supply colored (Lesson 8)\nIn our plot and the model, we treat food supply as a confounder\nIf food supply is a confounder in the relationship between life expectancy and female literacy rate, then we only use main effects in the model:\n\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-1",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-1",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Poll everywhere question 1",
    "text": "Poll everywhere question 1"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#what-is-an-effect-modifier",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#what-is-an-effect-modifier",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "What is an effect modifier?",
    "text": "What is an effect modifier?\n\n\n\nAn additional variable in the model\n\nOutside of the main relationship between \\(Y\\) and \\(X_1\\) that we are studying\n\nAn effect modifier will change the effect of \\(X_1\\) on \\(Y\\) depending on its value\n\nAka: as the effect modifier’s values change, so does the association between \\(Y\\) and \\(X_1\\)\nSo the coefficient estimating the relationship between \\(Y\\) and \\(X_1\\) changes with another variable\n\nExample: A breast cancer education program (the exposure) that is much more effective in reducing breast cancer (outcome) in rural areas than urban areas.\n\nLocation (rural vs. urban) is the EMM"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#how-do-we-include-an-effect-modifier-in-the-model",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#how-do-we-include-an-effect-modifier-in-the-model",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "How do we include an effect modifier in the model?",
    "text": "How do we include an effect modifier in the model?\n\nInteractions!!\nWe can incorporate interactions into our model through product terms: \\[Y  =  \\beta_0 + \\beta_1X_{1}+ \\beta_2X_{2} +\n\\beta_3X_{1}X_{2} + \\epsilon\\]\nTerminology:\n\nmain effect parameters: \\(\\beta_1,\\beta_2\\)\n\nThe main effect models estimate the average \\(X_{1}\\) and \\(X_{2}\\) effects\n\ninteraction parameter: \\(\\beta_3\\)"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#types-of-interactions-non-interactions",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#types-of-interactions-non-interactions",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Types of interactions / non-interactions",
    "text": "Types of interactions / non-interactions\n\n\n\nCommon types of interactions:\n\nSynergism: \\(X_{2}\\) strengthens the \\(X_{1}\\) effect\nAntagonism:\\(X_{2}\\) weakens the \\(X_{1}\\) effect\n\n\n \n\nIf the interaction coefficient is not significant\n\nNo evidence of effect modification, i.e., the effect of \\(X_{1}\\) does not vary with \\(X_{2}\\)\n\n\n \n\nIf the main effect of \\(X_2\\) is also not significant\n\nNo evidence that \\(X_2\\) is a confounder"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#do-we-think-income-level-is-an-effect-modifier-for-female-literacy-rate",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#do-we-think-income-level-is-an-effect-modifier-for-female-literacy-rate",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Do we think income level is an effect modifier for female literacy rate?",
    "text": "Do we think income level is an effect modifier for female literacy rate?\n\n\n\nLet’s say we only have two income groups: low income and high income\nWe can start by visualizing the relationship between life expectancy and female literacy rate by income level\nQuestions of interest: Is the effect of female literacy rate on life expectancy differ depending on income level?\n\nThis is the same as: Is income level is an effect modifier for female literacy rate?\n“effect of female literacy rate” differing = different slopes between FLR and LE depending on the income group\n\nLet’s run an interaction model to see!"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#model-with-interaction-between-a-binary-categorical-and-continuous-variables",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#model-with-interaction-between-a-binary-categorical-and-continuous-variables",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Model with interaction between a binary categorical and continuous variables",
    "text": "Model with interaction between a binary categorical and continuous variables\nModel we are fitting:\n\\[ LE = \\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{high income}) + \\beta_3 FLR \\cdot I(\\text{high income}) + \\epsilon\\]\n\n\\(LE\\) as outcome\n\\(FLR\\) as continuous variable that is our main variable of interest\n\\(I(\\text{high income})\\) as the indicator that income level is “high income” (binary categorical variable)\n\n\nm_int_inc2 = gapm_sub %&gt;% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2 +\n       FemaleLiteracyRate*income_levels2)\n\nOR\n\nm_int_inc2 = gapm_sub %&gt;% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate*income_levels2)"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#displaying-the-regression-table-and-writing-fitted-regression-equation",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#displaying-the-regression-table-and-writing-fitted-regression-equation",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Displaying the regression table and writing fitted regression equation",
    "text": "Displaying the regression table and writing fitted regression equation\n\ntidy(m_int_inc2, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n54.849\n2.846\n19.270\n0.000\n49.169\n60.529\n    FemaleLiteracyRate\n0.156\n0.039\n3.990\n0.000\n0.078\n0.235\n    income_levels2Higher income\n−16.649\n15.364\n−1.084\n0.282\n−47.308\n14.011\n    FemaleLiteracyRate:income_levels2Higher income\n0.228\n0.164\n1.392\n0.168\n−0.099\n0.555\n  \n  \n  \n\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{high income}) + \\widehat\\beta_3 FLR \\cdot I(\\text{high income}) \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot I(\\text{high income}) + 0.228 \\cdot FLR \\cdot I(\\text{high income})\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-2",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-2",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#comparing-fitted-regression-lines-for-each-income-level",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#comparing-fitted-regression-lines-for-each-income-level",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Comparing fitted regression lines for each income level",
    "text": "Comparing fitted regression lines for each income level\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{high income}) + \\widehat\\beta_3 FLR \\cdot I(\\text{high income}) \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot I(\\text{high income}) + 0.228 \\cdot FLR \\cdot I(\\text{high income})\n\\end{aligned}\\]\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 FLR \\cdot 0 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot 0 + \\\\\n& 0.228 \\cdot FLR \\cdot 0 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR\\\\\n\\end{aligned}\\]\n\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 FLR \\cdot 1 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot 1 + \\\\ & 0.228 \\cdot FLR \\cdot 1 \\\\\n\\widehat{LE} = & (54.85 - 16.65 \\cdot 1) + \\\\ & (0.156 \\cdot FLR + 0.228 \\cdot FLR \\cdot 1) \\\\\n\\widehat{LE} = & (54.85 - 16.65) + (0.156 + 0.228) \\cdot FLR\\\\\n\\widehat{LE} = & 38.2 + 0.384 \\cdot FLR\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#lets-take-a-look-back-at-the-plot",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#lets-take-a-look-back-at-the-plot",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Let’s take a look back at the plot",
    "text": "Let’s take a look back at the plot\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR  \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR\\\\\n\\end{aligned}\\]\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & (\\widehat\\beta_0 +\\widehat\\beta_2) + (\\widehat\\beta_1 +\\widehat\\beta_3) FLR \\\\\n\\widehat{LE} = & (54.85 - 16.65) + (0.156 + 0.228) \\cdot FLR\\\\\n\\widehat{LE} = & 38.2 + 0.384 \\cdot FLR\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#interpretation-for-interaction-between-binary-categorical-and-continuous-variables",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#interpretation-for-interaction-between-binary-categorical-and-continuous-variables",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Interpretation for interaction between binary categorical and continuous variables",
    "text": "Interpretation for interaction between binary categorical and continuous variables\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 I(\\text{high income}) + \\widehat\\beta_3 FLR^c \\cdot I(\\text{high income}) \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_2 \\cdot I(\\text{high income})\\bigg] + \\underbrace{\\bigg[\\widehat\\beta_1 + \\widehat\\beta_3 \\cdot I(\\text{high income}) \\bigg]}_\\text{FLR's effect} FLR^c \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in female literacy rate’s effect, comparing higher income to lower income levels\n\nAKA: the change in slopes (for line between FLR and LE) comparing high income to low income\n\nwhere the “female literacy rate effect” = change in mean life expectancy per percent increase in female literacy (slope) with income level held constant, i.e. “adjusted female literacy rate effect”\n\nIn summary, the interaction term can be interpreted as “difference in adjusted female literacy rate effect comparing higher income to lower income levels”\nIt will be helpful to test the interaction to round out this interpretation!!"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-binary-categorical-and-continuous-variables",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-binary-categorical-and-continuous-variables",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Test interaction between binary categorical and continuous variables",
    "text": "Test interaction between binary categorical and continuous variables\n\nWe run an F-test for a single coefficient (\\(\\beta_3\\)) in the below model (see Lesson 10, MLR: Using the F-test)\n\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 I(\\text{high income}) + \\beta_3 FLR^c \\cdot I(\\text{high income}) + \\epsilon\\]\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_3=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_3\\neq0\\)\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[\\begin{aligned}\nLE = & \\beta_0 + \\beta_1 FLR^c + \\beta_2 I(\\text{high income}) + \\\\ &\\epsilon\n\\end{aligned}\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}\nLE = & \\beta_0 + \\beta_1 FLR^c + \\beta_2 I(\\text{high income}) + \\\\ &\\beta_3 FLR^c \\cdot I(\\text{high income}) + \\epsilon\n\\end{aligned}\\]\n\n\n\n\n\n\n\nI’m going to be skipping steps so please look back at Lesson 10 for full steps (required in HW 4)"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-binary-categorical-and-continuous-variables-1",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-binary-categorical-and-continuous-variables-1",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Test interaction between binary categorical and continuous variables",
    "text": "Test interaction between binary categorical and continuous variables\n\nFit the reduced and full model\n\n\nm_int_inc_red = lm(LifeExpectancyYrs ~ FLR_c + income_levels2, \n                   data = gapm_sub)\nm_int_inc_full = lm(LifeExpectancyYrs ~ FLR_c + income_levels2 +\n                  FLR_c*income_levels2, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FLR_c + income_levels2\n69.000\n2,407.667\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FLR_c + income_levels2 + FLR_c * income_levels2\n68.000\n2,340.948\n1.000\n66.719\n1.938\n0.168\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and income level (p = 0.168).\n\nIf significant, we say more: For higher income levels, for every one percent increase in female literacy rate, the mean life expectancy increases 0.384 years. For lower income levels, for every one percent increase in female literacy rate, the mean life expectancy increases 0.156 years. Thus, the female literacy rate almost doubles comparing high income to low income levels."
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#do-we-think-world-region-is-an-effect-modifier-for-female-literacy-rate",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#do-we-think-world-region-is-an-effect-modifier-for-female-literacy-rate",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Do we think world region is an effect modifier for female literacy rate?",
    "text": "Do we think world region is an effect modifier for female literacy rate?\n\n\n\nWe can start by visualizing the relationship between life expectancy and female literacy rate by world region\nQuestions of interest: Does the effect of female literacy rate on life expectancy differ depending on world region?\n\nThis is the same as: Is world region is an effect modifier for female literacy rate?\n\nLet’s run an interaction model to see!"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#model-with-interaction-between-a-multi-level-categorical-and-continuous-variables",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#model-with-interaction-between-a-multi-level-categorical-and-continuous-variables",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Model with interaction between a multi-level categorical and continuous variables",
    "text": "Model with interaction between a multi-level categorical and continuous variables\nModel we are fitting:\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 FLR^c + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\\\ & \\beta_5 FLR^c \\cdot I(\\text{Americas}) + \\beta_6 FLR^c \\cdot I(\\text{Asia})+ \\beta_7 FLR^c \\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]\n\n\\(LE\\) as life expectancy\n\\(FLR^c\\) as centered female literacy rate (continuous variable)\n\\(I(\\text{Americas})\\), \\(I(\\text{Asia})\\), \\(I(\\text{Europe})\\) as the indicator for each world region\n\nIn R:\n\nm_int_wr = gapm_sub %&gt;% lm(formula = LifeExpectancyYrs ~ FLR_c + four_regions +\n                  FLR_c*four_regions)\n\nOR\n\nm_int_wr = gapm_sub %&gt;% lm(formula = LifeExpectancyYrs ~ FLR_c * four_regions)"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#displaying-the-regression-table-and-writing-fitted-regression-equation-1",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#displaying-the-regression-table-and-writing-fitted-regression-equation-1",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Displaying the regression table and writing fitted regression equation",
    "text": "Displaying the regression table and writing fitted regression equation\n\ntidy(m_int_wr, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n62.906\n2.050\n30.680\n0.000\n58.810\n67.002\n    FLR_c\n0.051\n0.053\n0.957\n0.342\n−0.055\n0.157\n    four_regionsAmericas\n12.706\n2.518\n5.046\n0.000\n7.676\n17.737\n    four_regionsAsia\n7.910\n2.477\n3.193\n0.002\n2.962\n12.859\n    four_regionsEurope\n15.732\n3.485\n4.514\n0.000\n8.770\n22.694\n    FLR_c:four_regionsAmericas\n0.164\n0.197\n0.830\n0.410\n−0.231\n0.558\n    FLR_c:four_regionsAsia\n0.061\n0.073\n0.830\n0.410\n−0.086\n0.208\n    FLR_c:four_regionsEurope\n−0.519\n0.476\n−1.090\n0.280\n−1.471\n0.432\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 FLR \\cdot I(\\text{Americas}) + \\widehat\\beta_6 FLR \\cdot I(\\text{Asia})+ \\widehat\\beta_7 FLR \\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 62.906 + 0.051 \\cdot FLR + 12.706 \\cdot I(\\text{Americas}) + 7.91 \\cdot I(\\text{Asia}) + 15.732 \\cdot I(\\text{Europe}) + \\\\ & 0.164  \\cdot FLR \\cdot I(\\text{Americas}) + 0.061 \\cdot FLR \\cdot I(\\text{Asia}) -0.519    \\cdot FLR \\cdot I(\\text{Europe})\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#comparing-fitted-regression-lines-for-each-world-region",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#comparing-fitted-regression-lines-for-each-world-region",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Comparing fitted regression lines for each world region",
    "text": "Comparing fitted regression lines for each world region\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 FLR \\cdot I(\\text{Americas}) + \\widehat\\beta_6 FLR \\cdot I(\\text{Asia})+ \\widehat\\beta_7 FLR \\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 62.906 + 0.051 \\cdot FLR + 12.706 \\cdot I(\\text{Americas}) + 7.91 \\cdot I(\\text{Asia}) + 15.732 \\cdot I(\\text{Europe}) + \\\\ & 0.164  \\cdot FLR \\cdot I(\\text{Americas}) + 0.061 \\cdot FLR \\cdot I(\\text{Asia}) -0.519    \\cdot FLR \\cdot I(\\text{Europe})\n\\end{aligned}\\]\n\n\n\n\nAfrica\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 + \\\\ & \\widehat\\beta_4 \\cdot 0 + \\widehat\\beta_5 FLR \\cdot 0 + \\\\ & \\widehat\\beta_6 FLR \\cdot 0+ \\widehat\\beta_7 FLR \\cdot 0 \\\\\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR\\\\\n\\end{aligned}\\]\n\n\n\n\n\nThe Americas\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\\\ & \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 \\cdot 0 + \\\\ & \\widehat\\beta_4 \\cdot 0 + \\widehat\\beta_5 FLR \\cdot 1 + \\\\ & \\widehat\\beta_6 FLR \\cdot 0+ \\widehat\\beta_7 FLR \\cdot 0 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_2\\big) + \\\\ &\\big(\\widehat\\beta_1 + \\widehat\\beta_5\\big)FLR \\\\\n\\end{aligned}\\]\n\n\n\n\n\nAsia\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 1 + \\\\ & \\widehat\\beta_4 \\cdot 0 + \\widehat\\beta_5 FLR \\cdot 0 + \\\\ & \\widehat\\beta_6 FLR \\cdot 1+ \\widehat\\beta_7 FLR \\cdot 0 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_3\\big) + \\\\ &\\big(\\widehat\\beta_1 + \\widehat\\beta_6\\big)FLR \\\\\n\\end{aligned}\\]\n\n\n\n\n\nEurope\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 + \\\\ & \\widehat\\beta_4 \\cdot 1 + \\widehat\\beta_5 FLR \\cdot 0 + \\\\ & \\widehat\\beta_6 FLR \\cdot 0+ \\widehat\\beta_7 FLR \\cdot 1 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_4\\big) + \\\\ & \\big(\\widehat\\beta_1 + \\widehat\\beta_7\\big)FLR \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-3",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-3",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#centering-continuous-variables-when-we-are-including-interactions",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#centering-continuous-variables-when-we-are-including-interactions",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Centering continuous variables when we are including interactions",
    "text": "Centering continuous variables when we are including interactions\n\n\nFor Europe, the mean life expectancy had a regression line with a large intercept\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_4\\big) + \\big(\\widehat\\beta_1 + \\widehat\\beta_7\\big)FLR \\\\\n\\widehat{LE} = & (58.23 + 63.63) + (0.051 - 0.519)FLR \\\\\n\\widehat{LE} = & 121.86 -0.468FLR \\\\\n\\end{aligned}\\]\n\nCentering the continuous variables in a model (when they are involved in interactions) helps with:\n\nInterpretations of the coefficient estimates\nCorrelation between the main effect for the variable and the interaction that it is involved with\n\nTo be discussed in future lecture: leads to multicollinearity issues\n\n\nOther online sources about when and when not to center:\n\nThe why and when of centering continuous predictors in regression modeling\nWhen not to center a predictor variable in regression"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#itll-be-helpful-to-center-female-literacy-rate",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#itll-be-helpful-to-center-female-literacy-rate",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "It’ll be helpful to center female literacy rate",
    "text": "It’ll be helpful to center female literacy rate\n\n\nCentering female literacy rate: \\[ FLR^c = FLR - \\overline{FLR}\\]\nCentering in R:\n\n\ngapm_sub = gapm_sub %&gt;% \n  mutate(FLR_c = FemaleLiteracyRate - median(FemaleLiteracyRate))\n\n\nI’m going to print the mean so I can use it for my interpretations\n\n\n(mean_FLR = mean(gapm_sub$FemaleLiteracyRate))\n\n[1] 82.03056\n\n\n\nNow all intercept values (in each respective world region) will be the mean life expectancy when female literacy rate is 82.03%\nWe will used center FLR for the rest of the lecture"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#now-we-refit-the-model-with-the-centered-flr",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#now-we-refit-the-model-with-the-centered-flr",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Now we refit the model with the centered FLR",
    "text": "Now we refit the model with the centered FLR\n\n\nm_int_wr_flrc = lm(LifeExpectancyYrs ~ FLR_c*four_regions, \n                data = gapm_sub)\ntidy(m_int_wr_flrc, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n62.387\n1.626\n38.358\n0.000\n59.138\n65.637\n    FLR_c\n0.051\n0.053\n0.957\n0.342\n−0.055\n0.157\n    four_regionsAmericas\n11.032\n2.918\n3.781\n0.000\n5.203\n16.862\n    four_regionsAsia\n7.287\n2.042\n3.568\n0.001\n3.207\n11.367\n    four_regionsEurope\n21.038\n7.698\n2.733\n0.008\n5.659\n36.417\n    FLR_c:four_regionsAmericas\n0.164\n0.197\n0.830\n0.410\n−0.231\n0.558\n    FLR_c:four_regionsAsia\n0.061\n0.073\n0.830\n0.410\n−0.086\n0.208\n    FLR_c:four_regionsEurope\n−0.519\n0.476\n−1.090\n0.280\n−1.471\n0.432\n  \n  \n  \n\n\n\n\n\nWhat changed? What stayed the same? What’s the new intercept for Europe?"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#interpretation-for-interaction-between-multi-level-categorical-and-continuous-variables",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#interpretation-for-interaction-between-multi-level-categorical-and-continuous-variables",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Interpretation for interaction between multi-level categorical and continuous variables",
    "text": "Interpretation for interaction between multi-level categorical and continuous variables\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 FLR \\cdot I(\\text{Americas}) + \\widehat\\beta_6 FLR \\cdot I(\\text{Asia})+ \\widehat\\beta_7 FLR \\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe})\\bigg] + \\\\ &\\underbrace{\\bigg[\\widehat\\beta_1 +  \\widehat\\beta_5 \\cdot I(\\text{Americas}) + \\widehat\\beta_6 \\cdot I(\\text{Asia})+ \\widehat\\beta_7 \\cdot I(\\text{Europe}) \\bigg]}_\\text{FLR's effect} FLR \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_5\\) = mean change in female literacy rate’s effect, comparing countries in the Americas to countries in Africa\n\\(\\beta_6\\) = mean change in female literacy rate’s effect, comparing countries in Asia to countries in Africa\n\\(\\beta_7\\) = mean change in female literacy rate’s effect, comparing countries in Europe to countries in Africa\n\nIt will be helpful to test the interaction to round out this interpretation!!"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-multi-level-categorical-continuous-variables",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-multi-level-categorical-continuous-variables",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Test interaction between multi-level categorical & continuous variables",
    "text": "Test interaction between multi-level categorical & continuous variables\n\nWe run an F-test for a group of coefficients (\\(\\beta_5\\), \\(\\beta_6\\), \\(\\beta_7\\)) in the below model (see lesson 10)\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\\\ & \\beta_5 FLR \\cdot I(\\text{Americas}) + \\beta_6 FLR \\cdot I(\\text{Asia})+ \\beta_7 FLR \\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_5= \\beta_6 = \\beta_7 =0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_5\\neq0\\) and/or \\(\\beta_6\\neq0\\) and/or \\(\\beta_7\\neq0\\)\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{Americas}) + \\\\ & \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\epsilon \\end{aligned}\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\\\ & \\beta_4 I(\\text{Europe}) + \\beta_5 FLR \\cdot I(\\text{Americas}) + \\\\ & \\beta_6 FLR \\cdot I(\\text{Asia})+ \\beta_7 FLR \\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-multi-level-categorical-continuous-variables-1",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-multi-level-categorical-continuous-variables-1",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Test interaction between multi-level categorical & continuous variables",
    "text": "Test interaction between multi-level categorical & continuous variables\n\nFit the reduced and full model\n\n\nm_int_wr_red = lm(LifeExpectancyYrs ~ FLR_c + four_regions, \n                   data = gapm_sub)\nm_int_wr_full = lm(LifeExpectancyYrs ~ FLR_c + four_regions+\n                  FLR_c*four_regions, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FLR_c + four_regions\n67.000\n1,705.881\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FLR_c + four_regions + FLR_c * four_regions\n64.000\n1,641.151\n3.000\n64.731\n0.841\n0.476\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and world region (p = 0.478).\nWorld region is NOT an effect measure modifier of FLR on LE\n\n\n\nLesson 11: Interactions Pt 1"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#do-we-think-income-level-can-be-an-effect-modifier-for-world-region",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#do-we-think-income-level-can-be-an-effect-modifier-for-world-region",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Do we think income level can be an effect modifier for world region?",
    "text": "Do we think income level can be an effect modifier for world region?\n\n\n\nTaking a break from female literacy rate to demonstrate interactions for two categorical variables\nWe can start by visualizing the relationship between life expectancy and world region by income level\nQuestions of interest: Does the effect of world region on life expectancy differ depending on income level?\n\nThis is the same as: Is income level an effect modifier for world region?\n\nLet’s run an interaction model to see!"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#model-with-interaction-between-a-multi-level-categorical-and-continuous-variables-1",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#model-with-interaction-between-a-multi-level-categorical-and-continuous-variables-1",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Model with interaction between a multi-level categorical and continuous variables",
    "text": "Model with interaction between a multi-level categorical and continuous variables\nModel we are fitting:\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\\\ & \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]\n\n\\(LE\\) as life expectancy\n\\(I(\\text{high income})\\) as indicator of high income\n\\(I(\\text{Americas})\\), \\(I(\\text{Asia})\\), \\(I(\\text{Europe})\\) as the indicator for each world region\n\nIn R:\n\n# gapm_sub = gapm_sub %&gt;% mutate(income_levels2 = relevel(income_levels2, ref = \"Higher income\")) # for poll everywhere\n\nm_int_wr_inc = lm(LifeExpectancyYrs ~ income_levels2 + four_regions +\n                  income_levels2*four_regions, data = gapm_sub)\nm_int_wr_inc = lm(LifeExpectancyYrs ~ income_levels2*four_regions, \n                data = gapm_sub)"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#displaying-the-regression-table-and-writing-fitted-regression-equation-2",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#displaying-the-regression-table-and-writing-fitted-regression-equation-2",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Displaying the regression table and writing fitted regression equation",
    "text": "Displaying the regression table and writing fitted regression equation\n\ntidy(m_int_wr_inc, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 25) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n60.850\n1.281\n47.488\n0.000\n58.290\n63.410\n    income_levels2Higher income\n2.100\n2.865\n0.733\n0.466\n−3.624\n7.824\n    four_regionsAmericas\n10.800\n3.844\n2.810\n0.007\n3.121\n18.479\n    four_regionsAsia\n7.467\n1.957\n3.815\n0.000\n3.556\n11.377\n    four_regionsEurope\n11.500\n2.865\n4.014\n0.000\n5.776\n17.224\n    income_levels2Higher income:four_regionsAmericas\n2.640\n4.896\n0.539\n0.592\n−7.141\n12.421\n    income_levels2Higher income:four_regionsAsia\n1.543\n3.956\n0.390\n0.698\n−6.360\n9.447\n    income_levels2Higher income:four_regionsEurope\n2.382\n4.020\n0.592\n0.556\n−5.649\n10.412\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\widehat\\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 60.85 + 2.10 \\cdot I(\\text{high income}) + 10.8 \\cdot I(\\text{Americas}) + 7.47\\cdot  I(\\text{Asia}) + 11.50 \\cdot I(\\text{Europe}) + \\\\ & 2.64 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + 1.54 \\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & 2.38 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-4",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-4",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#comparing-fitted-regression-means-for-each-world-region",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#comparing-fitted-regression-means-for-each-world-region",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Comparing fitted regression means for each world region",
    "text": "Comparing fitted regression means for each world region\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\widehat\\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 60.85 + 2.10 \\cdot I(\\text{high income}) + 10.8 \\cdot I(\\text{Americas}) + 7.47\\cdot  I(\\text{Asia}) + 11.50 \\cdot I(\\text{Europe}) + \\\\ & 2.64 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + 1.54 \\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & 2.38 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\nAfrica\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 + \\widehat\\beta_4 \\cdot 0 + \\\\ & \\widehat\\beta_5 I(\\text{high income}) \\cdot 0 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 0+ \\\\& \\widehat\\beta_7 I(\\text{high income}) \\cdot 0 \\\\\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income})\\\\\n\\end{aligned}\\]\n\n\n\n\n\nThe Americas\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 \\cdot 0 + \\widehat\\beta_4 \\cdot 0 + \\\\ &  \\widehat\\beta_5 I(\\text{high income}) \\cdot 1 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 0+ \\\\ & \\widehat\\beta_7 I(\\text{high income}) \\cdot 0 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_2\\big) + \\\\ &\\big(\\widehat\\beta_1 + \\widehat\\beta_5\\big)I(\\text{high income}) \\\\\n\\end{aligned}\\]\n\n\n\n\n\nAsia\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 1 + \\widehat\\beta_4 \\cdot 0 + \\\\ & \\widehat\\beta_5 I(\\text{high income}) \\cdot 0 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 1+ \\\\ & \\widehat\\beta_7 I(\\text{high income}) \\cdot 0 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_3\\big) + \\\\ &\\big(\\widehat\\beta_1 + \\widehat\\beta_6\\big)I(\\text{high income}) \\\\\n\\end{aligned}\\]\n\n\n\n\n\nEurope\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 + \\widehat\\beta_4 \\cdot 1 + \\\\ & \\widehat\\beta_5 I(\\text{high income}) \\cdot 0 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 0+ \\\\ & \\widehat\\beta_7 I(\\text{high income}) \\cdot 1 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_4\\big) + \\\\ & \\big(\\widehat\\beta_1 + \\widehat\\beta_7\\big)I(\\text{high income}) \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#comparing-fitted-regression-means-for-each-income-level",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#comparing-fitted-regression-means-for-each-income-level",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Comparing fitted regression means for each income level",
    "text": "Comparing fitted regression means for each income level\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\widehat\\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 60.85 + 2.10 \\cdot I(\\text{high income}) + 10.8 \\cdot I(\\text{Americas}) + 7.47\\cdot  I(\\text{Asia}) + 11.50 \\cdot I(\\text{Europe}) + \\\\ & 2.64 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + 1.54 \\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & 2.38 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 0 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot 0\\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot 0 \\cdot I(\\text{Asia})+ \\widehat\\beta_7 \\cdot 0\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 1 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot 1\\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot 1 \\cdot I(\\text{Asia})+ \\widehat\\beta_7 \\cdot 1\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & (\\widehat\\beta_0 + \\widehat\\beta_1)  + (\\widehat\\beta_2 + \\widehat\\beta_5) I(\\text{Americas}) + (\\widehat\\beta_3 + \\widehat\\beta_6)  I(\\text{Asia}) + \\\\ & (\\widehat\\beta_4 + \\widehat\\beta_7)  I(\\text{Europe}) \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#lets-take-a-look-back-at-the-plot-1",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#lets-take-a-look-back-at-the-plot-1",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Let’s take a look back at the plot",
    "text": "Let’s take a look back at the plot\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\\\ & \\widehat\\beta_4 I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & (\\widehat\\beta_0 + \\widehat\\beta_1)  + (\\widehat\\beta_2 + \\widehat\\beta_5) I(\\text{Americas}) + \\\\& (\\widehat\\beta_3 + \\widehat\\beta_6)  I(\\text{Asia}) +  (\\widehat\\beta_4 + \\widehat\\beta_7)  I(\\text{Europe}) \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#interpretation-for-interaction-between-two-categorical-variables",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#interpretation-for-interaction-between-two-categorical-variables",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Interpretation for interaction between two categorical variables",
    "text": "Interpretation for interaction between two categorical variables\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot I(\\text{high income})\\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\widehat\\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{high income})\\bigg]  + \\bigg[\\widehat\\beta_2 + \\widehat\\beta_5 \\cdot I(\\text{high income})\\bigg] I(\\text{Americas}) + \\\\ & \\bigg[\\widehat\\beta_3 + \\widehat\\beta_6 \\cdot I(\\text{high income})\\bigg]  I(\\text{Asia}) +  \\bigg[\\widehat\\beta_4 + \\widehat\\beta_7 \\cdot I(\\text{high income})\\bigg]  I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_1\\) = mean change in the Africa’s life expectancy, comparing high income to low income countries\n\\(\\beta_5\\) = mean change in the Americas’ effect, comparing high income to low income countries\n\\(\\beta_6\\) = mean change in Asia’s effect, comparing high income to low income countries\n\\(\\beta_7\\) = mean change in Europe’s effect, comparing high income to low income countries"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-two-categorical-variables",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-two-categorical-variables",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Test interaction between two categorical variables",
    "text": "Test interaction between two categorical variables\n\nWe run an F-test for a group of coefficients (\\(\\beta_5\\), \\(\\beta_6\\), \\(\\beta_7\\)) in the below model (see lesson 9)\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\\\ & \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_5= \\beta_6 = \\beta_7 =0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_5\\neq0\\) and/or \\(\\beta_6\\neq0\\) and/or \\(\\beta_7\\neq0\\)\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\\\& \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\epsilon \\end{aligned}\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\\\ & \\beta_4 I(\\text{Europe}) + \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\\\ & \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-multi-level-categorical-continuous-variables-2",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#test-interaction-between-multi-level-categorical-continuous-variables-2",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Test interaction between multi-level categorical & continuous variables",
    "text": "Test interaction between multi-level categorical & continuous variables\n\nFit the reduced and full model\n\n\nm_int_wr_inc_red = lm(LifeExpectancyYrs ~ income_levels2 + four_regions, \n                   data = gapm_sub)\nm_int_wr_inc_full = lm(LifeExpectancyYrs ~ income_levels2 + four_regions +\n                          income_levels2*four_regions, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ income_levels2 + four_regions\n67.000\n1,693.242\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ income_levels2 + four_regions + income_levels2 * four_regions\n64.000\n1,681.304\n3.000\n11.938\n0.151\n0.928\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and income level (p = 0.928)."
  },
  {
    "objectID": "lessons/12_Interactions_02/01_Review_key_info.html#key-dates",
    "href": "lessons/12_Interactions_02/01_Review_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "",
    "text": "Understand equations and visualizations that helps us build multiple linear regression model.\nFit MLR model (in R) and understand the difference between fitted regression plane and regression lines.\nIdentify the population multiple linear regression model and define statistics language for key notation.\nBased off of previous SLR work, understand how the population MLR is estimated.\nInterpret MLR (population) coefficient estimates with additional variable in model\n\n\n\n\nSLR helped us establish the foundation for a lot of regression\n\nBut we do not usually use SLR in analysis\n\n\nWhat did we learn in SLR??\n\n\n\n\nModel Fitting\n\n\n\nOrdinary least squares (OLS)\nlm() function in R\n\n\n\n\n\n\nModel Use\n\n\n\nInference for variance of residuals\nHypothesis testing for coefficients\nInterpreting population coefficient estimates\nCalculated the expected mean for specific \\(X\\) values\nInterpreted coefficient of determination\n\n\n\n\n\n\nModel Evaluation/Diagnostics\n\n\n\nLINE Assumptions\nInfluential points\nData Transformations\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#reminder-of-what-we-learned-in-the-context-of-slr",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#reminder-of-what-we-learned-in-the-context-of-slr",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Reminder of what we learned in the context of SLR",
    "text": "Reminder of what we learned in the context of SLR\n\nSLR helped us establish the foundation for a lot of regression\n\nBut we do not usually use SLR in analysis\n\n\nWhat did we learn in SLR??\n\n\n\n\nModel Fitting\n\n\n\nOrdinary least squares (OLS)\nlm() function in R\n\n\n\n\n\n\nModel Use\n\n\n\nInference for variance of residuals\nHypothesis testing for coefficients\nInterpreting population coefficient estimates\nCalculated the expected mean for specific \\(X\\) values\nInterpreted coefficient of determination\n\n\n\n\n\n\nModel Evaluation/Diagnostics\n\n\n\nLINE Assumptions\nInfluential points\nData Transformations"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#lets-map-that-to-our-regression-analysis-process",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#lets-map-that-to-our-regression-analysis-process",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Let’s map that to our regression analysis process",
    "text": "Let’s map that to our regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#simple-linear-regression-vs.-multiple-linear-regression",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#simple-linear-regression-vs.-multiple-linear-regression",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Simple Linear Regression vs. Multiple Linear Regression",
    "text": "Simple Linear Regression vs. Multiple Linear Regression\n\n\n\nSimple Linear Regression\n\n \n\nWe use one predictor to try to explain the variance of the outcome\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\\]\n\n\n\nMultiple Linear Regression\n\n \n\nWe use multiple predictors to try to explain the variance of the outcome\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_{k}X_{k}+ \\epsilon\n\\]\n\n \n\nHas \\(k+1\\) total coefficients (including intercept) for \\(k\\) predictors/covariates\nSometimes referred to as multivariable linear regression, but never multivariate\n\n\n\n \n\nThe models have similar “LINE” assumptions and follow the same general diagnostic procedure"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#going-back-to-our-life-expectancy-example",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#going-back-to-our-life-expectancy-example",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Going back to our life expectancy example",
    "text": "Going back to our life expectancy example\n\nLet’s say many other variables were measured for each country, including food supply\n\nFood Supply (kilocalories per person per day, kc PPD): the average kilocalories consumed by a person each day.\n\nIn SLR, we only had one predictor and one outcome in the model:\n\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same.\nAdult literacy rate is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life.\n\n\n \n\nDo we think adult female literacy rate is going to explain a lot of the variance of life expectancy between countries?"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#loading-the-new-ish-data",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#loading-the-new-ish-data",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Loading the (new-ish) data",
    "text": "Loading the (new-ish) data\n\n# Load the data - update code if the csv file is not in the same location on your computer\n# If you need to download the file, please go to ur shared folder under Data &gt; Slides\ngapm &lt;- read_excel(\"data/Gapminder_vars_2011.xlsx\", \n                   na = \"NA\")  # important!!!! \n\ngapm_sub &lt;- gapm %&gt;% \n  drop_na(LifeExpectancyYrs, FemaleLiteracyRate, FoodSupplykcPPD)\n\nglimpse(gapm_sub)\n\nRows: 72\nColumns: 18\n$ country                            &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\",…\n$ CO2emissions                       &lt;dbl&gt; 0.4120, 1.7900, 1.2500, 5.3600, 4.6…\n$ ElectricityUsePP                   &lt;dbl&gt; NA, 2210, 207, NA, 2900, 1810, 258,…\n$ FoodSupplykcPPD                    &lt;dbl&gt; 2110, 3130, 2410, 2370, 3160, 2790,…\n$ IncomePP                           &lt;dbl&gt; 1660, 10200, 5910, 18600, 19600, 70…\n$ LifeExpectancyYrs                  &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8,…\n$ FemaleLiteracyRate                 &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5,…\n$ population                         &lt;dbl&gt; 2.97e+07, 2.93e+06, 2.42e+07, 9.57e…\n$ WaterSourcePrct                    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8,…\n$ geo                                &lt;chr&gt; \"afg\", \"alb\", \"ago\", \"atg\", \"arg\", …\n$ four_regions                       &lt;chr&gt; \"asia\", \"europe\", \"africa\", \"americ…\n$ eight_regions                      &lt;chr&gt; \"asia_west\", \"europe_east\", \"africa…\n$ six_regions                        &lt;chr&gt; \"south_asia\", \"europe_central_asia\"…\n$ members_oecd_g77                   &lt;chr&gt; \"g77\", \"others\", \"g77\", \"g77\", \"g77…\n$ Latitude                           &lt;dbl&gt; 33.00000, 41.00000, -12.50000, 17.0…\n$ Longitude                          &lt;dbl&gt; 66.00000, 20.00000, 18.50000, -61.8…\n$ `World bank region`                &lt;chr&gt; \"South Asia\", \"Europe & Central Asi…\n$ `World bank, 4 income groups 2017` &lt;chr&gt; \"Low income\", \"Upper middle income\"…"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#can-we-improve-our-model-by-adding-food-supply-as-a-covariate",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#can-we-improve-our-model-by-adding-food-supply-as-a-covariate",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Can we improve our model by adding food supply as a covariate?",
    "text": "Can we improve our model by adding food supply as a covariate?\n\n\n\n\n\n\nSimple linear regression population model\n\n\n\\[\\begin{aligned}\n\\text{Life expectancy} & = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\epsilon \\\\\n\\text{LE} & = \\beta_0 + \\beta_1 \\text{FLR} + \\epsilon\n\\end{aligned}\\]\n\n\n\n\nMultiple linear regression population model (with added Food Supply)\n\n\n\\[\\begin{aligned}\n\\text{Life expectancy} & = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_2 \\text{Food supply} + \\epsilon \\\\\n\\text{LE} & = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#visualize-relationship-between-life-expectancy-female-literacy-rate-and-food-supply",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#visualize-relationship-between-life-expectancy-female-literacy-rate-and-food-supply",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Visualize relationship between life expectancy, female literacy rate, and food supply",
    "text": "Visualize relationship between life expectancy, female literacy rate, and food supply"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#visualize-relationship-in-3-d",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#visualize-relationship-in-3-d",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Visualize relationship in 3-D",
    "text": "Visualize relationship in 3-D"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#poll-everywhere-question-1",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#poll-everywhere-question-1",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#how-do-we-fit-a-multiple-linear-regression-model-in-r",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#how-do-we-fit-a-multiple-linear-regression-model-in-r",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "How do we fit a multiple linear regression model in R?",
    "text": "How do we fit a multiple linear regression model in R?\nNew population model for example:\n\\[\\text{Life expectancy} = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_2 \\text{Food supply} + \\epsilon\\]\n\n# Fit regression model:\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\ntidy(mr1, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{Life expectancy}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{Female literacy rate} + \\widehat{\\beta}_2 \\text{Food supply} \\\\\n\\widehat{\\text{Life expectancy}} &= 33.595 + 0.157\\ \\text{Female literacy rate}\n+ 0.008\\ \\text{Food supply}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#dont-forget-summary-to-extract-information",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#dont-forget-summary-to-extract-information",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Don’t forget summary() to extract information!",
    "text": "Don’t forget summary() to extract information!\n\nsummary(mr1)\n\n\nCall:\nlm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n    data = gapm_sub)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.715  -2.328   1.052   3.022   9.083 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        33.595479   4.472049   7.512 1.56e-10 ***\nFemaleLiteracyRate  0.156699   0.032158   4.873 6.75e-06 ***\nFoodSupplykcPPD     0.008482   0.001795   4.726 1.17e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.391 on 69 degrees of freedom\nMultiple R-squared:  0.563, Adjusted R-squared:  0.5503 \nF-statistic: 44.44 on 2 and 69 DF,  p-value: 3.958e-13"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#visualize-the-fitted-multiple-regression-model",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#visualize-the-fitted-multiple-regression-model",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Visualize the fitted multiple regression model",
    "text": "Visualize the fitted multiple regression model\n\n\n\nThe fitted model equation \\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X_1 + \\widehat{\\beta}_2 \\cdot X_2\\] has three variables (\\(Y, X_1,\\) and \\(X_2\\)) and thus we need 3 dimensions to plot it\n\n \n\nInstead of a regression line, we get a regression plane\n\nSee code in .qmd- file. I hid it from view in the html file."
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#regression-lines-for-varying-values-of-food-supply",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#regression-lines-for-varying-values-of-food-supply",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Regression lines for varying values of food supply",
    "text": "Regression lines for varying values of food supply\n\\[\\begin{aligned}\n\\widehat{\\text{Life expectancy}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{Female literacy rate} + \\widehat{\\beta}_2 \\text{Food supply} \\\\\n\\widehat{\\text{Life expectancy}} &= 33.595 + 0.157 \\text{ Female literacy rate}\n+ 0.008 \\text{ Food supply}\n\\end{aligned}\\]\n\n\n\nNote: when the food supply is held constant but the female literacy rate varies…\n\nthen the outcome values change along a line\n\nDifferent values of food supply give different lines\n\nThe intercepts change, but\nthe slopes stay the same (parallel lines)\n\n\n\n\n(mr1_2d = ggPredict(mr1, interactive = T))"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#how-do-we-calculate-the-regression-line-for-3000-kc-ppd-food-supply",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#how-do-we-calculate-the-regression-line-for-3000-kc-ppd-food-supply",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "How do we calculate the regression line for 3000 kc PPD food supply?",
    "text": "How do we calculate the regression line for 3000 kc PPD food supply?\n\n\n \n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\ \\text{FS}\\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\cdot 3000 \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 24 \\\\\n\\widehat{\\text{LE}} &= 57.6 + 0.157\\ \\text{FLR}\n\\end{aligned}\\]\n\n\n(mr1_2d = ggPredict(mr1, interactive = T))"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#poll-everwhere-question-2",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#poll-everwhere-question-2",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everwhere Question 2",
    "text": "Poll Everwhere Question 2"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#population-multiple-regression-model",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#population-multiple-regression-model",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Population multiple regression model",
    "text": "Population multiple regression model\n\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\nor on the individual (observation) level:\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}+ \\ldots + \\beta_k x_{ik} + \\epsilon_i,\\ \\ \\text{for}\\ i = 1, 2, \\ldots, n\\]\n\n\nObservable sample data\n\n\\(Y\\) is our dependent variable\n\nAka outcome or response variable\n\n\\(X_1, X_2, \\ldots, X_k\\) are our \\(k\\) independent variables\n\nAka predictors or covariates\n\n\n\nUnobservable population parameters\n\n\\(\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k\\) are unknown population parameters\n\nFrom our sample, we find the population parameter estimates: \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, \\ldots, \\widehat{\\beta}_k\\)\n\n\\(\\epsilon\\) is the random error\n\nAnd is still normally distributed\n\\(\\epsilon \\sim N(0, \\sigma^2)\\) where \\(\\sigma^2\\) is the population parameter of the variance"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#how-do-we-estimate-the-model-parameters",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#how-do-we-estimate-the-model-parameters",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "How do we estimate the model parameters?",
    "text": "How do we estimate the model parameters?\n\nWe need to estimate the population model coefficients \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, \\ldots, \\widehat{\\beta}_k\\)\nThis can be done using the ordinary least-squares method\n\nFind the \\(\\widehat{\\beta}\\) values that minimize the sum of squares due to error (\\(SSE\\))\n\n\n\\[ \\begin{aligned}\nSSE & = \\displaystyle\\sum^n_{i=1} \\widehat\\epsilon_i^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{Y}_i)^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - (\\widehat{\\beta}_0 +\\widehat{\\beta}_1 X_{i1}+ \\ldots+\\widehat{\\beta}_1 X_{ik}))^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0 -\\widehat{\\beta}_1 X_{i1}- \\ldots-\\widehat{\\beta}_1 X_{ik})^2\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#technical-side-note-not-needed-in-our-class",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#technical-side-note-not-needed-in-our-class",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Technical side note (not needed in our class)",
    "text": "Technical side note (not needed in our class)\n\nThe equations for calculating the \\(\\boldsymbol{\\widehat{\\beta}}\\) values is best done using matrix notation (not required for our class)\nWe will be using R to get the coefficients instead of the equation (already did this a few slides back!)\nHow we have represented the population regression model: \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\n\n\n\nHow to represent population model with matrix notation:\n\n\\[\\begin{aligned}\n\\boldsymbol{Y} &= \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\\\\n\\boldsymbol{Y}_{n \\times 1}& = \\boldsymbol{X}_{n \\times (k+1)}\\boldsymbol{\\beta}_{(k+1)\\times 1} + \\boldsymbol{\\epsilon}_{n \\times 1}\n\\end{aligned}\\]\n\n\\[\n\\boldsymbol{Y} = \\left[\\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\n\\end{array} \\right]_{n \\times 1}\n\\] \\[\n\\boldsymbol{\\epsilon} = \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n\n\\end{array} \\right]_{n \\times 1}  \n\\]\n\n\\[\n\\boldsymbol{X} = \\left[ \\begin{array}{ccccc} 1 &  X_{11} &  X_{12} & \\ldots & X_{1,k} \\\\\n1 &X_{21} &  X_{22} & \\ldots & X_{2,k} \\\\\n\\vdots&\\vdots & \\vdots &  \\ldots & \\vdots \\\\\n1 & X_{n1} &  X_{n2} & \\ldots & X_{n,k} \\end{array} \\right]_{n \\times (k+1)}\n\\]\n\\[\n\\boldsymbol{\\beta}  = \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1\\\\  \\vdots \\\\\n\\beta_{k}\n\\end{array} \\right]_{(k+1)\\times 1}\n\\]"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#line-model-assumptions",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#line-model-assumptions",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "LINE model assumptions",
    "text": "LINE model assumptions\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nThe mean value of \\(Y\\) given any combination of \\(X_1, X_2, \\ldots, X_k\\) values, is a linear function of \\(\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k\\):\n\\[\\mu_{Y|X_1, \\ldots, X_k} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k\\]\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nObservations (\\(X_1, X_2, \\ldots, X_k, Y\\)) are independent from one another\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\n\\(Y\\) has a normal distribution for any any combination of \\(X_1, X_2, \\ldots, X_k\\) values\n\nThus, the residuals are normally distributed\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nThe variance of \\(Y\\) is the same for any any combination of \\(X_1, X_2, \\ldots, X_k\\) values\n\\[\\sigma^2_{Y|X_1, X_2, \\ldots, X_k} = Var(Y|X_1, X_2, \\ldots, X_k) = \\sigma^2\\]"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#summary-of-the-line-assumptions",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#summary-of-the-line-assumptions",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Summary of the LINE assumptions",
    "text": "Summary of the LINE assumptions\n\nEquivalently, the residuals are independently and identically distributed (iid):\n\nnormal\nwith mean 0 and\nconstant variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#variation-explained-vs.-unexplained",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#variation-explained-vs.-unexplained",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Variation: Explained vs. Unexplained",
    "text": "Variation: Explained vs. Unexplained\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\bar{Y})^2 &= \\sum_{i=1}^n (\\widehat{Y}_i- \\bar{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY &= SSR + SSE\n\\end{aligned}\\]\n\n\\(Y_i - \\bar{Y}\\) = the deviation of \\(Y_i\\) around the mean \\(\\bar{Y}\\)\n\n(the total amount deviation unexplained at \\(X_{i1},\\ldots,X_{ik}\\) ).\n\n\\(\\widehat{Y}_i- \\bar{Y}\\) = the deviation of the fitted value \\(\\widehat{Y}_i\\) around the mean \\(\\bar{Y}\\)\n\n(the amount deviation explained by the regression at \\(X_{i1},\\ldots,X_{ik}\\) ).\n\n\\(Y_i - \\widehat{Y}_i\\) = the deviation of the observation \\(Y\\) around the fitted regression line\n\n(the amount deviation unexplained by the regression at \\(X_{i1},\\ldots,X_{ik}\\) )."
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#poll-everywhere-question-3",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#poll-everywhere-question-3",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#building-the-anova-table",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#building-the-anova-table",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Building the ANOVA table",
    "text": "Building the ANOVA table\nANOVA table (\\(k\\) = # of predictors, \\(n\\) = # of observations)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariation Source\ndf\nSS\nMS\ntest statistic\np-value\n\n\n\n\nRegression\n\\(k\\)\n\\(SSR\\)\n\\(MSR = \\frac{SSR}{k}\\)\n\\(F = \\frac{MSR}{MSE}\\)\n\\(F \\sim F_{(k, n-k-1)}\\)\n\n\nError\n\\(n-k-1\\)\n\\(SSE\\)\n\\(MSE = \\frac{SSE}{n-k-1}\\)\n\n\n\n\nTotal\n\\(n-1\\)\n\\(SSY\\)\n\n\n\n\n\n\n\n\n \n\nanova(mr1) %&gt;% tidy() %&gt;% gt() %&gt;%\n   tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1.000\n1,934.245\n1,934.245\n66.547\n0.000\n    FoodSupplykcPPD\n1.000\n649.319\n649.319\n22.339\n0.000\n    Residuals\n69.000\n2,005.556\n29.066\nNA\nNA"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#interpreting-the-estimated-population-coefficients",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#interpreting-the-estimated-population-coefficients",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Interpreting the estimated population coefficients",
    "text": "Interpreting the estimated population coefficients\n\nFor a population model: \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\epsilon\\]\n\nWhere \\(X_1\\) and \\(X_2\\) are continuous variables\nNo need to specify \\(Y\\) because it required to be continuous in linear regression\n\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB)."
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#poll-everywhere-question-4",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#poll-everywhere-question-4",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#getting-these-interpretations-from-our-regression-table",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#getting-these-interpretations-from-our-regression-table",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Getting these interpretations from our regression table",
    "text": "Getting these interpretations from our regression table\nWe fit the regression model in R and printed the regression table:\n\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model: \\(\\widehat{\\text{LE}} = 33.595 + 0.157 \\text{ FLR} + 0.008 \\text{ FS}\\)\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 0 kcal PPD (95% CI: 24.674, 41.517).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012)."
  },
  {
    "objectID": "lessons/08_MLR_Intro/08_MLR_Intro.html#lets-just-examine-the-general-interpretation-vs.-the-example",
    "href": "lessons/08_MLR_Intro/08_MLR_Intro.html#lets-just-examine-the-general-interpretation-vs.-the-example",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "Let’s just examine the general interpretation vs. the example",
    "text": "Let’s just examine the general interpretation vs. the example\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 0 kcal PPD (95% CI: 24.674, 41.517).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012).\n\n\n\n\n\n\nLesson 8: MLR 1"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred_key_info.html#key-dates",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates\n\nLab 1 due this Thursday at 11pm"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "",
    "text": "Describe the model assumptions made in linear regression using ordinary least squares\nDetermine if the relationship between our sampled X and Y is linear\nUse QQ plots to determine if our fitted model holds the normality assumption\nUse residual plots to determine if our fitted model holds the equality of variance assumption\n\n\n\n\nWe have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]\n\n\n\n\n\nRows: 188 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (2): life_expectancy_years_2011, female_literacy_rate_2011\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe residuals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "Let’s remind ourselves of the model that we have been working with",
    "text": "Let’s remind ourselves of the model that we have been working with\n\nWe have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "Our residuals will help us a lot in our diagnostics!",
    "text": "Our residuals will help us a lot in our diagnostics!\n\n\n\nThe residuals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#least-squares-model-assumptions-line",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#least-squares-model-assumptions-line",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "Least-squares model assumptions: LINE",
    "text": "Least-squares model assumptions: LINE\n \nThese are the model assumptions made in ordinary least squares:\n \n\n[L] Linearity of relationship between variables\n\n\n[I] Independence of the \\(Y\\) values\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\n[E] Equality of variance of the residuals (homoscedasticity)"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#l-linearity",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#l-linearity",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "L: Linearity",
    "text": "L: Linearity\n\n\n\nThe relationship between the variables is linear (a straight line):\n\nThe mean value of \\(Y\\) given \\(X\\), \\(\\mu_{y|x}\\) or \\(E[Y|X]\\), is a straight-line function of \\(X\\)\n\n\n\\[\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\]"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#i-independence-of-observations",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#i-independence-of-observations",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "I: Independence of observations",
    "text": "I: Independence of observations\n\nThe \\(Y\\)-values are statistically independent of one another\nExamples of when they are not independent, include\n\nrepeated measures (such as baseline, 3 months, 6 months)\ndata from clusters, such as different hospitals or families\n\nThis condition is checked by reviewing the study design and not by inspecting the data\n\n \n\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#poll-everywhere-question-1",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#poll-everywhere-question-1",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-normality",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-normality",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "N: Normality",
    "text": "N: Normality\n\n\n\nFor any fixed value of \\(X\\), \\(Y\\) has normal distribution.\n\nNote: This is not about \\(Y\\) alone, but \\(Y|X\\)\n\nEquivalently, the measurement (random) errors \\(\\epsilon_i\\) ’s normally distributed\n\nThis is more often what we check"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#e-equality-of-variance-of-the-residuals",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#e-equality-of-variance-of-the-residuals",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "E: Equality of variance of the residuals",
    "text": "E: Equality of variance of the residuals\n\nThe variance of \\(Y\\) given \\(X\\) (\\(\\sigma_{Y|X}^2\\)), is the same for any \\(X\\)\n\nWe use just \\(\\sigma^2\\) to denote the common variance\n\nThis is also called homoscedasticity"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#summary-of-line-model-assumptions",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#summary-of-line-model-assumptions",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "Summary of LINE model assumptions",
    "text": "Summary of LINE model assumptions\n\n\\(Y\\) values are independent (check study design!)\n\n\n\n\n\n\nThe distribution of \\(Y\\) given \\(X\\) is\n\nnormal\nwith mean \\(\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\)\nand common variance \\(\\sigma^2\\)\n\n\nThis means that the residuals are\n\nnormal\nwith mean = 0\nand common variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#how-do-we-determine-if-our-model-follows-the-line-assumptions",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#how-do-we-determine-if-our-model-follows-the-line-assumptions",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "How do we determine if our model follows the LINE assumptions?",
    "text": "How do we determine if our model follows the LINE assumptions?\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nCheck if there is a linear relationship between the mean response (Y) and the explanatory variable (X)\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nCheck that the observations are independent\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\nCheck that the responses (at each level X) are normally distributed\n\nUsually measured through the residuals\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nCheck that the variance (or standard deviation) of the responses is equal for all levels of X\n\nUsually measured through the residuals"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#l-linearity-of-relationship-between-variables",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#l-linearity-of-relationship-between-variables",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "L: Linearity of relationship between variables",
    "text": "L: Linearity of relationship between variables\n\n\n\n\n\nIs the association between the variables linear?\n\n\n\n\nDiagnostic tool: Scatterplot of \\(X\\) vs. \\(Y\\)"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#poll-everywhere-question-2",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#poll-everywhere-question-2",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#i-independence-of-the-residuals-y-values",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#i-independence-of-the-residuals-y-values",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "I: Independence of the residuals (\\(Y\\) values)",
    "text": "I: Independence of the residuals (\\(Y\\) values)\n\nAre the data points independent of each other?\n\n \n\nDiagnostic tool: reviewing the study design and not by inspecting the data"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-normality-of-the-residuals",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-normality-of-the-residuals",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "N: Normality of the residuals",
    "text": "N: Normality of the residuals\n\nWe need to check if the errors/residuals (\\(\\epsilon_i\\)’s) are normally distributed\n\n \n\nDiagnostic tools:\n\nDistribution plots of residuals\nQQ plots of residuals\n\n\n \n\nExtra resource on how QQ plots are made"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-extract-models-residuals-in-r",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-extract-models-residuals-in-r",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "N: Extract model’s residuals in R",
    "text": "N: Extract model’s residuals in R\n\nFirst extract the residuals’ values from the model output using the augment() function from the broom package.\nGet a tibble with the orginal data, as well as the residuals and some other important values.\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, \n                data = gapm)\naug1 &lt;- augment(model1) \n\nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-check-normality-with-usual-distribution-plots",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-check-normality-with-usual-distribution-plots",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "N: Check normality with “usual” distribution plots",
    "text": "N: Check normality with “usual” distribution plots\nNote that below I save each figure as an object, and then combine them together in one row of output using grid.arrange() from the gridExtra package\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_boxplot()\n\ngrid.arrange(hist1, density1, box1, nrow = 1)"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-normal-qq-plots-qq-quantile-quantile",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-normal-qq-plots-qq-quantile-quantile",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "N: Normal QQ plots (QQ = quantile-quantile)",
    "text": "N: Normal QQ plots (QQ = quantile-quantile)\n\nIt can be tricky to eyeball with a histogram or density plot whether the residuals are normal or not\nQQ plots are often used to help with this\n\n\n\n\nVertical axis: data quantiles\n\ndata points are sorted in order and\nassigned quantiles based on how many data points there are\n\nHorizontal axis: theoretical quantiles\n\nmean and standard deviation (SD) calculated from the data points\ntheoretical quantiles are calculated for each point, assuming the data are modeled by a normal distribution with the mean and SD of the data\n\n\n \n\nData are approximately normal if points fall on a line."
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-examples-of-normal-qq-plots-from-n100-observations",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-examples-of-normal-qq-plots-from-n100-observations",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "N: Examples of Normal QQ plots (from \\(n=100\\) observations)",
    "text": "N: Examples of Normal QQ plots (from \\(n=100\\) observations)\n\n\n\n\nNormal\n\nUniform\n\nT\n\nSkewed"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-examples-of-normal-qq-plots-from-n10-observations",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-examples-of-normal-qq-plots-from-n10-observations",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "N: Examples of Normal QQ plots (from \\(n=10\\) observations)",
    "text": "N: Examples of Normal QQ plots (from \\(n=10\\) observations)\n\n\n\n\nNormal\n\nUniform\n\nT\n\nSkewed"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-examples-of-normal-qq-plots-from-n1000-observations",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-examples-of-normal-qq-plots-from-n1000-observations",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "N: Examples of Normal QQ plots (from \\(n=1000\\) observations)",
    "text": "N: Examples of Normal QQ plots (from \\(n=1000\\) observations)\n\n\n\n\nNormal\n\nUniform\n\nT\n\nSkewed"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-we-can-compare-the-qq-plots-model-vs.-theoretical",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-we-can-compare-the-qq-plots-model-vs.-theoretical",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "N: We can compare the QQ plots: model vs. theoretical",
    "text": "N: We can compare the QQ plots: model vs. theoretical\n\n\n\nResiduals from Life Expectancy vs. Female Literacy Rate Regression\n\n\nggplot(aug1, \n      aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line() \n\n\n\n\n\n\n\n\n\n\nSimulated QQ plot of Normal Residuals with \\(n = 80\\)\n\n\nggplot() +\n  stat_qq(aes(\n    sample = rnorm(80))) + \n  geom_abline(\n    intercept = 0, slope = 1, \n    color = \"blue\")"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-shapiro-wilk-test-of-normality",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#n-shapiro-wilk-test-of-normality",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "N: Shapiro-Wilk Test of Normality",
    "text": "N: Shapiro-Wilk Test of Normality\n\nGoodness-of-fit test for the normal distribution: Is there evidence that our residuals are from a normal distribution?\nHypothesis test:\n\n\\[\\begin{aligned}\nH_0 & : \\text{data are from a normally distributed population} \\\\\nH_1 & : \\text{data are NOT from a normally distributed population}\n\\end{aligned}\\]\n\n\n\nshapiro.test(aug1$.resid)\n\n\n    Shapiro-Wilk normality test\n\ndata:  aug1$.resid\nW = 0.90575, p-value = 2.148e-05\n\n\n\n\n\nConclusion\n\n\nReject the null. Data are not from a normal distribution."
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#e-equality-of-variance-of-the-residuals-1",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#e-equality-of-variance-of-the-residuals-1",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "E: Equality of variance of the residuals",
    "text": "E: Equality of variance of the residuals\n\nHomoscedasticity: How do we determine if the variance across X values is constant?\nDiagnostic tool: residual plot"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#e-creating-a-residual-plot",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#e-creating-a-residual-plot",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "E: Creating a residual plot",
    "text": "E: Creating a residual plot\n\n\n\n\\(x\\) = explanatory variable from regression model\n\n(or the fitted values for a multiple regression)\n\n\\(y\\) = residuals from regression model\n\n\nggplot(aug1, \n       aes(x = female_literacy_rate_2011, \n           y = .resid)) + \n  geom_point(size = 2) +\n  geom_abline( intercept = 0, slope = 0,\n    size = 2, color = \"#FF8021\") +\n  labs(title = \"Residual plot\") +\n  theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 30))"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#autoplot-can-be-a-helpful-tool",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#autoplot-can-be-a-helpful-tool",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "autoplot() can be a helpful tool",
    "text": "autoplot() can be a helpful tool\n\nlibrary(ggfortify)\nautoplot(model1) + theme(text=element_text(size=14))"
  },
  {
    "objectID": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#summary-of-the-assumptions-and-their-diagnostic-tool",
    "href": "lessons/06_SLR_Diag_01/06_SLR_Diag_01.html#summary-of-the-assumptions-and-their-diagnostic-tool",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "Summary of the assumptions and their diagnostic tool",
    "text": "Summary of the assumptions and their diagnostic tool\n\n\n\n\n\n\n\n\nAssumption\nWhat needs to hold?\nDiagnostic tool\n\n\n\n\nLinearity\n\\(\\text{}\\)\n\nRelationship between \\(X\\) and \\(Y\\) is linear\n\n\nScatterplot of \\(Y\\) vs. \\(X\\)\n\n\\(\\text{}\\)\n\n\nIndependence\n\\(\\text{}\\)\n\nObservations are independent from each other\n\n\nStudy design\n\n\\(\\text{}\\)\n\n\nNormality\n\\(\\text{}\\)\n\nResiduals (and thus \\(Y|X\\)) are normally distributed\n\n\nQQ plot of residuals\nDistribution of residuals\n\n\n\nEquality of variance\n\\(\\text{}\\)\n\nVariance of residuals (and thus \\(Y|X\\)) is same across \\(X\\) values (homoscedasticity)\n\n\nResidual plot\n\n\\(\\text{}\\)\n\n\n\n\n\nLesson 6: SLR 4"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html",
    "title": "Lesson 13: Purposeful model selection",
    "section": "",
    "text": "Understand the overall steps for purposeful selection as a model building strategy\nApply purposeful selection to a dataset using R\nUse different approaches to assess the linear scale of continuous variables in logistic regression\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#regression-analysis-process",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#regression-analysis-process",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Regression analysis process",
    "text": "Regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#section",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#section",
    "title": "Lesson 14: Purposeful model selection",
    "section": "",
    "text": "“Successful modeling of a complex data set is part science, part statistical methods, and part experience and common sense.”\n\n \n\nHosmer, Lemeshow, and Sturdivant Textbook, pg. 101"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#overall-process",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#overall-process",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Overall Process",
    "text": "Overall Process\n\nExploratory data analysis\nCheck unadjusted associations in simple linear regression\nEnter all covariates in model that meet some threshold\n\nOne textbook suggest \\(p&lt;0.2\\) or \\(p&lt;0.25\\): great for modest sized datasets\nPLEASE keep in mind sample size in your study\nCan also use magnitude of association rather than, or along with, p-value\n\nRemove those that no longer reach some threshold\n\nCompare magnitude of associations to unadjusted version (univariable)\n\nCheck scaling of continuous and coding of categorical covariates\nCheck for interactions\nAssess model fit\n\nModel assumptions, diagnostics, overall fit"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#process-with-snappier-step-names",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#process-with-snappier-step-names",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Process with snappier step names",
    "text": "Process with snappier step names\n\n\nPre-step:\n \nStep 1:\n \nStep 2:\n \nStep 3:\n \nStep 4:\n \nStep 5:\n \nStep 6:\n\nExploratory data analysis (EDA)\n \nSimple linear regressions / analysis\n \nPreliminary variable selection\n \nAssess change in coefficients\n \nAssess scale for continuous variables\n \nCheck for interactions\n \nAssess model fit"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis",
    "text": "Pre-step: Exploratory data analysis\n\nThings we have been doing over the quarter in class and in our project\nI will not discuss some of the methods mentioned in our lab and data management class\n\nI am only going to introduce additional exploratory functions\n\n\n \nA few things we can do:\n\nCheck the data\nStudy your variables\nMissing data?\nExplore simple relationships and assumptions"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-check-the-data",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-check-the-data",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Check the data",
    "text": "Pre-step: Exploratory data analysis: Check the data\n\n\n\nGet to know the potential values for the data\n\nCategories\nUnits\n\nThen make sure the summary of values makes sense\n\nIf minimum or maximum look outside appropriate range\nFor example: a negative value for a measurement that is inherently positive (like population or income)\n\n\n\n\n\n\nhttps://www.gapminder.org/data/documentation/"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-check-the-data-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-check-the-data-1",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Check the data",
    "text": "Pre-step: Exploratory data analysis: Check the data\n\n\n\nLook at a summary for the raw data\nTypical use:\n\n\nlibrary(skimr)\nskim(gapm)\n\n\nSome skim() help"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-check-the-data-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-check-the-data-2",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Check the data",
    "text": "Pre-step: Exploratory data analysis: Check the data\n\n\n\nLook at a summary for the raw data\nTypical use:\n\n\nlibrary(skimr)\nskim(gapm)\n\n\nSome skim() help\nNote that skim(gapm) looks different because I had to create factors\nI am breaking down the skim() function into the categorical and continuous variables only because I want to show them on the slides\n\n\n\n\n\nskim(gapm_sub1) %&gt;% yank(\"factor\")\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nfour_regions\n0\n1.00\nFALSE\n4\nAsi: 57, Afr: 54, Eur: 49, Ame: 35\n\n\nincome_levels1\n1\n0.99\nFALSE\n4\nHig: 56, Upp: 55, Low: 52, Low: 31\n\n\nincome_levels2\n1\n0.99\nFALSE\n2\nHig: 111, Low: 83"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-check-the-data-3",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-check-the-data-3",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Check the data",
    "text": "Pre-step: Exploratory data analysis: Check the data\n\nskim(gapm_sub1) %&gt;% yank(\"numeric\")\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nCO2emissions\n4\n0.98\n4.55\n6.10\n0.03\n0.64\n2.41\n6.22\n41.20\n▇▁▁▁▁\n\n\nElectricityUsePP\n58\n0.70\n4220.92\n5964.07\n31.10\n699.00\n2410.00\n5600.00\n52400.00\n▇▁▁▁▁\n\n\nFoodSupplykcPPD\n27\n0.86\n2825.06\n443.59\n1910.00\n2490.00\n2775.00\n3172.50\n3740.00\n▅▇▇▇▅\n\n\nIncomePP\n2\n0.99\n16704.45\n19098.61\n614.00\n3370.00\n10100.00\n22700.00\n129000.00\n▇▂▁▁▁\n\n\nLifeExpectancyYrs\n8\n0.96\n70.66\n8.44\n47.50\n64.30\n72.70\n76.90\n82.90\n▁▃▃▇▇\n\n\nFemaleLiteracyRate\n115\n0.41\n81.65\n21.95\n13.00\n70.97\n91.60\n98.03\n99.80\n▁▁▂▁▇\n\n\nWaterSourcePrct\n1\n0.99\n84.84\n18.64\n18.30\n74.90\n93.50\n99.07\n100.00\n▁▁▂▂▇\n\n\nLatitude\n0\n1.00\n19.11\n23.93\n-42.00\n4.00\n17.33\n40.00\n65.00\n▁▃▇▆▅\n\n\nLongitude\n0\n1.00\n21.98\n66.52\n-175.00\n-5.75\n21.00\n49.27\n179.14\n▁▃▇▃▂\n\n\npopulation_mill\n0\n1.00\n35.95\n136.87\n0.00\n1.73\n7.57\n24.50\n1370.00\n▇▁▁▁▁"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-1",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-study-your-variables",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-study-your-variables",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Study your variables",
    "text": "Pre-step: Exploratory data analysis: Study your variables\n\nStarted this a little bit in previous slide (skim()), but you may want to look at things like:\n\nSample size\nCounts of missing data\nMeans and standard deviations\nIQRs\nMedians\nMinimums and maximums\n\nCan also look at visuals\n\nContinuous variables: histograms (in `skimr() a little)\nCategorical variables: frequency plots"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-study-your-variables-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-study-your-variables-1",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Study your variables",
    "text": "Pre-step: Exploratory data analysis: Study your variables\n\nlibrary(Hmisc)\nhist.data.frame(gapm %&gt;% select(-Longitude, -Latitude, -eight_regions, -six_regions, -geo, -`World bank, 4 income groups 2017`, -country, -population, -`World bank region`, -ElectricityUsePP))"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-2",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-missing-data",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-exploratory-data-analysis-missing-data",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Missing data",
    "text": "Pre-step: Exploratory data analysis: Missing data\n\nWhy are there missing data?\nWhich variables and observations should be excluded because of missing data?\nWill I impute missing data?\n\n \n\nUnfortunately, we don’t have time to discuss missing data more thoroughly\n\nI will try to cover this topic more thoroughly in BSTA 513\n\n \n\nFor the Gapminder dataset, we chose to use complete cases"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-step-1-explore-simple-relationships-and-assumptions",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#pre-step-step-1-explore-simple-relationships-and-assumptions",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Pre-step / Step 1 : Explore simple relationships and assumptions",
    "text": "Pre-step / Step 1 : Explore simple relationships and assumptions\n\ngapm2 %&gt;% ggpairs() # gapm2 is a new dataset with some variables selected"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-3",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-3",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nFor each covariate, we want to see how it relates to the outcome (without adjusting for other covariates)\nWe can partially do this with visualizations\n\nHelps us see the data we throw it into regression that makes assumptions (like our LINE assumptions)\nggpairs() can be a quick way to do it\nggplot() can make each plot\n\n+ geom_boxplot() to make boxplots by groups for categorical covariates\n+ geom_jitter() + stat_summary() to make non-overlaping points with group means for categorical covariates\n+ geom_point() to make scatterplots for continuous covariates\n\n\nWe need to run simple linear regression\n\nWe’re calling regression with multi-level categories “simple” even though there are multiple coefficients"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-1",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nLet’s think back to our Gapminder dataset\nAlways good to start with our main relationship: life expectancy vs. female literacy rate\n\nThrowback to Lesson 3 SLR when we first visualized and ran lm() for this relationship\n\n\n\nmodel_FLR = lm(LifeExpectancyYrs ~ FemaleLiteracyRate, data = gapm_sub)\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n51.438\n2.739\n18.782\n0.000\n    FemaleLiteracyRate\n0.230\n0.032\n7.141\n0.000"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-4",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-4",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-2",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nLet’s do this with one other variable before I show you a streamlined version of SLR\n\n\nmodel_WR = lm(LifeExpectancyYrs ~ four_regions, data = gapm_sub)\n\n \n\n\n\n\nCode\nggplot(gapm_sub, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_jitter(size = 1, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\",\n       caption = \"Diamonds = region averages\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\n\nanova(model_WR) %&gt;% tidy() %&gt;% gt() %&gt;%\n   tab_options(table.font.size = 40) %&gt;%\n   fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    four_regions\n3.000\n2,743.042\n914.347\n33.680\n0.000\n    Residuals\n68.000\n1,846.077\n27.148\nNA\nNA\n  \n  \n  \n\n\n\n\n \n\nRecall from Lesson 5 (SLR: More inference + Evaluation):\n\nanova() with one model name will compare the model (model_WR) to the intercept model"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-3",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-3",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nIf we do a good job visualizing the relationship between our outcome and each covariate, then we can proceed to a streamlined version of the F-test for each relationship\nFirst, I will select the variables that we are considering for model selection:\n\n\ngapm2 = gapm_sub %&gt;% select(LifeExpectancyYrs, CO2emissions, FoodSupplykcPPD, \n                            IncomePP, FemaleLiteracyRate, WaterSourcePrct, \n                            four_regions, members_oecd_g77)\n\n\nWe need to make sure our dataset only contains the variables we are considering for the model:\n\n\ngapm3 = gapm2 %&gt;% select(-LifeExpectancyYrs)"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-4",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-4",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nNow I can run the lapply() function, which allows me to run the same function multiple times over all the columns in gapm3\nFor each covariate I am running: lm(gapm2$LifeExpectancyYrs ~ x) %&gt;% anova()\n\nSo I am fitting the simple linear regression and printing the ANOVA table with F-test (comparing model with a without the covariate)\n\n\n\nlapply( gapm3, function(x) lm(gapm2$LifeExpectancyYrs ~ x) %&gt;% anova() )\n\n$CO2emissions\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx          1  452.3  452.31  7.6536 0.007241 **\nResiduals 70 4136.8   59.10                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$FoodSupplykcPPD\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 1893.4 1893.44  49.168 1.188e-09 ***\nResiduals 70 2695.7   38.51                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$IncomePP\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 1220.3 1220.34  25.358 3.557e-06 ***\nResiduals 70 3368.8   48.13                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$FemaleLiteracyRate\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 1934.2 1934.24  50.999 6.895e-10 ***\nResiduals 70 2654.9   37.93                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$WaterSourcePrct\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 2988.2 2988.20  130.66 &lt; 2.2e-16 ***\nResiduals 70 1600.9   22.87                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$four_regions\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          3 2743.0  914.35   33.68 1.858e-13 ***\nResiduals 68 1846.1   27.15                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$members_oecd_g77\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          2 1103.7  551.85  10.925 7.553e-05 ***\nResiduals 69 3485.4   50.51                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nWe can scroll through the output to see the ANOVA table for each covariate"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-5",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-1-simple-linear-regressions-analysis-5",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nWe can also filter the ANOVA table to just show the p-value for each F-test\n\n\nsapply( gapm3, function(x) anova( lm(gapm2$LifeExpectancyYrs ~ x) )$`Pr(&gt;F)` )\n\n     CO2emissions FoodSupplykcPPD     IncomePP FemaleLiteracyRate\n[1,]  0.007241207    1.187753e-09 3.557341e-06       6.894997e-10\n[2,]           NA              NA           NA                 NA\n     WaterSourcePrct four_regions members_oecd_g77\n[1,]    1.148644e-17 1.857818e-13      7.55261e-05\n[2,]              NA           NA               NA\n\n\n\nRow 1 is the p-value for the F-test\n\nThis will help us in Step 2"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-2-preliminary-variable-selection",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-2-preliminary-variable-selection",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 2: Preliminary variable selection",
    "text": "Step 2: Preliminary variable selection\n\nIdentify candidates for your first multivariable model by performing an F-test on each covariate’s SLR\n\nUsing p-values from previous slide\nIf the p-value of the test is less than 0.25, then consider the variable a candidate\n\n\n \n\nCandidates for first multivariable model\n\nAll clinically important variables (regardless of p-value)\nVariables with univariate test with p-value &lt; 0.25\n\n\n \n\nWith more experience, you won’t need to rely on these strict rules as much"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-2-preliminary-variable-selection-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-2-preliminary-variable-selection-1",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 2: Preliminary variable selection",
    "text": "Step 2: Preliminary variable selection\n\nFrom the previous p-values from the F-test on each covariate’s SLR\n\nDecision: we keep all the covariates since they all have a p-value &lt; 0.25\n\n\n\nsapply( gapm3, function(x) anova( lm(gapm2$LifeExpectancyYrs ~ x) )$`Pr(&gt;F)` )\n\n     CO2emissions FoodSupplykcPPD     IncomePP FemaleLiteracyRate\n[1,]  0.007241207    1.187753e-09 3.557341e-06       6.894997e-10\n[2,]           NA              NA           NA                 NA\n     WaterSourcePrct four_regions members_oecd_g77\n[1,]    1.148644e-17 1.857818e-13      7.55261e-05\n[2,]              NA           NA               NA"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-2-preliminary-variable-selection-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-2-preliminary-variable-selection-2",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 2: Preliminary variable selection",
    "text": "Step 2: Preliminary variable selection\n\nFit an initial model including any independent variable with p-value &lt; 0.25 and clinically important variables\n\n\ninit_model = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP +\n               four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77, \n                 data = gapm2)\ntidy(init_model, conf.int = T) %&gt;% gt() %&gt;% tab_options(table.font.size = 30) %&gt;% \n  fmt_number(decimals = 4)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n37.5560\n4.4083\n8.5194\n0.0000\n28.7410\n46.3710\n    FemaleLiteracyRate\n0.0020\n0.0352\n0.0580\n0.9539\n−0.0684\n0.0725\n    CO2emissions\n−0.2860\n0.1340\n−2.1344\n0.0368\n−0.5539\n−0.0181\n    IncomePP\n0.0002\n0.0001\n2.4133\n0.0188\n0.0000\n0.0003\n    four_regionsAmericas\n9.8963\n2.0031\n4.9405\n0.0000\n5.8909\n13.9017\n    four_regionsAsia\n5.7849\n1.5993\n3.6172\n0.0006\n2.5870\n8.9829\n    four_regionsEurope\n7.1421\n2.6994\n2.6458\n0.0104\n1.7442\n12.5399\n    WaterSourcePrct\n0.1377\n0.0658\n2.0928\n0.0405\n0.0061\n0.2693\n    FoodSupplykcPPD\n0.0052\n0.0021\n2.4961\n0.0153\n0.0010\n0.0093\n    members_oecd_g77oecd\n−0.3317\n2.5476\n−0.1302\n0.8968\n−5.4259\n4.7625\n    members_oecd_g77others\n0.3341\n2.2986\n0.1453\n0.8849\n−4.2622\n4.9304"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 3: Assess change in coefficient",
    "text": "Step 3: Assess change in coefficient\n\n\n\nThis is where we start identifying covariates that we might remove\n\n \n\nI would start by using the p-value to guide me towards specific variables\n\nFemale literacy rate, but that’s our main covariate\nmembers_oecd_g77\nMaybe water source percent?\n\n\n \n\nSome people will say you can use the p-value alone\n\nI like to double check that those variables do not have a large effect on the other coefficients\n\n\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n37.5560\n4.4083\n8.5194\n0.0000\n    FemaleLiteracyRate\n0.0020\n0.0352\n0.0580\n0.9539\n    CO2emissions\n−0.2860\n0.1340\n−2.1344\n0.0368\n    IncomePP\n0.0002\n0.0001\n2.4133\n0.0188\n    four_regionsAmericas\n9.8963\n2.0031\n4.9405\n0.0000\n    four_regionsAsia\n5.7849\n1.5993\n3.6172\n0.0006\n    four_regionsEurope\n7.1421\n2.6994\n2.6458\n0.0104\n    WaterSourcePrct\n0.1377\n0.0658\n2.0928\n0.0405\n    FoodSupplykcPPD\n0.0052\n0.0021\n2.4961\n0.0153\n    members_oecd_g77oecd\n−0.3317\n2.5476\n−0.1302\n0.8968\n    members_oecd_g77others\n0.3341\n2.2986\n0.1453\n0.8849"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient-1",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 3: Assess change in coefficient",
    "text": "Step 3: Assess change in coefficient\n\nVery similar to the process we used when looking at confounders\n\n \n\nOne variable at a time, we run the multivariable model with and without the variable\n\nWe look at the p-value of the F-test for the coefficients of said variable\nWe look at the percent change for the coefficient (\\(\\Delta\\%\\)) of our explanatory variable\n\n\n \n\nGeneral rule: We can remove a variable if…\n\np-value &gt; 0.05 for the F-test of its own coefficients\nAND change in coefficient (\\(\\Delta\\%\\)) of our explanatory variable is &lt; 10%"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient-2",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 3: Assess change in coefficient",
    "text": "Step 3: Assess change in coefficient\n\nLet’s try this out on members_oecd_g77\n\n\n\nDisplay the ANOVA table with F-statistic and p-value\nmodel_full = init_model\nmodel_red = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP +\n               four_regions + WaterSourcePrct + FoodSupplykcPPD, \n                 data = gapm2)\nanova(model_full, model_red) %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP + four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n61.000\n999.201\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP + four_regions + WaterSourcePrct + FoodSupplykcPPD\n63.000\n1,000.988\n−2.000\n−1.787\n0.055\n0.947\n  \n  \n  \n\n\n\n\n\n\\(\\widehat\\beta_{FLR, full} = 0.002\\), \\(\\widehat\\beta_{FLR, red} = 0.0036\\)\n\n\\[\n\\Delta\\% = 100\\% \\cdot \\frac{\\widehat\\beta_{FLR, full} - \\widehat\\beta_{FLR, red}}{\\widehat\\beta_{FLR, full}} = 100\\% \\cdot \\frac{0.002 - 0.0036}{0.002} = -74.41\\%\n\\]\n\nBased off the percent change, I would keep this in the model"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient-3",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient-3",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 3: Assess change in coefficient",
    "text": "Step 3: Assess change in coefficient\n\nLet’s try this out on water source percent (even though the p-value was &lt; 0.05)\n\n\n\nDisplay the ANOVA table with F-statistic and p-value\nmodel_full = init_model\nmodel_red = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP +\n               four_regions + members_oecd_g77 + FoodSupplykcPPD, \n                 data = gapm2)\nanova(model_full, model_red) %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP + four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n61.000\n999.201\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP + four_regions + members_oecd_g77 + FoodSupplykcPPD\n62.000\n1,070.944\n−1.000\n−71.744\n4.380\n0.041\n  \n  \n  \n\n\n\n\n\n\\(\\widehat\\beta_{FLR, full} = 0.002\\), \\(\\widehat\\beta_{FLR, red} = 0.034\\)\n\n\\[\n\\Delta\\% = 100\\% \\cdot \\frac{\\widehat\\beta_{FLR, full} - \\widehat\\beta_{FLR, red}}{\\widehat\\beta_{FLR, full}} = 100\\% \\cdot \\frac{0.002 - 0.034}{0.002} = -1561.06\\%\n\\]\n\nBased off the percent change (and p-value), I would keep this in the model"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-5",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-question-5",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient-4",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-3-assess-change-in-coefficient-4",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 3: Assess change in coefficient",
    "text": "Step 3: Assess change in coefficient\n\nAt the end of this step, we have a preliminary main effects model\nWhere the variables are excluded that met the following criteria:\n\nP-value &gt; 0.05 for the F-test of its own coefficients\nChange in coefficient (\\(\\Delta\\%\\)) of our explanatory variable is &lt; 10%\n\nIn our example, the preliminary main effects model (end of Step 3) was the same as the initial model (end of Step 2)\nPreliminary main effects model includes:\n\nFemaleLiteracyRate\nCO2emissions\nIncomePP\nfour_regions\nmembers_oecd_g77\nFoodSupplykcPPD\nWaterSupplePct"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#recap-of-steps-1-3",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#recap-of-steps-1-3",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Recap of Steps 1-3",
    "text": "Recap of Steps 1-3\n\nPre-step: Exploratory data analysis\nStep 1: Simple linear regressions / analysis\n\nLook at each covariate with outcome\nPerform SLR for each covariate\n\nStep 2: Preliminary variable selection\n\nFrom SLR, decide which variables go into the initial model\nUse F-test to see if each covariate (on its own) explains enough variation in outcome\nEnd with initial model\n\nStep 3: Assess change in coefficients\n\nFrom the initial model at end of step 2, we take a variable out of the model if:\n\nP-value &gt; 0.05 for the F-test of its own coefficients\nChange in coefficient (\\(\\Delta\\%\\)) of our explanatory variable is &lt; 10%\n\nEnd with preliminary main effects model"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables",
    "text": "Step 4: Assess scale for continuous variables\n\nWe assume the linear regression model is linear for each continuous variable\nWe need to assess linearity for continuous variables in the model\n\nDo this through smoothed scatterplots that we introduced in Lesson 6 (SLR Diagnostics)\nResidual plots (can be used in SLR) does not help us in MLR\nEach term in MLR model needs to have linearity with outcome\n\nThree methods/approaches to address the violation of linearity assumption:\n\nApproach 1: Categorize continuous variable\nApproach 2: Fractional Polynomials\nApproach 3: Spline functions\n\nApproach will depend on the covariate!!\nFor our class, only implement Approach 1 or 2\nModel at the end of Step 4 is the main effects model"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables",
    "text": "Step 4: Assess scale for continuous variables\n\n\nResidual plot does not help us with linearity in MLR\nlibrary(ggfortify)\nautoplot(model_full) + theme(text=element_text(size=14))"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables: Smoothed scatterplots",
    "text": "Step 4: Assess scale for continuous variables: Smoothed scatterplots\n\nSmoother scatterplots only check linearity, not addressing linearity issues\n\n \n\nCan also identify extreme observations\n\nAgain, just want to flag these values\nCan influence the assessment of linearity when using fractional polynomials or spline functions\n\n\n \n\nHelps us decide if the continuous variable can stay as is in the model\n\nProblem: if not linear, then we need to represent the variable in a new way (Approaches 1-3)"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-1",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables: Smoothed scatterplots",
    "text": "Step 4: Assess scale for continuous variables: Smoothed scatterplots\n\nIn Gapminder dataset, we have 5 continuous variables:\n\nCO2 Emissions\nFood Supply\nIncome\nFemale Literacy Rate\nWater source percent\n\nPlot each of these agains the outcome, life expectancy"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-2",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables: Smoothed scatterplots",
    "text": "Step 4: Assess scale for continuous variables: Smoothed scatterplots\n\n\nWe can quickly look at ggpairs() to identify variables\ngapm2 %&gt;% select(where(is.numeric)) %&gt;% \n  relocate(LifeExpectancyYrs, .after = last_col()) %&gt;% ggpairs()"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-3",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-3",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables: Smoothed scatterplots",
    "text": "Step 4: Assess scale for continuous variables: Smoothed scatterplots\n\n\nTake a look at C02, Food Supply, and Income\nCO2 = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = CO2emissions)) + \n  geom_point() +\n  geom_smooth(se=F) + labs(x = \"CO2 Emissions (kt)\", y = \"Life Expectancy (yrs)\")\n\nFS = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = FoodSupplykcPPD)) + \n  geom_point() +\n  geom_smooth(se=F) + labs(x = \"Food Supply (kcal PPD)\", y = \"Life Expectancy (yrs)\")\n\nIncome = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = IncomePP)) + \n  geom_point() +\n  geom_smooth(se=F) + labs(x = \"Income (GDP per capita)\", y = \"Life Expectancy (yrs)\")\n\ngrid.arrange(CO2, FS, Income, nrow=1)\n\n\n\n\nFood Supply looks admissible\nCO2 Emissions and Income do not look very linear, but I want to zoom into the area of the plots that have most of the data"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-4",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-4",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables: Smoothed scatterplots",
    "text": "Step 4: Assess scale for continuous variables: Smoothed scatterplots\n\n\nZoom into areas on plots with more data\nCO2 = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = CO2emissions)) + \n  geom_point() + xlim(0,10) +\n  geom_smooth(se=F) + labs(x = \"CO2 Emissions (kt)\", y = \"Life Expectancy (yrs)\")\n\nFS = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = FoodSupplykcPPD)) + \n  geom_point() +\n  geom_smooth(se=F) + labs(x = \"Food Supply (kcal PPD)\", y = \"Life Expectancy (yrs)\")\n\nIncome = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = IncomePP)) + \n  geom_point() + xlim(0,40000) +\n  geom_smooth(se=F) + labs(x = \"Income (GDP per capita)\", y = \"Life Expectancy (yrs)\")\n\ngrid.arrange(CO2, FS, Income, nrow=1)\n\n\n\n\nFood Supply still looks admissible\nCO2 Emissions and Income not linear: will address this!!"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-assess-scale-for-continuous-variables-2",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables",
    "text": "Step 4: Assess scale for continuous variables\n\nThree methods/approaches to address the violation of linearity assumption:\n \n\nApproach 1: Categorize continuous variable\n\n \n\nApproach 2: Fractional Polynomials\n\n \n\nApproach 3: Spline functions"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4: Approach 1: Categorize continuous variable",
    "text": "Step 4: Approach 1: Categorize continuous variable\n\nCategorize continuous variables\n\nPercentiles, quartiles, quantiles\n\nCreate indicator variables corresponding to each quartile\n\nMeaningful thresholds\n\nExample: income level groups discussed by Gapminder\n\n\nDisadvantages:\n\nTakes some time to create new variables, especially with multiple continuous covariates\nStart with quartiles, but might be more appropriate to use different splits\n\nNo set rules on this\n\n\nAdvantage: graphical and visually helps"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable-1",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4: Approach 1: Categorize continuous variable",
    "text": "Step 4: Approach 1: Categorize continuous variable\n\n\n\nFor income, I would use Gapminder’s income level groups\n\nDiscussed in Lesson 10 Categorical Covariates (slide 43)\n\n\n \n\nExperts in the field have developed these income groups\n\nI think this is best solution for income (that was not meeting linearity as a continuous variable)"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable-2",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4: Approach 1: Categorize continuous variable",
    "text": "Step 4: Approach 1: Categorize continuous variable\n\n\n\nLet’s still try it out with CO2 Emissions (kt)\nI have plotted the quartile lines of food supply with red lines\n\n\n\n\nTake a look at the quartiles within the scatterplot\nvline_coordinates= data.frame(Quantile_Name=names(quantile(gapm2$CO2emissions)),\n                          quantile_values=as.numeric(quantile(gapm2$CO2emissions)))\n\nggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = CO2emissions)) + \n  geom_point(size = 3) +\n  #geom_smooth(se=F) + \n  labs(x = \"CO2 Emissions (kt)\", y = \"Life Expectancy (yrs)\") +\n  geom_vline(data = vline_coordinates, aes(xintercept = quantile_values), \n             color = \"red\", linetype = \"dashed\", size = 2) +\n    theme(axis.title = element_text(size = 25), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 25))"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable-3",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable-3",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4: Approach 1: Categorize continuous variable",
    "text": "Step 4: Approach 1: Categorize continuous variable\n\nLet’s make the quartiles for CO2 emissions:\n\n\nlibrary(dvmisc)\ngapm2 = gapm2 %&gt;% \n  mutate(CO2_q = quant_groups(CO2emissions, groups = 4) %&gt;% factor())\n\n\n\nTake a look at the quartile means within the scatterplot\nggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = CO2_q)) + \n  # geom_point(size = 3, aes(y = LifeExpectancyYrs, x = CO2emissions)) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  #geom_smooth(se=F) + \n  labs(x = \"CO2 Emissions (kt)\", y = \"Life Expectancy (yrs)\") +\n    theme(axis.title = element_text(size = 25), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 25))"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable-4",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-1-categorize-continuous-variable-4",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4: Approach 1: Categorize continuous variable",
    "text": "Step 4: Approach 1: Categorize continuous variable\n\n\n \n\nLet’s fit a new model with the two new representations for income and CO2 emissions\n\n \n\nRemember, this is the main effects model if we decide to make CO2 into quartiles\n\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n39.877\n4.889\n8.157\n0.000\n    FemaleLiteracyRate\n−0.073\n0.047\n−1.555\n0.125\n    CO2_q(0.806,2.54]\n1.099\n1.914\n0.574\n0.568\n    CO2_q(2.54,4.66]\n−0.292\n2.419\n−0.121\n0.904\n    CO2_q(4.66,35.2]\n−0.595\n2.524\n−0.236\n0.814\n    income_levels1Lower middle income\n5.441\n2.343\n2.322\n0.024\n    income_levels1Upper middle income\n6.111\n2.954\n2.069\n0.043\n    income_levels1High income\n7.959\n3.277\n2.429\n0.018\n    four_regionsAmericas\n9.003\n2.050\n4.391\n0.000\n    four_regionsAsia\n5.260\n1.637\n3.213\n0.002\n    four_regionsEurope\n6.855\n2.871\n2.387\n0.020\n    WaterSourcePrct\n0.166\n0.066\n2.496\n0.015\n    FoodSupplykcPPD\n0.004\n0.002\n1.825\n0.073\n    members_oecd_g77oecd\n1.119\n2.674\n0.418\n0.677\n    members_oecd_g77others\n1.047\n2.511\n0.417\n0.678"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-2-fractional-polynomials",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-2-fractional-polynomials",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4: Approach 2: Fractional Polynomials",
    "text": "Step 4: Approach 2: Fractional Polynomials\n\nMain concepts and transformations presented in Lesson 7 SLR: Model Evaluation and Diagnostics (slide 33 on)\nIdea: test many transformations of a continuous covariate\n\nBased on Royston and Altman, Applied Statistics, 1994\n\n\n \n\nRecall Tukey’s transformation (power) ladder\n\nAnd can use R’s gladder() to see the transformations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower p\n-3\n-2\n-1\n-1/2\n0\n1/2\n1\n2\n3\n\n\n\n\n\n\\(\\frac{1}{x^3}\\)\n\\(\\frac{1}{x^2}\\)\n\\(\\frac{1}{x}\\)\n\\(\\frac{1}{\\sqrt{x}}\\)\n\\(\\log(x)\\)\n\\(\\sqrt{x}\\)\n\\(x\\)\n\\(x^2\\)\n\\(x^3\\)\n\n\n\n \n\nWe can run through each and test different models, or use the approach from Lesson 7\nThere is also a package we can use!\n\nmfp package in R contains the fp() function"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-2-fractional-polynomials-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-2-fractional-polynomials-1",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4: Approach 2: Fractional Polynomials",
    "text": "Step 4: Approach 2: Fractional Polynomials\n\nlibrary(mfp)\n\nfp_model_CO2 = mfp(LifeExpectancyYrs ~ FemaleLiteracyRate + \n                     fp(CO2emissions, df = 4) + income_levels1 + four_regions +\n                     WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77,\n               data = gapm2, family = \"gaussian\")\n\nfp_model_CO2$fptable %&gt;% gt(rownames_to_stub = T) %&gt;% tab_options(table.font.size = 24)\n\n\n\n\n\n  \n    \n      \n      df.initial\n      select\n      alpha\n      df.final\n      power1\n      power2\n    \n  \n  \n    four_regionsAmericas\n1\n1\n0.05\n1\n1\n.\n    four_regionsAsia\n1\n1\n0.05\n1\n1\n.\n    four_regionsEurope\n1\n1\n0.05\n1\n1\n.\n    WaterSourcePrct\n1\n1\n0.05\n1\n1\n.\n    income_levels1Lower middle income\n1\n1\n0.05\n1\n1\n.\n    income_levels1Upper middle income\n1\n1\n0.05\n1\n1\n.\n    income_levels1High income\n1\n1\n0.05\n1\n1\n.\n    FoodSupplykcPPD\n1\n1\n0.05\n1\n1\n.\n    FemaleLiteracyRate\n1\n1\n0.05\n1\n1\n.\n    CO2emissions\n4\n1\n0.05\n1\n1\n.\n    members_oecd_g77oecd\n1\n1\n0.05\n1\n1\n.\n    members_oecd_g77others\n1\n1\n0.05\n1\n1\n."
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-2-fractional-polynomials-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-2-fractional-polynomials-2",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4: Approach 2: Fractional Polynomials",
    "text": "Step 4: Approach 2: Fractional Polynomials\n\n\n\n\n\n\n\n\n  \n    \n      \n      df.initial\n      select\n      alpha\n      df.final\n      power1\n      power2\n    \n  \n  \n    four_regionsAmericas\n1\n1\n0.05\n1\n1\n.\n    four_regionsAsia\n1\n1\n0.05\n1\n1\n.\n    four_regionsEurope\n1\n1\n0.05\n1\n1\n.\n    WaterSourcePrct\n1\n1\n0.05\n1\n1\n.\n    income_levels1Lower middle income\n1\n1\n0.05\n1\n1\n.\n    income_levels1Upper middle income\n1\n1\n0.05\n1\n1\n.\n    income_levels1High income\n1\n1\n0.05\n1\n1\n.\n    FoodSupplykcPPD\n1\n1\n0.05\n1\n1\n.\n    FemaleLiteracyRate\n1\n1\n0.05\n1\n1\n.\n    CO2emissions\n4\n1\n0.05\n1\n1\n.\n    members_oecd_g77oecd\n1\n1\n0.05\n1\n1\n.\n    members_oecd_g77others\n1\n1\n0.05\n1\n1\n.\n  \n  \n  \n\n\n\n\n\n\nConclusion from fractional polynomial is that CO2 does not need to be transformed\nA little counter-intuitive to what we saw in quartiles\nThus, I think leaving CO2 emissions as quartiles is best!"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-3-spline-functions",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-3-spline-functions",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4: Approach 3: Spline functions",
    "text": "Step 4: Approach 3: Spline functions\n\nSpline function is to fit a series of smooth curves that joined at specific points (called knots)"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-3-spline-functions-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-approach-3-spline-functions-1",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4: Approach 3: Spline functions",
    "text": "Step 4: Approach 3: Spline functions\n\nNeed to specify knots for spline functions\n\nMore knots are flexible, but requires more parameters to estimate\nIn most applications three to five knots are sufficient\n\n\n \n\nWithin our class, fractional polynomials will be sufficient\n\n \n\nIf you think this is cool, I highly suggest you look into Functional Data Analysis (FDA) or Functional Regression\n\nJeffrey Morris is a big name in that field\n\n\n \n\nIn R there are a few options to incorporate splines\n\npspline( ): More information\nsmoothHR(): More information"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-conclusion-main-effects-model",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-4-conclusion-main-effects-model",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 4 Conclusion: main effects model",
    "text": "Step 4 Conclusion: main effects model\n\n\n\nWe concluded that we will use:\n\nIncome levels (categorical) that Gapminder created\nQuartiles for CO2 Emissions\n\n\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n39.877\n4.889\n8.157\n0.000\n    FemaleLiteracyRate\n−0.073\n0.047\n−1.555\n0.125\n    CO2_q(0.806,2.54]\n1.099\n1.914\n0.574\n0.568\n    CO2_q(2.54,4.66]\n−0.292\n2.419\n−0.121\n0.904\n    CO2_q(4.66,35.2]\n−0.595\n2.524\n−0.236\n0.814\n    income_levels1Lower middle income\n5.441\n2.343\n2.322\n0.024\n    income_levels1Upper middle income\n6.111\n2.954\n2.069\n0.043\n    income_levels1High income\n7.959\n3.277\n2.429\n0.018\n    four_regionsAmericas\n9.003\n2.050\n4.391\n0.000\n    four_regionsAsia\n5.260\n1.637\n3.213\n0.002\n    four_regionsEurope\n6.855\n2.871\n2.387\n0.020\n    WaterSourcePrct\n0.166\n0.066\n2.496\n0.015\n    FoodSupplykcPPD\n0.004\n0.002\n1.825\n0.073\n    members_oecd_g77oecd\n1.119\n2.674\n0.418\n0.677\n    members_oecd_g77others\n1.047\n2.511\n0.417\n0.678"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 5: Check for interactions",
    "text": "Step 5: Check for interactions\n\nCreate a list of interaction terms from variables in the “main effects model” that has clinical plausibility\n\n \n\nAdd the interaction variables, one at a time, to the main effects model, and assess the significance using a likelihood ratio test or Wald test\n\nMay keep interaction terms with p-value &lt; 0.10 (or 0.05)\n\n\n \n\nKeep the main effects untouched, only simplify the interaction terms\n\n \n\nUse methods from Step 2 (comparing model with all interactions to a smaller model with interactions) to determine which interactions to keep\n\n \n\nThe model by the end of Step 5 is called the preliminary final model"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-1",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 5: Check for interactions",
    "text": "Step 5: Check for interactions\n\nWe test with \\(\\alpha = 0.10\\)\nFollow the F-test procedure in Lesson 10 (MLR: Using the F-test)\n\nThis means we need to follow the 7 steps of the general F-test in previous slide (taken from Lesson 10)\n\nUse the hypothesis tests for the specific variable combo:\n\n\n\n\n\nBinary & continuous variable (Lesson 11, LOB 2)\n\n\nTesting a single coefficient for the interaction term using F-test comparing full model to reduced model\n\n\n\n\n\nMulti-level & continuous variables (Lesson 11, LOB 3)\n\n\nTesting group of coefficients for the interaction terms using F-test comparing full to reduced model\n\n\n\n\n\n\n\n\nBinary & multi-level variable (Lesson 12, LOB 4)\n\n\nTesting group of coefficients for the interaction terms using F-test comparing full to reduced model\n\n\n\n\n\nTwo continuous variables (Lesson 12, LOB 5)\n\n\nTesting a single coefficient for the interaction term using F-test comparing full to reduced model"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-2",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-2",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 5: Check for interactions",
    "text": "Step 5: Check for interactions\n\nvars = names(model.frame(main_eff_model))[-1] \n\ninteractions = combn(vars, 2, function(x) paste(x, collapse=\" * \")) %&gt;% \n    grep(., pattern = \"FemaleLiteracyRate\", value = T) \n\n\nMLRs = lapply(interactions, function(int)\n  lm(reformulate(c(vars, int), \"LifeExpectancyYrs\"), data = gapm2))"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-3",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-3",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 5: Check for interactions",
    "text": "Step 5: Check for interactions\n\n\n\nMLRs[[1]] %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 33) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n37.326\n5.463\n6.833\n0.000\n    FemaleLiteracyRate\n−0.035\n0.058\n−0.602\n0.550\n    CO2_q(0.806,2.54]\n9.049\n7.248\n1.249\n0.217\n    CO2_q(2.54,4.66]\n7.843\n16.082\n0.488\n0.628\n    CO2_q(4.66,35.2]\n−5.980\n25.867\n−0.231\n0.818\n    income_levels1Lower middle income\n4.032\n2.661\n1.515\n0.136\n    income_levels1Upper middle income\n4.997\n3.239\n1.543\n0.129\n    income_levels1High income\n6.825\n3.549\n1.923\n0.060\n    four_regionsAmericas\n9.317\n2.193\n4.250\n0.000\n    four_regionsAsia\n5.412\n1.668\n3.246\n0.002\n    four_regionsEurope\n7.267\n2.992\n2.429\n0.018\n    WaterSourcePrct\n0.178\n0.070\n2.529\n0.014\n    FoodSupplykcPPD\n0.004\n0.002\n1.706\n0.094\n    members_oecd_g77oecd\n0.798\n2.731\n0.292\n0.771\n    members_oecd_g77others\n1.121\n2.588\n0.433\n0.667\n    FemaleLiteracyRate:CO2_q(0.806,2.54]\n−0.104\n0.091\n−1.144\n0.258\n    FemaleLiteracyRate:CO2_q(2.54,4.66]\n−0.104\n0.177\n−0.590\n0.557\n    FemaleLiteracyRate:CO2_q(4.66,35.2]\n0.038\n0.268\n0.141\n0.889"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-4",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-4",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 5: Check for interactions",
    "text": "Step 5: Check for interactions\nYou can alse go straight to using the anova() function to compare the preliminary model.\n\nanova_res = lapply(interactions,\n            function(int) anova(lm(reformulate(c(vars, int), \"LifeExpectancyYrs\"),\n                                    data = gapm2), main_eff_model)) \nanova_res[[1]] %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + FemaleLiteracyRate * CO2_q\n54.000\n919.287\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n57.000\n946.458\n−3.000\n−27.171\n0.532\n0.662"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-6-assess-model-fit",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-6-assess-model-fit",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 6: Assess model fit",
    "text": "Step 6: Assess model fit\n\nAssess the adequacy of the model and check its fit\n\n \n\nMethods will be discussed next class\n\nCombination of diagnostics and model fit statistics!\nLook at model fit statistics in this lesson\nLook at diagnostics in Lesson 14: MLR Diagnostics\n\n\n \n\nIf the model is adequate and fits well, then it is the Final model"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-6-assess-model-fit-1",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-6-assess-model-fit-1",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 6: Assess model fit",
    "text": "Step 6: Assess model fit\n\nOur final model contains\n\nFemale Literacy Rate FLR\nCO2 Emissions in quartiles CO2_q\nIncome levels in groups assigned by Gapminder income_levels\nWorld regions four_regions\nMembership of global and economic groups members_oecd_g77\n\nOECD: Organization for Economic Co-operation and Development\nG77: Group of 77\nOther\n\nFood Supply FoodSupplykcPPD\nClean Water Supply WaterSupplePct"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-6-assess-model-fit-model-fit-statistics",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-6-assess-model-fit-model-fit-statistics",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 6: Assess model fit: Model fit statistics",
    "text": "Step 6: Assess model fit: Model fit statistics\n\nWay I did it in the lab instructions\n\n\nsum_fm = summary(final_model)\nmodel_fit_stats = data.frame(Model = \"Final model\", \n                             Adjusted_R_sq = sum_fm$adj.r.squared, \n                             AIC = AIC(final_model), BIC = BIC(final_model))\n\nmodel_fit_stats %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      Model\n      Adjusted_R_sq\n      AIC\n      BIC\n    \n  \n  \n    Final model\n0.743\n421.804\n458.230\n  \n  \n  \n\n\n\n\n\nAnother (maybe faster?) way to do it (glance() in broom package)\n\n\nglance(final_model) %&gt;% mutate(Model = \"Final model\") %&gt;%\n  select(Model, adj.r.squared, AIC, BIC) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      Model\n      adj.r.squared\n      AIC\n      BIC\n    \n  \n  \n    Final model\n0.743\n421.804\n458.230"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-6-assess-model-fit-comparing-model-fits",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-6-assess-model-fit-comparing-model-fits",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 6: Assess model fit: Comparing model fits",
    "text": "Step 6: Assess model fit: Comparing model fits\n\nRemember the preliminary main effects model (at end of Step 3): same as final model but the continuous varaibles, income and CO2 emissions, were not categorized\nWe can compare model fit statistics of the preliminary main effects model and the final model\n\n\nfm_glance = glance(final_model) %&gt;% mutate(Model = \"Final model\") %&gt;%\n  select(Model, `Adj R-squared` = adj.r.squared, AIC, BIC) \npmem_glance = glance(prelim_me_model) %&gt;% \n  mutate(Model = \"Preliminary main effects model\") %&gt;%\n  select(Model, `Adj R-squared` = adj.r.squared, AIC, BIC) \nrbind(fm_glance, pmem_glance) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      Model\n      Adj R-squared\n      AIC\n      BIC\n    \n  \n  \n    Final model\n0.743\n421.804\n458.230\n    Preliminary main effects model\n0.747\n417.708\n445.028\n  \n  \n  \n\n\n\n\n\nRemember, adjusted \\(R^2\\), AIC, and BIC penalize models for more coefficients\nPreliminary main effects model: better fit statistics, but violates linearity assumption\n\n\n\nLesson 14: Purposeful Selection"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html",
    "href": "lessons/02_Data_Management/02_Data_Management.html",
    "title": "Data Management with the tidyverse",
    "section": "",
    "text": "Artwork by @allison_horst"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#section",
    "href": "lessons/02_Data_Management/02_Data_Management.html#section",
    "title": "Lesson 2: Data and File Management",
    "section": "",
    "text": "Artwork by @allison_horst"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#what-is-the-tidyverse",
    "href": "lessons/02_Data_Management/02_Data_Management.html#what-is-the-tidyverse",
    "title": "Lesson 2: Data and File Management",
    "section": "What is the tidyverse?",
    "text": "What is the tidyverse?\nThe tidyverse is a collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\n\n\nggplot2 - data visualisation\ndplyr - data manipulation\ntidyr - tidy data\nreadr - read rectangular data\npurrr - functional programming\ntibble - modern data frames\nstringr - string manipulation\nforcats - factors\nand many more …"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#tidy-data1",
    "href": "lessons/02_Data_Management/02_Data_Management.html#tidy-data1",
    "title": "Lesson 2: Data and File Management",
    "section": "Tidy data1",
    "text": "Tidy data1\n\n\n\n\n\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nSource: R for Data Science. Grolemund and Wickham."
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#pipe-operator-magrittr",
    "href": "lessons/02_Data_Management/02_Data_Management.html#pipe-operator-magrittr",
    "title": "Lesson 2: Data and File Management",
    "section": "Pipe operator (magrittr)",
    "text": "Pipe operator (magrittr)\n\nThe pipe operator (%&gt;%) allows us to step through sequential functions in the same way we follow if-then statements or steps from instructions\n\n \n\nI want to find my keys, then start my car, then drive to work, then park my car.\n\n \n\n\nNested\n\npark(drive(start_car(find(\"keys\")), \n           to = \"work\"))\n\n\nPiped\n\nfind(\"keys\") %&gt;%\n  start_car() %&gt;%\n  drive(to = \"work\") %&gt;%\n  park()"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#recoding-a-binary-variable-with-pipe-operator",
    "href": "lessons/02_Data_Management/02_Data_Management.html#recoding-a-binary-variable-with-pipe-operator",
    "title": "Lesson 2: Data and File Management",
    "section": "Recoding a binary variable with pipe operator",
    "text": "Recoding a binary variable with pipe operator\n \n\nLet’s say I want a variable transmission to show the category names that are assigned to numeric values in the code. I want 0 to be coded as automatic and 1 to be coded as manual.\n\n \n\n\nBase R:\n\nmtcars$transmission &lt;-\n  ifelse(\n    mtcars$am == 0,\n    \"automatic\",\n    \"manual\"\n  )\n\n\nTidyverse:\n\nmtcars &lt;- mtcars %&gt;%\n  mutate(\n    transmission = case_when(\n      am == 0 ~ \"automatic\",\n      am == 1 ~ \"manual\"\n    )\n  )\n\n \n\nmutate() creates new columns that are functions of existing variables"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#recoding-a-multi-level-variable",
    "href": "lessons/02_Data_Management/02_Data_Management.html#recoding-a-multi-level-variable",
    "title": "Lesson 2: Data and File Management",
    "section": "Recoding a multi-level variable",
    "text": "Recoding a multi-level variable\n \n\nLet’s say I want a variable gear to show the category names that are assigned to numeric values in the code. I want 3 to be coded as gear three, 4 to be coded as gear four, 5 to be coded as gear five.\n\n \n\n\nBase R:\n\nmtcars$gear_char &lt;-\n  ifelse(\n    mtcars$gear == 3,\n    \"three\",\n    ifelse(\n      mtcars$gear == 4,\n      \"four\",\n      \"five\"\n    )\n  )\n\n\nTidyverse:\n\nmtcars &lt;- mtcars %&gt;%\n  mutate(\n    gear_char = case_when(\n      gear == 3 ~ \"three\",\n      gear == 4 ~ \"four\",\n      gear == 5 ~ \"five\"\n    )\n  )"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#ggplot2-in-tidyverse",
    "href": "lessons/02_Data_Management/02_Data_Management.html#ggplot2-in-tidyverse",
    "title": "Lesson 2: Data and File Management",
    "section": "ggplot2 in tidyverse",
    "text": "ggplot2 in tidyverse\n\n\n\n\n\n\n\n\n\nWe talked about this in our review notes\n\nI want to revisit it: always helps to have more examples!\nThis example is closer to the multivariable work we’ll do in this class!\n\n\n \n\nggplot2 is tidyverse’s data visualization package\n\n \n\nThe gg in “ggplot2” stands for Grammar of Graphics\n\n \n\nIt is inspired by the book Grammar of Graphics by Leland Wilkinson"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#tidyverse-visualizing-multiple-variables",
    "href": "lessons/02_Data_Management/02_Data_Management.html#tidyverse-visualizing-multiple-variables",
    "title": "Lesson 2: Data and File Management",
    "section": "Tidyverse: Visualizing multiple variables",
    "text": "Tidyverse: Visualizing multiple variables\n \n\nggplot(\n  mtcars,\n  aes(x = disp, y = mpg, color = transmission)) +\n  geom_point()"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#poll-everywhere-question-1",
    "href": "lessons/02_Data_Management/02_Data_Management.html#poll-everywhere-question-1",
    "title": "Lesson 2: Data and File Management",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#tidyverse-visualizing-even-more-variables",
    "href": "lessons/02_Data_Management/02_Data_Management.html#tidyverse-visualizing-even-more-variables",
    "title": "Lesson 2: Data and File Management",
    "section": "Tidyverse: Visualizing even more variables",
    "text": "Tidyverse: Visualizing even more variables\n\nggplot(\n  mtcars,\n  aes(x = disp, y = mpg, color = transmission)) +\n  geom_point() +\n  facet_wrap(~ cyl)"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#base-r-visualizing-even-more-variables",
    "href": "lessons/02_Data_Management/02_Data_Management.html#base-r-visualizing-even-more-variables",
    "title": "Lesson 2: Data and File Management",
    "section": "Base R: Visualizing even more variables",
    "text": "Base R: Visualizing even more variables\n\nmtcars$trans_color &lt;- ifelse(mtcars$transmission == \"automatic\", \"green\", \"blue\")\nmtcars_cyl4 = mtcars[mtcars$cyl == 4, ]\nmtcars_cyl6 = mtcars[mtcars$cyl == 6, ]\nmtcars_cyl8 = mtcars[mtcars$cyl == 8, ]\npar(mfrow = c(1, 3), mar = c(2.5, 2.5, 2, 0), mgp = c(1.5, 0.5, 0))\nplot(mpg ~ disp, data = mtcars_cyl4, col = trans_color, main = \"Cyl 4\")\nplot(mpg ~ disp, data = mtcars_cyl6, col = trans_color, main = \"Cyl 6\")\nplot(mpg ~ disp, data = mtcars_cyl8, col = trans_color, main = \"Cyl 8\")\nlegend(\"topright\", legend = c(\"automatic\", \"manual\"), pch = 1, col = c(\"green\", \"blue\"))"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#important-functions-for-data-management",
    "href": "lessons/02_Data_Management/02_Data_Management.html#important-functions-for-data-management",
    "title": "Lesson 2: Data and File Management",
    "section": "Important functions for data management",
    "text": "Important functions for data management\n \nData manipulation\n\npivot_longer() and pivot_wider() (not covered today)\nrename()\nmutate()\nfilter()\nselect()\n\nSummarizing data\n\ntbl_summary()\ngroup_by()\nsummarize()\nacross()"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#example-for-pivot_longer-instructional-staff-employment-trends",
    "href": "lessons/02_Data_Management/02_Data_Management.html#example-for-pivot_longer-instructional-staff-employment-trends",
    "title": "Lesson 2: Data Management with the tidyverse",
    "section": "Example for pivot_longer(): Instructional staff employment trends",
    "text": "Example for pivot_longer(): Instructional staff employment trends\nThe American Association of University Professors (AAUP) is a nonprofit membership association of faculty and other academic professionals. This report by the AAUP shows trends in instructional staff employees between 1975 and 2011, and contains an image very similar to the one given below."
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#data",
    "href": "lessons/02_Data_Management/02_Data_Management.html#data",
    "title": "Lesson 2: Data Management with the tidyverse",
    "section": "Data",
    "text": "Data\nEach row in this dataset represents a faculty type, and the columns are the years for which we have data. The values are percentage of hires of that type of faculty for each year.\n   \n\n(staff &lt;- read_csv(here(\"./data/instructional-staff.csv\")))\n\n# A tibble: 5 × 12\n  faculty_type    `1975` `1989` `1993` `1995` `1999` `2001` `2003` `2005` `2007`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Full-Time Tenu…   29     27.6   25     24.8   21.8   20.3   19.3   17.8   17.2\n2 Full-Time Tenu…   16.1   11.4   10.2    9.6    8.9    9.2    8.8    8.2    8  \n3 Full-Time Non-…   10.3   14.1   13.6   13.6   15.2   15.5   15     14.8   14.9\n4 Part-Time Facu…   24     30.4   33.1   33.2   35.5   36     37     39.3   40.5\n5 Graduate Stude…   20.5   16.5   18.1   18.8   18.7   19     20     19.9   19.5\n# ℹ 2 more variables: `2009` &lt;dbl&gt;, `2011` &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#poll-everywhere-question-2",
    "href": "lessons/02_Data_Management/02_Data_Management.html#poll-everywhere-question-2",
    "title": "Lesson 2: Data Management with the tidyverse",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#recreate-the-visualization",
    "href": "lessons/02_Data_Management/02_Data_Management.html#recreate-the-visualization",
    "title": "Lesson 2: Data Management with the tidyverse",
    "section": "Recreate the visualization",
    "text": "Recreate the visualization\n \n\nIn order to recreate this visualization we need to first reshape the data:\n\none variable for faculty type\none variable for year\n\n\n \n\nConvert the data from the wide format to long format\n\npivot_longer()"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#pivot_-functions",
    "href": "lessons/02_Data_Management/02_Data_Management.html#pivot_-functions",
    "title": "Lesson 2: Data Management with the tidyverse",
    "section": "pivot_*() functions",
    "text": "pivot_*() functions"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#poll-everywhere-question-3",
    "href": "lessons/02_Data_Management/02_Data_Management.html#poll-everywhere-question-3",
    "title": "Lesson 2: Data Management with the tidyverse",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#pivot-staff-data-and-mutate-percentage",
    "href": "lessons/02_Data_Management/02_Data_Management.html#pivot-staff-data-and-mutate-percentage",
    "title": "Lesson 2: Data Management with the tidyverse",
    "section": "Pivot staff data and mutate percentage",
    "text": "Pivot staff data and mutate percentage\n\n(staff_long &lt;- staff %&gt;%\n  pivot_longer(\n    cols = -faculty_type,    # columns to pivot\n    names_to = \"year\",       # name of new column for variable names\n    values_to = \"percentage\" # name of new column for values\n  ) %&gt;%\n  mutate(percentage = as.numeric(percentage))\n)\n\n# A tibble: 55 × 3\n   faculty_type              year  percentage\n   &lt;chr&gt;                     &lt;chr&gt;      &lt;dbl&gt;\n 1 Full-Time Tenured Faculty 1975        29  \n 2 Full-Time Tenured Faculty 1989        27.6\n 3 Full-Time Tenured Faculty 1993        25  \n 4 Full-Time Tenured Faculty 1995        24.8\n 5 Full-Time Tenured Faculty 1999        21.8\n 6 Full-Time Tenured Faculty 2001        20.3\n 7 Full-Time Tenured Faculty 2003        19.3\n 8 Full-Time Tenured Faculty 2005        17.8\n 9 Full-Time Tenured Faculty 2007        17.2\n10 Full-Time Tenured Faculty 2009        16.8\n# ℹ 45 more rows"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#a-meh-plot-over-the-years",
    "href": "lessons/02_Data_Management/02_Data_Management.html#a-meh-plot-over-the-years",
    "title": "Lesson 2: Data Management with the tidyverse",
    "section": "A “meh” plot over the years",
    "text": "A “meh” plot over the years\n\nggplot(staff_long, aes(x = percentage, y = year, fill = faculty_type)) +\n  geom_col()"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#more-improvement",
    "href": "lessons/02_Data_Management/02_Data_Management.html#more-improvement",
    "title": "Lesson 2: Data Management with the tidyverse",
    "section": "More improvement",
    "text": "More improvement\n\n\n\nstaff_long %&gt;%\n  mutate( \n    part_time = if_else(faculty_type == \"Part-Time Faculty\",\n                        \"Part-Time Faculty\", \"Other Faculty\"),\n    year = as.numeric(year)) %&gt;% \n  ggplot(\n    aes(x = year, y = percentage/100, group = faculty_type, color = part_time)) +\n  geom_line() +\n  scale_color_manual(values = c(\"gray\", \"red\")) + \n  scale_y_continuous(labels = label_percent(accuracy = 1)) + \n  theme_minimal() +\n  labs(\n    title = \"Instructional staff employment trends\",\n    x = \"Year\", y = \"Percentage\", color = NULL) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#all-that-just-to-show-one-helpful-function",
    "href": "lessons/02_Data_Management/02_Data_Management.html#all-that-just-to-show-one-helpful-function",
    "title": "Lesson 2: Data Management with the tidyverse",
    "section": "All that just to show one helpful function",
    "text": "All that just to show one helpful function\nNow we can move onto the other functions mentioned:\n \nData manipulation\n\npivot_longer() and pivot_wider()\nrename()\nmutate()\nfilter()\nselect()\n\nSummarizing data\n\ntbl_summary()\ngroup_by()\nsummarize()\nacross()"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#lets-look-back-at-the-dds.discr-dataset-that-i-briefly-used-last-class",
    "href": "lessons/02_Data_Management/02_Data_Management.html#lets-look-back-at-the-dds.discr-dataset-that-i-briefly-used-last-class",
    "title": "Lesson 2: Data and File Management",
    "section": "Let’s look back at the dds.discr dataset that I briefly used last class",
    "text": "Let’s look back at the dds.discr dataset that I briefly used last class\n   \n\nWe will load the data (This is a special case! dds.discr is a built-in R dataset)\n\n\ndata(\"dds.discr\")\n\n\nNow, let’s take a glimpse at the dataset:\n\n\nglimpse(dds.discr)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ gender       &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ ethnicity    &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#rename-one-of-the-first-things-i-usually-do",
    "href": "lessons/02_Data_Management/02_Data_Management.html#rename-one-of-the-first-things-i-usually-do",
    "title": "Lesson 2: Data and File Management",
    "section": "rename(): one of the first things I usually do",
    "text": "rename(): one of the first things I usually do\n\nI notice that two variables have values that don’t necessarily match the variable name\n\nFemale and male are not genders\n“White not Hispanic” combines race and ethnicity into one category\n\n\n\nI want to rename gender to SAB (sex assigned at birth) and rename ethnicity to R_E (race and ethnicity)\n\n \n\ndds.discr1 = dds.discr %&gt;% \n  rename(SAB = gender, \n         R_E = ethnicity)\n\nglimpse(dds.discr1)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ SAB          &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ R_E          &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#mutate-constructing-new-variables-from-what-you-have",
    "href": "lessons/02_Data_Management/02_Data_Management.html#mutate-constructing-new-variables-from-what-you-have",
    "title": "Lesson 2: Data and File Management",
    "section": "mutate(): constructing new variables from what you have",
    "text": "mutate(): constructing new variables from what you have\n\nWe’ve seen a couple examples for mutate() so far (mostly because its used so often!)\nWe haven’t seen an example where we make a new variable from two variables\n\n\nI want to make a variable that is the ratio of expenditures over age\n\n \n\ndds.discr2 = dds.discr1 %&gt;%\n  mutate(exp_to_age = expenditures/age)\n\nglimpse(dds.discr2)\n\nRows: 1,000\nColumns: 7\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ SAB          &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ R_E          &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…\n$ exp_to_age   &lt;dbl&gt; 124.2941, 1133.0811, 484.6667, 336.8421, 339.3846, 304.40…"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#poll-everywhere-question-4",
    "href": "lessons/02_Data_Management/02_Data_Management.html#poll-everywhere-question-4",
    "title": "Lesson 2: Data and File Management",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#filter-keep-rows-that-match-a-condition",
    "href": "lessons/02_Data_Management/02_Data_Management.html#filter-keep-rows-that-match-a-condition",
    "title": "Lesson 2: Data and File Management",
    "section": "filter(): keep rows that match a condition",
    "text": "filter(): keep rows that match a condition\n\nWhat if I want to subset the data frame? (keep certain rows of observations)\n\n\nI want to look at the data for people who between 50 and 60 years old\n\n \n\ndds.discr3 = dds.discr2 %&gt;%\n  filter(age &gt;= 50 & age &lt;= 60)\n\nglimpse(dds.discr3)\n\nRows: 23\nColumns: 7\n$ id           &lt;int&gt; 15970, 19412, 29506, 31658, 36123, 39287, 39672, 43455, 4…\n$ age.cohort   &lt;fct&gt; 51+, 51+, 51+, 51+, 51+, 51+, 51+, 51+, 51+, 51+, 51+, 51…\n$ age          &lt;int&gt; 51, 60, 56, 60, 59, 59, 54, 57, 52, 57, 55, 52, 59, 54, 5…\n$ SAB          &lt;fct&gt; Female, Female, Female, Female, Male, Female, Female, Mal…\n$ expenditures &lt;int&gt; 54267, 57702, 48215, 46873, 42739, 44734, 52833, 48363, 5…\n$ R_E          &lt;fct&gt; White not Hispanic, White not Hispanic, White not Hispani…\n$ exp_to_age   &lt;dbl&gt; 1064.0588, 961.7000, 860.9821, 781.2167, 724.3898, 758.20…"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#select-keep-or-drop-columns-using-their-names-and-types",
    "href": "lessons/02_Data_Management/02_Data_Management.html#select-keep-or-drop-columns-using-their-names-and-types",
    "title": "Lesson 2: Data and File Management",
    "section": "select(): keep or drop columns using their names and types",
    "text": "select(): keep or drop columns using their names and types\n\nWhat if I want to remove or keep certain variables?\n\n\nI want to only have age and expenditure in my data frame\n\n \n\ndds.discr4 = dds.discr2 %&gt;%\n  select(age, expenditures)\n\nglimpse(dds.discr4)\n\nRows: 1,000\nColumns: 2\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#tbl_summary-table-summary-12",
    "href": "lessons/02_Data_Management/02_Data_Management.html#tbl_summary-table-summary-12",
    "title": "Lesson 2: Data and File Management",
    "section": "tbl_summary() : table summary (1/2)",
    "text": "tbl_summary() : table summary (1/2)\n\nWhat if I want one of those fancy summary tables that are at the top of most research articles? (lovingly called “Table 1”)\n\n\n\n\nlibrary(gtsummary)\ntbl_summary(dds.discr2)\n\n\n\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 1,0001\n    \n  \n  \n    id\n55,385 (31,759, 76,205)\n    age.cohort\n\n        0-5\n82 (8.2%)\n        6-12\n175 (18%)\n        13-17\n212 (21%)\n        18-21\n199 (20%)\n        22-50\n226 (23%)\n        51+\n106 (11%)\n    age\n18 (12, 26)\n    SAB\n\n        Female\n503 (50%)\n        Male\n497 (50%)\n    expenditures\n7,026 (2,898, 37,718)\n    R_E\n\n        American Indian\n4 (0.4%)\n        Asian\n129 (13%)\n        Black\n59 (5.9%)\n        Hispanic\n376 (38%)\n        Multi Race\n26 (2.6%)\n        Native Hawaiian\n3 (0.3%)\n        Other\n2 (0.2%)\n        White not Hispanic\n401 (40%)\n    exp_to_age\n462 (273, 938)\n  \n  \n  \n    \n      1 Median (Q1, Q3); n (%)"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#tbl_summary-table-summary-22",
    "href": "lessons/02_Data_Management/02_Data_Management.html#tbl_summary-table-summary-22",
    "title": "Lesson 2: Data and File Management",
    "section": "tbl_summary() : table summary (2/2)",
    "text": "tbl_summary() : table summary (2/2)\n\nLet’s make this more presentable\n\n \n\n\n\ndds.discr2 %&gt;%\n  select(-id, -age.cohort, -exp_to_age) %&gt;%\n  tbl_summary(label = c(age ~ \"Age\", \n                        R_E ~ \"Race/Ethnicity\", \n                        SAB ~ \"Sex Assigned at Birth\", \n                        expenditures ~ \"Expenditures\") ,\n              statistic = list(all_continuous() ~ \"{mean} ({sd})\"))\n\n\n\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 1,0001\n    \n  \n  \n    Age\n23 (18)\n    Sex Assigned at Birth\n\n        Female\n503 (50%)\n        Male\n497 (50%)\n    Expenditures\n18,066 (19,543)\n    Race/Ethnicity\n\n        American Indian\n4 (0.4%)\n        Asian\n129 (13%)\n        Black\n59 (5.9%)\n        Hispanic\n376 (38%)\n        Multi Race\n26 (2.6%)\n        Native Hawaiian\n3 (0.3%)\n        Other\n2 (0.2%)\n        White not Hispanic\n401 (40%)\n  \n  \n  \n    \n      1 Mean (SD); n (%)"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#group_by-group-by-one-or-more-variables",
    "href": "lessons/02_Data_Management/02_Data_Management.html#group_by-group-by-one-or-more-variables",
    "title": "Lesson 2: Data and File Management",
    "section": "group_by(): group by one or more variables",
    "text": "group_by(): group by one or more variables\n\nWhat if I want to quickly look at group differences?\nIt will not change how the data look, but changes the actions of following functions\n\n\nI want to group my data by sex assigned at birth.\n\n \n\ndds.discr5 = dds.discr2 %&gt;%\n  group_by(SAB)\nglimpse(dds.discr5)\n\nRows: 1,000\nColumns: 7\nGroups: SAB [2]\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ SAB          &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ R_E          &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…\n$ exp_to_age   &lt;dbl&gt; 124.2941, 1133.0811, 484.6667, 336.8421, 339.3846, 304.40…\n\n\n\nLet’s see how the groups change something like the summarize() function in the next slide"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#summarize-summarize-your-data-or-grouped-data-into-one-row",
    "href": "lessons/02_Data_Management/02_Data_Management.html#summarize-summarize-your-data-or-grouped-data-into-one-row",
    "title": "Lesson 2: Data and File Management",
    "section": "summarize(): summarize your data or grouped data into one row",
    "text": "summarize(): summarize your data or grouped data into one row\n\nWhat if I want to calculate specific descriptive statistics for my variables?\nThis function is often best used with group_by()\nIf only presenting the summaries, functions like tbl_summary() is better\nsummarize() creates a new data frame, which means you can plot and manipulate the summarized data\n\n \n\n\nOver whole sample:\n\ndds.discr2 %&gt;% \n  summarize(\n    ave = mean(expenditures),\n    SD = sd(expenditures),\n    med = median(expenditures))\n\n# A tibble: 1 × 3\n     ave     SD   med\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 18066. 19543.  7026\n\n\n\nGrouped by sex assigned at birth:\n\ndds.discr2 %&gt;% \n  group_by(SAB) %&gt;% \n  summarize(\n    ave = mean(expenditures),\n    SD = sd(expenditures),\n    med = median(expenditures))\n\n# A tibble: 2 × 4\n  SAB       ave     SD   med\n  &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 Female 18130. 20020.  6400\n2 Male   18001. 19068.  7219"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#across-apply-a-function-across-multiple-columns",
    "href": "lessons/02_Data_Management/02_Data_Management.html#across-apply-a-function-across-multiple-columns",
    "title": "Lesson 2: Data and File Management",
    "section": "across(): apply a function across multiple columns",
    "text": "across(): apply a function across multiple columns\n\nLike group_by(), this function is often paired with another transformation function\n\n\nI want all my integer values to have two significant figures.\n\n \n\ndds.discr6 = dds.discr2 %&gt;%\n  mutate(across(where(is.integer), signif, digits = 2))\n\nglimpse(dds.discr6)\n\nRows: 1,000\nColumns: 7\n$ id           &lt;dbl&gt; 10000, 10000, 10000, 11000, 11000, 11000, 11000, 11000, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;dbl&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ SAB          &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;dbl&gt; 2100, 42000, 1500, 6400, 4400, 4600, 3900, 3900, 5000, 29…\n$ R_E          &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…\n$ exp_to_age   &lt;dbl&gt; 124.2941, 1133.0811, 484.6667, 336.8421, 339.3846, 304.40…"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#dplyr-resources",
    "href": "lessons/02_Data_Management/02_Data_Management.html#dplyr-resources",
    "title": "Lesson 2: Data and File Management",
    "section": "dplyr resources",
    "text": "dplyr resources\n\nMore dpylr functions to reference!\n\nAdditional details and examples are available in the vignettes:\n\ncolumn-wise operations vignette\nrow-wise operations vignette\n\n \nand the dplyr 1.0.0 release blog posts:\n\nworking across columns\nworking within rows"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#r-programming-class-at-ohsu",
    "href": "lessons/02_Data_Management/02_Data_Management.html#r-programming-class-at-ohsu",
    "title": "Lesson 2: Data and File Management",
    "section": "R programming class at OHSU!",
    "text": "R programming class at OHSU!\nYou can check out Dr. Jessica Minnier’s R class page if you want more notes, videos, etc."
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#the-larger-tidy-ecosystem",
    "href": "lessons/02_Data_Management/02_Data_Management.html#the-larger-tidy-ecosystem",
    "title": "Lesson 2: Data and File Management",
    "section": "The larger tidy ecosystem",
    "text": "The larger tidy ecosystem\nJust to name a few…\n\njanitor\nkableExtra\npatchwork\ngghighlight\ntidybayes"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#credit-to-mine-çetinkaya-rundel",
    "href": "lessons/02_Data_Management/02_Data_Management.html#credit-to-mine-çetinkaya-rundel",
    "title": "Lesson 2: Data and File Management",
    "section": "Credit to Mine Çetinkaya-Rundel",
    "text": "Credit to Mine Çetinkaya-Rundel\n\nThese notes were built from Mine’s notes\n\nMost pages and code were left as she made them\nI changed a few things to match our class\n\nPlease see her Github repository for the original notes\n\n\n\nLesson 2: Data Management"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#footnotes",
    "href": "lessons/02_Data_Management/02_Data_Management.html#footnotes",
    "title": "Data Management with the tidyverse",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSource: R for Data Science. Grolemund and Wickham.↩︎"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management_key_info.html#key-dates",
    "href": "lessons/02_Data_Management/02_Data_Management_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates\n\nHomework 0 due this Thursday at 11pm\n\nLet me know if you have issues with turning it in on Sakai"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval_key_info.html#key-dates",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html",
    "title": "SLR: More inference + Evaluation",
    "section": "",
    "text": "Identify different sources of variation in an Analysis of Variance (ANOVA) table\nUsing the F-test, determine if there is enough evidence that population slope \\(\\beta_1\\) is not 0\nCalculate and interpret the coefficient of determination\nDescribe the model assumptions made in linear regression using ordinary least squares\n\n\n\n\n\nLesson 1 of SLR:\n\nFit regression line\nCalculate slope & intercept\nInterpret slope & intercept\n\nLesson 2 of SLR:\n\nEstimate variance of the residuals\nInference for slope & intercept: CI, p-value\nConfidence bands of regression line for mean value of Y|X\n\n\n\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#so-far-in-our-regression-example",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#so-far-in-our-regression-example",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "So far in our regression example…",
    "text": "So far in our regression example…\n\n\nLesson 3: SLR 1\n\nFit regression line\nCalculate slope & intercept\nInterpret slope & intercept\n\nLesson 4: SLR 2\n\nEstimate variance of the residuals\nInference for slope & intercept: CI, p-value\nConfidence bands of regression line for mean value of Y|X\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#lets-revisit-the-regression-analysis-process",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#lets-revisit-the-regression-analysis-process",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Let’s revisit the regression analysis process",
    "text": "Let’s revisit the regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#getting-to-the-f-test",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#getting-to-the-f-test",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Getting to the F-test",
    "text": "Getting to the F-test\nThe F statistic in linear regression is essentially a proportion of the variance explained by the model vs. the variance not explained by the model\n\nStart with visual of explained vs. unexplained variation\nFigure out the mathematical representations of this variation\nLook at the ANOVA table to establish key values measuring our variance from our model\nBuild the F-test"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#explained-vs.-unexplained-variation",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#explained-vs.-unexplained-variation",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Explained vs. Unexplained Variation",
    "text": "Explained vs. Unexplained Variation\n\n\\[ \\begin{aligned}\nY_i - \\overline{Y} & = (Y_i - \\widehat{Y}_i) + (\\widehat{Y}_i- \\overline{Y})\\\\\n\\text{Total unexplained variation} & = \\text{Residual variation after regression} + \\text{Variation explained by regression}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#more-on-the-equation",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#more-on-the-equation",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "More on the equation",
    "text": "More on the equation\n\n\n\\[Y_i - \\overline{Y} = (Y_i - \\widehat{Y}_i) + (\\widehat{Y}_i- \\overline{Y})\\]\n\n\\(Y_i - \\overline{Y}\\) = the deviation of \\(Y_i\\) around the mean \\(\\overline{Y}\\)\n\n(the total amount deviation unexplained at \\(X_i\\) ).\n\n\\(Y_i - \\widehat{Y}_i\\) = the deviation of the observation \\(Y\\) around the fitted regression line\n\n(the amount deviation unexplained by the regression at \\(X_i\\) ).\n\n\\(\\widehat{Y_i}- \\overline{Y}\\) = the deviation of the fitted value \\(\\widehat{Y}_i\\) around the mean \\(\\overline{Y}\\)\n\n(the amount deviation explained by the regression at \\(X_i\\) )"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question-1",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question-1",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#how-is-this-actually-calculated-for-our-fitted-model",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#how-is-this-actually-calculated-for-our-fitted-model",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "How is this actually calculated for our fitted model?",
    "text": "How is this actually calculated for our fitted model?\n\\[ \\begin{aligned}\nY_i - \\overline{Y} & = (Y_i - \\hat{Y}_i) + (\\hat{Y}_i- \\overline{Y})\\\\\n\\text{Total unexplained variation} & = \\text{Variation due to regression} + \\text{Residual variation after regression}\n\\end{aligned}\\]\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 & = \\sum_{i=1}^n (\\hat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 \\\\\nSSY & = SSR + SSE\n\\end{aligned}\\] \\[\\text{Total Sum of Squares} = \\text{Sum of Squares due to Regression} + \\text{Sum of Squares due to Error (residuals)}\\]\n\n\n\n\nANOVA table:\n\n\n\n\n\n\n\n\n\n\n\nVariation Source\ndf\nSS\nMS\ntest statistic\np-value\n\n\n\n\nRegression\n\\(1\\)\n\\(SSR\\)\n\\(MSR = \\frac{SSR}{1}\\)\n\\(F = \\frac{MSR}{MSE}\\)\n\n\n\nError\n\\(n-2\\)\n\\(SSE\\)\n\\(MSE = \\frac{SSE}{n-2}\\)\n\n\n\n\nTotal\n\\(n-1\\)\n\\(SSY\\)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#analysis-of-variance-anova-table-in-r",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#analysis-of-variance-anova-table-in-r",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Analysis of Variance (ANOVA) table in R",
    "text": "Analysis of Variance (ANOVA) table in R\n\n# Fit regression model:\nmodel1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate,\n             data = gapm)\n\nanova(model1)\n\nAnalysis of Variance Table\n\nResponse: LifeExpectancyYrs\n                   Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nFemaleLiteracyRate  1 2052.8 2052.81  54.414 1.501e-10 ***\nResiduals          78 2942.6   37.73                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n   tab_options(table.font.size = 40) %&gt;%\n   fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1.000\n2,052.812\n2,052.812\n54.414\n0.000\n    Residuals\n78.000\n2,942.635\n37.726\nNA\nNA"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#what-is-the-f-statistic-testing",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#what-is-the-f-statistic-testing",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "What is the F statistic testing?",
    "text": "What is the F statistic testing?\n\\[F = \\frac{MSR}{MSE}\\]\n\nIt can be shown that\n\n\\[E(MSE)=\\sigma^2\\ \\text{and}\\ E(MSR) = \\sigma^2 + \\beta_1^2\\sum_{i=1}^n (X_i- \\overline{X})^2\\]\n\nRecall that \\(\\sigma^2\\) is the variance of the residuals\nThus if\n\n\\(\\beta_1 = 0\\), then \\(F \\approx \\frac{\\widehat{\\sigma}^2}{\\widehat{\\sigma}^2} = 1\\)\n\\(\\beta_1 \\neq 0\\), then \\(F \\approx \\frac{\\widehat{\\sigma}^2 + \\widehat{\\beta}_1^2\\sum_{i=1}^n (X_i- \\overline{X})^2}{\\widehat{\\sigma}^2} &gt; 1\\)\n\nSo the \\(F\\) statistic can also be used to test \\(\\beta_1\\)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#f-test-vs.-t-test-for-the-population-slope",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#f-test-vs.-t-test-for-the-population-slope",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "F-test vs. t-test for the population slope",
    "text": "F-test vs. t-test for the population slope\nThe square of a \\(t\\)-distribution with \\(df = \\nu\\) is an \\(F\\)-distribution with \\(df = 1, \\nu\\)\n\\[T_{\\nu}^2 \\sim F_{1,\\nu}\\]\n\nWe can use either F-test or t-test to run the following hypothesis test:\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote that the F-test does not support one-sided alternative tests, but the t-test does!\n\nF-test cannot handle alternatives like \\(\\beta_1 &gt; 0\\) nor \\(\\beta_2 &lt; 0\\)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#planting-a-seed-about-the-f-test",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#planting-a-seed-about-the-f-test",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Planting a seed about the F-test",
    "text": "Planting a seed about the F-test\nWe can think about the hypothesis test for the slope…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_1\\neq0\\)\n\n\n\n\nin a slightly different way…\n\n\n\n\n\n\nNull model (\\(\\beta_1=0\\))\n\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\nSmaller (reduced) model\n\n\n\n\n\n\nAlternative model (\\(\\beta_1\\neq0\\))\n\n\n\n\\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)\nLarger (full) model\n\n\n\n\n\n\nIn multiple linear regression, we can start using this framework to test multiple coefficient parameters at once\n\nDecide whether or not to reject the smaller reduced model in favor of the larger full model\nCannot do this with the t-test when we have multiple coefficients!"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question-2",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question-2",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#f-test-general-steps-for-hypothesis-test-for-population-slope-beta_1",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#f-test-general-steps-for-hypothesis-test-for-population-slope-beta_1",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "F-test: general steps for hypothesis test for population slope \\(\\beta_1\\)",
    "text": "F-test: general steps for hypothesis test for population slope \\(\\beta_1\\)\n\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nOften, we are curious if the coefficient is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=1\\) and denominator \\(df=n-2\\).\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic for \\(\\widehat\\beta_1\\) is\n\\[F = \\frac{MSR}{MSE}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{1, n-2} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject: \\(P(F_{1, n-2} &gt; F) &lt; \\alpha\\)\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at the \\(100\\alpha\\%\\) significiance level. There is (sufficient/insufficient) evidence that there is significant association between (\\(Y\\)) and (\\(X\\)) (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\)\n\nSteps 1-4 are setting up our hypothesis test: not much change from the general steps\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nWe are testing if the slope is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=1\\) and denominator \\(df=n-2 = 80-2\\).\n\n\nnobs(model1)\n\n[1] 80"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-24",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-24",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/4)\n\n\n\nCompute the value of the test statistic\n\n\n\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1\n2052.812\n2052.81234\n54.4136\n1.501286e-10\n    Residuals\n78\n2942.635\n37.72609\nNA\nNA\n  \n  \n  \n\n\n\n\n\nOption 1: Calculate the test statistic using the values in the ANOVA table\n\n\\[F = \\frac{MSR}{MSE} = \\frac{2052.81}{37.73}=54.414\\]\n\nOption 2: Get the test statistic value (F) from the ANOVA table\n\n\nI tend to skip this step because I can do it all with step 6"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-34",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-34",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (3/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (3/4)\n\n\n\nCalculate the p-value\n\n\n\n\nAs per Step 4, test statistic \\(F\\) can be modeled by a \\(F\\)-distribution with \\(df1 = 1\\) and \\(df2 = n-2\\).\n\nWe had 80 countries’ data, so \\(n=80\\)\n\nOption 1: Use pf() and our calculated test statistic\n\n\n# p-value is ALWAYS the right tail for F-test\npf(54.414, df1 = 1, df2 = 78, lower.tail = FALSE)\n\n[1] 1.501104e-10\n\n\n\nOption 2: Use the ANOVA table\n\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1\n2052.812\n2052.81234\n54.4136\n1.501286e-10\n    Residuals\n78\n2942.635\n37.72609\nNA\nNA"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-44",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-44",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (4/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (4/4)\n\n\n\nWrite conclusion for the hypothesis test\n\n\n\nWe reject the null hypothesis that the slope is 0 at the \\(5\\%\\) significance level. There is sufficient evidence that there is significant association between female life expectancy and female literacy rates (p-value &lt; 0.0001)."
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#did-you-notice-anything-about-the-p-value",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#did-you-notice-anything-about-the-p-value",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Did you notice anything about the p-value?",
    "text": "Did you notice anything about the p-value?\nThe p-value of the t-test and F-test are the same!!\n\nFor the t-test:\n\n\ntidy(model1) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    FemaleLiteracyRate\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\nFor the F-test:\n\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1\n2052.812\n2052.81234\n54.4136\n1.501286e-10\n    Residuals\n78\n2942.635\n37.72609\nNA\nNA\n  \n  \n  \n\n\n\n\nThis is true when we use the F-test for a single coefficient!"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#correlation-coefficient-from-511",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#correlation-coefficient-from-511",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Correlation coefficient from 511",
    "text": "Correlation coefficient from 511\n\n\nCorrelation coefficient \\(r\\) can tell us about the strength of a relationship\n\nIf \\(r = -1\\), then there is a perfect negative linear relationship between \\(X\\) and \\(Y\\)\nIf \\(r = 1\\), then there is a perfect positive linear relationship between \\(X\\) and \\(Y\\)\nIf \\(r = 0\\), then there is no linear relationship between \\(X\\) and \\(Y\\)\n\nNote: All other values of \\(r\\) tell us that the relationship between \\(X\\) and \\(Y\\) is not perfect. The closer \\(r\\) is to 0, the weaker the linear relationship."
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#correlation-coefficient",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#correlation-coefficient",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Correlation coefficient",
    "text": "Correlation coefficient\n\n\nThe (Pearson) correlation coefficient \\(r\\) of variables \\(X\\) and \\(Y\\) can be computed using the formula:\n\\[\\begin{aligned}\nr  & = \\frac{\\sum_{i=1}^n (X_i - \\overline{X})(Y_i - \\overline{Y})}{\\Big(\\sum_{i=1}^n (X_i - \\overline{X})^2 \\sum_{i=1}^n (Y_i - \\overline{Y})^2\\Big)^{1/2}} \\\\\n& = \\frac{SSXY}{\\sqrt{SSX \\cdot SSY}}\n\\end{aligned}\\]\nwe have the relationship\n\\[\\widehat{\\beta}_1 = r\\frac{SSY}{SSX},\\ \\ \\text{or},\\ \\  r = \\widehat{\\beta}_1\\frac{SSX}{SSY}\\]"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#coefficient-of-determination-r2",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#coefficient-of-determination-r2",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Coefficient of determination: \\(R^2\\)",
    "text": "Coefficient of determination: \\(R^2\\)\nIt can be shown that the square of the correlation coefficient \\(r\\) is equal to\n\\[R^2 = \\frac{SSR}{SSY} = \\frac{SSY - SSE}{SSY}\\]\n\n\\(R^2\\) is called the coefficient of determination.\nInterpretation: The proportion of variation in the \\(Y\\) values explained by the regression model\n\\(R^2\\) measures the strength of the linear relationship between \\(X\\) and \\(Y\\):\n\n\\(R^2 = \\pm 1\\): Perfect relationship\n\nHappens when \\(SSE = 0\\), i.e. no error, all points on the line\n\n\\(R^2 = 0\\): No relationship\n\nHappens when \\(SSY = SSE\\), i.e. using the line doesn’t not improve model fit over using \\(\\overline{Y}\\) to model the \\(Y\\) values."
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Poll Everywhere Question",
    "text": "Poll Everywhere Question"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-correlation-coeffiicent-r-and-coefficient-of-determination-r2",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#life-expectancy-example-correlation-coeffiicent-r-and-coefficient-of-determination-r2",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Life expectancy example: correlation coeffiicent \\(r\\) and coefficient of determination \\(R^2\\)",
    "text": "Life expectancy example: correlation coeffiicent \\(r\\) and coefficient of determination \\(R^2\\)\n\n\n\n(r = cor(x = gapm$LifeExpectancyYrs, \n    y = gapm$FemaleLiteracyRate,\n    use =  \"complete.obs\"))\n\n[1] 0.6410434\n\nr^2\n\n[1] 0.4109366\n\n(sum_m1 = summary(model1)) # for R^2 value\n\n\nCall:\nlm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate, data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        50.92790    2.66041  19.143  &lt; 2e-16 ***\nFemaleLiteracyRate  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\nsum_m1$r.squared\n\n[1] 0.4109366\n\n\n\n   \n\n\nInterpretation\n\n\n41.1% of the variation in countries’ life expectancy is explained by the linear model with female literacy rate as the independent variable."
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#what-does-r2-not-measure",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#what-does-r2-not-measure",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "What does \\(R^2\\) not measure?",
    "text": "What does \\(R^2\\) not measure?\n\n\n\n\\(R^2\\) is not a measure of the magnitude of the slope of the regression line\n\nExample: can have \\(R^2 = 1\\) for many different slopes!!\n\n\\(R^2\\) is not a measure of the appropriateness of the straight-line model\n\nExample: figure"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#least-squares-model-assumptions-eline",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#least-squares-model-assumptions-eline",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Least-squares model assumptions: eLINE",
    "text": "Least-squares model assumptions: eLINE\nThese are the model assumptions made in ordinary least squares:\n \n\ne xistence: For any \\(X\\), there exists a distribution for \\(Y\\)\n\n \n\nL inearity of relationship between variables\n\n \n\nI ndependence of the \\(Y\\) values\n\n \n\nN ormality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n \n\nE quality of variance of the residuals (homoscedasticity)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#e-existence-of-ys-distribution",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#e-existence-of-ys-distribution",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "e: Existence of Y’s distribution",
    "text": "e: Existence of Y’s distribution\n\nFor any fixed value of the variable \\(X\\), \\(Y\\) is a\n\nrandom variable with a certain probability distribution\nhaving finite\n\nmean and\nvariance\n\n\nThis leads to the normality assumption\nNote: This is not about \\(Y\\) alone, but \\(Y|X\\)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#l-linearity",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#l-linearity",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "L: Linearity",
    "text": "L: Linearity\n\n\n\nThe relationship between the variables is linear (a straight line):\n\nThe mean value of \\(Y\\) given \\(X\\), \\(\\mu_{y|x}\\) or \\(E[Y|X]\\), is a straight-line function of \\(X\\)\n\n\n\\[\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\]"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#i-independence-of-observations",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#i-independence-of-observations",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "I: Independence of observations",
    "text": "I: Independence of observations\n\nThe \\(Y\\)-values are statistically independent of one another\nExamples of when they are not independent, include\n\nrepeated measures (such as baseline, 3 months, 6 months)\ndata from clusters, such as different hospitals or families\n\nThis condition is checked by reviewing the study design and not by inspecting the data\n\n \n\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question-3",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question-3",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#n-normality",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#n-normality",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "N: Normality",
    "text": "N: Normality\n\n\n\nFor any fixed value of \\(X\\), \\(Y\\) has normal distribution.\n\nNote: This is not about \\(Y\\) alone, but \\(Y|X\\)\n\nEquivalently, the measurement (random) errors \\(\\epsilon_i\\) ’s normally distributed\n\nThis is more often what we check"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#e-equality-of-variance-of-the-residuals",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#e-equality-of-variance-of-the-residuals",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "E: Equality of variance of the residuals",
    "text": "E: Equality of variance of the residuals\n\nThe variance of \\(Y\\) given \\(X\\) (\\(\\sigma_{Y|X}^2\\)), is the same for any \\(X\\)\n\nWe use just \\(\\sigma^2\\) to denote the common variance\n\nThis is also called homoscedasticity"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#summary-of-eline-model-assumptions",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#summary-of-eline-model-assumptions",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Summary of eLINE model assumptions",
    "text": "Summary of eLINE model assumptions\n\n\\(Y\\) values are independent (check study design!)\n\n\n\n\n\nThe distribution of \\(Y\\) given \\(X\\) is\n\nnormal\nwith mean \\(\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\)\nand common variance \\(\\sigma^2\\)\n\n\n\n\nThis means that the residuals are\n\nnormal\nwith mean = 0\nand common variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#anscombes-quartet",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#anscombes-quartet",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet\n\n\n\nLesson 5: SLR 3"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-1",
    "href": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-1",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "syllabus.html#key-course-info",
    "href": "syllabus.html#key-course-info",
    "title": "BSTA 512/612 Syllabus",
    "section": "Key Course Info",
    "text": "Key Course Info\n\nIf an assignment on Sakai is closed or you are submitting late work, please email me AND the TAs your work\nIf you are emailing for an extension, please email me AND the TAs.\nLatework policy\n\nFor homework: can be turned in any time before course ends, but you will not receive feedback on late work\nFor labs: one “no questions asked” extension\n\nAttendance policy: attend in person and fill out exit tickets for 12 out of the 19 classes\nThe class will end on March 17, 2025. All coursework MUST be completed by March 21, 2025 at 11pm."
  },
  {
    "objectID": "project.html#poster-and-presentation",
    "href": "project.html#poster-and-presentation",
    "title": "Project Central",
    "section": "Poster and presentation",
    "text": "Poster and presentation\nLast Year’s Report Instructions\n\nI am currently updating the Poster instructions\n\nPoster due 3/17\n\nReading and listening sources\nIf you are interested in sources that discuss the social complexities of anti-fat bias, feel free to take a look at the following sources. Please be aware that these resources will discuss anti-fat bias and related histories, including racism and sexism.\n\nArticle: Implicit and explicit anti-fat bias: The role of weight-related attitudes and beliefs\nPodcast: Anti-Fat Bias by Maintenance Phase\nBook: Fearing the Black Body: The Racial Origins of Fat Phobia\n\nMultnomah County Library has unlimited loans for the audiobook\n\nBlog: Dances with Fat\n\nYou can subscribe to Ragen’s weekly newsletter for free\n\n\nIf you have additional sources that you would like to share, please send them to me!"
  },
  {
    "objectID": "labs/Project_poster_instructions.html",
    "href": "labs/Project_poster_instructions.html",
    "title": "Project Poster Instructions",
    "section": "",
    "text": "Important\n\n\n\nInstructions and rubric are still in progress!\nThese instructions were partially developed using ChatGPT by feeding in my previous project report instructions and asking ChatGPT to edit for a poster."
  },
  {
    "objectID": "labs/Project_poster_instructions.html#directions",
    "href": "labs/Project_poster_instructions.html#directions",
    "title": "Project Poster Instructions",
    "section": "1 Directions",
    "text": "1 Directions\n\n\n\n\n\n\nProject template\n\n\n\nYou may use this project template to get started on the report. It is your responsibility to meet the formatting guidelines below!!\nDO NOT USE SITE PAGE (“Project Report Instructions”, current page) as your template!!\n\n\n\n1.1 Purpose\nA scientific poster serves as a visual and concise way to communicate research findings. For this project, your poster should highlight your linear regression analysis and results while ensuring the context and methods are clearly explained. Posters should balance visuals (e.g., tables, figures) with text to engage an audience effectively.\n\n\n1.2 Formatting guide\n\nThe report will be written in Quarto. Turn in both the qmd and html files\n\nNo code should appear in the html document\n\nThis means all R code chunks should have #| echo: false\nThis also means warnings and messages should be turned off\n\n\nThe report should be 10 - 14 paragraphs long\nTables and figures should NOT have variable names as they appear in the data frame\n\nVariable names should be understood by a reader\nVariable names should be written in full words\nInclude a title or caption for all figures\nFigure and tables appear on same page or close to same page where they are first referenced\nTables and figures are an appropriate size in the html - Nicky is able to read all words in figures and tables\n\nWriting, spelling, and grammar should be admissable\n\nThis means I can generally follow your thought/what you are trying to communicate\nSome spelling and grammar mistakes are allowed\n\nI will not take off points if there are a few sprinkled in\nIf every or close to every sentence has mistakes, then I will take off\n\n\nSectioning of the report\n\nMain sections that were required: Introduction, Statistical Methods, Results, Discussion, Conclusion, and References\nOther sections that might help group specific methods or results\n\nTitle information at the top of the html\n\nThis includes the title itself, your name, and the date\n\n\n\n\n\n\n\n\nThe project report is a separate file from the labs\n\n\n\nYou can save tables and figures from labs or separate files, then load them in the report\n\nSave R objects in analyses file:\n\nSuppose you named the Table 1 as table1\nsave(table1, file = \"table1.Rdata\")\n\nLoad R objects in report file: load(file = \"table1.Rdata\")\n\n\n\n\n\n1.3 Examples of reports\nThe following are examples of reports from BSTA 513 with the feedback that I gave them.\nPlease note that 513 uses a different type of outcome than our class. These examples are meant to help guide you with the formatting and some appropriate content.\nAlso note that these were converted to PDFs so I could write in feedback. Some of the tables and figure sizes were distorted. They need to be legible in the html.\n\nReport 1 with my feedback\nReport 2 with my feedback\n\nThe above reports have code showing in their html. Remember that I am asking you to hide all code, warnings, and messages.\n\n\n1.4 Grading\nThe project report is out of 36 points. Note that the Statistical Methods and Results sections are graded on an 8-point scale, while all other components are graded on a 4-point scale.\n\n1.4.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences with very few grammatical or spelling errors. With little editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences with some (around 2 per section) grammatical or spelling errors. With some editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences, but have many grammatical or spelling errors. With major editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences, but are very hard to follow due to grammar mistakes.\nLab not submitted on Sakai (or by email if late) with .html file. Report is not written with complete sentences. With major editing, the report can be distributed.\n\n\nFigures and work\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables look professional, are easily interpreted by the reader, and easily convey the intended message.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. For the most part, figures and tables look professional, are easily interpreted by the reader, and easily convey the intended message. A few mistakes in the figures are made.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables look semi-professional, are not so easily interpreted by the reader, and convey the intended message but after some work by the reader. Some mistakes in the figures are made.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables do not look professional, are not easily interpreted by the reader, and/or do not convey the intended message. Many mistakes in the figures are made.\nRequested output is not displayed, Missing one or more figures.\n\n\nIntroduction\nProvides a good background for the research question, includes motivation for the question, and references previous research that justifies this analysis.\nProvides a decent background for the research question and includes motivation for the question. Previous research is mentioned, but feels disconnected to the current analysis.\nProvides a decent background for the research question and includes motivation for the question. Previous research is mentioned, but feels disconnected to the current analysis.\nDoes not provide a background that connects to the research question. Motivation and previous research are not mentioned.\nNo introduction included.\n\n\nMethods (8 points)\nDescribes statistical methods concisely and highlights pertinent information to the reader (listed Sections below). Demonstrates proper analyses were performed.\nDescribes statistical methods and highlights pertinent information to the reader (listed Sections below). Details were omitted or added that were not needed to explain the overarching methods. Demonstrates proper analyses were performed.\nDescribes statistical methods and highlights pertinent information to the reader (listed Sections below). Details were omitted or added that were not needed to explain the overarching methods. Some incorrect analyses included in the description.\nDescribes statistical methods, but lacks clarity. Demonstrates a lack of understanding about the overall process of regression analysis. Incorrect analyses included in the description.\nNo methods included.\n\n\nResults (8 points)\nCorrectly interprets coefficients for the explanatory variable and identifies any other interesting trends. Highlights pertinent results to the reader (listed Sections below).\nCorrectly interprets coefficients, but does correctly incorporate the interaction (if in the model). Highlights pertinent results to the reader (listed Sections below).\nIncorrectly interprets coefficients. Highlights pertinent results to the reader (listed Sections below).\nIncorrectly interprets coefficients.Omits pertinent results to the reader (listed Sections below).\nNo results included.\n\n\nDiscussion\nThoroughly and concisely discusses limitations and considerations of the results, and their consequences.\nDiscusses limitations and considerations of the results and their consequences, but misses some big considerations.\nDiscusses limitations and considerations of the results, but does not discuss the consequences.\nDiscusses limitations and considerations of the results, but misses many considerations and does not discuss consequences.\nNo discussion included.\n\n\nConclusion and References\nFor the conclusion, main research question is answered and statistical caveats described to non-technical person. References are mostly cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is answered and statistical caveats described to non-technical person. References are sometimes cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is somewhat answered (but focus is not on the research question) and statistical caveats described to non-technical person. References are sometimes cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is somewhat answered (but not the focus at all) and statistical caveats are not described. References are not cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is not answered. Or references are not included at all.\n\n\n\n\nIn formatting, an example of a report with little editing needed is one that has zero to some grammar or spelling mistakes, no code chunks showing, and no output warnings nor messages showing.\nProfessional figures mean\n\nI can read the words and numbers in the html\n\nVariable names are converted from the data frame version to readable text\nFor example: iam_001 does not show up on axes, instead something like: Response to \"Currently, I am...\"\n\nColors are only used if conveying information\nIntended message of the figure is easily understood\n\nIf you are trying to show a trend of mean IAT vs. an ordered categorical variable, then the variable is ordered on the x-axis\n\n\nFor the references\n\nI will not be overly critical about the formatting\nBy consistency, I mean that you if you are citing things like (Last Name, Year) it doesn’t suddenly change to number citations.\nIf you would like to use Quarto’s citation tool, you can! I actually pair it with Zotero and it works beautifully! (But I would not embark on this if you haven’t used Zotero before)"
  },
  {
    "objectID": "labs/Project_poster_instructions.html#sections",
    "href": "labs/Project_poster_instructions.html#sections",
    "title": "Project Poster Instructions",
    "section": "2 Sections",
    "text": "2 Sections\n\n2.1 Title\n\nPurpose: Create an identifiable name for your research project that includes the main research question’s variables and gives some context to the analysis or results\n\n\n\n2.2 Introduction\n\nLength: 1-2 paragraphs\nPurpose: Introduce the research question and why it is important to study\nThis section is non-technical.\n\nBy reading just the introduction and conclusion, someone without a technical background should have an idea of what they study was about, why it is important, and what the main results are\n\nYou may start with the introduction written in Lab 1, but you should edit it and make sure it flows into your report well!\nShould contain some references\n\n\n\n2.3 Statistical Methods\n\nLength: 3-5 paragraphs\nPurpose: Describe the analyses that were conducted and methods used to select variables and check diagnostics\nImportant to keep in mind: methods typically describe your approach and process, not the results of that process\n\nFor example: I might say “We investigated the linearity of each continuous covariate visually. If continuous variables were not linear, then we divided the variable into categories using existing guidelines from &lt;insert reference here&gt; or creating quartiles.”\n\nIn the methods section, I would NOT say: “We investigated the linearity of each continuous covariate visually. We found that age was not linearly related to IAT scores. Thus, we categorized age into the following groups: ___, ____, ____, ____, and ____.”\n\nThe last two sentences about age would be more appropriate in the Results section\n\n\n\nSome important methods to discuss (You may divide these into your sections, not necessarily with these names)\n\nGeneral approach to the dataset\n\n3-5 sentences\nDid you need to do any quality control?\nMissing data: we performed complete case analysis\n\n1 sentence\nCan be included in the Exploratory data analysis section\n\n\nVariables and variable creation\n\nThis includes a description of analyses for Table 1 and what statistics were used to summarize the variables\n\nMore on creation of Table 1, not discussing the results of Table 1\n\nIncludes (not required)\n\nIndicators for gender identity or race\nCreating BMI\nCategorizing a continuous variable (even if performed in model selection)\nUsing scoring for an ordered categorical variable (that is not your explanatory variable)\n\n1-2 sentences per variable\n\nModel building: we performed purposeful selection\n\n3-5 sentences\nIncludes\n\nDescribe purposeful selection: combining existing literature, clinical significance, and analysis\nHow did you build the model? Describe the process\nDid you consider confounders and effect modifiers?\n\n\nModel diagnostics\n\n2-5 sentences\nIncludes\n\nProcess of investigating model diagnostics\nBy the time you build the model, LINE assumptions should be met\nIf assumptions were not met, what process did you use to fix it?\n\n\n\n\n\n\n2.4 Results\n\nLength: ~3 paragraphs\nPurpose: Relay the results from our sample’s analysis typically focusing on the numbers and interpretations\nSome important results to discuss (also could be sections)\n\nSample data set statistics (Table 1)\n\n3-5 sentences\nInclude a brief description of the sample’s characteristics\nTable 1 should be referenced and appear here!\n\nFinal model\n\n1-2 sentences\nDescribe final model (or models if comparing a few)\n\nWhat variables were included in your final model?\nWhat interactions with your explanatory variable did you include?\n\n\nInterpret the model coefficients in the context of the research question\n\n1-2 paragraphs\nInterpreting the explanatory variable’s relationship with IAT score is the most important thing to report!!\n\nWhen doing this, make sure you account for ALL interactions: If your explanatory variable has multiple interactions and you are trying to interpret one, then what does that mean about the other variables involved in the other interactions? If this is confusing, please make an appointment with me!!\n\n\nResults of model diagnostics if there is anything worth noting\n\nTables & figures\n\nThe following are required tables or figures\n\nTable 1 summarizing participant characteristics both overall and stratified by your primary independent variable\nTable or figure with regression results\n\nCan be a forest plot\nIf you have A LOT of coefficient estimates, the forest plot may not work well!\n\n\n1-3 figures that you think are helpful in understanding the results, for example\n\nDAG explaining connection between variables (if you did this)\nTable or figure to compare model fit statistics (if you did this)\nTable or figure for unadjusted relationship between outcome and explanatory variables\n\n\n\n\n\n2.5 Discussion\n\nLength: 2-3 paragraphs\nPurpose: Discuss the results and give them context outside of the sample and its analysis\nSome important things to include\n\nInclude a paragraph on the limitations of the results\n\nYou don’t need to hit all the limitations, but think about the big ones (generalizability? independence of samples? large sample size vs. clinical significance? the way we handled variables?)\n\nAfter limitations, discuss the positive parts of the results\n\nWhat can we do with these results? What impact can it have?\n\nAny overarching trends that are worth noting? (Giebel et al. 2024)\n\nShould contain some references\n\n\n\n2.6 Conclusion\n\nLength: 1 short paragraph (more like ~3 sentences)\nPurpose: Describe the main conclusions to a non-technical audience\n\n\n\n2.7 References\n\nInclude your references here!\nYou introduction should have references, especially when discussing the social science behind the analysis\nYou must reference the IAT data source!!"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-1",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#poll-everywhere-question-1",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#what-did-we-learn-in-511611",
    "href": "lessons/01_Review/01_Review.html#what-did-we-learn-in-511611",
    "title": "Lesson 1: Review",
    "section": "What did we learn in 511/611?",
    "text": "What did we learn in 511/611?\n\nIn 511, we talked about categorical and continuous outcomes (dependent variables)\n \nWe also talked about their relationship with 1-2 continuous or categorical exposure (independent variables or predictor)\n \nWe had many good ways to assess the relationship between an outcome and exposure:\n \n\n\n\n\n\n\n\n\n\n\nContinuous Outcome\nCategorical Outcome\n\n\nContinuous Exposure\nCorrelation, simple linear regression\n??\n\n\nCategorical Exposure\nt-tests, paired t-tests, 2 sample t-tests, ANOVA\nproportion t-test, Chi-squared goodness of fit test, Fisher’s Exact test, Chi-squared test of independence, etc."
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#another-way-of-thinking-about-ssy-ssr-and-sse",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#another-way-of-thinking-about-ssy-ssr-and-sse",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Another way of thinking about SSY, SSR, and SSE",
    "text": "Another way of thinking about SSY, SSR, and SSE"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#what-did-we-learn-in-511611-12",
    "href": "lessons/01_Review/01_Review.html#what-did-we-learn-in-511611-12",
    "title": "Lesson 1: Review",
    "section": "What did we learn in 511/611? (1/2)",
    "text": "What did we learn in 511/611? (1/2)\n\nIn 511, we talked about categorical and continuous outcomes (dependent variables)\n \nWe also talked about their relationship with 1-2 continuous or categorical exposure (independent variables or predictor)\n \nWe had many good ways to assess the relationship between an outcome and exposure:\n \n\n\n\n\n\n\n\n\n\n\nContinuous Outcome\nCategorical Outcome\n\n\nContinuous Exposure\nCorrelation, simple linear regression\n??\n\n\nCategorical Exposure\nt-tests, paired t-tests, 2 sample t-tests, ANOVA\nproportion t-test, Chi-squared goodness of fit test, Fisher’s Exact test, Chi-squared test of independence, etc."
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#what-did-we-learn-in-511611-22",
    "href": "lessons/01_Review/01_Review.html#what-did-we-learn-in-511611-22",
    "title": "Lesson 1: Review",
    "section": "What did we learn in 511/611? (2/2)",
    "text": "What did we learn in 511/611? (2/2)\n\nYou set up a really important foundation\n\nIncluding distributions, mathematical definitions, hypothesis testing, and more!\n\n \nTests and statistical approaches learned are incredibly helpful!\n \nWhile you had to learn a lot of different tests and approaches for each combination of categorical/continuous exposure with categorical/continuous outcome\n\nThose tests cannot handle more complicated data\n\n \nWhat happens when other variables influence the relationship between your exposure and outcome?\n\nDo we just ignore them?"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#chi-squared-distribution",
    "href": "lessons/01_Review/01_Review.html#chi-squared-distribution",
    "title": "Lesson 1: Review",
    "section": "Chi-squared distribution",
    "text": "Chi-squared distribution\n\n\n\nWhere did we see this?\n\nHypothesis test if two categorical variables were independent\n\n\n \n\nNotation: \\(X \\sim \\chi^2_{df}\\) OR \\(X \\sim \\chi^2_{\\nu}\\)\n\nDegrees of freedom (df): \\(df=n-1\\)\n\\(X\\) takes on only positive values\n\n\n \n\nIf \\(Z_i\\sim \\mbox{N}(0,1)\\), then \\(Z_i^2\\sim \\chi^2_1\\)\n\nA standard normal distribution squared is the Chi squared distribution with df of 1."
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#reference-steps-in-a-hypothesis-test",
    "href": "lessons/01_Review/01_Review.html#reference-steps-in-a-hypothesis-test",
    "title": "Lesson 1: Review",
    "section": "Reference: Steps in a Hypothesis Test",
    "text": "Reference: Steps in a Hypothesis Test\n\nCheck the assumptions\n\nWhat sampling distribution are you using? What assumptions are required for it?\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols and/or in words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#another-view-steps-in-a-hypothesis-test",
    "href": "lessons/01_Review/01_Review.html#another-view-steps-in-a-hypothesis-test",
    "title": "Lesson 1: Review",
    "section": "Another view: Steps in a Hypothesis Test",
    "text": "Another view: Steps in a Hypothesis Test"
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#reference-what-does-it-all-look-like-together",
    "href": "lessons/01_Review/01_Review.html#reference-what-does-it-all-look-like-together",
    "title": "Lesson 1: Review",
    "section": "Reference: what does it all look like together?",
    "text": "Reference: what does it all look like together?\n\n\nExample of hypothesis test based on the 1992 JAMA data\n\n\nIs there evidence to support that the population mean body temperature is different from 98.6°F?\n\n\n\n\n\nAssumptions: The individual observations are independent and the number of individuals in our sample is 130. Thus, we can use CLT to approximate the sampling distribution.\n\n4-5. Test statistic and p-value\n\n\nSet \\(\\alpha = 0.05\\)\n\n\n\nHypothesis:\n\\[\\begin{aligned}\nH_0 &: \\mu = 98.6\\\\\n\\text{vs. } H_A&: \\mu \\neq 98.6\n\\end{aligned}\\]\n\n\n\n\n\nCode\ntemps_ttest &lt;- t.test(x = BodyTemps$Temperature, mu = 98.6)\ntidy(temps_ttest) %&gt;% gt() %&gt;% tab_options(table.font.size = 36)\n\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    98.24923\n-5.454823\n2.410632e-07\n129\n98.122\n98.37646\nOne Sample t-test\ntwo.sided\n  \n  \n  \n\n\n\n\n\nConclusion: We reject the null hypothesis. The average body temperature in the sample was 98.25°F (95% CI 98.12, 98.38°F), which is discernibly different from 98.6°F ( \\(p\\)-value &lt; 0.001)."
  },
  {
    "objectID": "lessons/01_Review/01_Review.html#do-your-exit-ticket",
    "href": "lessons/01_Review/01_Review.html#do-your-exit-ticket",
    "title": "Lesson 1: Review",
    "section": "Do your exit ticket!!",
    "text": "Do your exit ticket!!\n\nDon’t forget to go online and fill it out!\n\nThis will count as your attendance\n\n\n \n\nI look forward to the quarter with you!\n\n\n\nLesson 1: Review"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome_key_info.html",
    "href": "lessons/00_Welcome/00_Welcome_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Hello! Welcome!\nLet’s take a second to check that Echo360 works\n\nBut please mute your computer first!!\n\nI’ll discuss more in the Welcome and Review lesson\nAny initial questions"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome_key_info.html#announcements",
    "href": "lessons/00_Welcome/00_Welcome_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Hello! Welcome!\nLet’s take a second to check that Echo360 works\n\nBut please mute your computer first!!\n\nI’ll discuss more in the Welcome and Review lesson\nAny initial questions"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website-homepage",
    "href": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website-homepage",
    "title": "Welcome to BSTA 512/612!",
    "section": "Let’s visit the website: Homepage",
    "text": "Let’s visit the website: Homepage\n\nHomepage"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website-syllabus",
    "href": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website-syllabus",
    "title": "Welcome to BSTA 512/612!",
    "section": "Let’s visit the website: Syllabus",
    "text": "Let’s visit the website: Syllabus\n\nCourse learning objectives\nTextbook: two online textbooks\nR: we will continue to use and learn this programming language\nAssessments and grade breakdowns\nWeekly assignments: homeworks and labs\nFeedback: in the form of exit tickets, ongoing feedback forms, midterm feedback, and final course\nHow to succeed in this course: resources and assignments explained\nLate work policy / Attendance policy\nChatGPT and other AI technology\nCourse expectations: a few ways that I will show you respect and commitment to you as students\n\nAnd a few ways I expect from you!\n\nCommunicating with me: give me 24 hours to reply M-F\n\nOnline communication is not my strength!"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website-schedule-12",
    "href": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website-schedule-12",
    "title": "Welcome to BSTA 512/612!",
    "section": "Let’s visit the website: Schedule (1/2)",
    "text": "Let’s visit the website: Schedule (1/2)\n\nWeeks, class info, homeworks, labs"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website-schedule-22",
    "href": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website-schedule-22",
    "title": "Welcome to BSTA 512/612!",
    "section": "Let’s visit the website: Schedule (2/2)",
    "text": "Let’s visit the website: Schedule (2/2)\n\n\n\n\nKey Info\nI will post announcements and other important class related info here. For example, if I change a due date or discuss a common mistake in homework, I will put it here.\n\n\n\nSlides QMD\nThese are the basic slides that will open in your browser.\n\n\n\nSlides PDF\nThese are the slides in pdf form for easy note taking. I’m not always the best at posting these before class, so make sure you know how to save your own copy of pdf slides!\n\n\n\nSlides Notes\nThese are the annotated slides in pdf form. In class, I add my own notes to slides. After class, I will post them here.\n\n\n\nExit tix\nThese are links to that day’s exit ticket.\n\n\n\nRecording\nI record our classes. This will be a link to the OneDrive folder containing this recording.\n\n\n\nMuddy Points\nYou will have a chance to ask questions about class in your exit tickets. If I notice a trend in confusion, I will add explanations to these “Muddy Points”"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website-search",
    "href": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website-search",
    "title": "Welcome to BSTA 512/612!",
    "section": "Let’s visit the website: Search",
    "text": "Let’s visit the website: Search"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website-homework",
    "href": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website-homework",
    "title": "Welcome to BSTA 512/612!",
    "section": "Let’s visit the website: Homework",
    "text": "Let’s visit the website: Homework"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#decision-on-homework-due-dates",
    "href": "lessons/00_Welcome/00_Welcome.html#decision-on-homework-due-dates",
    "title": "Welcome to BSTA 512/612!",
    "section": "Decision on Homework due dates",
    "text": "Decision on Homework due dates\n\nI have some set due dates in the schedule\nPlease look at your other classes, your calendar, etc\nConsider what day of the week you would like to turn in your assignment, solutions, and video/meeting\nQuestion in HW 0 to cast your vote and share your opinion\nWe have homework every week\n\nHomework 8 is due on 11/26 (Wednesday before Thanksgiving break)\nYou will have the option of turning it in with Homework 9"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#structure-for-this-course",
    "href": "lessons/00_Welcome/00_Welcome.html#structure-for-this-course",
    "title": "Welcome to BSTA 512/612!",
    "section": "Structure for this course",
    "text": "Structure for this course\n\nWe will use the foundation built in BSTA 511/611 or EPID 525\n\n \n\nWe will be building towards models that can handle many variables!\n \n\nRegression is the building block for modeling multivariable relationships\n\n \nIn Linear Models we will build, interpret, and evaluate linear regression models"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#what-we-will-cover",
    "href": "lessons/00_Welcome/00_Welcome.html#what-we-will-cover",
    "title": "Welcome to BSTA 512/612!",
    "section": "What we will cover",
    "text": "What we will cover\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#let-me-know-if-you-have-questions",
    "href": "lessons/00_Welcome/00_Welcome.html#let-me-know-if-you-have-questions",
    "title": "Welcome to BSTA 512/612!",
    "section": "Let me know if you have questions",
    "text": "Let me know if you have questions\nOr if there’s any contradicting information in the course site… I’m sure I made a mistake somewhere!!\n\nFor example: we do NOT have quizzes. If you see a mention of quizzes anywhere in the course, then I simply overlooked it and need to fix it!\n\n\n\nWelcome"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website-project-and-labs",
    "href": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website-project-and-labs",
    "title": "Welcome to BSTA 512/612!",
    "section": "Let’s visit the website: Project and labs",
    "text": "Let’s visit the website: Project and labs"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#what-we-will-cover-process-for-regression-data-analysis",
    "href": "lessons/00_Welcome/00_Welcome.html#what-we-will-cover-process-for-regression-data-analysis",
    "title": "Welcome to BSTA 512/612!",
    "section": "What we will cover: process for regression data analysis",
    "text": "What we will cover: process for regression data analysis\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website-instructors",
    "href": "lessons/00_Welcome/00_Welcome.html#lets-visit-the-website-instructors",
    "title": "Welcome to BSTA 512/612!",
    "section": "Let’s visit the website: Instructors",
    "text": "Let’s visit the website: Instructors"
  },
  {
    "objectID": "instructors.html#tutors",
    "href": "instructors.html#tutors",
    "title": "Instructors",
    "section": "Tutors",
    "text": "Tutors\nThe Academic Success Center has biostatistics/epidemiology tutors available. Unlike TAs, tutors are not closely associated with any particular course sections or professors, nor do they have any grading responsibilities. Consider meeting with a tutor when you feel stuck on a project, need help with R coding, want to review course concepts, or simply complete assignments in a social learning environment. There is no cost to use tutoring.\nYour tutors this year are Mickey McVeety (MPH Epi program) and Sofia Chapela Lara (PhD program). They are looking forward to working with you in a one-to-one or group setting, depending on your needs. Please schedule to meet with them at least 24 hours in advance. If you have questions about tutoring or need assistance with the schedule, contact one of the tutors or the Academic Success Center: mcveety@ohsu.edu, chapelal@ohsu.edu, learningsupport@ohsu.edu"
  },
  {
    "objectID": "instructors.html#academic-success-center-tutors",
    "href": "instructors.html#academic-success-center-tutors",
    "title": "Instructors",
    "section": "Academic Success Center Tutors",
    "text": "Academic Success Center Tutors\nThe Academic Success Center has biostatistics/epidemiology tutors available. Unlike TAs, tutors are not closely associated with any particular course sections or professors, nor do they have any grading responsibilities. Consider meeting with a tutor when you feel stuck on a project, need help with R coding, want to review course concepts, or simply complete assignments in a social learning environment. There is no cost to use tutoring.\nYour tutors this year are Mickey McVeety (MPH Epi program) and Sofia Chapela Lara (PhD program). They are looking forward to working with you in a one-to-one or group setting, depending on your needs. Please schedule to meet with them at least 24 hours in advance. If you have questions about tutoring or need assistance with the schedule, contact one of the tutors or the Academic Success Center: mcveety@ohsu.edu, chapelal@ohsu.edu, learningsupport@ohsu.edu"
  },
  {
    "objectID": "lessons/01_Review/01_Review_muddy_points.html",
    "href": "lessons/01_Review/01_Review_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Just to clarify! We will be using the distributions in the context of hypothesis testing. I just wanted you to see some of the cool connections between the distributions. (We don’t need to know the connections for a lot of what we do in this class.)\nAlso, I will discuss each distribution again as we hit the hypothesis tests that use them!\nThere is a big, scary (but fun!) infographic at the end of a famous stat textbook (Casella and Berger) that shows all the connections between distributions:\n\nWe mostly talk about the red, circled area. Each line with the directional arrow represents a specific transformation that is needed to go from the starting distribution to the distribution at the end of the arrow.\nAgain, this is NOT information we need to perform regression, but it is really interesting to see the connections between these distributions.\n\n\n\nI feel like I might’ve scared us with all the distribution talk.\nI want to be clear: We will further discuss and explore the distributions as we use them within the course. It will be more important to understand their use within regression then knowing the distribution in depth. Basically, when we implement specific hypothesis tests, we just need to know which distribution is most appropriate for the test.\n\n\n\nYes! In regression, we often use the F-distribution through the F-test (same as ANOVA) However, in regression, we are comparing the variance of two models, that may differ by a coefficient. See the STAT 501 page for more info if interested. In ANOVA, we are comparing variance between and within groups. Both use the same test, but with different goals!\n\n\n\nSome people misuse “multivariate” instead of “multivariable” modeling. In this class, we will only look at multivariable regression. Here’s the big difference:\n\nMultivariable: model with multiple independent variables (covariates, predictors)\n\nIf we want to see how our outcome (height) is related to parent height, birth country, sex assigned at birth, etc.\n\nMultivariate: model with multiple dependent variables (outcome)\n\nIf we want to extend the outcome from height to height and head circumference. Multivariate modeling would try to model both outcomes together and see how they are related to other variables."
  },
  {
    "objectID": "lessons/01_Review/01_Review_muddy_points.html#muddy-points-from-winter-2024",
    "href": "lessons/01_Review/01_Review_muddy_points.html#muddy-points-from-winter-2024",
    "title": "Muddy Points",
    "section": "",
    "text": "Just to clarify! We will be using the distributions in the context of hypothesis testing. I just wanted you to see some of the cool connections between the distributions. (We don’t need to know the connections for a lot of what we do in this class.)\nAlso, I will discuss each distribution again as we hit the hypothesis tests that use them!\nThere is a big, scary (but fun!) infographic at the end of a famous stat textbook (Casella and Berger) that shows all the connections between distributions:\n\nWe mostly talk about the red, circled area. Each line with the directional arrow represents a specific transformation that is needed to go from the starting distribution to the distribution at the end of the arrow.\nAgain, this is NOT information we need to perform regression, but it is really interesting to see the connections between these distributions.\n\n\n\nI feel like I might’ve scared us with all the distribution talk.\nI want to be clear: We will further discuss and explore the distributions as we use them within the course. It will be more important to understand their use within regression then knowing the distribution in depth. Basically, when we implement specific hypothesis tests, we just need to know which distribution is most appropriate for the test.\n\n\n\nYes! In regression, we often use the F-distribution through the F-test (same as ANOVA) However, in regression, we are comparing the variance of two models, that may differ by a coefficient. See the STAT 501 page for more info if interested. In ANOVA, we are comparing variance between and within groups. Both use the same test, but with different goals!\n\n\n\nSome people misuse “multivariate” instead of “multivariable” modeling. In this class, we will only look at multivariable regression. Here’s the big difference:\n\nMultivariable: model with multiple independent variables (covariates, predictors)\n\nIf we want to see how our outcome (height) is related to parent height, birth country, sex assigned at birth, etc.\n\nMultivariate: model with multiple dependent variables (outcome)\n\nIf we want to extend the outcome from height to height and head circumference. Multivariate modeling would try to model both outcomes together and see how they are related to other variables."
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management_muddy_points.html",
    "href": "lessons/02_Data_Management/02_Data_Management_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Muddy points from Winter 2024\n\n\n1. More on the functions and problems we had in class!\n\n\n\n\n\n\nImportant\n\n\n\nThis section has some examples from this YouTube video series. There is a video on mutate, pipe, filter, select, rename, arrange, and summarize. Note that some of the function in the series is outdated. The use of if_else() in the video is outdated, and it is more common to use case_when() now. The use of gather and spread have been replaced by pivot_longer and pivot_wider.\n\n\nTo discuss these functions below, I want to use a different dataset than what we used for examples in class. I’m hoping this allows us to see each function from a different angle. I’ll use the dataset that we used for some of the ggplot examples: mtcars. Let’s load the tidyverse and take a look at the dataset:\n\nlibrary(tidyverse)\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\nmutate()\nFor mutate, there are a few common uses:\n\ncreate another numeric variable by manipulating other variables\ncreate a categorical variable by creating cases from other variables\n\n\nCreate another numeric variable by manipulating other variables\nThe wt variable is the weight of the car in tons. Let’s say we want the full weight in pounds (lbs). I’ll create a new variable that is 1000 times (1 ton = 1000 lbs) the weight in the dataset.\n\nmtcars1 = mtcars %&gt;%\n  mutate(weight_lb = wt * 1000)\n\nglimpse(mtcars1)\n\nRows: 32\nColumns: 12\n$ mpg       &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, …\n$ cyl       &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, …\n$ disp      &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.…\n$ hp        &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180…\n$ drat      &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, …\n$ wt        &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.15…\n$ qsec      &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.9…\n$ vs        &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, …\n$ am        &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, …\n$ gear      &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, …\n$ carb      &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, …\n$ weight_lb &lt;dbl&gt; 2620, 2875, 2320, 3215, 3440, 3460, 3570, 3190, 3150, 3440, …\n\n\nWe can also perform several mutations at the same time. Let’s say we want the weight in pounds AND the horse power per cylinder (hp per cyl). We can perform both manipulations:\n\nmtcars2 = mtcars %&gt;%\n  mutate(weight_lb = wt * 1000, \n         hp_per_cyl = hp/cyl)\n\nglimpse(mtcars2)\n\nRows: 32\nColumns: 13\n$ mpg        &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2,…\n$ cyl        &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4,…\n$ disp       &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140…\n$ hp         &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 18…\n$ drat       &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92,…\n$ wt         &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.1…\n$ qsec       &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.…\n$ vs         &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,…\n$ am         &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,…\n$ gear       &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4,…\n$ carb       &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1,…\n$ weight_lb  &lt;dbl&gt; 2620, 2875, 2320, 3215, 3440, 3460, 3570, 3190, 3150, 3440,…\n$ hp_per_cyl &lt;dbl&gt; 18.33333, 18.33333, 23.25000, 18.33333, 21.87500, 17.50000,…\n\n\nYou can even use the same syntax if you need to change a variable that depends on a previous mutation. Let’s say I want the ratio of weight in pounds to the car’s horse power.\n\nmtcars3 = mtcars %&gt;%\n  mutate(weight_lb = wt * 1000, \n         w_to_hp = weight_lb / hp)\n\nglimpse(mtcars3)\n\nRows: 32\nColumns: 13\n$ mpg       &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, …\n$ cyl       &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, …\n$ disp      &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.…\n$ hp        &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180…\n$ drat      &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, …\n$ wt        &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.15…\n$ qsec      &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.9…\n$ vs        &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, …\n$ am        &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, …\n$ gear      &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, …\n$ carb      &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, …\n$ weight_lb &lt;dbl&gt; 2620, 2875, 2320, 3215, 3440, 3460, 3570, 3190, 3150, 3440, …\n$ w_to_hp   &lt;dbl&gt; 23.81818, 26.13636, 24.94624, 29.22727, 19.65714, 32.95238, …\n\n\n\n\nCreate a categorical variable by creating cases from other variables\nRecall in class we used mutate to label the numeric values of am to a categorical variable transmission. We create a new categorical variable from a binary, numeric variable.\n\nmtcars4 = mtcars %&gt;%\n  mutate(transmission = case_when(am == 0 ~ \"automatic\",\n                                  am == 1 ~ \"manual\"))\nglimpse(mtcars4)\n\nRows: 32\nColumns: 12\n$ mpg          &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.…\n$ cyl          &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, …\n$ disp         &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 1…\n$ hp           &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, …\n$ drat         &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.9…\n$ wt           &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3…\n$ qsec         &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 2…\n$ vs           &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, …\n$ am           &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, …\n$ gear         &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, …\n$ carb         &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, …\n$ transmission &lt;chr&gt; \"manual\", \"manual\", \"manual\", \"automatic\", \"automatic\", \"…\n\n\nWe can also create a categorical variable from a continuous numeric variable. Let’s say we want to divide the miles per gallon into three categories: low, medium, and high. We can use mutate() and case_when() to do so:\n\nmtcars5 = mtcars %&gt;%\n  mutate(mpg_cat = case_when(mpg &gt; 22 ~ \"high\",\n                             mpg &gt; 15 ~ \"medium\", \n                             .default = \"low\"))\nglimpse(mtcars5)\n\nRows: 32\nColumns: 12\n$ mpg     &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17…\n$ cyl     &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4,…\n$ disp    &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8,…\n$ hp      &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, …\n$ drat    &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.…\n$ wt      &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150,…\n$ qsec    &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90,…\n$ vs      &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,…\n$ am      &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,…\n$ gear    &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3,…\n$ carb    &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1,…\n$ mpg_cat &lt;chr&gt; \"medium\", \"medium\", \"high\", \"medium\", \"medium\", \"medium\", \"low…\n\n\nNotice that I used .default in the last case. This means “for all other values of mpg, assign it to ‘low’.”\n\n\n\npipe %&gt;%\nCheck out the videos explanation! I can’t explain it much better!!\n\n\nselect() everything but a certain variable\nIt seems like we’re mostly okay with the select() function, but want more information on selecting everything but a certain varaible.\nWhen we select variables, we have the option to identify variables we want to keep or remove. If we want to keep a variable, we would just list the variable’s column name. If we want to remove a variable, we use the minus sign to let R know that we do NOT want that variable. We typically do not tell R to remove and keep variables within the same select() function. Let’s recall the variables within the original mtcars dataset:\n\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\nNow we want to keep the following variables: cyl, mpg, disp, and qsec.\n\nmtcars6 = mtcars %&gt;%\n  select(mpg, cyl, disp, qsec)\nglimpse(mtcars6)\n\nRows: 32\nColumns: 4\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n\n\nIn this case there was less typing to ID the variables we wanted to keep, so we inputted the column names. If we wanted to ID the variables we wanted to remove, what variables would identify to get the same remaining variables that are in mtcars6?\nSo we want to remove the following variables:\n\nmtcars7 = mtcars %&gt;%\n  select(-hp, -drat, -wt, -vs, -am, -gear, -carb)\nglimpse(mtcars7)\n\nRows: 32\nColumns: 4\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n\n\nSo both get the job done, but one is definitely quicker to type!\n\n\n\n\n\n\nWe don’t have to write the minus in front of every variable\n\n\n\nWhen we are removing several variables, we can combine them into a vector to remove:\n\nmtcars8 = mtcars %&gt;%\n  select(-c(hp, drat, wt, vs, am, gear, carb))\nglimpse(mtcars8)\n\nRows: 32\nColumns: 4\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n\n\nThe vector above is: c(hp, drat, wt, vs, am, gear, carb) and the minus sign in front of the vector will be applied to each variable.\n\n\n\n\npivot_longer()\nI want to address the function with the faculty dataset from class.\n\n# Note, I've put the data in a folder \"data\" that is in the same folder as this page's file\nstaff = read_csv(here(\"data/instructional-staff.csv\"))\nstaff\n\n# A tibble: 5 × 12\n  faculty_type    `1975` `1989` `1993` `1995` `1999` `2001` `2003` `2005` `2007`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Full-Time Tenu…   29     27.6   25     24.8   21.8   20.3   19.3   17.8   17.2\n2 Full-Time Tenu…   16.1   11.4   10.2    9.6    8.9    9.2    8.8    8.2    8  \n3 Full-Time Non-…   10.3   14.1   13.6   13.6   15.2   15.5   15     14.8   14.9\n4 Part-Time Facu…   24     30.4   33.1   33.2   35.5   36     37     39.3   40.5\n5 Graduate Stude…   20.5   16.5   18.1   18.8   18.7   19     20     19.9   19.5\n# ℹ 2 more variables: `2009` &lt;dbl&gt;, `2011` &lt;dbl&gt;\n\n\nNote that I am not using glimpse() here because we only have 5 rows corresponding to the faculty type. What would a glimpse of the data look like?\n\nglimpse(staff)\n\nRows: 5\nColumns: 12\n$ faculty_type &lt;chr&gt; \"Full-Time Tenured Faculty\", \"Full-Time Tenure-Track Facu…\n$ `1975`       &lt;dbl&gt; 29.0, 16.1, 10.3, 24.0, 20.5\n$ `1989`       &lt;dbl&gt; 27.6, 11.4, 14.1, 30.4, 16.5\n$ `1993`       &lt;dbl&gt; 25.0, 10.2, 13.6, 33.1, 18.1\n$ `1995`       &lt;dbl&gt; 24.8, 9.6, 13.6, 33.2, 18.8\n$ `1999`       &lt;dbl&gt; 21.8, 8.9, 15.2, 35.5, 18.7\n$ `2001`       &lt;dbl&gt; 20.3, 9.2, 15.5, 36.0, 19.0\n$ `2003`       &lt;dbl&gt; 19.3, 8.8, 15.0, 37.0, 20.0\n$ `2005`       &lt;dbl&gt; 17.8, 8.2, 14.8, 39.3, 19.9\n$ `2007`       &lt;dbl&gt; 17.2, 8.0, 14.9, 40.5, 19.5\n$ `2009`       &lt;dbl&gt; 16.8, 7.6, 15.1, 41.1, 19.4\n$ `2011`       &lt;dbl&gt; 16.7, 7.4, 15.4, 41.3, 19.3\n\n\nBoth views are indicators that the dataset is in a “wide” format where each year has its own column. We want our data to be in a tidy format, which means each column is a variable and each cell has a value. However, the years are actually values for a variable “year.” By using pivot_longer(), we can tell R to take those columns for years and make them their own column where year is the value. That means for a year like 1975, there are five numbers corresponding to the five faculty types. Those five numbers are the percentage of the specific faculty type in that year. So we want to end with columns: faculty type, year, and percentage.\nTo start with an easier implementation of pivot_longer(), let’s remove the faculty type using select()\n\nstaff2 = staff %&gt;%\n  select(-faculty_type)\nstaff2\n\n# A tibble: 5 × 11\n  `1975` `1989` `1993` `1995` `1999` `2001` `2003` `2005` `2007` `2009` `2011`\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   29     27.6   25     24.8   21.8   20.3   19.3   17.8   17.2   16.8   16.7\n2   16.1   11.4   10.2    9.6    8.9    9.2    8.8    8.2    8      7.6    7.4\n3   10.3   14.1   13.6   13.6   15.2   15.5   15     14.8   14.9   15.1   15.4\n4   24     30.4   33.1   33.2   35.5   36     37     39.3   40.5   41.1   41.3\n5   20.5   16.5   18.1   18.8   18.7   19     20     19.9   19.5   19.4   19.3\n\n\nNow we can pivot the years! We first identify the columns that we want to pivot. To pivot all the columns, we say cols = everything(). pivot_longer() knows you want the column names to now become values of a variable, but it does not know what to call that variable. So now we identify the new variable name of the column that will contain all the years (our old column names). We identify the new variable name with: names_to. Finally, we need to adress the old cell values that were under each year in our wide dataset. Those cell values will make up a new column/variable. Remember that each year had 5 values underneath it, so we need to include all 55 cell values. Similar to names_to, we need to identify the new column names for all those values. We use values_to to identify the column name for the cell values of our old wide formatted data.\n\nstaff_long = staff2 %&gt;%\n  pivot_longer(\n    cols = everything(),    # columns to pivot\n    names_to = \"year\",       # name of new column for variable names\n    values_to = \"percentage\" # name of new column for values\n  ) %&gt;%\n  mutate(percentage = as.numeric(percentage))\n\nhead(staff_long, 20) # I'm asking R to show me the first 20 rows\n\n# A tibble: 20 × 2\n   year  percentage\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 1975        29  \n 2 1989        27.6\n 3 1993        25  \n 4 1995        24.8\n 5 1999        21.8\n 6 2001        20.3\n 7 2003        19.3\n 8 2005        17.8\n 9 2007        17.2\n10 2009        16.8\n11 2011        16.7\n12 1975        16.1\n13 1989        11.4\n14 1993        10.2\n15 1995         9.6\n16 1999         8.9\n17 2001         9.2\n18 2003         8.8\n19 2005         8.2\n20 2007         8  \n\n\nI included the first 20 rows so we could see that the years repeat. This is because there were 5 percentages for each year. While this is in the desired long format, we now see that we’re missing the information on faculty type. Each percentage in each year corresponded to a specific faculty type:\n\nstaff_long %&gt;% filter(year == 1975)\n\n# A tibble: 5 × 2\n  year  percentage\n  &lt;chr&gt;      &lt;dbl&gt;\n1 1975        29  \n2 1975        16.1\n3 1975        10.3\n4 1975        24  \n5 1975        20.5\n\n\nWhich percentage is for which faculty??\nWe could use another function called join() to try to remedy the situation, but it’s much easier to redo the pivot function. We will go back to staff which still has the faculty type:\n\nstaff\n\n# A tibble: 5 × 12\n  faculty_type    `1975` `1989` `1993` `1995` `1999` `2001` `2003` `2005` `2007`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Full-Time Tenu…   29     27.6   25     24.8   21.8   20.3   19.3   17.8   17.2\n2 Full-Time Tenu…   16.1   11.4   10.2    9.6    8.9    9.2    8.8    8.2    8  \n3 Full-Time Non-…   10.3   14.1   13.6   13.6   15.2   15.5   15     14.8   14.9\n4 Part-Time Facu…   24     30.4   33.1   33.2   35.5   36     37     39.3   40.5\n5 Graduate Stude…   20.5   16.5   18.1   18.8   18.7   19     20     19.9   19.5\n# ℹ 2 more variables: `2009` &lt;dbl&gt;, `2011` &lt;dbl&gt;\n\n\nNow we can implement pivot_longer(). We will identify the columns we want to pivot as column 2 through 12 so we exclude the faculty type from the pivoting. BUT the really nice thing is that pivot_longer() will remember the percentages that correspond to a specific combination of faculty type and year! Let’s try it again:\n\nstaff_long2 = staff %&gt;%\n  pivot_longer(\n    cols = 2:12,    # columns to pivot\n    names_to = \"year\",       # name of new column for variable names\n    values_to = \"percentage\" # name of new column for values\n  ) %&gt;%\n  mutate(percentage = as.numeric(percentage))\n\nhead(staff_long2, 20) # I'm asking R to show me the first 20 rows\n\n# A tibble: 20 × 3\n   faculty_type                   year  percentage\n   &lt;chr&gt;                          &lt;chr&gt;      &lt;dbl&gt;\n 1 Full-Time Tenured Faculty      1975        29  \n 2 Full-Time Tenured Faculty      1989        27.6\n 3 Full-Time Tenured Faculty      1993        25  \n 4 Full-Time Tenured Faculty      1995        24.8\n 5 Full-Time Tenured Faculty      1999        21.8\n 6 Full-Time Tenured Faculty      2001        20.3\n 7 Full-Time Tenured Faculty      2003        19.3\n 8 Full-Time Tenured Faculty      2005        17.8\n 9 Full-Time Tenured Faculty      2007        17.2\n10 Full-Time Tenured Faculty      2009        16.8\n11 Full-Time Tenured Faculty      2011        16.7\n12 Full-Time Tenure-Track Faculty 1975        16.1\n13 Full-Time Tenure-Track Faculty 1989        11.4\n14 Full-Time Tenure-Track Faculty 1993        10.2\n15 Full-Time Tenure-Track Faculty 1995         9.6\n16 Full-Time Tenure-Track Faculty 1999         8.9\n17 Full-Time Tenure-Track Faculty 2001         9.2\n18 Full-Time Tenure-Track Faculty 2003         8.8\n19 Full-Time Tenure-Track Faculty 2005         8.2\n20 Full-Time Tenure-Track Faculty 2007         8  \n\n\nAnd now I’ll just dump a couple other ways to identify the columns we want to pivot:\n\nIn this one, we name the column years. It’s like 2:12, but it’s helpful when it’s hard to see what number the column is. Also, this is a special case because the columns are numbers, so we need to use ’ to wrap around the year. In the mtcars dataset, a similar approach would be cyl:vs to select all the variables between cyl and vs. You can also make a vector of variable names if they are not next to each other.\n\n\nstaff_long3 = staff %&gt;%\n  pivot_longer(\n    cols = '1975':'2011',    # columns to pivot\n    names_to = \"year\",       # name of new column for variable names\n    values_to = \"percentage\" # name of new column for values\n  ) %&gt;%\n  mutate(percentage = as.numeric(percentage))\n\nhead(staff_long3, 20) # I'm asking R to show me the first 20 rows\n\n# A tibble: 20 × 3\n   faculty_type                   year  percentage\n   &lt;chr&gt;                          &lt;chr&gt;      &lt;dbl&gt;\n 1 Full-Time Tenured Faculty      1975        29  \n 2 Full-Time Tenured Faculty      1989        27.6\n 3 Full-Time Tenured Faculty      1993        25  \n 4 Full-Time Tenured Faculty      1995        24.8\n 5 Full-Time Tenured Faculty      1999        21.8\n 6 Full-Time Tenured Faculty      2001        20.3\n 7 Full-Time Tenured Faculty      2003        19.3\n 8 Full-Time Tenured Faculty      2005        17.8\n 9 Full-Time Tenured Faculty      2007        17.2\n10 Full-Time Tenured Faculty      2009        16.8\n11 Full-Time Tenured Faculty      2011        16.7\n12 Full-Time Tenure-Track Faculty 1975        16.1\n13 Full-Time Tenure-Track Faculty 1989        11.4\n14 Full-Time Tenure-Track Faculty 1993        10.2\n15 Full-Time Tenure-Track Faculty 1995         9.6\n16 Full-Time Tenure-Track Faculty 1999         8.9\n17 Full-Time Tenure-Track Faculty 2001         9.2\n18 Full-Time Tenure-Track Faculty 2003         8.8\n19 Full-Time Tenure-Track Faculty 2005         8.2\n20 Full-Time Tenure-Track Faculty 2007         8  \n\n\n\nThis one is the same as our in-class code. We “remove” faculty type from our identified columns\n\n\nstaff_long4 = staff %&gt;%\n  pivot_longer(\n    cols = -faculty_type,    # columns to pivot\n    names_to = \"year\",       # name of new column for variable names\n    values_to = \"percentage\" # name of new column for values\n  ) %&gt;%\n  mutate(percentage = as.numeric(percentage))\n\nhead(staff_long4, 20) # I'm asking R to show me the first 20 rows\n\n# A tibble: 20 × 3\n   faculty_type                   year  percentage\n   &lt;chr&gt;                          &lt;chr&gt;      &lt;dbl&gt;\n 1 Full-Time Tenured Faculty      1975        29  \n 2 Full-Time Tenured Faculty      1989        27.6\n 3 Full-Time Tenured Faculty      1993        25  \n 4 Full-Time Tenured Faculty      1995        24.8\n 5 Full-Time Tenured Faculty      1999        21.8\n 6 Full-Time Tenured Faculty      2001        20.3\n 7 Full-Time Tenured Faculty      2003        19.3\n 8 Full-Time Tenured Faculty      2005        17.8\n 9 Full-Time Tenured Faculty      2007        17.2\n10 Full-Time Tenured Faculty      2009        16.8\n11 Full-Time Tenured Faculty      2011        16.7\n12 Full-Time Tenure-Track Faculty 1975        16.1\n13 Full-Time Tenure-Track Faculty 1989        11.4\n14 Full-Time Tenure-Track Faculty 1993        10.2\n15 Full-Time Tenure-Track Faculty 1995         9.6\n16 Full-Time Tenure-Track Faculty 1999         8.9\n17 Full-Time Tenure-Track Faculty 2001         9.2\n18 Full-Time Tenure-Track Faculty 2003         8.8\n19 Full-Time Tenure-Track Faculty 2005         8.2\n20 Full-Time Tenure-Track Faculty 2007         8  \n\n\n\n\nacross()\nI really ran out of time before getting to this one. For now, you can look at the examples from this site to see the capabilities of across() . I invite you to try them out on the various datasets in our lecture.\n\n\n\n2. tbl_summary(): Trying to figure out how to change the median values to mean\nOh, wow! Turns out we solved it in class, but I made big mistake with my slides. The code for the table was in two places, but we fixed the one that was NOT running and showing on the slide!\nSo turns out, it worked!!\nHere’s the code:\n\nlibrary(tidyverse)  ## Need to load to use selec() and %&gt;%\nlibrary(gtsummary)  ## Needed package for tbl_summary()\n\ndata(\"dds.discr\")\n\ndds.discr1 = dds.discr %&gt;% \n  rename(SAB = gender, \n         R_E = ethnicity)\n\ndds.discr1 %&gt;%\n  select(-id, -age.cohort) %&gt;%\n  tbl_summary(label = c(age ~ \"Age\", \n                        R_E ~ \"Race/Ethnicity\", \n                        SAB ~ \"Sex Assigned at Birth\", \n                        expenditures ~ \"Expenditures\") ,\n              statistic = list(all_continuous() ~ \"{mean} ({sd})\"))\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 1,0001\n    \n  \n  \n    Age\n23 (18)\n    Sex Assigned at Birth\n\n        Female\n503 (50%)\n        Male\n497 (50%)\n    Expenditures\n18,066 (19,543)\n    Race/Ethnicity\n\n        American Indian\n4 (0.4%)\n        Asian\n129 (13%)\n        Black\n59 (5.9%)\n        Hispanic\n376 (38%)\n        Multi Race\n26 (2.6%)\n        Native Hawaiian\n3 (0.3%)\n        Other\n2 (0.2%)\n        White not Hispanic\n401 (40%)\n  \n  \n  \n    \n      1 Mean (SD); n (%)\n    \n  \n\n\n\n\n\n\n3. Are there benefits to ggplot compared to the base R graphing functions?\nThe main benefit I see for ggplot is that the syntax and grammar of our coding in tidyr and dplyr is very similar to ggplot. Your effort in strengthening one will help with the others.\nI am certainly not going to force you to use ggplot over base R. At the end of the day, it is really whatever makes the most sense to you. I will say: ggplot2 seems to be where most statisticians and epidemiologists are headed. And I really believe that ggplot is more efficient with coding."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02_key_info.html#key-dates",
    "href": "lessons/12_Interactions_02/12_Interactions_02_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates_key_info.html#key-dates",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics_key_info.html#key-dates",
    "href": "lessons/15_MLR_Diagnostics/15_MLR_Diagnostics_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/13_Model_selection/13_Model_selection_info.html#key-dates",
    "href": "lessons/13_Model_selection/13_Model_selection_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01_key_info.html#key-dates",
    "href": "lessons/11_Interactions_01/11_Interactions_01_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates\n\nHW 3 due this Friday\nMidterm feedback due 2/14 with HW 3\nLab 3 due 2/21"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection_key_info.html#key-dates",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#poll-everywhere-question",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#poll-everywhere-question",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Poll Everywhere Question??",
    "text": "Poll Everywhere Question??"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#regression-analysis-process",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#regression-analysis-process",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Regression analysis process",
    "text": "Regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#process-of-regression-data-analysis",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#process-of-regression-data-analysis",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Process of regression data analysis",
    "text": "Process of regression data analysis\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#process-of-regression-data-analysis",
    "href": "lessons/03_SLR/03_SLR.html#process-of-regression-data-analysis",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Process of regression data analysis",
    "text": "Process of regression data analysis\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#folder-organization-1",
    "href": "lessons/02_Data_Management/02_Data_Management.html#folder-organization-1",
    "title": "Lesson 2: Data and File Management",
    "section": "Folder organization",
    "text": "Folder organization\n\nMake a folder for our class!\n\nI suggest naming it something like BSTA_512_W25 to indicate the class and the term\n\nMake these folders in your computer (or in OneDrive if you prefer)\n\nOnly make them in OneDrive if you have a desktop connection\n\n\n\n\n\nFor a project, I have the following folders\n\nBackground\nCode\nData_Raw\nData_Processed\nDissemination\nReports\nMeetings\n\n\n\n\nFor our class, I suggest making one folder for the course with the following folders in it:\n\nData\nHomework\nProject (with above subfolders)\nLessons\nAnd other folders if you want"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#aside-folder-and-file-naming",
    "href": "lessons/02_Data_Management/02_Data_Management.html#aside-folder-and-file-naming",
    "title": "Lesson 2: Data and File Management",
    "section": "Aside: folder and file naming",
    "text": "Aside: folder and file naming\nThere are a few good practices for naming files and folders for easy tracking:\n\nKeep the name short and relevant\nUse leading numbers to help organize sequential items\n\nI can show you my lessons folders as an example\n\nUse dates in the format “YYYY-MM-DD” so that files are in chronological order\nYou can label different versions if you would like to\nUse “_” to separate sections of the name\n\nI also use this to separate words, but some people say you should use “-” to separate words"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#make-an-r-project",
    "href": "lessons/02_Data_Management/02_Data_Management.html#make-an-r-project",
    "title": "Lesson 2: Data and File Management",
    "section": "Make an R project",
    "text": "Make an R project\n\nI suggest you make an .Rproj file in the overall class folder (BSTA_512_W25)"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#creating-project-in-rstudio",
    "href": "lessons/02_Data_Management/02_Data_Management.html#creating-project-in-rstudio",
    "title": "Lesson 2: Data and File Management",
    "section": "Creating project in RStudio",
    "text": "Creating project in RStudio\n\nWay to designate a working directory: basically your home base when working in R\n\nWe have to tell R exactly where we are in our folders and where to find other things\nA project makes it easier to tell R where we are\n\nBasic steps to create a project\n\nGo into RStudio\nCreate new project for this class (under File or top right corner)\n\nI would chose “Existing Directory” since we have already set up our folders\nMake the new project in the BSTA_512_W25 folder\n\n\nOnce we have projects, we can open one and R will automatically know that its location is the start of our working directory\nOnly make one project for now!!"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#the-nice-thing-about-r-projects",
    "href": "lessons/02_Data_Management/02_Data_Management.html#the-nice-thing-about-r-projects",
    "title": "Lesson 2: Data and File Management",
    "section": "The nice thing about R projects",
    "text": "The nice thing about R projects\n\n5 minute video explaining some of the nice features of R projects\n\nhttps://rfortherestofus.com/2022/10/rstudio-projects"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#reproducibility",
    "href": "lessons/02_Data_Management/02_Data_Management.html#reproducibility",
    "title": "Lesson 2: Data and File Management",
    "section": "Reproducibility",
    "text": "Reproducibility\n\nResearch data and code can reach the same results regardless of who is running the code\n\nThis can also refer to future or past you!\n\n\n \n\nWe want to set up our work so the entire folder can be moved around and work in its new location\n\n \n\nProjects work well in combination with the here package"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#here-package",
    "href": "lessons/02_Data_Management/02_Data_Management.html#here-package",
    "title": "Lesson 2: Data and File Management",
    "section": "here package",
    "text": "here package\n\nIllustration by Allison Horst"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#here-package-1",
    "href": "lessons/02_Data_Management/02_Data_Management.html#here-package-1",
    "title": "Lesson 2: Data and File Management",
    "section": "here package",
    "text": "here package\n\n\n\nGood source for the here package\n\nJust substitute .Rmd with .qmd\n\nBasically, a .qmd file and .R file work differently\n\nWe haven’t worked much with .R files\n\nFor .qmd files, the automatic directory is the folder it is in\n\nBut we want it to be the main project folder\n\nhere can help with that\n\n \n\nVery important for reproducibility!!"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#using-here-package",
    "href": "lessons/02_Data_Management/02_Data_Management.html#using-here-package",
    "title": "Lesson 2: Data and File Management",
    "section": "Using here package",
    "text": "Using here package\n\nWithin your console, type here() and enter\n\nTry this with getwd() as well\n\n\n\nlibrary(here)\nhere()\n\n[1] \"/Users/wakim/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/Teaching/Classes/W25_BSTA_512_612/BSTA_512_W25_site\"\n\ngetwd()\n\n[1] \"/Users/wakim/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/Teaching/Classes/W25_BSTA_512_612/BSTA_512_W25_site\"\n\n\n \n\nhere can be used whenever we need to access a file path in R code\n\nImporting data\nSaving output\nAccessing files"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#using-here-to-load-data",
    "href": "lessons/02_Data_Management/02_Data_Management.html#using-here-to-load-data",
    "title": "Lesson 2: Data and File Management",
    "section": "Using here() to load data",
    "text": "Using here() to load data\n\nThe here() function will start at the working directory (where your .Rproj file is) and let you write out a file path for anything\nTo load the dataset in our .qmd file, we will use:\n\n\nlibrary(readxl)\ndata = read_excel(here(\"./data/BodyTemperatures.xlsx\"))\ndata = read_excel(here(\"data\", \"BodyTemperatures.xlsx\"))"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#common-functions-to-load-data",
    "href": "lessons/02_Data_Management/02_Data_Management.html#common-functions-to-load-data",
    "title": "Lesson 2: Data and File Management",
    "section": "Common functions to load data",
    "text": "Common functions to load data\n\n\n\nFunction\nData file type\nPackage needed\n\n\n\n\nread_excel()\n.xls, .xlsx\nreadxl\n\n\nread.csv()\n.csv\nBuilt in\n\n\nload()\n.Rdata\nBuilt in\n\n\nread_sas()\n.sas7bdat\nhaven"
  },
  {
    "objectID": "labs/Lab_01_instructions.html",
    "href": "labs/Lab_01_instructions.html",
    "title": "Lab 1 Instructions",
    "section": "",
    "text": "Caution\n\n\n\nLab is ready! 1/9/2025"
  },
  {
    "objectID": "labs/Lab_01_instructions.html#directions",
    "href": "labs/Lab_01_instructions.html#directions",
    "title": "Lab 1 Instructions",
    "section": "Directions",
    "text": "Directions\nPlease turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy. We can work out another way for you to turn in the labs.\nYou can download the .qmd file for this lab here.\n\nPurpose\nThis lab will serve as an introduction to our quarter long project.\nThere will be no analysis in this lab. Instead, we are building our knowledge around the research question and setting up our folder.\n\n\nGrading\nEach lab will follow the rubric on the Project page. Since this lab does not include coding nor analysis, this portion of the rubric is excluded."
  },
  {
    "objectID": "labs/Lab_01_instructions.html#lab-activities",
    "href": "labs/Lab_01_instructions.html#lab-activities",
    "title": "Lab 1 Instructions",
    "section": "Lab activities",
    "text": "Lab activities\n\n1. Reading and listening activities\n\n1.1 Article: Implicit and explicit anti-fat bias: The role of weight-related attitudes and beliefs\nThis article will serve as a reference point for our project. The article is meant to introduce social scientists’ approaches to research and analyses. However, the article is not meant to be a basis for which we perform our analysis.\n\n\n\n\n\n\nWarning\n\n\n\nThis article discusses anti-fat bias. It uses words that may be triggering to larger-bodied people.\n\n\nPlease read sections 1 - 2, through 2.2 (“Procedures and measures”). Answer the following questions:\n\nIn your own words, what is anti-fat bias?\nWhat were the three social theoretical models that the paper discusses? Which do you personally think is the biggest contributor to anti-fat bias and why?\nFrom the following measures in section 2.2, select two and discuss why the named measure may or may not accurately represent the italicized statement taken from the IAT questionnaire. Feel free to answer this question after taking the IAT yourself.\n\nSelf-perception of weight\nThin/fat group identity\nControllability of weight\nAwareness of societal standards\nInternalization of societal standards\n\nFor example, for Self-perception of weight, the italicized statement is the following statement outlined in red:\n\n\n\n\n\n\n\nTask\n\n\n\nAnswer the above questions.\n\n\n\n\n1.2 Podcast: Anti-Fat Bias by Maintenance Phase\n\n\n\n\n\n\nWarning\n\n\n\nThis podcast shares the experience of one of its hosts that involves anti-fat bias. This may be triggering if you have experienced this type of bias.\n\n\nThis is an optional listening for this lab, but I highly encourage you listen at some point this quarter. This is a really good way to see how research can be integrated into conversation and experience.\nIf you decide to listen, feel free to share a quote that most impacted you.\n\n\n\n\n\n\nTask\n\n\n\nIf you decide to listen, feel free to share a quote that most impacted you.\n\n\n\n\n\n2. Familiarizing ourselves with the Implicit Association Test (IAT)\n\n2.1 Learn more about the test\nVisit the Project Implicit site, and read about the test. What is your initial reaction to the test? What questions about the test do you have? Do you have any questions about the test’s validity? The point here is not to attempt to discredit the test itself, but see what specific questions the test can help us answer and what is outside the scope of our analysis. For example, are there any potential issues with the fact that people are self-selected to take the test? Does that mean our sample is representative of our population? Is it an issue that someone can take the test more than once?\nThis exercise will serve as a good starting point for the discussion section of our project report. The more effort you put in here and now, the more prepared you will be for the report.\n\n\n\n\n\n\nTask\n\n\n\nIn 5-10 bullet points, write down some of your ideas on the study design that you may want to mention.\n\n\n\n\n2.2 Take the test\nYou will spend 15 minutes taking the IAT. You can go to the Project Implicit website, register, and select a specific test to take. Once registered, you can click “Take a Test,” read the Preliminary Information, and then click “I wish to proceed” at the bottom. Then you can click the button “Weight IAT” to take this particular test.\nI will not check that you have completed this test, but it will help you understand the data you are analyzing.\n\n\n\n\n\n\nTask\n\n\n\nTake the Weight IAT.\n\n\n\n\n\n3. Get a sense of how you would like to analyze the data\nFor our project, we will examine the association betwen the IAT score and one other variable. From the above article, and the introduced variables in section 2.2, which association are you most interested in analyzing? Please write this in the form of a research question.\nWe will have a chance to adjust our research question once we have explored the data in Lab 2.\n\n\n\n\n\n\nTask\n\n\n\nWrite your research question\n\n\n\n\n4. Organize your “Project” folder\nBefore downloading the data, go back to Lesson 2 and follow the file setup for our project. This includes making an .Rproj file within the main folder. Make sure you are working with the project by using the here() function to display your working directory.\n\n\n\n\n\n\nTask\n\n\n\nDisplay your working directory using the here package and here() function.\n\n\n\n\n5. Compile above work into an introduction\nAt this point, you have done a lot of the work needed to write an introduction for your poster. In 3-5 bullet points, write a description of anti-fat bias, IAT, your research question, and the context for the question.\nIn the next lab, we will work on a summary of the dataset (e.g. where are the data from, when were they collected, how many subjects, what are the variables, what are the exposure and outcomes variables of interest, etc.).\n\n\n\n\n\n\nTask\n\n\n\nWrite your introduction"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#mutate-constructing-new-variables-from-what-you-have-1",
    "href": "lessons/02_Data_Management/02_Data_Management.html#mutate-constructing-new-variables-from-what-you-have-1",
    "title": "Lesson 2: Data and File Management",
    "section": "mutate(): constructing new variables from what you have",
    "text": "mutate(): constructing new variables from what you have\n\ndds.discr3 = dds.discr1 %&gt;%\n  mutate(exp_200 = expenditures - 200, \n         expend_20perc = expenditures * 0.2, \n         expend_sq = expenditures^2, \n         expend_over_5000 = case_when(\n           expenditures &gt; 5000 ~ \"Yes\", \n           expenditures &lt;= 5000 ~ \"No\"\n         )\n  )\nglimpse(dds.discr3)\n\nRows: 1,000\nColumns: 10\n$ id               &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 1077…\n$ age.cohort       &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17,…\n$ age              &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17…\n$ SAB              &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Mal…\n$ expenditures     &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021…\n$ R_E              &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, His…\n$ exp_200          &lt;dbl&gt; 1913, 41724, 1254, 6200, 4212, 4366, 3715, 3673, 4821…\n$ expend_20perc    &lt;dbl&gt; 422.6, 8384.8, 290.8, 1280.0, 882.4, 913.2, 783.0, 77…\n$ expend_sq        &lt;dbl&gt; 4464769, 1757621776, 2114116, 40960000, 19465744, 208…\n$ expend_over_5000 &lt;chr&gt; \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"Ye…"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management.html#mutate-other-examples",
    "href": "lessons/02_Data_Management/02_Data_Management.html#mutate-other-examples",
    "title": "Lesson 2: Data and File Management",
    "section": "mutate(): other examples",
    "text": "mutate(): other examples\n\ndds.discr3 = dds.discr1 %&gt;%\n  mutate(expend_20perc = expenditures * 0.2, \n         expend_sq = expenditures^2, \n         expend_over_5000 = case_when(\n           expenditures &gt; 5000 ~ \"Yes\", \n           expenditures &lt;= 5000 ~ \"No\"\n         ), \n         expend_log = log(expenditures)\n  )\nglimpse(dds.discr3)\n\nRows: 1,000\nColumns: 10\n$ id               &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 1077…\n$ age.cohort       &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17,…\n$ age              &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17…\n$ SAB              &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Mal…\n$ expenditures     &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021…\n$ R_E              &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, His…\n$ expend_20perc    &lt;dbl&gt; 422.6, 8384.8, 290.8, 1280.0, 882.4, 913.2, 783.0, 77…\n$ expend_sq        &lt;dbl&gt; 4464769, 1757621776, 2114116, 40960000, 19465744, 208…\n$ expend_over_5000 &lt;chr&gt; \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"Ye…\n$ expend_log       &lt;dbl&gt; 7.655864, 10.643614, 7.282074, 8.764053, 8.392083, 8.…"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management_key_info.html",
    "href": "lessons/02_Data_Management/02_Data_Management_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "I saw that we had some issues with getting into the OneDrive\n\nLet’s see if this link works\nI might need to manually add you all to the folder UGH\n\nNew to R? Check out my old class site\n\nVideo recordings will be in Student folder\n\nAnything else?"
  },
  {
    "objectID": "lessons/02_Data_Management/02_Data_Management_key_info.html#announcements",
    "href": "lessons/02_Data_Management/02_Data_Management_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "I saw that we had some issues with getting into the OneDrive\n\nLet’s see if this link works\nI might need to manually add you all to the folder UGH\n\nNew to R? Check out my old class site\n\nVideo recordings will be in Student folder\n\nAnything else?"
  },
  {
    "objectID": "project.html#lab-rubric",
    "href": "project.html#lab-rubric",
    "title": "Project Central",
    "section": "Lab rubric",
    "text": "Lab rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written as asked (bullets or complete sentences) with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project poster.\nLab submitted on Sakai with .html file. Answers are written as asked (bullets or complete sentences) with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written as asked (bullets or complete sentences) with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are not written as asked.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work*\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nSome tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning**\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*While there is not a large emphasis on “correctness” in the labs, you must follow the correct procedure for certain tasks. The code/work grade will reflect whether or not you followed the procedure for analysis correctly.\n**Applies to questions with reasoning (like target population, choosing variables, revisiting research question)"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR_key_info.html",
    "href": "lessons/03_SLR/03_SLR_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "There is no Slack for this class\nAttendance policy: Attendance in-person or online to all (ish) classes. Attendance is taken by completing exit ticket within 1 week of class\nExtra question on preference for Thursday morning office hours\nTA emails are up!\n\nWe are also working on office hours"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR_key_info.html#announcements",
    "href": "lessons/03_SLR/03_SLR_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "There is no Slack for this class\nAttendance policy: Attendance in-person or online to all (ish) classes. Attendance is taken by completing exit ticket within 1 week of class\nExtra question on preference for Thursday morning office hours\nTA emails are up!\n\nWe are also working on office hours"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#get-to-know-the-data-13",
    "href": "lessons/03_SLR/03_SLR.html#get-to-know-the-data-13",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Get to know the data (1/3)",
    "text": "Get to know the data (1/3)\n\nLoad data\n\n\nlibrary(readxl)\ngapm1 &lt;- read_excel(here(\"data/Gapminder_vars_2011.xlsx\"), na = \"NA\")"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#get-to-know-the-data-23",
    "href": "lessons/03_SLR/03_SLR.html#get-to-know-the-data-23",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Get to know the data (2/3)",
    "text": "Get to know the data (2/3)\n\nGlimpse of the data\n\n\nglimpse(gapm1)\n\nRows: 195\nColumns: 18\n$ country                            &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\"…\n$ CO2emissions                       &lt;dbl&gt; 0.412, 1.790, 3.290, 5.870, 1.250, …\n$ ElectricityUsePP                   &lt;dbl&gt; NA, 2210.0, 1120.0, NA, 207.0, NA, …\n$ FoodSupplykcPPD                    &lt;dbl&gt; 2110, 3130, 3220, NA, 2410, 2370, 3…\n$ IncomePP                           &lt;dbl&gt; 1660, 10200, 13000, 42000, 5910, 18…\n$ LifeExpectancyYrs                  &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9,…\n$ FemaleLiteracyRate                 &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.…\n$ population                         &lt;dbl&gt; 2.97e+07, 2.93e+06, 3.68e+07, 8.38e…\n$ WaterSourcePrct                    &lt;dbl&gt; 52.6, 88.1, 92.6, 100.0, 40.3, 97.0…\n$ geo                                &lt;chr&gt; \"afg\", \"alb\", \"dza\", \"and\", \"ago\", …\n$ four_regions                       &lt;chr&gt; \"asia\", \"europe\", \"africa\", \"europe…\n$ eight_regions                      &lt;chr&gt; \"asia_west\", \"europe_east\", \"africa…\n$ six_regions                        &lt;chr&gt; \"south_asia\", \"europe_central_asia\"…\n$ members_oecd_g77                   &lt;chr&gt; \"g77\", \"others\", \"g77\", \"others\", \"…\n$ Latitude                           &lt;dbl&gt; 33.00000, 41.00000, 28.00000, 42.50…\n$ Longitude                          &lt;dbl&gt; 66.00000, 20.00000, 3.00000, 1.5210…\n$ `World bank region`                &lt;chr&gt; \"South Asia\", \"Europe & Central Asi…\n$ `World bank, 4 income groups 2017` &lt;chr&gt; \"Low income\", \"Upper middle income\"…\n\n\n\nNote the missing values for our variables of interest"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#get-to-know-the-data-33",
    "href": "lessons/03_SLR/03_SLR.html#get-to-know-the-data-33",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Get to know the data (3/3)",
    "text": "Get to know the data (3/3)\n\nGet a sense of the summary statistics\n\n\ngapm1 %&gt;% \n  select(LifeExpectancyYrs, \n         FemaleLiteracyRate) %&gt;% \n  summary()\n\n LifeExpectancyYrs FemaleLiteracyRate\n Min.   :47.50     Min.   :13.00     \n 1st Qu.:64.30     1st Qu.:70.97     \n Median :72.70     Median :91.60     \n Mean   :70.66     Mean   :81.65     \n 3rd Qu.:76.90     3rd Qu.:98.03     \n Max.   :82.90     Max.   :99.80     \n NA's   :8         NA's   :115"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#research-and-dataset-description",
    "href": "lessons/03_SLR/03_SLR.html#research-and-dataset-description",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Research and dataset description",
    "text": "Research and dataset description\n\nResearch question: Is there an association between life expectancy and female literacy rates?\n\n\nData file: Gapminder_vars_2011.xlsx\nData were downloaded from Gapminder\n\n2011 is the most recent year with the most complete data\nObservational study measuring different characteristics of countries, including population, health, environment, work, etc.\n\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same.\nAdult literacy rate is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life.\n\n \n\nNational Literacy Trust in England has studied the link between these two variables\n\nPlease note that they clearly state that literacy is linked to life expectancy through many socioeconomic and health factors"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-6",
    "href": "lessons/03_SLR/03_SLR.html#poll-everywhere-question-6",
    "title": "Lesson 3: Introduction to Simple Linear Regression (SLR)",
    "section": "Poll Everywhere Question 6",
    "text": "Poll Everywhere Question 6"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR_muddy_points.html",
    "href": "lessons/03_SLR/03_SLR_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "The population model is only used to describe what we will fit. Before we use the data to estimate anything, we need a reference model for what we will fit. Once we fit the population model using our data, we report the estimated model with the best fit line."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred_key_info.html",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "No class on Monday! MLKJ day!\nEveryone should be added to the OneDrive\nExit ticket today will ask you about Thursday office hours\nI will be working on muddy points from Lesson 2, 3, and 4 tonight or tomorrow!\n\nI’ve been trying to paste the muddy points from last year so you at least have a reference\n\nAnything else?"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred_key_info.html#announcements",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "No class on Monday! MLKJ day!\nEveryone should be added to the OneDrive\nExit ticket today will ask you about Thursday office hours\nI will be working on muddy points from Lesson 2, 3, and 4 tonight or tomorrow!\n\nI’ve been trying to paste the muddy points from last year so you at least have a reference\n\nAnything else?"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred_key_info.html#additional-announcements-from-student-success-center",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred_key_info.html#additional-announcements-from-student-success-center",
    "title": "Key Info and Announcements",
    "section": "Additional Announcements from Student Success Center",
    "text": "Additional Announcements from Student Success Center\n\nCheck Out a Laptop and/or Noise Cancelling Headphones\nStarting Winter Term, the Student Success Center is offering students the opportunity to check out in-person a laptop and/or a pair of noise cancelling headphones for up to 3 hours from the SSC Front desk VPT 620.\nAlex, our new Administrative Coordinator, can help you with the checkout process! She is available in-person Monday through Friday from 8:30 am – 5:00 pm in VPT 620.\nPlease reach out to Alex Garfias Barajas if you have any questions: sphsuccess@ohsu.edu\n\n\nATLAS TI and SAS\nWe have installed ATLAS TI and SAS on our Student Success Center desktops. Currently, only our desktops have SAS, and we are working on installing SAS to our laptops. Please let us know if there is any other software that would be beneficial to your academic success.\n\n\nLinguistic Equity Research Gathering.\nJanuary 14th 11:30 am - 1:00 pm in Vanport 620M or Zoom. Lunch provided for those participating in person.\nWe hope to create a dedicated space for students to learn and engage with ongoing research on linguistic equity and health at the School of Public Health (SPH). Most importantly, the goal is to use this time to connect with one another, validate our collective knowledge, and share how we’d like to frame and shape SPH research that will directly impact our health and well-being (and those of future multilingual students!), de-centering English along the way. Please see the attached flyer for more information. Please RSVP here: https://portlandstate.qualtrics.com/jfe/form/SV_6StuUq6kI2JG5xA NEXT WEEK! Talk by Dr. Mariana Chilton: The Painful Truth About Hunger in America. Wednesday, January 15th 3:00 pm – 5:00 pm in Vanport 515. Join the SPH as they host the esteemed Dr. Mariana Chilton for an enlightening talk on “The Painful Truth About Hunger in America.” With over 25 years of dedicated research, programming, and advocacy in the field, Dr. Chilton is a leading expert in addressing the systemic roots of hunger and poverty in the United States. Her book challenges conventional wisdom about hunger, highlighting that the issue extends beyond mere food scarcity to deeper, systemic socio-political issues. Drawing on interviews and extensive research, Dr. Chilton argues that to truly eradicate hunger, a holistic approach involving personal, political, and spiritual transformation is necessary. This engaging talk will not only shed light on the hidden truths of food insecurity but also inspire actionable change. Don’t miss this opportunity to gain profound insights from a pioneering voice in public health advocacy! This is a hybrid event and open to the community! If you need accommodations, email sphcomms@ohsu.edu.\n\n\nManifesting Your Success hosted by SSC and TRIO-SSS.\nJanuary 22nd 2:00 pm – 3:00 pm in Vanport 620M.\nThe Student Success Center and TRIO-SSS invite you to make vision boards to celebrate the new year and make your own yogurt parfait! Supplies will be provided!\nUse this link to RSVP: https://forms.gle/QhAjkxtyHNe2x3X89\n\n\nWriting Resumes & Cover Letters for Public Health Opportunities.\nFriday, January 24th 12:00 pm – 1:00 pm via Zoom\nHave you ever felt like you don’t have the right experience to apply for a job, internship, or summer research program within Public Health? Join this resume and cover letter workshop to learn how to demonstrate that you meet qualifications while highlighting your experience, knowledge, and skillset for the opportunities you are applying for. We’ll share the latest information on how to effectively write and format a resume and cover letter, and where to go for 1:1 support if needed. Please RSVP: https://portlandstate.joinhandshake.com/events/1679779/share_preview\n\n\nCareers in Public Health: Insights from Industry Professionals.\nThursday, February 20th 12:00 – 1:00 pm. Virtual Event.\nThe Student Success Center and PSU’s Career Center are hosting a virtual panel featuring professionals from public service, health care, nonprofit education, and education. You’ll also learn insider tips and job search advice!\nPlease register for the event using Handshake: https://portlandstate.joinhandshake.com/events/1668028/share_preview"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#interpreting-the-coefficient-estimate-of-the-population-slope-with-cis",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#interpreting-the-coefficient-estimate-of-the-population-slope-with-cis",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Interpreting the coefficient estimate of the population slope with CIs",
    "text": "Interpreting the coefficient estimate of the population slope with CIs\n\nWhen we report our results to someone else, we don’t usually show them our full hypothesis test\n\nIn an informal setting, someone may want to see it\n\nTypically, we report the estimate with the confidence interval\n\nFrom the confidence interval, your audience can also deduce the results of a hypothesis test\n\nOnce we found our CI, we often just write the interpretation of the coefficient estimate:\n\n\n\nGeneral statement for population slope inference\n\n\nFor every increase of 1 unit in the \\(X\\)-variable, there is an expected/average (pick one) increase of \\(\\widehat\\beta_1\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\nIn our example: For every 1% increase in female literacy rate, life expectancy increases, on average, 0.232 years (95% CI: 0.170, 0.295)."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#usually-three-options-for-your-interpretations",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#usually-three-options-for-your-interpretations",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Usually three options for your interpretations",
    "text": "Usually three options for your interpretations\n\nOption 1: For every 1% increase in female literacy rate, life expectancy increases, on average, 0.232 years (95% CI: 0.170, 0.295).\n\n \n\nOption 2: For every 1% increase in female literacy rate, average life expectancy increases 0.232 years (95% CI: 0.170, 0.295).\n\n \n\nOption 3: For every 1% increase in female literacy rate, expected life expectancy increases 0.232 years (95% CI: 0.170, 0.295)."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#reference-steps-in-a-hypothesis-test",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#reference-steps-in-a-hypothesis-test",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Reference: Steps in a Hypothesis Test",
    "text": "Reference: Steps in a Hypothesis Test\n\nCheck the assumptions\n\nWhat sampling distribution are you using? What assumptions are required for it?\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols and/or in words\nAlternative: one- or two-sided?\n\nSpecify the test statistic and its distribution under the null\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#steps-for-hypothesis-test-for-population-slope-beta_1-using-t-test",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#steps-for-hypothesis-test-for-population-slope-beta_1-using-t-test",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Steps for hypothesis test for population slope \\(\\beta_1\\) (using t-test)",
    "text": "Steps for hypothesis test for population slope \\(\\beta_1\\) (using t-test)\n\n\n\nCheck the assumptions\nSet the level of significance\n\nOften we use \\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nOften, we are curious if the coefficient is 0 or not:\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nSpecify the test statistic and its distribution under the null\n\nThe test statistic is \\(t\\), and follows a Student’s t-distribution.\n\n\n\n\n\n\nCalculate the test statistic.\n\nThe calculated test statistic for \\(\\widehat\\beta_1\\) is \\[t = \\frac{ \\widehat\\beta_1 - \\beta_1}{ \\text{SE}_{\\widehat\\beta_1}} = \\frac{ \\widehat\\beta_1}{ \\text{SE}_{\\widehat\\beta_1}}\\] when we assume \\(H_0: \\beta_1 = 0\\) is true.\n\nCalculate the p-value\n\nWe are generally calculating: \\(2\\cdot P(T &gt; t)\\)\n\nWrite a conclusion\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at the \\(100\\alpha\\%\\) significiance level. There is (sufficient/insufficient) evidence that there is significant association between (\\(Y\\)) and (\\(X\\)) (p-value = \\(P(T &gt; t)\\))."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#widehatsigma2-i-hope-r-can-calculate-that-for-me-12",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#widehatsigma2-i-hope-r-can-calculate-that-for-me-12",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "\\(\\widehat\\sigma^2\\): I hope R can calculate that for me… (1/2)",
    "text": "\\(\\widehat\\sigma^2\\): I hope R can calculate that for me… (1/2)\n\nThe standard deviation \\(\\widehat{\\sigma}\\) is given in the R output as the Residual standard error\n\n\\(4^{th}\\) line from the bottom in the summary() output of the model:\n\n\n\nsummary(model1)\n\n\nCall:\nlm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        50.92790    2.66041  19.143  &lt; 2e-16 ***\nFemaleLiteracyRate  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#widehatsigma2-i-hope-r-can-calculate-that-for-me-22",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#widehatsigma2-i-hope-r-can-calculate-that-for-me-22",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "\\(\\widehat\\sigma^2\\): I hope R can calculate that for me… (2/2)",
    "text": "\\(\\widehat\\sigma^2\\): I hope R can calculate that for me… (2/2)\n\nIt can!!\n\n\nm1_sum = summary(model1)\nm1_sum$sigma\n\n[1] 6.142157\n\n# number of observations (pairs of data) used to run the model\nnobs(model1) \n\n[1] 80"
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#textbook-readings",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred.html#textbook-readings",
    "title": "Lesson 4: SLR Inference and Prediction",
    "section": "Textbook readings",
    "text": "Textbook readings\n\nIntroduction to Regression Methods for Public Health Using R\n\n4.5 Interpreting p-values\n4.6 Predictions from the model\n4.7 Confidence intervals and prediction intervals\n\nA Progressive Introduction to Linear Models\n\nNot really any good sections"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR_muddy_points.html#muddy-points-from-2024",
    "href": "lessons/03_SLR/03_SLR_muddy_points.html#muddy-points-from-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from 2024",
    "text": "Muddy Points from 2024\nIf I saw overlap between questions from this year and 2024, I am leaving my old answer below:\n\n1. What does the epsilon mean and how does it relate to the line in the linear model?\n\\(\\epsilon\\) is our error term, our residual. It is the difference between our observed value \\(Y\\) and the expected value of \\(Y\\) given \\(X\\). It’s a mathematical way to represent the fact that not every oberved \\(Y\\) value directly falls on our line. \\(\\epsilon\\) is the difference between our line and our observed value for \\(Y\\).\n\n\n2. Different betas and stuff: make the table for the class!! and epsilon\nBelow is a table that I started to construct with a student after class. We often use the model or the line to represent linear regression. When we refer to the model, most people think of the row named model. The line is just another way to represent the model. Remember that \\(\\epsilon = Y - E(Y|X)\\) and \\(\\widehat\\epsilon = Y - \\widehat{E}(Y|X)\\). Try substituting \\(\\epsilon = Y - E(Y|X)\\) into the population model \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\). Does it simplify to the population line?\nI think it can help a lot with this confusion.\n\n\n\n\n\n\n\n\n\nPopulation\nEstimated\n\n\n\n\nModel\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\\[Y = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X + \\widehat\\epsilon \\]\n\n\nLine\n\\[E(Y|X) = \\beta_0 + \\beta_1 X \\] OR\n\\[\\mu_Y = \\beta_0 + \\beta_1 X \\]\n\\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X \\]\nOR\n\\[\n\\widehat{E}[Y|X] = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X\n\\]\nOR\n\\[\n\\widehat{E[Y|X]} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X\n\\]\n\n\n\n\n2.1 Someone else asked: Why does the population model have an error term epsilon in the equation but the estimated line does not?\nI think this is referring to this slide. This was because I wanted to put the population model next to the estimated line. I realize this is very confusing. Both estimated and population models can be represented as the lines and models in the above table.\n\n\n2.2 Someone else asked: Why does the population equation even matter?\nHuh, I’m scratching my head with this one. Why does it matter? We basically mirror all the mathematical manipulations with the estimated model anyway…\nBut then I thought: What would our world or class lectures look like without the population model? The answer might be more philosophical than mathematical. The representation of the true, underlying model that we are aspiring for with our sample data reminds us that our estimated model is not perfect. That we are just trying out best to uncover some fraction of the truth. And at the end of the day, when we perform hypothesis tests, we’re working to provide evidence for the value of the population parameters from the population model. We know what the estimated values are, but can they help us get an idea of what the parameter values are?\n\n\n\n3. Math for minimizing SSE (aka OLS process)\nI am very sorry that this math was intimidating! Most of us don’t need to see the math, but there are a handful of students that should see it, and get a sense of the underlying math. Just wanted to make sure they saw it!\nThe important things for us to know is the information on the slide for Step 1 and 2, where we talk about the process itself. If I asked you why we minimize the SSE with respect to our coefficients, would you be able to answer?\n\n\n4. The lecture materials don’t always feel like they apply to the homework. If asked to “state the linear regression models,” are we just running lm()?\nStating the linear regression model is asking us to show the population model that we are fitting. This is just to make sure we are aware of the model that we plan to fit. So the generic form of this is: \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\).\nRunning lm() is the equivalent of fitting the model.\nKeep letting me know what feels disconnected in the class! Sometimes I purposefully say things in different ways to build our understanding, but sometimes that fails!"
  },
  {
    "objectID": "lessons/03_SLR/03_SLR_muddy_points.html#muddy-points-from-2025",
    "href": "lessons/03_SLR/03_SLR_muddy_points.html#muddy-points-from-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "The population model is only used to describe what we will fit. Before we use the data to estimate anything, we need a reference model for what we will fit. Once we fit the population model using our data, we report the estimated model with the best fit line."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred_muddy_points.html",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Exactly. We can get an idea of the “trustworthiness” from a graph, but we can never be certain. That’s why we gotta use the math! The math will exactly quantify that “trustworthiness.”\n\n\n\nBig question. For this class, when each is asked (heh). This is mostly so we get practice doing both and can do either in a quicker, more informal way.\nFor real life, people will report the p-value of a hypothesis test as they make a more meaningful statement about the coefficient estimate. No one really writes “We rejected the null hypothesis that …” However, they DO write “We found significant association with number of visits to the doctor (p-value = 0.008).” And even more likely, they are writing “We found that for an increase of one doctor visit per year, there was a 0.5 decrease in the expected number of 911 calls (95%CI: 0.2, 0.8). All that to say that people typically use CIs when doing a presentation or written report. When chatting with colleagues, you may informally discuss the hypothesis test.\n\n\n\nIt’s hard! My main advice is to keep practicing! Hats mean we are referring to the result of our estimate. We can replace any time we write “\\(\\hat{\\beta_1}\\)” with the actual value we found in the regression table. For \\(\\beta_1\\), we cannot replace it with an actual value. We can hypothesize what that value is, but we can never replace \\(\\beta_1\\) with a 100% certain value.\n\n\n\n\n\n\nThe population model is basically the plan for what we will do. It’s our guide, our blueprint. It does not actually exist in the world though! That’s becuase it is just a way to represent the true, underlying relationship that we cannot fully uncover.\nThat’s where the sample estimated model comes in. We can take the population model (our plan, guide, blueprint) and then implement it using data. Then we have the estimates! Those estimates are our closest bet to the population from our data.\nYou can think of it like the blueprint to a house. We have a plan. It’s our ideal structure for the home. If we have really good materials and evreyhting we need (think really good sample), then we can execute the blueprint pretty well. Our house will resemble the blueprint closely. It will never exactly be the blueprint, but it will get close. However, if our materials are lacking, and maybe someone forgot their measuring tape at home, then our house will not resemble our blueprint as nicely. There are many different houses that can come from the same blueprint.\n\n\n\nPrediction means we are using the model to predict the outcome. Inference means we are using the model to discuss the relationship between variables.\nFor the most part, models learned in this class will be most appropriate for inference. Prediction requires much more information, flexibility, and variables to make the predicted value more accurate.\n\n\n\nRemember that the residuals can be negative or positive depending what side of the best-fit line they fall. If we sum them, no matter how far they are from the line, it is likely the sum will be close to zero. To make sure the negative and positive residuals do not cancel each other out, we square them!\n\n\n\nThis is because our model is what gives us the degrees of freedom. We are performing the hypothesis test on one coefficient, \\(\\beta_1\\), but our model is estimating 2 coefficients! Thus we are restricted to \\(n-2\\) degrees of freedom.\n\n\n\nThe best way is to look at the std. error column and then find the row that has the X-variable. The number in the table is the standard error for \\(\\beta_1\\)."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred_muddy_points.html#muddy-points-from-2025",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred_muddy_points.html#muddy-points-from-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "Exactly. We can get an idea of the “trustworthiness” from a graph, but we can never be certain. That’s why we gotta use the math! The math will exactly quantify that “trustworthiness.”\n\n\n\nBig question. For this class, when each is asked (heh). This is mostly so we get practice doing both and can do either in a quicker, more informal way.\nFor real life, people will report the p-value of a hypothesis test as they make a more meaningful statement about the coefficient estimate. No one really writes “We rejected the null hypothesis that …” However, they DO write “We found significant association with number of visits to the doctor (p-value = 0.008).” And even more likely, they are writing “We found that for an increase of one doctor visit per year, there was a 0.5 decrease in the expected number of 911 calls (95%CI: 0.2, 0.8). All that to say that people typically use CIs when doing a presentation or written report. When chatting with colleagues, you may informally discuss the hypothesis test.\n\n\n\nIt’s hard! My main advice is to keep practicing! Hats mean we are referring to the result of our estimate. We can replace any time we write “\\(\\hat{\\beta_1}\\)” with the actual value we found in the regression table. For \\(\\beta_1\\), we cannot replace it with an actual value. We can hypothesize what that value is, but we can never replace \\(\\beta_1\\) with a 100% certain value.\n\n\n\n\n\n\nThe population model is basically the plan for what we will do. It’s our guide, our blueprint. It does not actually exist in the world though! That’s becuase it is just a way to represent the true, underlying relationship that we cannot fully uncover.\nThat’s where the sample estimated model comes in. We can take the population model (our plan, guide, blueprint) and then implement it using data. Then we have the estimates! Those estimates are our closest bet to the population from our data.\nYou can think of it like the blueprint to a house. We have a plan. It’s our ideal structure for the home. If we have really good materials and evreyhting we need (think really good sample), then we can execute the blueprint pretty well. Our house will resemble the blueprint closely. It will never exactly be the blueprint, but it will get close. However, if our materials are lacking, and maybe someone forgot their measuring tape at home, then our house will not resemble our blueprint as nicely. There are many different houses that can come from the same blueprint.\n\n\n\nPrediction means we are using the model to predict the outcome. Inference means we are using the model to discuss the relationship between variables.\nFor the most part, models learned in this class will be most appropriate for inference. Prediction requires much more information, flexibility, and variables to make the predicted value more accurate.\n\n\n\nRemember that the residuals can be negative or positive depending what side of the best-fit line they fall. If we sum them, no matter how far they are from the line, it is likely the sum will be close to zero. To make sure the negative and positive residuals do not cancel each other out, we square them!\n\n\n\nThis is because our model is what gives us the degrees of freedom. We are performing the hypothesis test on one coefficient, \\(\\beta_1\\), but our model is estimating 2 coefficients! Thus we are restricted to \\(n-2\\) degrees of freedom.\n\n\n\nThe best way is to look at the std. error column and then find the row that has the X-variable. The number in the table is the standard error for \\(\\beta_1\\)."
  },
  {
    "objectID": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred_muddy_points.html#muddy-points-from-2024",
    "href": "lessons/04_SLR_Inf_Pred/04_SLR_Inf_Pred_muddy_points.html#muddy-points-from-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from 2024",
    "text": "Muddy Points from 2024\n\n1. “all of the different manifestations of t”\nI love the way this person said it!\nSo I’ve sorted this out:\n\nWe say \\(T\\) follows a t-distribution\n\n\\(T\\) is the general name for the variable (like \\(X\\) or \\(Y\\))\n\nWe calculate a given \\(t\\)-value and call that \\(t\\)\n\nWe also call this the test statistic\n\nThe critical value that corresponds to a specific confidence interval and \\(\\alpha\\) is labelled \\(t^*\\)\n\n\n\n2. What’s the difference between SD and variance?\nSD (standard deviation) is the square root of the variance. That’s why I sometimes write \\(\\sigma\\) (standard deviation) or \\(\\sigma^2\\) (variance) when I’m talking about the distribution of residuals.\n\\[\n\\sigma = \\sqrt{\\sigma^2}\n\\]\nVariance is usually easier to work with mathematically, but standard deviation is in the units that match a variable. For example, the variance of 10 height measurements are in square inches, but the standard deviation are in inches.\n\n\n3. Why is it important to test if \\(\\beta_1\\) is equal to zero? Is \\(\\beta_1=0\\) the same as the x and y variables having no correlation?\nLet’s answer the second question: Yes! It is the same in simple linear regression. When we get to multiple linear regression, and have several variables/coefficients in our model, testing \\(\\beta_1=0\\) won’t be the same as testing the correlation.\nIn simple linear regression, it is important to test \\(\\beta_1\\) mostly for pedagogical reasons. It’s just helpful to establish the process in a simpler setting.\n\n\n4. SSE and sigma\nWe were looking at the relationship between SSE and \\(\\widehat\\sigma^2\\):\n\\[ \\widehat{\\sigma}^2 = \\frac{1}{n-2}SSE \\]\nThe sum of square errors is \\(SSE = \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = \\sum_{i=1}^n \\epsilon_i^2\\)\n\n\n\n\n\n\nAn aside on variance\n\n\n\nThe definition of variance is the sum of the squared differences between values and their mean.\nSo if I had a variable \\(S\\), with 100 observations, the mean of \\(S\\), which we call \\(\\overline{S}\\), would be \\(\\frac{\\sum_{i=1}^{100} S_i}{100}\\). The variance of \\(S\\) would be \\(\\sum_{i=1}^{100} (S_i - \\overline{S})^2\\).\n\n\nNow, let’s get back to the sum of square errors: \\(SSE = \\sum_{i=1}^n \\epsilon_i^2\\)\nThe variance of the residuals would be \\(\\sum_{i=1}^n (\\epsilon_i - \\overline{\\epsilon})^2\\). The mean of \\(\\epsilon\\), \\(\\overline\\epsilon\\), should be 0 by our assumptions. So the variance of the residuals is \\(\\sum_{i=1}^n \\epsilon_i^2\\) which is our SSE!\nThere is some more complicated math that goes into why our variance is divided by n-2 to get the estimated variance of the residuals, but that’s basically it!"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval_key_info.html",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Question in exit ticket: Could you please repeat questions for the live-streamers?\n\nI will try to remember! This is one of those cases where the recording is not the exact substitute for the class. It’s just hard for me to think about the mic picking up everything while I’m speaking.\n\n\n\n\n\n\n\nOffice hours: Fridays 10-11am\n\nOnline Zoom Link\n\nEmail: ainsworl@ohsu.edu\n\n\n\n\n\nOffice hours: Mondays 4:30-5:30 pm\n\nOnline Webex Link\n\nEmail: handka@ohsu.edu\n\n\n\n\n\nOffice hours: Thursdays 10-11am\n\nOnline Webex Link\n\nEmail: sunm@ohsu.edu"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval_key_info.html#announcements",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Question in exit ticket: Could you please repeat questions for the live-streamers?\n\nI will try to remember! This is one of those cases where the recording is not the exact substitute for the class. It’s just hard for me to think about the mic picking up everything while I’m speaking.\n\n\n\n\n\n\n\nOffice hours: Fridays 10-11am\n\nOnline Zoom Link\n\nEmail: ainsworl@ohsu.edu\n\n\n\n\n\nOffice hours: Mondays 4:30-5:30 pm\n\nOnline Webex Link\n\nEmail: handka@ohsu.edu\n\n\n\n\n\nOffice hours: Thursdays 10-11am\n\nOnline Webex Link\n\nEmail: sunm@ohsu.edu"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#process-of-regression-data-analysis",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#process-of-regression-data-analysis",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Process of regression data analysis",
    "text": "Process of regression data analysis\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#how-is-this-actually-calculated-for-our-fitted-model-12",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#how-is-this-actually-calculated-for-our-fitted-model-12",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "How is this actually calculated for our fitted model? (1/2)",
    "text": "How is this actually calculated for our fitted model? (1/2)\n\\[ \\begin{aligned}\nY_i - \\overline{Y} & = (Y_i - \\widehat{Y}_i) + (\\widehat{Y}_i- \\overline{Y})\\\\\n\\text{Total unexplained variation} & = \\text{Variation due to regression} + \\text{Residual variation after regression}\n\\end{aligned}\\]\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 & = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY & = SSR + SSE\n\\end{aligned}\\] \\[\\text{Total Sum of Squares} = \\text{Sum of Squares due to Regression} + \\text{Sum of Squares due to Error (residuals)}\\]\n\n\n\n\nANOVA table:\n\n\n\n\n\n\n\n\n\n\n\nVariation Source\ndf\nSS\nMS\ntest statistic\np-value\n\n\n\n\nRegression\n\\(1\\)\n\\(SSR\\)\n\\(MSR = \\frac{SSR}{1}\\)\n\\(F = \\frac{MSR}{MSE}\\)\n\n\n\nError\n\\(n-2\\)\n\\(SSE\\)\n\\(MSE = \\frac{SSE}{n-2}\\)\n\n\n\n\nTotal\n\\(n-1\\)\n\\(SSY\\)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#how-is-this-actually-calculated-for-our-fitted-model-22",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#how-is-this-actually-calculated-for-our-fitted-model-22",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "How is this actually calculated for our fitted model? (2/2)",
    "text": "How is this actually calculated for our fitted model? (2/2)\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 & = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY & = SSR + SSE\n\\end{aligned}\\] \\[\\text{Total Sum of Squares} = \\text{Sum of Squares due to Regression} + \\text{Sum of Squares due to Error (residuals)}\\]\n\n\n\n\nANOVA table:\n\n\n\n\n\n\n\n\n\n\n\nVariation Source\ndf\nSS\nMS\ntest statistic\np-value\n\n\n\n\nRegression\n\\(1\\)\n\\(SSR\\)\n\\(MSR = \\frac{SSR}{1}\\)\n\\(F = \\frac{MSR}{MSE}\\)\n\n\n\nError\n\\(n-2\\)\n\\(SSE\\)\n\\(MSE = \\frac{SSE}{n-2}\\)\n\n\n\n\nTotal\n\\(n-1\\)\n\\(SSY\\)\n\n\n\n\n\n\n\n\n\n\n \n\nF-statistic: Proportion of variation that is explained by the model to variation not explained by the model"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#least-squares-model-assumptions-line",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#least-squares-model-assumptions-line",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Least-squares model assumptions: LINE",
    "text": "Least-squares model assumptions: LINE\n \nThese are the model assumptions made in ordinary least squares:\n \n\n[L] Linearity of relationship between variables\n\n\n[I] Independence of the \\(Y\\) values\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\n[E] Equality of variance of the residuals (homoscedasticity)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question-1-1",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question-1-1",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#summary-of-line-model-assumptions",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#summary-of-line-model-assumptions",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Summary of LINE model assumptions",
    "text": "Summary of LINE model assumptions\n\n\\(Y\\) values are independent (check study design!)\n\n\n\n\n\n\nThe distribution of \\(Y\\) given \\(X\\) is\n\nnormal\nwith mean \\(\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\)\nand common variance \\(\\sigma^2\\)\n\n\nThis means that the residuals are\n\nnormal\nwith mean = 0\nand common variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#how-do-we-determine-if-our-model-follows-the-line-assumptions",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#how-do-we-determine-if-our-model-follows-the-line-assumptions",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "How do we determine if our model follows the LINE assumptions?",
    "text": "How do we determine if our model follows the LINE assumptions?\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nCheck if there is a linear relationship between the mean response (Y) and the explanatory variable (X)\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nCheck that the observations are independent\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\nCheck that the responses (at each level X) are normally distributed\n\nUsually measured through the residuals\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nCheck that the variance (or standard deviation) of the responses is equal for all levels of X\n\nUsually measured through the residuals\n\n\n\n\n\n\n\nLesson 5: SLR 3"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question-4",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#poll-everywhere-question-4",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/05_SLR_Eval/05_SLR_Eval.html#another-way-of-thinking-about-the-different-deviations",
    "href": "lessons/05_SLR_Eval/05_SLR_Eval.html#another-way-of-thinking-about-the-different-deviations",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "Another way of thinking about the different deviations",
    "text": "Another way of thinking about the different deviations"
  },
  {
    "objectID": "lessons/10_Cat_covariates/10_Cat_covariates.html#why-slr-ish",
    "href": "lessons/10_Cat_covariates/10_Cat_covariates.html#why-slr-ish",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Why “SLR-ish”?",
    "text": "Why “SLR-ish”?"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html",
    "title": "Lesson 8: Introduction to Multiple Linear Regression (MLR)",
    "section": "",
    "text": "Understand equations and visualizations that helps us build multiple linear regression model.\nFit MLR model (in R) and understand the difference between fitted regression plane and regression lines.\nIdentify the population multiple linear regression model and define statistics language for key notation.\nBased off of previous SLR work, understand how the population MLR is estimated.\nInterpret MLR (population) coefficient estimates with additional variable in model\n\n\n\n\nSLR helped us establish the foundation for a lot of regression\n\nBut we do not usually use SLR in analysis\n\n\nWhat did we learn in SLR??\n\n\n\n\nModel Fitting\n\n\n\nOrdinary least squares (OLS)\nlm() function in R\n\n\n\n\n\n\nModel Use\n\n\n\nInference for variance of residuals\nHypothesis testing for coefficients\nInterpreting population coefficient estimates\nCalculated the expected mean for specific \\(X\\) values\nInterpreted coefficient of determination\n\n\n\n\n\n\nModel Evaluation/Diagnostics\n\n\n\nLINE Assumptions\nInfluential points\nData Transformations\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#reminder-of-what-we-learned-in-the-context-of-slr",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#reminder-of-what-we-learned-in-the-context-of-slr",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Reminder of what we learned in the context of SLR",
    "text": "Reminder of what we learned in the context of SLR\n\nSLR helped us establish the foundation for a lot of regression\n\nBut we do not usually use SLR in analysis\n\n\nWhat did we learn in SLR??\n\n\n\n\nModel Fitting\n\n\n\nOrdinary least squares (OLS)\nlm() function in R\n\n\n\n\n\n\nModel Use\n\n\n\nInference for variance of residuals\nHypothesis testing for coefficients\nInterpreting population coefficient estimates\nCalculated the expected mean for specific \\(X\\) values\nInterpreted coefficient of determination\n\n\n\n\n\n\nModel Evaluation/Diagnostics\n\n\n\nLINE Assumptions\nInfluential points\nData Transformations"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#lets-map-that-to-our-regression-analysis-process",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#lets-map-that-to-our-regression-analysis-process",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Let’s map that to our regression analysis process",
    "text": "Let’s map that to our regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#simple-linear-regression-vs.-multiple-linear-regression",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#simple-linear-regression-vs.-multiple-linear-regression",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Simple Linear Regression vs. Multiple Linear Regression",
    "text": "Simple Linear Regression vs. Multiple Linear Regression\n\n\n\nSimple Linear Regression\n\n \n\nWe use one predictor to try to explain the variance of the outcome\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\\]\n\n\n\nMultiple Linear Regression\n\n \n\nWe use multiple predictors to try to explain the variance of the outcome\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_{k}X_{k}+ \\epsilon\n\\]\n\n \n\nHas \\(k+1\\) total coefficients (including intercept) for \\(k\\) predictors/covariates\nSometimes referred to as multivariable linear regression, but never multivariate\n\n\n\n \n\nThe models have similar “LINE” assumptions and follow the same general diagnostic procedure"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#going-back-to-our-life-expectancy-example",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#going-back-to-our-life-expectancy-example",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Going back to our life expectancy example",
    "text": "Going back to our life expectancy example\n\nLet’s say many other variables were measured for each country, including food supply\n\nFood Supply (kilocalories per person per day, kc PPD): the average kilocalories consumed by a person each day.\n\nIn SLR, we only had one predictor and one outcome in the model:\n\nOutcome: Life expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same.\nPredictor: Adult literacy rate(predictor) is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life.\n\n\n \n\nDo we think adult female literacy rate is going to explain a lot of the variance of life expectancy between countries?"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#loading-the-new-ish-data",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#loading-the-new-ish-data",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Loading the (new-ish) data",
    "text": "Loading the (new-ish) data\n\n# Load the data - update code if the csv file is not in the same location on your computer\n# If you need to download the file, please go to ur shared folder under Data &gt; Slides\ngapm &lt;- read_excel(\"data/Gapminder_vars_2011.xlsx\", \n                   na = \"NA\")  # important!!!! \n\ngapm_sub &lt;- gapm %&gt;% \n  drop_na(LifeExpectancyYrs, FemaleLiteracyRate, FoodSupplykcPPD)\n\nglimpse(gapm_sub)\n\nRows: 72\nColumns: 18\n$ country                            &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\",…\n$ CO2emissions                       &lt;dbl&gt; 0.4120, 1.7900, 1.2500, 5.3600, 4.6…\n$ ElectricityUsePP                   &lt;dbl&gt; NA, 2210, 207, NA, 2900, 1810, 258,…\n$ FoodSupplykcPPD                    &lt;dbl&gt; 2110, 3130, 2410, 2370, 3160, 2790,…\n$ IncomePP                           &lt;dbl&gt; 1660, 10200, 5910, 18600, 19600, 70…\n$ LifeExpectancyYrs                  &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8,…\n$ FemaleLiteracyRate                 &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5,…\n$ population                         &lt;dbl&gt; 2.97e+07, 2.93e+06, 2.42e+07, 9.57e…\n$ WaterSourcePrct                    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8,…\n$ geo                                &lt;chr&gt; \"afg\", \"alb\", \"ago\", \"atg\", \"arg\", …\n$ four_regions                       &lt;chr&gt; \"asia\", \"europe\", \"africa\", \"americ…\n$ eight_regions                      &lt;chr&gt; \"asia_west\", \"europe_east\", \"africa…\n$ six_regions                        &lt;chr&gt; \"south_asia\", \"europe_central_asia\"…\n$ members_oecd_g77                   &lt;chr&gt; \"g77\", \"others\", \"g77\", \"g77\", \"g77…\n$ Latitude                           &lt;dbl&gt; 33.00000, 41.00000, -12.50000, 17.0…\n$ Longitude                          &lt;dbl&gt; 66.00000, 20.00000, 18.50000, -61.8…\n$ `World bank region`                &lt;chr&gt; \"South Asia\", \"Europe & Central Asi…\n$ `World bank, 4 income groups 2017` &lt;chr&gt; \"Low income\", \"Upper middle income\"…"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#can-we-improve-our-model-by-adding-food-supply-as-a-covariate",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#can-we-improve-our-model-by-adding-food-supply-as-a-covariate",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Can we improve our model by adding food supply as a covariate?",
    "text": "Can we improve our model by adding food supply as a covariate?\n\n\n\n\n\n\nSimple linear regression population model\n\n\n\\[\\begin{aligned}\n\\text{Life expectancy} & = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\epsilon \\\\\n\\text{LE} & = \\beta_0 + \\beta_1 \\text{FLR} + \\epsilon\n\\end{aligned}\\]\n\n\n\n\nMultiple linear regression population model (with added Food Supply)\n\n\n\\[\\begin{aligned}\n\\text{Life expectancy} & = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_2 \\text{Food supply} + \\epsilon \\\\\n\\text{LE} & = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#visualize-relationship-between-life-expectancy-female-literacy-rate-and-food-supply",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#visualize-relationship-between-life-expectancy-female-literacy-rate-and-food-supply",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Visualize relationship between life expectancy, female literacy rate, and food supply",
    "text": "Visualize relationship between life expectancy, female literacy rate, and food supply"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#visualize-relationship-in-3-d",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#visualize-relationship-in-3-d",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Visualize relationship in 3-D",
    "text": "Visualize relationship in 3-D"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#poll-everywhere-question-1",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#poll-everywhere-question-1",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#how-do-we-fit-a-multiple-linear-regression-model-in-r",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#how-do-we-fit-a-multiple-linear-regression-model-in-r",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "How do we fit a multiple linear regression model in R?",
    "text": "How do we fit a multiple linear regression model in R?\nNew population model for example:\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]\n\n# Fit regression model:\nmr1 &lt;- gapm_sub %&gt;% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)\ntidy(mr1, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157 \\ \\text{FLR}\n+ 0.008\\ \\text{FS}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#dont-forget-summary-to-extract-information",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#dont-forget-summary-to-extract-information",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Don’t forget summary() to extract information!",
    "text": "Don’t forget summary() to extract information!\n\nsummary(mr1)\n\n\nCall:\nlm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n    data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.715  -2.328   1.052   3.022   9.083 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        33.595479   4.472049   7.512 1.56e-10 ***\nFemaleLiteracyRate  0.156699   0.032158   4.873 6.75e-06 ***\nFoodSupplykcPPD     0.008482   0.001795   4.726 1.17e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.391 on 69 degrees of freedom\nMultiple R-squared:  0.563, Adjusted R-squared:  0.5503 \nF-statistic: 44.44 on 2 and 69 DF,  p-value: 3.958e-13"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#visualize-the-fitted-multiple-regression-model",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#visualize-the-fitted-multiple-regression-model",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Visualize the fitted multiple regression model",
    "text": "Visualize the fitted multiple regression model\n\n\n\nThe fitted model equation \\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X_1 + \\widehat{\\beta}_2 \\cdot X_2\\] has three variables (\\(Y, X_1,\\) and \\(X_2\\)) and thus we need 3 dimensions to plot it\n\n \n\nInstead of a regression line, we get a regression plane\n\nSee code in .qmd- file. I hid it from view in the html file."
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#regression-lines-for-varying-values-of-food-supply",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#regression-lines-for-varying-values-of-food-supply",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Regression lines for varying values of food supply",
    "text": "Regression lines for varying values of food supply\n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157 \\ \\text{FLR}\n+ 0.008\\ \\text{FS}\n\\end{aligned}\\]\n\n\n\nNote: when the food supply is held constant but the female literacy rate varies…\n\nthen the outcome values change along a line\n\nDifferent values of food supply give different lines\n\nThe intercepts change, but\nthe slopes stay the same (parallel lines)\n\n\n\n\n(mr1_2d = ggPredict(mr1, interactive = T))"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#how-do-we-calculate-the-regression-line-for-3000-kc-ppd-food-supply",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#how-do-we-calculate-the-regression-line-for-3000-kc-ppd-food-supply",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "How do we calculate the regression line for 3000 kc PPD food supply?",
    "text": "How do we calculate the regression line for 3000 kc PPD food supply?\n\n\n \n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= 33.595 + 0.157 \\ \\text{FLR}\n+ 0.008\\ \\text{FS}\\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157 \\ \\text{FLR}\n+ 0.008\\cdot 3000\\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157 \\ \\text{FLR}\n+ 25.446 \\\\\n\\widehat{\\text{LE}} &= 59.042 + 0.157 \\ \\text{FLR}\n\\end{aligned}\\]\n\n\n(mr1_2d = ggPredict(mr1, interactive = T))"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#poll-everwhere-question-2",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#poll-everwhere-question-2",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everwhere Question 2",
    "text": "Poll Everwhere Question 2"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#population-multiple-regression-model",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#population-multiple-regression-model",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Population multiple regression model",
    "text": "Population multiple regression model\n\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\nor on the individual (observation) level:\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}+ \\ldots + \\beta_k x_{ik} + \\epsilon_i,\\ \\ \\text{for}\\ i = 1, 2, \\ldots, n\\]\n\n\nObservable sample data\n\n\\(Y\\) is our dependent variable\n\nAka outcome or response variable\n\n\\(X_1, X_2, \\ldots, X_k\\) are our \\(k\\) independent variables\n\nAka predictors or covariates\n\n\n\nUnobservable population parameters\n\n\\(\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k\\) are unknown population parameters\n\nFrom our sample, we find the population parameter estimates: \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, \\ldots, \\widehat{\\beta}_k\\)\n\n\\(\\epsilon\\) is the random error\n\nAnd is still normally distributed\n\\(\\epsilon \\sim N(0, \\sigma^2)\\) where \\(\\sigma^2\\) is the population parameter of the variance"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#how-do-we-estimate-the-model-parameters",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#how-do-we-estimate-the-model-parameters",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "How do we estimate the model parameters?",
    "text": "How do we estimate the model parameters?\n\nWe need to estimate the population model coefficients \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, \\ldots, \\widehat{\\beta}_k\\)\nThis can be done using the ordinary least-squares method\n\nFind the \\(\\widehat{\\beta}\\) values that minimize the sum of squares due to error (\\(SSE\\))\n\n\n\\[ \\begin{aligned}\nSSE & = \\displaystyle\\sum^n_{i=1} \\widehat\\epsilon_i^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{Y}_i)^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - (\\widehat{\\beta}_0 +\\widehat{\\beta}_1 X_{i1}+ \\ldots+\\widehat{\\beta}_1 X_{ik}))^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0 -\\widehat{\\beta}_1 X_{i1}- \\ldots-\\widehat{\\beta}_1 X_{ik})^2\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#technical-side-note-not-needed-in-our-class",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#technical-side-note-not-needed-in-our-class",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Technical side note (not needed in our class)",
    "text": "Technical side note (not needed in our class)\n\nThe equations for calculating the \\(\\boldsymbol{\\widehat{\\beta}}\\) values is best done using matrix notation (not required for our class)\nWe will be using R to get the coefficients instead of the equation (already did this a few slides back!)\nHow we have represented the population regression model: \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\n\n\n\nHow to represent population model with matrix notation:\n\n\\[\\begin{aligned}\n\\boldsymbol{Y} &= \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\\\\n\\boldsymbol{Y}_{n \\times 1}& = \\boldsymbol{X}_{n \\times (k+1)}\\boldsymbol{\\beta}_{(k+1)\\times 1} + \\boldsymbol{\\epsilon}_{n \\times 1}\n\\end{aligned}\\]\n\n\\(\\boldsymbol{X}\\) is often called the design matrix\n\nEach row represents an individual\nEach column represents a covariate\n\n\n\n\\[\n\\boldsymbol{Y} = \\left[\\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\n\\end{array} \\right]_{n \\times 1}\n\\] \\[\n\\boldsymbol{\\epsilon} = \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n\n\\end{array} \\right]_{n \\times 1}  \n\\]\n\n\\[\n\\boldsymbol{X} = \\left[ \\begin{array}{ccccc} 1 &  X_{11} &  X_{12} & \\ldots & X_{1,k} \\\\\n1 &X_{21} &  X_{22} & \\ldots & X_{2,k} \\\\\n\\vdots&\\vdots & \\vdots &  \\ldots & \\vdots \\\\\n1 & X_{n1} &  X_{n2} & \\ldots & X_{n,k} \\end{array} \\right]_{n \\times (k+1)}\n\\]\n\\[\n\\boldsymbol{\\beta}  = \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1\\\\  \\vdots \\\\\n\\beta_{k}\n\\end{array} \\right]_{(k+1)\\times 1}\n\\]"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#line-model-assumptions",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#line-model-assumptions",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "LINE model assumptions",
    "text": "LINE model assumptions\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nThe mean value of \\(Y\\) given any combination of \\(X_1, X_2, \\ldots, X_k\\) values, is a linear function of \\(\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k\\):\n\\[\\mu_{Y|X_1, \\ldots, X_k} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k\\]\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nObservations (\\(X_1, X_2, \\ldots, X_k, Y\\)) are independent from one another\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\n\\(Y\\) has a normal distribution for any any combination of \\(X_1, X_2, \\ldots, X_k\\) values\n\nThus, the residuals are normally distributed\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nThe variance of \\(Y\\) is the same for any any combination of \\(X_1, X_2, \\ldots, X_k\\) values\n\\[\\sigma^2_{Y|X_1, X_2, \\ldots, X_k} = Var(Y|X_1, X_2, \\ldots, X_k) = \\sigma^2\\]"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#summary-of-the-line-assumptions",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#summary-of-the-line-assumptions",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Summary of the LINE assumptions",
    "text": "Summary of the LINE assumptions\n\nEquivalently, the residuals are independently and identically distributed (iid):\n\nnormal\nwith mean 0 and\nconstant variance \\(\\sigma^2\\)\n\n\n \n\nResiduals are still \\(\\widehat{\\epsilon}_i=Y_i - \\widehat{Y}_i\\) for each observation\n\nIt’s just that \\(\\widehat{Y}_i\\) is now calculated with many covariates (\\(X_1, X_2, \\ldots, X_k\\))"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#variation-explained-vs.-unexplained",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#variation-explained-vs.-unexplained",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Variation: Explained vs. Unexplained",
    "text": "Variation: Explained vs. Unexplained\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 &= \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY &= SSR + SSE\n\\end{aligned}\\]\n\n\\(Y_i - \\overline{Y}\\) = the deviation of \\(Y_i\\) around the mean \\(\\overline{Y}\\)\n\nthe total amount deviation\n\n\\(\\widehat{Y}_i- \\overline{Y}\\) = the deviation of the fitted value \\(\\widehat{Y}_i\\) around the mean \\(\\overline{Y}\\)\n\nthe amount deviation explained by the regression at \\(X_{i1},\\ldots,X_{ik}\\)\n\n\\(Y_i - \\widehat{Y}_i\\) = the deviation of the observation \\(Y\\) around the fitted regression line\n\nthe amount deviation unexplained by the regression at \\(X_{i1},\\ldots,X_{ik}\\)"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#poll-everywhere-question-3",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#poll-everywhere-question-3",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#building-the-anova-table",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#building-the-anova-table",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Building the ANOVA table",
    "text": "Building the ANOVA table\nANOVA table (\\(k\\) = # of predictors, \\(n\\) = # of observations)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariation Source\ndf\nSS\nMS\ntest statistic\np-value\n\n\n\n\nRegression\n\\(k\\)\n\\(SSR\\)\n\\(MSR = \\frac{SSR}{k}\\)\n\\(F = \\frac{MSR}{MSE}\\)\n\\(P(F_{(k, n-k-1)}&gt;F)\\)\n\n\nError\n\\(n-k-1\\)\n\\(SSE\\)\n\\(MSE = \\frac{SSE}{n-k-1}\\)\n\n\n\n\nTotal\n\\(n-1\\)\n\\(SSY\\)"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#interpreting-the-estimated-population-coefficients",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#interpreting-the-estimated-population-coefficients",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Interpreting the estimated population coefficients",
    "text": "Interpreting the estimated population coefficients\n\nFor a population model: \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\epsilon\\]\n\nWhere \\(X_1\\) and \\(X_2\\) are continuous variables\nNo need to specify \\(Y\\) because it required to be continuous in linear regression\n\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB)."
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#poll-everywhere-question-4",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#poll-everywhere-question-4",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#getting-these-interpretations-from-our-regression-table",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#getting-these-interpretations-from-our-regression-table",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Getting these interpretations from our regression table",
    "text": "Getting these interpretations from our regression table\nWe fit the regression model in R and printed the regression table:\n\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model: \\(\\widehat{\\text{LE}} = 33.595 + 0.157 \\text{ FLR} + 0.008 \\text{ FS}\\)\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe average life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 kcal PPD (95% CI: 24.674, 41.517).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012)."
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#lets-just-examine-the-general-interpretation-vs.-the-example",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#lets-just-examine-the-general-interpretation-vs.-the-example",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Let’s just examine the general interpretation vs. the example",
    "text": "Let’s just examine the general interpretation vs. the example\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 0 kcal PPD (95% CI: 24.674, 41.517).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012)."
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro_key_info.html#key-dates",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions_key_info.html#key-dates",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates\n\nHomework 2 due on Friday!!"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html",
    "title": "Lesson 5: SLR: More inference + Evaluation",
    "section": "",
    "text": "Identify different sources of variation in an Analysis of Variance (ANOVA) table\nUsing the F-test, determine if there is enough evidence that population slope \\(\\beta_1\\) is not 0\nCalculate and interpret the coefficient of determination\nDescribe the model assumptions made in linear regression using ordinary least squares\n\n\n\n\n\nLesson 3: SLR 1\n\nFit regression line\nCalculate slope & intercept\nInterpret slope & intercept\n\nLesson 4: SLR 2\n\nEstimate variance of the residuals\nInference for slope & intercept: CI, p-value\nConfidence bands of regression line for mean value of Y|X\n\n\n\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]\n\n\n\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#so-far-in-our-regression-example",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#so-far-in-our-regression-example",
    "title": "Lesson 6: SLR: More inference",
    "section": "So far in our regression example…",
    "text": "So far in our regression example…\n\n\nLesson 3: SLR 1\n\nFit regression line\nCalculate slope & intercept\nInterpret slope & intercept\n\nLesson 4: SLR 2\n\nEstimate variance of the residuals\nInference for slope & intercept: CI, p-value\nConfidence bands of regression line for mean value of Y|X\n\nLesson 5: Categorical Covariates\n\nInference for different categories\n\n\n\n\n\n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{LE}} &= 50.9 + 0.232 \\cdot \\text{FLR}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#process-of-regression-data-analysis",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#process-of-regression-data-analysis",
    "title": "Lesson 6: SLR: More inference",
    "section": "Process of regression data analysis",
    "text": "Process of regression data analysis\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#getting-to-the-f-test",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#getting-to-the-f-test",
    "title": "Lesson 6: SLR: More inference",
    "section": "Getting to the F-test",
    "text": "Getting to the F-test\nThe F statistic in linear regression is essentially a proportion of the variance explained by the model vs. the variance not explained by the model\n\nStart with visual of explained vs. unexplained variation\nFigure out the mathematical representations of this variation\nLook at the ANOVA table to establish key values measuring our variance from our model\nBuild the F-test"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#explained-vs.-unexplained-variation",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#explained-vs.-unexplained-variation",
    "title": "Lesson 6: SLR: More inference",
    "section": "Explained vs. Unexplained Variation",
    "text": "Explained vs. Unexplained Variation\n\n\\[ \\begin{aligned}\nY_i - \\overline{Y} & = (Y_i - \\widehat{Y}_i) + (\\widehat{Y}_i- \\overline{Y})\\\\\n\\text{Total variation} & = \\text{Residual variation after regression} + \\text{Variation explained by regression}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#more-on-the-equation",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#more-on-the-equation",
    "title": "Lesson 6: SLR: More inference",
    "section": "More on the equation",
    "text": "More on the equation\n\n\n\\[Y_i - \\overline{Y} = (Y_i - \\widehat{Y}_i) + (\\widehat{Y}_i- \\overline{Y})\\]\n\n\\(Y_i - \\overline{Y}\\) = the deviation of \\(Y_i\\) around the mean \\(\\overline{Y}\\)\n\n(the total amount deviation)\n\n\\(Y_i - \\widehat{Y}_i\\) = the deviation of the observation \\(Y\\) around the fitted regression line\n\n(the amount deviation unexplained by the regression at \\(X_i\\) ).\n\n\\(\\widehat{Y_i}- \\overline{Y}\\) = the deviation of the fitted value \\(\\widehat{Y}_i\\) around the mean \\(\\overline{Y}\\)\n\n(the amount deviation explained by the regression at \\(X_i\\) )"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#another-way-of-thinking-about-the-different-deviations",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#another-way-of-thinking-about-the-different-deviations",
    "title": "Lesson 6: SLR: More inference",
    "section": "Another way of thinking about the different deviations",
    "text": "Another way of thinking about the different deviations"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#poll-everywhere-question-1",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#poll-everywhere-question-1",
    "title": "Lesson 6: SLR: More inference",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#how-is-this-actually-calculated-for-our-fitted-model-12",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#how-is-this-actually-calculated-for-our-fitted-model-12",
    "title": "Lesson 6: SLR: More inference",
    "section": "How is this actually calculated for our fitted model? (1/2)",
    "text": "How is this actually calculated for our fitted model? (1/2)\n\\[ \\begin{aligned}\nY_i - \\overline{Y} & = (Y_i - \\widehat{Y}_i) + (\\widehat{Y}_i- \\overline{Y})\\\\\n\\text{Total variation} & = \\text{Variation explained by regression} + \\text{Residual variation after regression}\n\\end{aligned}\\]\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 & = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY & = SSR + SSE\n\\end{aligned}\\] \\[\\text{Total Sum of Squares} = \\text{Sum of Squares explained by Regression} + \\text{Sum of Squares due to Error (residuals)}\\]\n\n\n\n\nANOVA table:\n\n\n\n\n\n\n\n\n\n\n\nVariation Source\ndf\nSS\nMS\ntest statistic\np-value\n\n\n\n\nRegression\n\\(1\\)\n\\(SSR\\)\n\\(MSR = \\frac{SSR}{1}\\)\n\\(F = \\frac{MSR}{MSE}\\)\n\n\n\nError\n\\(n-2\\)\n\\(SSE\\)\n\\(MSE = \\frac{SSE}{n-2}\\)\n\n\n\n\nTotal\n\\(n-1\\)\n\\(SSY\\)"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#how-is-this-actually-calculated-for-our-fitted-model-22",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#how-is-this-actually-calculated-for-our-fitted-model-22",
    "title": "Lesson 6: SLR: More inference",
    "section": "How is this actually calculated for our fitted model? (2/2)",
    "text": "How is this actually calculated for our fitted model? (2/2)\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 & = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY & = SSR + SSE\n\\end{aligned}\\] \\[\\text{Total Sum of Squares} = \\text{Sum of Squares explained by Regression} + \\text{Sum of Squares due to Error (residuals)}\\]\n\n\n\n\nANOVA table:\n\n\n\n\n\n\n\n\n\n\n\nVariation Source\ndf\nSS\nMS\ntest statistic\np-value\n\n\n\n\nRegression\n\\(1\\)\n\\(SSR\\)\n\\(MSR = \\frac{SSR}{1}\\)\n\\(F = \\frac{MSR}{MSE}\\)\n\n\n\nError\n\\(n-2\\)\n\\(SSE\\)\n\\(MSE = \\frac{SSE}{n-2}\\)\n\n\n\n\nTotal\n\\(n-1\\)\n\\(SSY\\)\n\n\n\n\n\n\n\n\n\n\n \n\nF-statistic: Proportion of variation that is explained by the model to variation not explained by the model"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#analysis-of-variance-anova-table-in-r",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#analysis-of-variance-anova-table-in-r",
    "title": "Lesson 6: SLR: More inference",
    "section": "Analysis of Variance (ANOVA) table in R",
    "text": "Analysis of Variance (ANOVA) table in R\n\n# Fit regression model:\nmodel1 &lt;- gapm %&gt;% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\n\nanova(model1)\n\nAnalysis of Variance Table\n\nResponse: LifeExpectancyYrs\n                   Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nFemaleLiteracyRate  1 2052.8 2052.81  54.414 1.501e-10 ***\nResiduals          78 2942.6   37.73                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n   tab_options(table.font.size = 40) %&gt;%\n   fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1.000\n2,052.812\n2,052.812\n54.414\n0.000\n    Residuals\n78.000\n2,942.635\n37.726\nNA\nNA"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#what-is-the-f-statistic-testing",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#what-is-the-f-statistic-testing",
    "title": "Lesson 6: SLR: More inference",
    "section": "What is the F statistic testing?",
    "text": "What is the F statistic testing?\n\\[F = \\frac{MSR}{MSE}\\]\n\nIt can be shown that\n\n\\[E(MSE)=\\sigma^2\\ \\text{and}\\ E(MSR) = \\sigma^2 + \\beta_1^2\\sum_{i=1}^n (X_i- \\overline{X})^2\\]\n\nRecall that \\(\\sigma^2\\) is the variance of the population residuals\nThus if\n\n\\(\\beta_1 = 0\\), then \\(F \\approx \\frac{\\widehat{\\sigma}^2}{\\widehat{\\sigma}^2} = 1\\)\n\\(\\beta_1 \\neq 0\\), then \\(F \\approx \\frac{\\widehat{\\sigma}^2 + \\widehat{\\beta}_1^2\\sum_{i=1}^n (X_i- \\overline{X})^2}{\\widehat{\\sigma}^2} &gt; 1\\)\n\nSo the \\(F\\) statistic can also be used to test \\(\\beta_1\\)"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#f-test-vs.-t-test-for-the-population-slope",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#f-test-vs.-t-test-for-the-population-slope",
    "title": "Lesson 6: SLR: More inference",
    "section": "F-test vs. t-test for the population slope",
    "text": "F-test vs. t-test for the population slope\nThe square of a \\(t\\)-distribution with \\(df = \\nu\\) is an \\(F\\)-distribution with \\(df = 1, \\nu\\)\n\\[T_{\\nu}^2 \\sim F_{1,\\nu}\\]\n\nWe can use either F-test or t-test to run the following hypothesis test:\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote that the F-test does not support one-sided alternative tests, but the t-test does!\n\nF-test cannot handle alternatives like \\(\\beta_1 &gt; 0\\) nor \\(\\beta_2 &lt; 0\\)"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#planting-a-seed-about-the-f-test",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#planting-a-seed-about-the-f-test",
    "title": "Lesson 6: SLR: More inference",
    "section": "Planting a seed about the F-test",
    "text": "Planting a seed about the F-test\nWe can think about the hypothesis test for the slope…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_1\\neq0\\)\n\n\n\n\nin a slightly different way…\n\n\n\n\n\n\nNull model (\\(\\beta_1=0\\))\n\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\nSmaller (reduced) model\n\n\n\n\n\n\nAlternative model (\\(\\beta_1\\neq0\\))\n\n\n\n\\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)\nLarger (full) model\n\n\n\n\n\n\nIn multiple linear regression, we can start using this framework to test multiple coefficient parameters at once\n\nDecide whether or not to reject the smaller reduced model in favor of the larger full model\nCannot do this with the t-test when we have multiple coefficients!"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#poll-everywhere-question-2",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#poll-everywhere-question-2",
    "title": "Lesson 6: SLR: More inference",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#f-test-general-steps-for-hypothesis-test-for-population-slope-beta_1",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#f-test-general-steps-for-hypothesis-test-for-population-slope-beta_1",
    "title": "Lesson 6: SLR: More inference",
    "section": "F-test: general steps for hypothesis test for population slope \\(\\beta_1\\)",
    "text": "F-test: general steps for hypothesis test for population slope \\(\\beta_1\\)\n\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nOften, we are curious if the coefficient is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=1\\) and denominator \\(df=n-2\\).\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic for \\(\\widehat\\beta_1\\) is\n\\[F = \\frac{MSR}{MSE}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{1, n-2} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject: \\(P(F_{1, n-2} &gt; F) &lt; \\alpha\\)\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at the \\(100\\alpha\\%\\) significiance level. There is (sufficient/insufficient) evidence that there is significant association between (\\(Y\\)) and (\\(X\\)) (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1",
    "title": "Lesson 6: SLR: More inference",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\)\n\nSteps 1-4 are setting up our hypothesis test: not much change from the general steps\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nWe are testing if the slope is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=1\\) and denominator \\(df=n-2 = 80-2\\).\n\n\nnobs(model1)\n\n[1] 80"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-24",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-24",
    "title": "Lesson 6: SLR: More inference",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/4)\n\n\n\nCompute the value of the test statistic\n\n\n\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1\n2052.812\n2052.81234\n54.4136\n1.501286e-10\n    Residuals\n78\n2942.635\n37.72609\nNA\nNA\n  \n  \n  \n\n\n\n\n\nOption 1: Calculate the test statistic using the values in the ANOVA table\n\n\\[F = \\frac{MSR}{MSE} = \\frac{2052.81}{37.73}=54.414\\]\n\nOption 2: Get the test statistic value (F) from the ANOVA table\n\n\nI tend to skip this step because I can do it all with step 6"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-34",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-34",
    "title": "Lesson 6: SLR: More inference",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (3/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (3/4)\n\n\n\nCalculate the p-value\n\n\n\n\nAs per Step 4, test statistic \\(F\\) can be modeled by a \\(F\\)-distribution with \\(df1 = 1\\) and \\(df2 = n-2\\).\n\nWe had 80 countries’ data, so \\(n=80\\)\n\nOption 1: Use pf() and our calculated test statistic\n\n\n# p-value is ALWAYS the right tail for F-test\npf(54.414, df1 = 1, df2 = 78, lower.tail = FALSE)\n\n[1] 1.501104e-10\n\n\n\nOption 2: Use the ANOVA table\n\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1\n2052.812\n2052.81234\n54.4136\n1.501286e-10\n    Residuals\n78\n2942.635\n37.72609\nNA\nNA"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-44",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-44",
    "title": "Lesson 6: SLR: More inference",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (4/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (4/4)\n\n\n\nWrite conclusion for the hypothesis test\n\n\n\nWe reject the null hypothesis that the slope is 0 at the \\(5\\%\\) significance level. There is sufficient evidence that there is significant association between female life expectancy and female literacy rates (p-value &lt; 0.0001)."
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#did-you-notice-anything-about-the-p-value",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#did-you-notice-anything-about-the-p-value",
    "title": "Lesson 6: SLR: More inference",
    "section": "Did you notice anything about the p-value?",
    "text": "Did you notice anything about the p-value?\nThe p-value of the t-test and F-test are the same!!\n\nFor the t-test:\n\n\ntidy(model1) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    FemaleLiteracyRate\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\nFor the F-test:\n\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1\n2052.812\n2052.81234\n54.4136\n1.501286e-10\n    Residuals\n78\n2942.635\n37.72609\nNA\nNA\n  \n  \n  \n\n\n\n\nThis is true when we use the F-test for a single coefficient!"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#correlation-coefficient-from-511",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#correlation-coefficient-from-511",
    "title": "Lesson 6: SLR: More inference",
    "section": "Correlation coefficient from 511",
    "text": "Correlation coefficient from 511\n\n\nCorrelation coefficient \\(r\\) can tell us about the strength of a relationship between two continuous variables\n\nIf \\(r = -1\\), then there is a perfect negative linear relationship between \\(X\\) and \\(Y\\)\nIf \\(r = 1\\), then there is a perfect positive linear relationship between \\(X\\) and \\(Y\\)\nIf \\(r = 0\\), then there is no linear relationship between \\(X\\) and \\(Y\\)\n\nNote: All other values of \\(r\\) tell us that the relationship between \\(X\\) and \\(Y\\) is not perfect. The closer \\(r\\) is to 0, the weaker the linear relationship."
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#correlation-coefficient",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#correlation-coefficient",
    "title": "Lesson 6: SLR: More inference",
    "section": "Correlation coefficient",
    "text": "Correlation coefficient\n\n\nThe (Pearson) correlation coefficient \\(r\\) of variables \\(X\\) and \\(Y\\) can be computed using the formula:\n\\[\\begin{aligned}\nr  & = \\frac{\\sum_{i=1}^n (X_i - \\overline{X})(Y_i - \\overline{Y})}{\\Big(\\sum_{i=1}^n (X_i - \\overline{X})^2 \\sum_{i=1}^n (Y_i - \\overline{Y})^2\\Big)^{1/2}} \\\\\n& = \\frac{SSXY}{\\sqrt{SSX \\cdot SSY}}\n\\end{aligned}\\]\nwe have the relationship\n\\[\\widehat{\\beta}_1 = r\\frac{SSY}{SSX},\\ \\ \\text{or},\\ \\  r = \\widehat{\\beta}_1\\frac{SSX}{SSY}\\]"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#coefficient-of-determination-r2",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#coefficient-of-determination-r2",
    "title": "Lesson 6: SLR: More inference",
    "section": "Coefficient of determination: \\(R^2\\)",
    "text": "Coefficient of determination: \\(R^2\\)\nIt can be shown that the square of the correlation coefficient \\(r\\) is equal to\n\\[R^2 = \\frac{SSR}{SSY} = \\frac{SSY - SSE}{SSY}\\]\n\n\\(R^2\\) is called the coefficient of determination.\nInterpretation: The proportion of variation in the \\(Y\\) values explained by the regression model\n\\(R^2\\) measures the strength of the linear relationship between \\(X\\) and \\(Y\\):\n\n\\(R^2 = \\pm 1\\): Perfect relationship\n\nHappens when \\(SSE = 0\\), i.e. no error, all points on the line\n\n\\(R^2 = 0\\): No relationship\n\nHappens when \\(SSY = SSE\\), i.e. using the line doesn’t not improve model fit over using \\(\\overline{Y}\\) to model the \\(Y\\) values."
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-correlation-coeffiicent-r-and-coefficient-of-determination-r2",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-correlation-coeffiicent-r-and-coefficient-of-determination-r2",
    "title": "Lesson 6: SLR: More inference",
    "section": "Life expectancy example: correlation coeffiicent \\(r\\) and coefficient of determination \\(R^2\\)",
    "text": "Life expectancy example: correlation coeffiicent \\(r\\) and coefficient of determination \\(R^2\\)\n\n(r = cor(x = gapm$LifeExpectancyYrs, y = gapm$FemaleLiteracyRate, \n         use =  \"complete.obs\"))\n\n[1] 0.6410434\n\n\n\n\n\nsummary(model1) # for R^2 value\n\n\nCall:\nlm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        50.92790    2.66041  19.143  &lt; 2e-16 ***\nFemaleLiteracyRate  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\n\n\n\nr^2\n\n[1] 0.4109366\n\n\n   \n\n\nInterpretation\n\n\n41.1% of the variation in countries’ life expectancy is explained by the linear model with female literacy rate as the independent variable."
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#what-does-r2-not-measure",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#what-does-r2-not-measure",
    "title": "Lesson 6: SLR: More inference",
    "section": "What does \\(R^2\\) not measure?",
    "text": "What does \\(R^2\\) not measure?\n\n\n\n\\(R^2\\) is not a measure of the magnitude of the slope of the regression line\n\nExample: can have \\(R^2 = 1\\) for many different slopes!!\n\n\\(R^2\\) is not a measure of the appropriateness of the straight-line model\n\nExample: figure\n\n\n\n\n\n\n\n\n\n\n\n\nLesson 6: SLR 3"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#least-squares-model-assumptions-line",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#least-squares-model-assumptions-line",
    "title": "Lesson 6: SLR: More inference + Evaluation",
    "section": "Least-squares model assumptions: LINE",
    "text": "Least-squares model assumptions: LINE\n \nThese are the model assumptions made in ordinary least squares:\n \n\n[L] Linearity of relationship between variables\n\n\n[I] Independence of the \\(Y\\) values\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\n[E] Equality of variance of the residuals (homoscedasticity)"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#l-linearity",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#l-linearity",
    "title": "Lesson 6: SLR: More inference + Evaluation",
    "section": "L: Linearity",
    "text": "L: Linearity\n\n\n\nThe relationship between the variables is linear (a straight line):\n\nThe mean value of \\(Y\\) given \\(X\\), \\(\\mu_{y|x}\\) or \\(E[Y|X]\\), is a straight-line function of \\(X\\)\n\n\n\\[\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\]"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#i-independence-of-observations",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#i-independence-of-observations",
    "title": "Lesson 6: SLR: More inference + Evaluation",
    "section": "I: Independence of observations",
    "text": "I: Independence of observations\n\nThe \\(Y\\)-values are statistically independent of one another\nExamples of when they are not independent, include\n\nrepeated measures (such as baseline, 3 months, 6 months)\ndata from clusters, such as different hospitals or families\n\nThis condition is checked by reviewing the study design and not by inspecting the data\n\n \n\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#poll-everywhere-question-3",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#poll-everywhere-question-3",
    "title": "Lesson 6: SLR: More inference",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#n-normality",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#n-normality",
    "title": "Lesson 6: SLR: More inference + Evaluation",
    "section": "N: Normality",
    "text": "N: Normality\n\n\n\nFor any fixed value of \\(X\\), \\(Y\\) has normal distribution.\n\nNote: This is not about \\(Y\\) alone, but \\(Y|X\\)\n\nEquivalently, the measurement (random) errors \\(\\epsilon_i\\) ’s normally distributed\n\nThis is more often what we check"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#e-equality-of-variance-of-the-residuals",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#e-equality-of-variance-of-the-residuals",
    "title": "Lesson 6: SLR: More inference + Evaluation",
    "section": "E: Equality of variance of the residuals",
    "text": "E: Equality of variance of the residuals\n\nThe variance of \\(Y\\) given \\(X\\) (\\(\\sigma_{Y|X}^2\\)), is the same for any \\(X\\)\n\nWe use just \\(\\sigma^2\\) to denote the common variance\n\nThis is also called homoscedasticity"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#summary-of-line-model-assumptions",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#summary-of-line-model-assumptions",
    "title": "Lesson 6: SLR: More inference + Evaluation",
    "section": "Summary of LINE model assumptions",
    "text": "Summary of LINE model assumptions\n\n\\(Y\\) values are independent (check study design!)\n\n\n\n\n\n\nThe distribution of \\(Y\\) given \\(X\\) is\n\nnormal\nwith mean \\(\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\)\nand common variance \\(\\sigma^2\\)\n\n\nThis means that the residuals are\n\nnormal\nwith mean = 0\nand common variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#how-do-we-determine-if-our-model-follows-the-line-assumptions",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#how-do-we-determine-if-our-model-follows-the-line-assumptions",
    "title": "Lesson 6: SLR: More inference + Evaluation",
    "section": "How do we determine if our model follows the LINE assumptions?",
    "text": "How do we determine if our model follows the LINE assumptions?\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nCheck if there is a linear relationship between the mean response (Y) and the explanatory variable (X)\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nCheck that the observations are independent\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\nCheck that the responses (at each level X) are normally distributed\n\nUsually measured through the residuals\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nCheck that the variance (or standard deviation) of the responses is equal for all levels of X\n\nUsually measured through the residuals\n\n\n\n\n\n\n\nLesson 6: SLR 3"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "",
    "text": "Use visualizations and cut off points to flag potentially influential points using residuals, leverage, and Cook’s distance\nHandle influential points and assumption violations by checking data errors, reassessing the model, and making data transformations.\nImplement a model with data transformations and determine if it improves the model fit.\n\n\n\n\nWe have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\nThe residuals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun model1 through augment() (model1 is input)\n\nSo we assigned model1 as the output of the lm() function (model1 is output)\n\nWill give us values about each observation in the context of the fitted regression model\n\ncook’s distance (.cooksd), fitted value (.fitted, \\(\\widehat{Y}_i\\)), leverage (.hat), residual (.resid), standardized residuals (.std.resid)\n\n\n\naug1 &lt;- augment(model1) \nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…\n\n\nRDocumentation on the augment() function.\n\n\n\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nCheck if there is a linear relationship between the mean response (Y) and the explanatory variable (X)\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nCheck that the observations are independent\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\nCheck that the responses (at each level X) are normally distributed\n\nUsually measured through the residuals\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nCheck that the variance (or standard deviation) of the responses is equal for all levels of X\n\nUsually measured through the residuals"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Let’s remind ourselves of the model that we have been working with",
    "text": "Let’s remind ourselves of the model that we have been working with\n\nWe have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    FemaleLiteracyRate\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Our residuals will help us a lot in our diagnostics!",
    "text": "Our residuals will help us a lot in our diagnostics!\n\n\n \n\nThe residuals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#augment-getting-extra-information-on-the-fitted-model",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#augment-getting-extra-information-on-the-fitted-model",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "augment(): getting extra information on the fitted model",
    "text": "augment(): getting extra information on the fitted model\n\nRun model1 through augment() (model1 is input)\n\nSo we assigned model1 as the output of the lm() function (model1 is output)\n\nWill give us values about each observation in the context of the fitted regression model\n\ncook’s distance (.cooksd), fitted value (.fitted, \\(\\widehat{Y}_i\\)), leverage (.hat), residual (.resid), standardized residuals (.std.resid)\n\n\n\naug1 &lt;- augment(model1) \nglimpse(aug1)\n\nRows: 80\nColumns: 8\n$ LifeExpectancyYrs  &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 76.9, 58.…\n$ FemaleLiteracyRate &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 96.7, 85.…\n$ .fitted            &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.65980, 7…\n$ .resid             &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074, 2.3402…\n$ .hat               &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077123, 0.0…\n$ .sigma             &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.176043, 6…\n$ .cooksd            &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2.400993e…\n$ .std.resid         &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.47579667, 0.…\n\n\nRDocumentation on the augment() function."
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#revisiting-our-line-assumptions",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#revisiting-our-line-assumptions",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Revisiting our LINE assumptions",
    "text": "Revisiting our LINE assumptions\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nCheck if there is a linear relationship between the mean response (Y) and the explanatory variable (X)\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nCheck that the observations are independent\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\nCheck that the responses (at each level X) are normally distributed\n\nUsually measured through the residuals\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nCheck that the variance (or standard deviation) of the responses is equal for all levels of X\n\nUsually measured through the residuals"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#influential-points",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#influential-points",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Influential points",
    "text": "Influential points\n\n\n\n\nOutliers\n\n\n\nAn observation (\\(X_i, Y_i\\)) whose response \\(Y_i\\) does not follow the general trend of the rest of the data\n\n\n\n\n\n\n\n\n \n \n\n\n\nHigh leverage observations\n\n\n\nAn observation (\\(X_i, Y_i\\)) whose predictor \\(X_i\\) has an extreme value\n\\(X_i\\) can be an extremely high or low value compared to the rest of the observations"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#outliers",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#outliers",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Outliers",
    "text": "Outliers\n\n\n\nAn observation (\\(X_i, Y_i\\)) whose response \\(Y_i\\) does not follow the general trend of the rest of the data\n\n \n\nHow do we determine if a point is an outlier?\n\nScatterplot of \\(Y\\) vs. \\(X\\)\nFollowed by evaluation of its residual (and standardized residual)\n\nTypically use the internally standardized residual (aka studentized residual)"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#poll-everywhere-question-1",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#poll-everywhere-question-1",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#identifying-outliers",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#identifying-outliers",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Identifying outliers",
    "text": "Identifying outliers\n\n\n\n\nInternally standardized residual\n\n\n\\[\nr_i = \\frac{\\widehat\\epsilon_i}{\\sqrt{\\widehat\\sigma^2(1-h_{ii})}}\n\\]\n\n\n\n\nWe flag an observation if the standardized residual is “large”\n\nDifferent sources will define “large” differently\nPennState site uses \\(|r_i| &gt; 3\\)\nautoplot() shows the 3 observations with the highest standardized residuals\nOther sources use \\(|r_i| &gt; 2\\), which is a little more conservative\n\n\n\n\n\n\n \n\nggplot(data = aug1) + \n  geom_histogram(aes(x = .std.resid))"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#countries-that-are-outliers-r_i-2",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#countries-that-are-outliers-r_i-2",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Countries that are outliers (\\(|r_i| > 2\\))",
    "text": "Countries that are outliers (\\(|r_i| &gt; 2\\))\n\nWe can identify the countries that are outliers\n\n\naug1 %&gt;% \n  filter(abs(.std.resid) &gt; 3)\n\n# A tibble: 1 × 24\n  country  LifeExpectancyYrs FemaleLiteracyRate .std.resid .fitted .resid   .hat\n  &lt;chr&gt;                &lt;dbl&gt;              &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Swazila…              48.9               87.3      -3.65    71.2  -22.3 0.0133\n# ℹ 17 more variables: .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, CO2emissions &lt;dbl&gt;,\n#   ElectricityUsePP &lt;dbl&gt;, FoodSupplykcPPD &lt;dbl&gt;, IncomePP &lt;dbl&gt;,\n#   population &lt;dbl&gt;, WaterSourcePrct &lt;dbl&gt;, geo &lt;chr&gt;, four_regions &lt;chr&gt;,\n#   eight_regions &lt;chr&gt;, six_regions &lt;chr&gt;, members_oecd_g77 &lt;chr&gt;,\n#   Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt;, `World bank region` &lt;chr&gt;,\n#   `World bank, 4 income groups 2017` &lt;chr&gt;"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#high-leverage-observations",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#high-leverage-observations",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "High leverage observations",
    "text": "High leverage observations\n\n\n\nAn observation (\\(X_i, Y_i\\)) whose response \\(X_i\\) is considered “extreme” compared to the other values of \\(X\\)\n\n \n\nHow do we determine if a point has high leverage?\n\nScatterplot of \\(Y\\) vs. \\(X\\)\nCalculating the leverage of each observation"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#leverage-h_i",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#leverage-h_i",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Leverage \\(h_i\\)",
    "text": "Leverage \\(h_i\\)\n\n\nLeverage\n\n\nMeasure of the distance between the x value (\\(X_i\\)) for the data point (\\(i\\)) and the mean of the x values (\\(\\overline{X}\\)) for all \\(n\\) data points\n\n\n\nValues of leverage are: \\(0 \\leq h_i \\leq 1\\)\nWe flag an observation if the leverage is “high”\n\nDifferent sources will define “high” differently\nSome textbooks use \\(h_i &gt; 4/n\\) where \\(n\\) = sample size\nSome people suggest \\(h_i &gt; 6/n\\)\nPennState site uses \\(h_i &gt; 3p/n\\) where \\(p\\) = number of regression coefficients"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#countries-with-high-leverage-h_i-4n",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#countries-with-high-leverage-h_i-4n",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Countries with high leverage (\\(h_i > 4/n\\))",
    "text": "Countries with high leverage (\\(h_i &gt; 4/n\\))\n\nWe can look at the countries that have high leverage\n\n\naug1 = aug1 %&gt;% relocate(.hat, .after = FemaleLiteracyRate)\n\naug1 %&gt;% filter(.hat &gt; 4/80) %&gt;% arrange(desc(.hat))\n\n# A tibble: 6 × 24\n  country  LifeExpectancyYrs FemaleLiteracyRate   .hat .std.resid .fitted .resid\n  &lt;chr&gt;                &lt;dbl&gt;              &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghani…              56.7               13   0.136      0.482     53.9  2.75 \n2 Mali                  60                 24.6 0.0980     0.576     56.6  3.36 \n3 Chad                  57                 25.4 0.0956     0.0298    56.8  0.174\n4 Sierra …              55.7               32.6 0.0757    -0.474     58.5 -2.80 \n5 Gambia                66                 41.9 0.0540     0.894     60.7  5.34 \n6 Guinea-…              56.2               42.1 0.0536    -0.754     60.7 -4.50 \n# ℹ 17 more variables: .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, CO2emissions &lt;dbl&gt;,\n#   ElectricityUsePP &lt;dbl&gt;, FoodSupplykcPPD &lt;dbl&gt;, IncomePP &lt;dbl&gt;,\n#   population &lt;dbl&gt;, WaterSourcePrct &lt;dbl&gt;, geo &lt;chr&gt;, four_regions &lt;chr&gt;,\n#   eight_regions &lt;chr&gt;, six_regions &lt;chr&gt;, members_oecd_g77 &lt;chr&gt;,\n#   Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt;, `World bank region` &lt;chr&gt;,\n#   `World bank, 4 income groups 2017` &lt;chr&gt;"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#poll-everywhere-question-2",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#poll-everywhere-question-2",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#countries-with-high-leverage-h_i-4n-1",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#countries-with-high-leverage-h_i-4n-1",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Countries with high leverage (\\(h_i > 4/n\\))",
    "text": "Countries with high leverage (\\(h_i &gt; 4/n\\))\nLabel only countries with large leverage:\n\nggplot(aug1, aes(x = FemaleLiteracyRate, y = LifeExpectancyYrs,\n                 label = country)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  geom_text(aes(label = ifelse(.hat &gt; 0.05, as.character(country), ''))) +\n  geom_vline(xintercept = mean(aug1$FemaleLiteracyRate), color = \"grey\") +\n  geom_hline(yintercept = mean(aug1$LifeExpectancyYrs), color = \"grey\")"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#what-does-the-model-look-like-without-the-high-leverage-points",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#what-does-the-model-look-like-without-the-high-leverage-points",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "What does the model look like without the high leverage points?",
    "text": "What does the model look like without the high leverage points?\nSensitivity analysis removing countries with high leverage\n\naug1_lowlev &lt;- aug1 %&gt;% filter(.hat &lt;= 4/80)\n\nmodel1_lowlev &lt;- aug1_lowlev %&gt;% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\ntidy(model1_lowlev) %&gt;% gt() %&gt;% # Without high-leverage points\n tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n49.563\n3.888\n12.746\n0.000\n    FemaleLiteracyRate\n0.247\n0.044\n5.562\n0.000\n  \n  \n  \n\n\n\ntidy(model1) %&gt;% gt() %&gt;% # With high leverage points\n tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n    FemaleLiteracyRate\n0.232\n0.031\n7.377\n0.000"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#cooks-distance",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#cooks-distance",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Cook’s distance",
    "text": "Cook’s distance\n\nMeasures the overall influence of an observation\n\n \n\nAttempts to measure how much influence a single observation has over the fitted model\n\nMeasures how all fitted values change when the \\(ith\\) observation is removed from the model\nCombines leverage and outlier information"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#identifying-points-with-high-cooks-distance",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#identifying-points-with-high-cooks-distance",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Identifying points with high Cook’s distance",
    "text": "Identifying points with high Cook’s distance\n\n\nThe Cook’s distance for the \\(i^{th}\\) observation is\n\\[d_i = \\frac{h_i}{2(1-h_i)} \\cdot r_i^2\\] where \\(h_i\\) is the leverage and \\(r_i\\) is the studentized residual\n\n\nAnother rule for Cook’s distance that is not strict:\n\nInvestigate observations that have \\(d_i &gt; 1\\)\n\nCook’s distance values are already in the augment tibble: .cooksd\n\n\n\n\naug1 = aug1 %&gt;% relocate(.cooksd, .after = FemaleLiteracyRate)\naug1 %&gt;% arrange(desc(.cooksd))\n\n# A tibble: 80 × 24\n   country        LifeExpectancyYrs FemaleLiteracyRate .cooksd   .hat .std.resid\n   &lt;chr&gt;                      &lt;dbl&gt;              &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n 1 Central Afric…              48                 44.2  0.126  0.0493     -2.20 \n 2 Swaziland                   48.9               87.3  0.0903 0.0133     -3.65 \n 3 South Africa                55.8               92.2  0.0577 0.0154     -2.71 \n 4 Zimbabwe                    51.9               80.1  0.0531 0.0126     -2.89 \n 5 Morocco                     73.8               57.6  0.0350 0.0277      1.57 \n 6 Nepal                       68.7               46.7  0.0311 0.0446      1.15 \n 7 Bangladesh                  71                 53.4  0.0280 0.0335      1.27 \n 8 Botswana                    58.9               85.6  0.0249 0.0129     -1.95 \n 9 Equatorial Gu…              61.4               91.1  0.0231 0.0148     -1.75 \n10 Gambia                      66                 41.9  0.0228 0.0540      0.894\n# ℹ 70 more rows\n# ℹ 18 more variables: .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;, .sigma &lt;dbl&gt;,\n#   CO2emissions &lt;dbl&gt;, ElectricityUsePP &lt;dbl&gt;, FoodSupplykcPPD &lt;dbl&gt;,\n#   IncomePP &lt;dbl&gt;, population &lt;dbl&gt;, WaterSourcePrct &lt;dbl&gt;, geo &lt;chr&gt;,\n#   four_regions &lt;chr&gt;, eight_regions &lt;chr&gt;, six_regions &lt;chr&gt;,\n#   members_oecd_g77 &lt;chr&gt;, Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt;,\n#   `World bank region` &lt;chr&gt;, `World bank, 4 income groups 2017` &lt;chr&gt;"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#plotting-cooks-distance",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#plotting-cooks-distance",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Plotting Cook’s Distance",
    "text": "Plotting Cook’s Distance\n\nplot(model) shows figures similar to autoplot()\n\n4th plot is Cook’s distance (not available in autoplot())\n\n\n\nplot(model1, which = 4)"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#model-without-those-4-points-qq-plot-residual-plot",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#model-without-those-4-points-qq-plot-residual-plot",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Model without those 4 points: QQ Plot, Residual plot",
    "text": "Model without those 4 points: QQ Plot, Residual plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI am okay with this!\n\nAnd don’t forget: we may want more variables in our model!\nYou do not need to produce plots with the influential points taken out"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#model-without-those-4-points-qq-plot-residual-plot-1",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#model-without-those-4-points-qq-plot-residual-plot-1",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Model without those 4 points: QQ Plot, Residual plot",
    "text": "Model without those 4 points: QQ Plot, Residual plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI am okay with this!\n\nAnd don’t forget: we may want more variables in our model!\nYou do not need to produce plots with the influential points taken out"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#summary-of-how-we-identify-influential-points",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#summary-of-how-we-identify-influential-points",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Summary of how we identify influential points",
    "text": "Summary of how we identify influential points\n\nUse scatterplot of \\(Y\\) vs. \\(X\\) to see if any points fall outside of range we expect\nUse standardized residuals, leverage, and Cook’s distance to further identify those points\nLook at the models run with and without the identified points to check for drastic changes\n\nLook at QQ plot and residuals to see if assumptions hold without those points\nLook at coefficient estimates to see if they change in sign and large magnitude\n\n\n \n\nNext: how to handle? It’s a little wishy washy"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#how-do-we-deal-with-influential-points",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#how-do-we-deal-with-influential-points",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "How do we deal with influential points?",
    "text": "How do we deal with influential points?\n\nIf an observation is influential, we perform a sensitivity analysis:\n\nWe took out the influential points we identified then reran the model\nOften, you’ll see that the “influential points” have not drastically changed your estimates\n\nA change in sign (for example: positive slope to negative slope)\nA really large increase (think more than 2x the original value)\n\n\nIf an observation is influential, we check data errors:\n\nWas there a data entry or collection problem?\nIf you have reason to believe that the observation does not hold within the population (or gives you cause to redefine your population)\n\nIf an observation is influential, we check our model:\n\nDid you leave out any important predictors?\nShould you consider adding some interaction terms?\nIs there any nonlinearity that needs to be modeled?"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#when-we-have-detected-problems-in-our-model",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#when-we-have-detected-problems-in-our-model",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "When we have detected problems in our model…",
    "text": "When we have detected problems in our model…\n\nWe have talked about influential points\nWe have talked about identifying issues with our LINE assumptions\n\nWhat are our options once we have identified issues in our linear regression model?\n\nSee if we need to add predictors to our model\n\nNicky’s thought for our life expectancy example\n\nTry a transformation if there is an issue with linearity or normality\nTry a transformation if there is unequal variance\nTry a weighted least squares approach if unequal variance (might be lesson at end of course)\nTry a robust estimation procedure if we have a lot of outlier issues (outside scope of class)"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#transformations",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#transformations",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Transformations",
    "text": "Transformations\n\nWhen we have issues with our LINE (mostly linearity, normality, or equality of variance) assumptions\n\nWe can use transformations to improve the fit of the model\n\nTransformations can…\n\nMake the relationship more linear\nMake the residuals more normal\n“Stabilize” the variance so that it is more constant\nIt can also bring in or reduce outliers\n\nWe can transform the dependent (\\(Y\\)) variable and/or the independent (\\(X\\)) variable\n\nUsually we want to try transforming the \\(X\\) first\n\n\n \n\nRequires trial and error!!\nMajor drawback: interpreting the model becomes harder!"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#common-transformations",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#common-transformations",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Common transformations",
    "text": "Common transformations\n\nTukey’s transformation (power) ladder\n\nUse R’s gladder() command from the describedata package\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower p\n-3\n-2\n-1\n-1/2\n0\n1/2\n1\n2\n3\n\n\n\n\n\n\\(\\frac{1}{x^3}\\)\n\\(\\frac{1}{x^2}\\)\n\\(\\frac{1}{x}\\)\n\\(\\frac{1}{\\sqrt{x}}\\)\n\\(\\log(x)\\)\n\\(\\sqrt{x}\\)\n\\(x\\)\n\\(x^2\\)\n\\(x^3\\)\n\n\n\n\n\n\nHow to use the power ladder for the general distribution shape\n\nIf data are skewed left, we need to compress smaller values towards the rest of the data\n\nGo “up” ladder to transformations with power &gt; 1\n\nIf data are skewed right, we need to compress larger values towards the rest of the data\n\nGo “down” ladder to transformations with power &lt; 1\n\n\n\n\n\nHow to use the power ladder for heteroscedasticity\n\nIf higher \\(X\\) values have more spread\n\nCompress larger values towards the rest of the data\nGo “down” ladder to transformations with power &lt; 1\n\nIf lower \\(X\\) values have more spread\n\nCompress smaller values towards the rest of the data\nGo “up” ladder to transformations with power &gt; 1"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#poll-everywhere-question-3",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#poll-everywhere-question-3",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#transform-dependent-variable",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#transform-dependent-variable",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Transform dependent variable?",
    "text": "Transform dependent variable?\n\n\n\nggplot(gapm, \n       aes(x = LifeExpectancyYrs)) +\n  geom_histogram()\n\n\n\n\n\n\nLooks like more spread on the left side as well\nUse powers greater than 1\n\n\\(LE^2\\) and \\(LE^3\\)"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#gladder-of-life-expectancy",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#gladder-of-life-expectancy",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "gladder() of life expectancy",
    "text": "gladder() of life expectancy\n\ngladder(gapm$LifeExpectancyYrs)"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#ladder-of-life-expectancy",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#ladder-of-life-expectancy",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "ladder() of life expectancy",
    "text": "ladder() of life expectancy\n\n\n\nladder() output tests various transformations of the data for normality\nShapiro-Wilkes test is used to assess for normality\n\n\\(H_0\\): data are from a normal population\n\\(H_A\\): data are NOT from a normal population\n\n\n\n\nladder(gapm$life_expectancy_years_2011) %&gt;% \n  gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      Transformation\n      statistic\n      p.value\n    \n  \n  \n    cubic\n0.963\n0.000\n    square\n0.956\n0.000\n    identity\n0.944\n0.000\n    sqrt\n0.935\n0.000\n    log\n0.924\n0.000\n    1/sqrt\n0.911\n0.000\n    inverse\n0.896\n0.000\n    1/square\n0.860\n0.000\n    1/cubic\n0.815\n0.000"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#transform-independent-variable",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#transform-independent-variable",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Transform independent variable?",
    "text": "Transform independent variable?\n\n\n\nggplot(gapm, \n       aes(x = FemaleLiteracyRate)) +\n  geom_histogram()\n\n\n\n\n\n\nLooks like more spread on the left side\nUse powers greater than 1\n\n\\(FLR^2\\) and \\(FLR^3\\)"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#gladder-of-female-literacy-rate",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#gladder-of-female-literacy-rate",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "gladder() of female literacy rate",
    "text": "gladder() of female literacy rate\n\ngladder(gapm$FemaleLiteracyRate)"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#ladder-of-female-literacy-rate",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#ladder-of-female-literacy-rate",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "ladder() of female literacy rate",
    "text": "ladder() of female literacy rate\n\n\n\nladder() output tests various transformations of the data for normality\nShapiro-Wilkes test is used to assess for normality\n\n\\(H_0\\): data are from a normal population\n\\(H_A\\): data are NOT from a normal population\n\n\n\n\nladder(gapm$female_literacy_rate_2011) %&gt;% \n  gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      Transformation\n      statistic\n      p.value\n    \n  \n  \n    cubic\n0.850\n0.000\n    square\n0.830\n0.000\n    identity\n0.792\n0.000\n    sqrt\n0.755\n0.000\n    log\n0.693\n0.000\n    1/sqrt\n0.599\n0.000\n    inverse\n0.479\n0.000\n    1/square\n0.264\n0.000\n    1/cubic\n0.159\n0.000"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#tips",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#tips",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Tips",
    "text": "Tips\n\nRecall, assessing our LINE assumptions are not on \\(Y\\) alone!! (it’s \\(Y|X\\))\n\nWe can use gladder() to get a sense of what our transformations will do to the data, but we need to check with our residuals again!!\n\nTransformations usually work better if all values are positive (or negative)\nIf observation has a 0, then we cannot perform certain transformations\nLog function only defined for positive values\n\nWe might take the \\(log(X+1)\\) if \\(X\\) includes a 0 value\n\nWhen we make cubic or square transformations, we MUST include the original \\(X\\) in the model\n\nWe do not do this for \\(Y\\) though"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#add-quadratic-and-cubic-transformations-to-dataset",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#add-quadratic-and-cubic-transformations-to-dataset",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Add quadratic and cubic transformations to dataset",
    "text": "Add quadratic and cubic transformations to dataset\n\nHelpful to make a new variable with the transformation in your dataset\n\n\ngapm &lt;- gapm %&gt;% \n  mutate(LE_2 = LifeExpectancyYrs^2,\n         LE_3 = LifeExpectancyYrs^3,\n         FLR_2 = FemaleLiteracyRate^2,\n         FLR_3 = FemaleLiteracyRate^3)\n\ncolnames(gapm)\n\n [1] \"country\"                          \"CO2emissions\"                    \n [3] \"ElectricityUsePP\"                 \"FoodSupplykcPPD\"                 \n [5] \"IncomePP\"                         \"LifeExpectancyYrs\"               \n [7] \"FemaleLiteracyRate\"               \"population\"                      \n [9] \"WaterSourcePrct\"                  \"geo\"                             \n[11] \"four_regions\"                     \"eight_regions\"                   \n[13] \"six_regions\"                      \"members_oecd_g77\"                \n[15] \"Latitude\"                         \"Longitude\"                       \n[17] \"World bank region\"                \"World bank, 4 income groups 2017\"\n[19] \"LE_2\"                             \"LE_3\"                            \n[21] \"FLR_2\"                            \"FLR_3\""
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#we-are-going-to-compare-a-few-different-models-with-transformations",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#we-are-going-to-compare-a-few-different-models-with-transformations",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "We are going to compare a few different models with transformations",
    "text": "We are going to compare a few different models with transformations\nWe are going to call life expectancy \\(LE\\) and female literacy rate \\(FLR\\)\n\nModel 1: \\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\nModel 2: \\(LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\nModel 3: \\(LE^3 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\nModel 4: \\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\epsilon\\)\nModel 5: \\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon\\)\nModel 6: \\(LE^3 = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon\\)"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#poll-everywhere-question-4",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#poll-everywhere-question-4",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#compare-scatterplots-does-linearity-improve",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#compare-scatterplots-does-linearity-improve",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Compare Scatterplots: does linearity improve?",
    "text": "Compare Scatterplots: does linearity improve?"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#run-models-with-transformations-examples",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#run-models-with-transformations-examples",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Run models with transformations: examples",
    "text": "Run models with transformations: examples\nModel 2: \\(LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\nmodel2 &lt;- lm(LE_2 ~ FemaleLiteracyRate,\n             data = gapm)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n2,401.272\n352.070\n6.820\n0.000\n    FemaleLiteracyRate\n31.174\n4.166\n7.484\n0.000\n  \n  \n  \n\n\n\n\nModel 6: \\(LE^3 = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon\\)\n\nmodel6 &lt;- lm(LE_3 ~ \n               FemaleLiteracyRate + FLR_2 + FLR_3,\n             data = gapm)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n67,691.796\n149,056.945\n0.454\n0.651\n    FemaleLiteracyRate\n8,092.133\n8,473.154\n0.955\n0.343\n    FLR_2\n−128.596\n147.876\n−0.870\n0.387\n    FLR_3\n0.840\n0.794\n1.059\n0.293"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#normal-q-q-plots-comparison",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#normal-q-q-plots-comparison",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Normal Q-Q plots comparison",
    "text": "Normal Q-Q plots comparison"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#residual-plots-comparison",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#residual-plots-comparison",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Residual plots comparison",
    "text": "Residual plots comparison"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#summary-of-transformations",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#summary-of-transformations",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Summary of transformations",
    "text": "Summary of transformations\n\nIf the model without the transformation is blatantly violating a LINE assumption\n\nThen a transformation is a good idea\nIf transformations do not help, then keep it untransformed\n\n\n \n\nIf the model without a transformation is not following the LINE assumptions very well, but is mostly okay\n\nThen try to avoid a transformation\nThink about what predictors might need to be added\nEspecially if you keep seeing the same points as influential\n\n\n \n\nIf interpretability is important in your final work, then transformations are not a great solution"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#models-comparison",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#models-comparison",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Models comparison",
    "text": "Models comparison\n\n# library(gtsummary) for tbl_regression() and tbl_merge()\n\ntbl_model1 &lt;- tbl_regression(model1)\n\ntbl_model2 &lt;- tbl_regression(model2)\n\ntbl_model3 &lt;- tbl_regression(model3)\n\ntbl_model4 &lt;- tbl_regression(model4)\n\ntbl_model5 &lt;- tbl_regression(model5)\n\ntbl_model6 &lt;- tbl_regression(model6)\n\n# Compare models 1-3\ntbl_merge(\n  tbls = list(tbl_model1, tbl_model2, tbl_model3),\n  tab_spanner = c(\"Model 1: y=LE\", \"Model 2: y=LE^2\", \"Model 3: y=LE^3\")\n  )\n\n\n\n\n\n  \n    \n      Characteristic\n      \n        Model 1: y=LE\n      \n      \n        Model 2: y=LE^2\n      \n      \n        Model 3: y=LE^3\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    female_literacy_rate_2011\n0.23\n0.17, 0.29\n&lt;0.001\n31\n23, 39\n&lt;0.001\n3,166\n2,327, 4,006\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n# Compare models 4-6\ntbl_merge(\n  tbls = list(tbl_model4, tbl_model5, tbl_model6),\n  tab_spanner = c(\"Model 4: y=LE\", \"Model 5: y=LE\", \"Model 6: y=LE^3\")\n  )\n\n\n\n\n\n  \n    \n      Characteristic\n      \n        Model 4: y=LE\n      \n      \n        Model 5: y=LE\n      \n      \n        Model 6: y=LE^3\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    female_literacy_rate_2011\n0.02\n-0.38, 0.42\n&gt;0.9\n0.65\n-0.61, 1.9\n0.3\n8,092\n-8,784, 24,968\n0.3\n    FLR_2\n0.00\n0.00, 0.00\n0.3\n-0.01\n-0.03, 0.01\n0.4\n-129\n-423, 166\n0.4\n    FLR_3\n\n\n\n0.00\n0.00, 0.00\n0.3\n0.84\n-0.74, 2.4\n0.3\n  \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#other-fit-statistics-comparison",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#other-fit-statistics-comparison",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Other fit statistics comparison",
    "text": "Other fit statistics comparison\n\nglance(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4109366\n0.4033845\n6.142157\n54.4136\n1.501286e-10\n1\n-257.7164\n521.4329\n528.579\n2942.635\n78\n80\n  \n  \n  \n\n\n\nglance(model2) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4179234\n0.4104609\n812.8336\n56.00298\n9.352191e-11\n1\n-648.5445\n1303.089\n1310.235\n51534476\n78\n80\n  \n  \n  \n\n\n\nglance(model3) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4196986\n0.4122588\n82263.89\n56.41291\n8.285324e-11\n1\n-1017.917\n2041.835\n2048.981\n527853141587\n78\n80\n  \n  \n  \n\n\n\nglance(model4) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4195991\n0.4045238\n6.13629\n27.83346\n8.008115e-10\n2\n-257.1239\n522.2477\n531.7758\n2899.362\n77\n80\n  \n  \n  \n\n\n\nglance(model5) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4278828\n0.4052993\n6.132293\n18.94664\n2.844144e-09\n3\n-256.5488\n523.0977\n535.0078\n2857.981\n76\n80\n  \n  \n  \n\n\n\nglance(model6) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4414424\n0.419394\n81763.02\n20.02158\n1.160111e-09\n3\n-1016.39\n2042.78\n2054.69\n508074577758\n76\n80"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#example-chapter-5-problem-9",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#example-chapter-5-problem-9",
    "title": "Lesson 7: SLR: Model Evaluation and Diagnostics",
    "section": "Example: Chapter 5 Problem 9",
    "text": "Example: Chapter 5 Problem 9\n\nIn an experiment designed to describe the dose–response curve for vitamin K, individual rats were depleted of their vitamin K reserves and then fed dried liver for 4 days at different dosage levels.\nThe response of each rat was measured as the concentration of a clotting agent needed to clot a sample of its blood in 3 minutes.\nThe results of the experiment on 12 rats are given in the following table; values are expressed in common logarithms for both dose and response.\n\nNote: by “common logarithm” the authors mean a base 10 logarithm\n\n\n\nQuestion: why did they choose a log-log transformation?\n\n\nrats &lt;- read_excel(\"data/CH05Q09.xls\")\nglimpse(rats)\n\nRows: 12\nColumns: 3\n$ RAT     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ LOGCONC &lt;dbl&gt; 2.65, 2.25, 2.26, 1.95, 1.72, 1.60, 1.55, 1.32, 1.13, 1.07, 0.…\n$ LOGDOSE &lt;dbl&gt; 0.18, 0.33, 0.42, 0.54, 0.65, 0.75, 0.83, 0.92, 1.01, 1.04, 1.…\n\nloglog_plot &lt;- ggplot(rats, aes(x = LOGDOSE, y = LOGCONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Transformed variables\")\nloglog_plot"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#reference-all-run-models",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#reference-all-run-models",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Reference: all run models",
    "text": "Reference: all run models\n\n\nModel 2: \\(LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\nmodel2 &lt;- lm(LE_2 ~ FemaleLiteracyRate,\n             data = gapm)\ntidy(model2) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n2401.27207\n352.069818\n6.820443\n1.726640e-09\n    FemaleLiteracyRate\n31.17351\n4.165624\n7.483514\n9.352191e-11\n  \n  \n  \n\n\n\n\nModel 3: \\(LE^3 \\sim FLR\\)\n\nmodel3 &lt;- lm(LE_3 ~ FemaleLiteracyRate,\n             data = gapm)\ntidy(model3) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n95453.189\n35631.6898\n2.678885\n9.005716e-03\n    FemaleLiteracyRate\n3166.481\n421.5875\n7.510853\n8.285324e-11\n  \n  \n  \n\n\n\n\nModel 4: \\(LE \\sim FLR + FLR^2\\)\n\nmodel4 &lt;- lm(LifeExpectancyYrs ~ \n               FemaleLiteracyRate + FLR_2,\n             data = gapm)\ntidy(model4) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n57.030875456\n6.282845592\n9.07723652\n8.512585e-14\n    FemaleLiteracyRate\n0.019348795\n0.201021963\n0.09625215\n9.235704e-01\n    FLR_2\n0.001578649\n0.001472592\n1.07202008\n2.870595e-01\n  \n  \n  \n\n\n\n\n\nModel 5: \\(LE \\sim FLR + FLR^2 + FLR^3\\)\n\nmodel5 &lt;- lm(LifeExpectancyYrs ~ \n               FemaleLiteracyRate + FLR_2 + FLR_3,\n             data = gapm)\ntidy(model5) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n4.732796e+01\n1.117939e+01\n4.2335001\n6.373341e-05\n    FemaleLiteracyRate\n6.517986e-01\n6.354934e-01\n1.0256576\n3.083065e-01\n    FLR_2\n-9.952763e-03\n1.109080e-02\n-0.8973895\n3.723451e-01\n    FLR_3\n6.245016e-05\n5.953283e-05\n1.0490038\n2.975008e-01\n  \n  \n  \n\n\n\n\nModel 6: \\(LE^3 \\sim FLR + FLR^2 + FLR^3\\)\n\nmodel6 &lt;- lm(LE_3 ~ \n               FemaleLiteracyRate + FLR_2 + FLR_3,\n             data = gapm)\ntidy(model6) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n67691.7963283\n1.490569e+05\n0.4541338\n0.6510268\n    FemaleLiteracyRate\n8092.1325988\n8.473154e+03\n0.9550320\n0.3425895\n    FLR_2\n-128.5960879\n1.478757e+02\n-0.8696230\n0.3872447\n    FLR_3\n0.8404736\n7.937625e-01\n1.0588477\n0.2930229\n  \n  \n  \n\n\n\n\n\n\n\n\nLesson 8: SLR 5"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html",
    "title": "Lesson 10: MLR: Inference + F-test",
    "section": "",
    "text": "Interpret MLR (population) coefficient estimates with additional variable in model\nUnderstand the use of the general F-test and interpret what it measures.\nUnderstand the context of the Overall F-test, conduct the needed hypothesis test, and interpret the results.\nUnderstand the context of the single covariate F-test, conduct the needed hypothesis test, and interpret the results.\nUnderstand the context of the group of covariates F-test, conduct the needed hypothesis test, and interpret the results.\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#lets-map-that-to-our-regression-analysis-process",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#lets-map-that-to-our-regression-analysis-process",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Let’s map that to our regression analysis process",
    "text": "Let’s map that to our regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#interpreting-the-estimated-population-coefficients",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#interpreting-the-estimated-population-coefficients",
    "title": "Lesson 10: MLR: Inference + F-test",
    "section": "Interpreting the estimated population coefficients",
    "text": "Interpreting the estimated population coefficients\n\nFor a population model: \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\epsilon\\]\n\nWhere \\(X_1\\) and \\(X_2\\) are continuous variables\nNo need to specify \\(Y\\) because it required to be continuous in linear regression\n\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB)."
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#getting-these-interpretations-from-our-regression-table",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#getting-these-interpretations-from-our-regression-table",
    "title": "Lesson 10: MLR: Inference + F-test",
    "section": "Getting these interpretations from our regression table",
    "text": "Getting these interpretations from our regression table\nWe fit the regression model in R and printed the regression table:\n\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model: \\(\\widehat{\\text{LE}} = 33.595 + 0.157 \\text{ FLR} + 0.008 \\text{ FS}\\)\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 0 kcal PPD (95% CI: 24.674, 41.517).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012)."
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#lets-just-examine-the-general-interpretation-vs.-the-example",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#lets-just-examine-the-general-interpretation-vs.-the-example",
    "title": "Lesson 10: MLR: Inference + F-test",
    "section": "Let’s just examine the general interpretation vs. the example",
    "text": "Let’s just examine the general interpretation vs. the example\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 0 kcal PPD (95% CI: 24.674, 41.517).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012)."
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#what-we-need-in-our-interpretations-of-coefficients-reference",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#what-we-need-in-our-interpretations-of-coefficients-reference",
    "title": "Lesson 10: MLR: Inference + F-test",
    "section": "What we need in our interpretations of coefficients (reference)",
    "text": "What we need in our interpretations of coefficients (reference)\n\n\n\nUnits of Y\nUnits of X\nDiscussing intercept: Mean or average or expected before Y\nDiscussing coefficient for continuous covariate: Mean or average or expected before difference, increase, or decrease\n\nOR: Mean or average or expected before Y\nOnly need before difference or Y!!\n\nConfidence interval\n\n\n\nIf other covariates in the model\n\nDiscussing intercept: Must state that variables are equal to 0\n\nor at their centered value if centered!\n\nDiscussing coefficient for covariate: Must state “adjusting for all other variables”, “Controlling for all other variables”, or “Holding all other variables constant”\n\nIf only one other variable in the model, then replace “all other variables” with the single variable name"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#we-must-revisit-our-dear-friend-the-f-test",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#we-must-revisit-our-dear-friend-the-f-test",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "We must revisit our dear friend, the F-test!",
    "text": "We must revisit our dear friend, the F-test!\n\nhttps://www.writerswrite.co.za/foreshadowing/"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#remember-from-lesson-5-f-test-vs.-t-test-for-the-population-slope",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#remember-from-lesson-5-f-test-vs.-t-test-for-the-population-slope",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Remember from Lesson 5: F-test vs. t-test for the population slope",
    "text": "Remember from Lesson 5: F-test vs. t-test for the population slope\nThe square of a \\(t\\)-distribution with \\(df = \\nu\\) is an \\(F\\)-distribution with \\(df = 1, \\nu\\)\n\\[T_{\\nu}^2 \\sim F_{1,\\nu}\\]\n\nWe can use either F-test or t-test to run the following hypothesis test:\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote that the F-test does not support one-sided alternative tests, but the t-test does!"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#remember-from-lesson-5-planting-a-seed-about-the-f-test",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#remember-from-lesson-5-planting-a-seed-about-the-f-test",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Remember from Lesson 5: Planting a seed about the F-test",
    "text": "Remember from Lesson 5: Planting a seed about the F-test\nWe can think about the hypothesis test for the slope…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_1\\neq0\\)\n\n\n\n\nin a slightly different way…\n\n\n\n\n\n\nNull model (\\(\\beta_1=0\\))\n\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\nSmaller (reduced) model\n\n\n\n\n\n\nAlternative model (\\(\\beta_1\\neq0\\))\n\n\n\n\\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)\nLarger (full) model\n\n\n\n\n\n\nIn multiple linear regression, we can start using this framework to test multiple coefficient parameters at once\n\nDecide whether or not to reject the smaller reduced model in favor of the larger full model\nCannot do this with the t-test!"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#we-can-extend-this",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#we-can-extend-this",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "We can extend this!!",
    "text": "We can extend this!!\nWe can create a hypothesis test for more than one coefficient at a time…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=\\beta_2=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_1\\neq0\\) and/or \\(\\beta_2\\neq0\\)\n\n\n\n\nin a slightly different way…\n\n\n\n\n\n\nNull model\n\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\nSmaller (reduced) model\n\n\n\n\n\n\nAlternative* model\n\n\n\n\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\)\nLarger (full) model\n\n\n\n\n\n*This is not quite the alternative, but if we reject the null, then this is the model we move forward with"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#poll-everywhere-question-1",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#poll-everywhere-question-1",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#building-a-very-important-toolkit-three-types-of-tests",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#building-a-very-important-toolkit-three-types-of-tests",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Building a very important toolkit: three types of tests",
    "text": "Building a very important toolkit: three types of tests\n\n\nOverall test\n\n\nDoes at least one of the covariates/predictors contribute significantly to the prediction of Y?\n\n\n\n\nTest for addition of a single variable’s coefficient (covariate subset test)\n\n\nDoes the addition of one particular covariate (with a single coefficient) add significantly to the prediction of Y achieved by other covariates already present in the model?\n\n\n\n\nTest for addition of group of variables’ coefficient (covariate subset test)\n\n\nDoes the addition of some group of covariates (or one covariate with multiple coefficients) add significantly to the prediction of Y achieved by other covariates already present in the model?"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#variation-explained-vs.-unexplained",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#variation-explained-vs.-unexplained",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Variation: Explained vs. Unexplained",
    "text": "Variation: Explained vs. Unexplained\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 &= \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY &= SSR + SSE\n\\end{aligned}\\]\n\n\\(Y_i - \\overline{Y}\\) = the deviation of \\(Y_i\\) around the mean \\(\\overline{Y}\\)\n\nthe total amount deviation\n\n\\(\\widehat{Y}_i- \\overline{Y}\\) = the deviation of the fitted value \\(\\widehat{Y}_i\\) around the mean \\(\\overline{Y}\\)\n\nthe amount deviation explained by the regression at \\(X_{i1},\\ldots,X_{ik}\\)\n\n\\(Y_i - \\widehat{Y}_i\\) = the deviation of the observation \\(Y\\) around the fitted regression line\n\nthe amount deviation unexplained by the regression at \\(X_{i1},\\ldots,X_{ik}\\)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#another-way-to-think-of-ssy-ssr-and-sse",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#another-way-to-think-of-ssy-ssr-and-sse",
    "title": "Lesson 10: MLR: Inference + F-test",
    "section": "Another way to think of SSY, SSR, and SSE",
    "text": "Another way to think of SSY, SSR, and SSE\n\nLet’s create a data frame of each component within the SS’s\n\nDifference in SSY: \\(Y_i - \\overline{Y}\\)\nDifference in SSR: \\(\\widehat{Y}_i- \\overline{Y}\\)\nDifference in SSE: \\(Y_i - \\widehat{Y}_i\\)\n\nUsing our simple linear regression model as an example:\n\n\nslr1 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate, data = gapm_sub)\naug_slr1 = augment(slr1)\nSS_df = gapm_sub %&gt;% select(LifeExpectancyYrs) %&gt;%\n  mutate(SSY_diff = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         y_fit = aug_slr1$.fitted, \n         SSR_diff = y_fit - mean(LifeExpectancyYrs), \n         SSE_diff = aug_slr1$.resid)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#plot-the-components-of-each-sum-of-squares",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#plot-the-components-of-each-sum-of-squares",
    "title": "Lesson 10: MLR: Inference + F-test",
    "section": "Plot the components of each sum of squares",
    "text": "Plot the components of each sum of squares\n\nSSY_plot = ggplot(SS_df, aes(SSY_diff)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) \nSSR_plot = ggplot(SS_df, aes(SSR_diff)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) \nSSE_plot = ggplot(SS_df, aes(SSE_diff)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) \ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = 64.64\\]\n\\[SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = 27.24\\]\n\\[SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = 37.39\\]"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#when-running-a-f-test-for-linear-models",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#when-running-a-f-test-for-linear-models",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "When running a F-test for linear models…",
    "text": "When running a F-test for linear models…\n\nWe need to define a larger, full model (more parameters)\nWe need to define a smaller, reduced model (fewer parameters)\nUse the F-statistic to decide whether or not we reject the smaller model\n\nThe F-statistic compares the SSE of each model to determine if the full model explains a significant amount of additional variance\n\n\n\n\n \n\\[\nF = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\n\\]\n\n\n\\(SSE(R) \\geq SSE(F)\\)\nNumerator measures difference in unexplained variation between the models\n\nBig difference = added parameters greatly reduce the unexplained variation (increase explained variation)\nSmaller difference = added parameters don’t reduce the unexplained variation\n\nTake ratio of difference to the unexplained variation in the full model"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#poll-everywhere-question-2",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#poll-everywhere-question-2",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#we-will-keep-working-with-the-mlr-model-from-last-class",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#we-will-keep-working-with-the-mlr-model-from-last-class",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "We will keep working with the MLR model from last class",
    "text": "We will keep working with the MLR model from last class\nNew population model for example:\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]\n\n# Fit regression model:\nmr1 &lt;- gapm_sub %&gt;% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)\ntidy(mr1, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157 \\ \\text{FLR}\n+ 0.008\\ \\text{FS}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#overall-f-test",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#overall-f-test",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Overall F-test",
    "text": "Overall F-test\nDoes at least one of the covariates/predictors contribute significantly to the prediction of Y?\n\nFor a general population MLR model, \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\nWe can create a hypothesis test for all the covariate coefficients…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=\\beta_2= \\ldots=\\beta_k=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\nAt least one \\(\\beta_j\\neq0\\) (for \\(j=1, 2, \\ldots, k\\))\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k + \\epsilon\\)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#overall-f-test-general-steps-for-hypothesis-test",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#overall-f-test-general-steps-for-hypothesis-test",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Overall F-test: general steps for hypothesis test",
    "text": "Overall F-test: general steps for hypothesis test\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1=\\beta_2= \\ldots=\\beta_k=0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=1, 2, \\ldots, k\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}} = \\frac{MSR_{full}}{MSE_{full}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject if: \\(P(F_{k, n-k-1} &gt; F) &lt; \\alpha\\)\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that at least one predictor’s coefficient is not 0 (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#overall-f-test-a-word-on-the-conclusion",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#overall-f-test-a-word-on-the-conclusion",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Overall F-test: a word on the conclusion",
    "text": "Overall F-test: a word on the conclusion\n\nIf \\(H_0\\) is rejected, we conclude there is sufficient evidence that at least one predictor’s coefficient is different from zero.\nSame as: at least one independent variable contributes significantly to the prediction of \\(Y\\)\n\n \n\nIf \\(H_0\\) is not rejected, we conclude there is insufficient evidence that at least one predictor’s coefficient is different from zero.\nSame as: Not enough evidence that at least one independent variable contributes significantly to the prediction of \\(Y\\)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#lets-think-about-our-mlr-example-for-life-expectancy",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#lets-think-about-our-mlr-example-for-life-expectancy",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Let’s think about our MLR example for life expectancy",
    "text": "Let’s think about our MLR example for life expectancy\nOur proposed population model\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\ \\text{FS}\n\\end{aligned}\\]\nOur main question for the Overall F-test: Is the regression model containing female literacy rate and food supply useful in estimating countries’ life expectancy?\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(LE = \\beta_0 + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\nFit and get augmented values for reduced model:\n\n\nmod_red1 = gapm_sub %&gt;% \n  lm(formula = LifeExpectancyYrs ~ 1)\naug_red1  = augment(mod_red1)\n\n\nFit and get augmented values for full model:\n\n\nmod_full1 = gapm_sub %&gt;% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)\naug_full1  = augment(mod_full1)\n\n\nCalculate the deviances for each model:\n\n\nSS_df2 = gapm_sub %&gt;% select(LifeExpectancyYrs) %&gt;%\n  mutate(SSY_diff_r1 = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         SSR_diff_r1 = aug_red1$.fitted - mean(LifeExpectancyYrs), \n         SSE_diff_r1 = aug_red1$.resid, \n         SSY_diff_f1 = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         SSR_diff_f1 = aug_full1$.fitted - mean(LifeExpectancyYrs), \n         SSE_diff_f1 = aug_full1$.resid)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-1",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-1",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\n\n\nReduced / null model\n\n\\[LE = \\beta_0 + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 0\\]\n \n\\[SSE = 64.64\\]\n\n\n\n\nFull / Alternative model\n\n\\[LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 36.39\\]\n \n\\[SSE = 28.25\\]"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#poll-everywhere-question-3",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#poll-everywhere-question-3",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (1/3)",
    "text": "So let’s step through our hypothesis test (1/3)\n\n\n\nMet underlying LINE assumptions\n\n\n\n \n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1=\\beta_2=0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_1\\neq0 \\text{ or } \\beta_2\\neq0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k =2\\) and denominator \\(df=n-k-1 = 72 - 2-1=69\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (2/3)",
    "text": "So let’s step through our hypothesis test (2/3)\n\n\n\nCompute the value of the test statistic / 6. Calculate the p-value\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}=44.443\\] OR use ANOVA table:\n\nanova(mod_red1, mod_full1) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ 1\n71.000\n4,589.119\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD\n69.000\n2,005.556\n2.000\n2,583.563\n44.443\n0.000"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (3/3)",
    "text": "So let’s step through our hypothesis test (3/3)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n \nWe reject the null hypothesis at the 5% significance level. There is sufficient evidence that either countries’ female literacy rate or the food supply (or both) contributes significantly to the prediction of life expectancy (p-value &lt; 0.001)."
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#covariate-subset-test-single-variable",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#covariate-subset-test-single-variable",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Covariate subset test: Single variable",
    "text": "Covariate subset test: Single variable\nDoes the addition of one particular covariate of interest (a numeric covariate with only one coefficient) add significantly to the prediction of Y achieved by other covariates already present in the model?\n\nFor a general population MLR model, \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\beta_j X_j +\\ldots + \\beta_k X_k + \\epsilon\\]\n\nWe can create a hypothesis test for a single \\(j\\) covariate coefficient (where \\(j\\) can be any value \\(1, 2, \\ldots, k\\))…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_j=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_j\\neq0\\)\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(\\begin{aligned}Y = &\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_j X_j +\\\\ &\\ldots + \\beta_k X_k + \\epsilon \\end{aligned}\\)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#single-covariate-f-test-general-steps-for-hypothesis-test-reference",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#single-covariate-f-test-general-steps-for-hypothesis-test-reference",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Single covariate F-test: general steps for hypothesis test (reference)",
    "text": "Single covariate F-test: general steps for hypothesis test (reference)\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_j=0\\\\\n\\text{vs. } H_A&: \\beta_j\\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that predictor/covariate \\(j\\) significantly improves the prediction of Y, given all the other covariates are in the model (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#lets-think-about-our-mlr-example-for-life-expectancy-1",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#lets-think-about-our-mlr-example-for-life-expectancy-1",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Let’s think about our MLR example for life expectancy",
    "text": "Let’s think about our MLR example for life expectancy\nOur proposed population model\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\ \\text{FS}\n\\end{aligned}\\]\nOur main question for the single covariate subset F-test: Is the regression model containing food supply improve the estimation of countries’ life expectancy, given female literacy rate is already in the model?\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-2",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-2",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\n\nReduced / null model \\[LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 27.24\\]\n \n\\[SSE = 37.39\\]\n\n\n\nFull / Alternative model \\[LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 36.39\\]\n \n\\[SSE = 28.25\\]"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#poll-everywhere-question-4",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#poll-everywhere-question-4",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13-1",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13-1",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (1/3)",
    "text": "So let’s step through our hypothesis test (1/3)\n\n\n\nMet underlying LINE assumptions\n\n\n\n \n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_2=0\\\\\n\\text{vs. } H_A&: \\beta_2\\neq0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k =2\\) and denominator \\(df=n-k-1 = 72 - 2-1=69\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23-1",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23-1",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (2/3)",
    "text": "So let’s step through our hypothesis test (2/3)\n\n\n\nCompute the value of the test statistic / 6. Calculate the p-value\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\] ANOVA table:\n\nanova(mod_red2, mod_full2) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate\n70.000\n2,654.875\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD\n69.000\n2,005.556\n1.000\n649.319\n22.339\n0.000"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33-1",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33-1",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (3/3)",
    "text": "So let’s step through our hypothesis test (3/3)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n \nWe reject the null hypothesis at the 5% significance level. There is sufficient evidence that countries’ food supply contributes significantly to the prediction of life expectancy, given that female literacy rate is already in the model (p-value &lt; 0.001)."
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#covariate-subset-test-group-of-variables",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#covariate-subset-test-group-of-variables",
    "title": "Lesson 10: MLR: Inference + F-test",
    "section": "Covariate subset test: group of variables",
    "text": "Covariate subset test: group of variables\nDoes the addition of some group of covariates of interest (or a multi-level categorical variable) add significantly to the prediction of Y obtained through other independent variables already present in the model?\n\nFor a general population MLR model, \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\nWe can create a hypothesis test for a group of covariate coefficients (subset of many)… For example…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=\\beta_3 =0\\) (this can be any coefficients)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\nAt least one \\(\\beta_j\\neq0\\) (for \\(j=2,3\\))\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(Y = \\beta_0 + \\beta_2 X_2 + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X + \\beta_3 X_3+\\epsilon\\)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#covariate-subset-f-test-general-steps-for-hypothesis-test-reference",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#covariate-subset-f-test-general-steps-for-hypothesis-test-reference",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Covariate subset F-test: general steps for hypothesis test (reference)",
    "text": "Covariate subset F-test: general steps for hypothesis test (reference)\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\nFor example:\n\\[\\begin{align}\nH_0 &: \\beta_1 = \\beta_3 = 0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=1,3\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that predictors/covariates \\(2,3\\) significantly improve the prediction of Y, given all the other covariates are in the model (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#we-need-to-slightly-alter-our-mlr-example-for-life-expectancy",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#we-need-to-slightly-alter-our-mlr-example-for-life-expectancy",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "We need to slightly alter our MLR example for life expectancy",
    "text": "We need to slightly alter our MLR example for life expectancy\nOur proposed population model to include water source percent (WS):\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\beta_3 WS + \\epsilon\\]\n\nWe don’t have a fitted multiple regression model for this yet!\n\nOur main question for the group covariate subset F-test: Is the regression model containing food supply and water source percent improve the estimation of countries’ life expectancy, given percent female literacy rate is already in the model?\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\beta_3 WS + \\epsilon\\)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-3",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-3",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\n\nReduced / null model \\[LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 27.24\\]\n \n\\[SSE = 37.39\\]\n\n\n\nFull / Alternative model \\[LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\beta_3 WS + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 43.26\\]\n \n\\[SSE = 21.38\\]"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13-2",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-13-2",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (1/3)",
    "text": "So let’s step through our hypothesis test (1/3)\n\n\n\nMet underlying LINE assumptions\n\n\n\n \n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_2=\\beta_3=0\\\\\n\\text{vs. } H_A&: \\beta_2\\neq0 \\text{ and/or } \\beta_3\\neq0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k =2\\) and denominator \\(df=n-k-1 = 72 - 2-1=69\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23-2",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-23-2",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (2/3)",
    "text": "So let’s step through our hypothesis test (2/3)\n\n\n\nCompute the value of the test statistic / 6. Calculate the p-value\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\] ANOVA table:\n\nanova(mod_red3, mod_full3) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate\n70.000\n2,654.875\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD + WaterSourcePrct\n68.000\n1,517.916\n2.000\n1,136.959\n25.467\n0.000"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33-2",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#so-lets-step-through-our-hypothesis-test-33-2",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (3/3)",
    "text": "So let’s step through our hypothesis test (3/3)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n \nWe reject the null hypothesis at the 5% significance level. There is sufficient evidence that countries’ food supply or water source (or both) contribute significantly to the prediction of life expectancy, given that female literacy rate is already in the model (p-value &lt; 0.001)."
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#other-ways-to-word-the-hypothesis-tests-reference",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#other-ways-to-word-the-hypothesis-tests-reference",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Other ways to word the hypothesis tests (reference)",
    "text": "Other ways to word the hypothesis tests (reference)\n\nSingle covariate subset F-test\n\n\\(H_0:\\) \\(X^*\\) does not significantly improve the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_p\\) are already in the model\n\\(H_A:\\) \\(X^*\\) significantly improves the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_p\\) are already in the model\n\nGroup covariate subset F-test\n\n\\(H_0:\\) The addition of the \\(s\\) variables \\(X_1^*, X_2^*, \\ldots, X_s^*\\) does not significantly improve the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_q\\) are already in the model\n\\(H_A:\\) The addition of the \\(s\\) variables \\(X_1^*, X_2^*, \\ldots, X_s^*\\) significantly improves the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_q\\) are already in the model\n\n\n\n\nLesson 10: MLR 2"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "",
    "text": "Understand why we need a new way to code categorical variables compared to continuous variables\nWrite the regression equation for a categorical variable using reference cell coding\nCalculate and interpret coefficients for reference cell coding\nChange the reference level in a categorical variable for reference cell coding\nCreate new variables and interpret coefficient for ordinal / scoring coding\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#lets-map-that-to-our-regression-analysis-process",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#lets-map-that-to-our-regression-analysis-process",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Let’s map that to our regression analysis process",
    "text": "Let’s map that to our regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#still-looking-at-gapminder-life-expectancy-data",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#still-looking-at-gapminder-life-expectancy-data",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Still looking at Gapminder Life Expectancy data",
    "text": "Still looking at Gapminder Life Expectancy data\n\nWe will look at life expectancy vs. these world regions\nGapminder uses four world regions\n\nAfrica\nThe Americas\nAsia\nEurope\n\nWorld region is a multi-level categorical covariate: it has four regions\n\n \n\nNote: I am calling the expected life expectancy \\(\\widehat{LE}\\)\n\nPreviously, I have written \\(\\widehat{\\text{life expectancy}}\\)"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#linear-regression-with-a-categorical-covariate",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#linear-regression-with-a-categorical-covariate",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Linear regression with a categorical covariate",
    "text": "Linear regression with a categorical covariate\n\n\nBad option for visualization:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_point() +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nGood option for visualization:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_jitter(size = 1, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\",\n       caption = \"Diamonds = region averages\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nUsed geom_jitter()\n\n\nGood option for visualization:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_boxplot() +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#linear-regression-with-a-categorical-covariate-1",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#linear-regression-with-a-categorical-covariate-1",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Linear regression with a categorical covariate",
    "text": "Linear regression with a categorical covariate\n\n\n\nWhen using a categorical covariate/predictor (that is not ordered),\n\nWe do NOT, technically, find a best-fit line\n\nInstead we model the means of the outcome\n\nFor the different levels of the categorical variable\n\nIn 511, we used Kruskal-Wallis test and our ANOVA table to test if groups means were statistically different from one another\nWe can do this using linear models AND we can include other variables in the model"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#there-are-different-ways-to-code-categorical-variables",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#there-are-different-ways-to-code-categorical-variables",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "There are different ways to code categorical variables",
    "text": "There are different ways to code categorical variables\n\nReference cell coding (sometimes called dummy coding)\n\nCompares each level of a variable to the omitted (reference) level\n\nEffect coding (sometimes called sum coding or deviation coding)\n\nCompares deviations from the grand mean\nNot covered in our class\n\nOrdinal encoding (sometimes called scoring)\n\nCategories have a natural, even spaced ordering\n\n\n \nIf you want to learn more about these and other coding schemes:\n\nCoding Systems for Categorical Variables in Regression Analysis\nCategorical Data Encoding Techniques\nCoding Schemes for Categorical Variables"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#building-the-regression-equation-problem-with-a-single-coefficient",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#building-the-regression-equation-problem-with-a-single-coefficient",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Building the regression equation: problem with a single coefficient",
    "text": "Building the regression equation: problem with a single coefficient\n\n\nPreviously: simple linear regression\n\nOutcome \\(Y\\) = numerical variable\nPredictor \\(X\\) = numerical variable\n\nThe regression (best-fit) line is: \\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X \\]\n\nNew: what if the explanatory variable is categorical?\nNaively, we could write: \\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\)\nOr, with our variables: \\[\\widehat{\\textrm{LE}} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot \\textrm{WR} \\]\n\nBut what does \\(\\textrm{WR}\\) (world regions) mean in this equation?\n\nWhat values can it take? How do we represent each region?\n\n\n \n\nNote: the above is WRONG"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#building-the-regression-equation-how-do-we-map-categories-to-means",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#building-the-regression-equation-how-do-we-map-categories-to-means",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Building the regression equation: how do we map categories to means?",
    "text": "Building the regression equation: how do we map categories to means?\n\nIf we only have world region in our model and want to map it to an expected life expectancy…\n\n\n\n\nWe want to create a function that can map each region to life expectancy\n\nIf in Africa: \\(\\widehat{LE} = 61.32\\) years\nIf in the Americas: \\(\\widehat{LE} = 74.64\\) years\nIf in Asia: \\(\\widehat{LE} = 71.70\\) years\nIf in Europe: \\(\\widehat{LE} = 77.61\\) years\n\n\n \n\nCan we make one equation for \\(\\widehat{LE}\\) by putting the “if” statements within the equation?"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#building-the-regression-equation-indicator-functions",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#building-the-regression-equation-indicator-functions",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Building the regression equation: Indicator functions",
    "text": "Building the regression equation: Indicator functions\n\nIn order to represent each region in the equation, we need to introduce a new function:\n\nIndicator function:\n\n\\[I(X = x) \\text{ or } I(x) =\n\\left\\{\n\\begin{array}{@{}ll@{}}\n1, & \\text{if}\\ X = x \\\\\n  0, & \\text{else}\n\\end{array}\\right. \\]\n\nThis basically a binary yes/no if \\(X\\) is a specific value \\(x\\)\n\nFor example, if we want to identify a country as being in the Americas region, we can make:\n\\[I(WR = \\text{Americas}) \\text{ or }I(\\text{Americas}) =\n\\left\\{\n\\begin{array}{@{}ll@{}}\n1, & \\text{if}\\ WR = \\text{Americas} \\\\\n  0, & \\text{else}\n\\end{array}\\right. \\]"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#poll-everywhere-question-1",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#poll-everywhere-question-1",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#building-the-regression-equation-indicators-in-our-equation",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#building-the-regression-equation-indicators-in-our-equation",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Building the regression equation: Indicators in our equation",
    "text": "Building the regression equation: Indicators in our equation\n\n\n\\[\\begin{aligned}\n\\widehat{\\textrm{LE}} = & 61.32 \\cdot I(\\text{Africa}) + 74.64 \\cdot I(\\text{Americas}) + \\\\ &71.7 \\cdot I(\\text{Asia}) + 77.61 \\cdot I(\\text{Europe})\n\\end{aligned}\\]\n\nHowever, a linear regression equation still requires an intercept!\n\nSo one of our regions need to become our “reference” group\nWe’ll use Africa as our reference\nThat means we need to adjust all the numbers\n\n\n\\[\\begin{aligned}\n\\widehat{\\textrm{LE}} = & 61.32 + 13.32 \\cdot I(\\text{Americas}) + \\\\ &10.38 \\cdot I(\\text{Asia}) + 16.29 \\cdot I(\\text{Europe}) \\\\\n\\widehat{\\textrm{LE}} = & \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) + \\\\ & \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#viewing-the-regression-equation-another-way",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#viewing-the-regression-equation-another-way",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Viewing the regression equation another way",
    "text": "Viewing the regression equation another way\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) + \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\\]\n\n\n\n\n\n\n\n\n\n\nWorld region\nRegression equation for WR\nAverage Life Expectancy for WR\n\n\n\n\nAfrica\n\\(\\begin{aligned} \\widehat{\\textrm{LE}} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 0 + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 \\end{aligned}\\)\n\\(\\widehat{\\textrm{LE}} = \\widehat\\beta_0\\)\n\n\nAmericas\n\\(\\begin{aligned} \\widehat{\\textrm{LE}} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 1+ \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 \\end{aligned}\\)\n\\(\\widehat{\\textrm{LE}} = \\widehat\\beta_0 + \\widehat\\beta_1\\)\n\n\nAsia\n\\(\\begin{aligned} \\widehat{\\textrm{LE}} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 0 + \\\\ & \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 \\cdot 0 \\end{aligned}\\)\n\\(\\widehat{\\textrm{LE}} = \\widehat\\beta_0 + \\widehat\\beta_2\\)\n\n\nEurope\n\\(\\begin{aligned} \\widehat{\\textrm{LE}} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 0 + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 1 \\end{aligned}\\)\n\\(\\widehat{\\textrm{LE}} = \\widehat\\beta_0 + \\widehat\\beta_3\\)"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#interpretation-of-regression-equation-coefficients",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#interpretation-of-regression-equation-coefficients",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Interpretation of regression equation coefficients",
    "text": "Interpretation of regression equation coefficients\n\nRemember: expected, mean, and average are interchangeable\n\n\n\n\n\n\n\n\nCoefficient\nInterpretation\n\n\n\n\n\\(\\widehat{\\beta}_0\\)\nExpected/mean/average life expectancy of Africa\n\n\n\\(\\widehat{\\beta}_1\\)\nDifference in mean life expectancy of the Americas and Africa -OR-\nMean difference in life expectancy of the Americas and Africa\n\n\n\\(\\widehat{\\beta}_2\\)\nDifference in mean life expectancy between Asia and Africa -OR-\nMean difference in life expectancy between Asia and Africa\n\n\n\\(\\widehat{\\beta}_3\\)\nDifference in mean life expectancy between Europe and Africa -OR-\nMean difference in life expectancy between Europe and Africa"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#poll-everywhere-question-2",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#poll-everywhere-question-2",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#regression-table-with-lm-function",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#regression-table-with-lm-function",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Regression table with lm() function",
    "text": "Regression table with lm() function\n\nmodel1 &lt;- lm(LifeExpectancyYrs ~ four_regions, data = gapm2)\ntidy(model1, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 38) %&gt;% \n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n61.32\n0.76\n80.26\n0.00\n59.81\n62.83\n    four_regionsAmericas\n13.32\n1.23\n10.83\n0.00\n10.89\n15.74\n    four_regionsAsia\n10.38\n1.08\n9.61\n0.00\n8.25\n12.51\n    four_regionsEurope\n16.29\n1.13\n14.37\n0.00\n14.05\n18.52\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{LE}} = 61.32 + 13.32 \\cdot I(\\text{Americas}) + 10.38 \\cdot I(\\text{Asia}) + 16.29 \\cdot I(\\text{Europe})\\]\n\nWhich world region did R choose as the reference level?\nHow you would calculate the mean life expectancies of world regions using only the results from the regression table?"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#bringing-in-the-numbersunits95-ci",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#bringing-in-the-numbersunits95-ci",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Bringing in the numbers/units/95% CI",
    "text": "Bringing in the numbers/units/95% CI\n\n\n\n\n\n\n\nCoefficient\nInterpretation\n\n\n\n\n\\(\\widehat{\\beta}_0\\)\nAverage life expectancy of countries in Africa is 61.32 years (95% CI: 59.81, 62.83).\n\n\n\\(\\widehat{\\beta}_1\\)\nThe difference in mean life expectancy between countries in the Americas and Africa is 13.32 (95% CI: 10.89, 15.74).\n\n\n\\(\\widehat{\\beta}_2\\)\nThe difference in mean life expectancy between countries in the Americas and Africa is 10.38 (95% CI: 8.25, 12.51).\n\n\n\\(\\widehat{\\beta}_3\\)\nThe difference in mean life expectancy between countries in Europe and Africa is 18.52 (95% CI: 14.05, 18.52).\n\n\n\n \n\nDon’t forget that we can use the confidence intervals to assess whether the mean difference with Africa is significant or not"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#we-can-also-use-r-to-report-each-regions-average-life-expectancy",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#we-can-also-use-r-to-report-each-regions-average-life-expectancy",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "We can also use R to report each region’s average life expectancy",
    "text": "We can also use R to report each region’s average life expectancy\nFind the 95% CI’s for the mean life expectancy for the Americas, Asia, and Europe\n\nUse the base R predict() function (see Lesson 4 for more info)\nRequires specification of a newdata “value”\n\n\nnewdata &lt;- data.frame(four_regions = c(\"Africa\", \"Americas\", \"Asia\", \"Europe\")) \n\n\n\n\n(pred = predict(model1, \n                newdata=newdata, \n                interval=\"confidence\"))\n\n       fit      lwr      upr\n1 61.32037 59.81287 62.82787\n2 74.63824 72.73841 76.53806\n3 71.70185 70.19435 73.20935\n4 77.60889 75.95751 79.26027\n\n\n\n\n\nInterpretations\n\n\n\nThe average life expectancy for countries in the Americas is 74.64 years (95% CI: 72.74, 76.54).\nThe average life expectancy for countries in Asia is 71.7 years (95% CI: 70.19, 73.21).\nThe average life expectancy for countries in Europe is 77.61 years (95% CI: 75.96, 79.26)."
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#another-way-to-look-at-coefficient-values",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#another-way-to-look-at-coefficient-values",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Another way to look at coefficient values",
    "text": "Another way to look at coefficient values\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) +  \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\\]\n\n\nCode\n# means of each level of `four_regions`\ngapm2_ave &lt;- gapm2 %&gt;% \n  group_by(four_regions) %&gt;% \n  summarise(\n    life_ave = mean(LifeExpectancyYrs))\n\n# mean of `africa`\nmean_africa &lt;- gapm2_ave %&gt;% \n  filter(four_regions == \"Africa\") %&gt;% \n  pull(life_ave)\n\n# differences in means between levels of `four_regions` and `africa`\ngapm2_ave_diff &lt;- gapm2_ave %&gt;% \n  mutate(`Difference with Africa` = life_ave - mean_africa) %&gt;%\n  rename(`World regions` = four_regions, \n         `Average life expectancy` = life_ave)\n\n# At the beginning of the Rmd we loaded knitr, which is where the kable command is from\n# library(knitr)\ngapm2_ave_diff %&gt;% kable(\n  digits = 1,\n  format = \"markdown\"\n  ) \n\n\n\n\n\nWorld regions\nAverage life expectancy\nDifference with Africa\n\n\n\n\nAfrica\n61.3\n0.0\n\n\nAmericas\n74.6\n13.3\n\n\nAsia\n71.7\n10.4\n\n\nEurope\n77.6\n16.3\n\n\n\n\n\n\\[\\widehat{\\textrm{LE}} = 61.32 + 13.32 \\cdot I(\\text{Americas}) + 10.38 \\cdot I(\\text{Asia}) + 16.29 \\cdot I(\\text{Europe})\\]"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#reference-levels",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#reference-levels",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Reference levels",
    "text": "Reference levels\nWhy is Africa not one of the variables in the regression equation?\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) +  \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\\]\n\nCategorical variables have to have at least 2 levels. If they have 2 levels, we call them binary\n\n \n\nWe choose one level as our reference level to which all other levels of the categorical variable are compared\n\nThe levels \\(\\text{Americas}, \\text{Asia}, \\text{Europe}\\) are compared to the level \\(\\text{Africa}\\)\n\n\n \n\nThe intercept of the regression equation is the mean of the outcome restricted to the reference level\n\nRecall that the intercept is the mean life expectancy of Africa, which was our reference level\n\n\n \n\nIf the categorical variable has \\(r\\) levels, then we need \\(r-1\\) variables/coefficients to model it!"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#we-can-change-the-reference-level-to-europe-12",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#we-can-change-the-reference-level-to-europe-12",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "We can change the reference level to Europe (1/2)",
    "text": "We can change the reference level to Europe (1/2)\n\nSuppose we want to compare the mean life expectancies of world regions to the \\(\\text{Europe}\\) level instead of \\(\\text{Africa}\\)\nBelow is the estimated regression equation for when \\(Africa\\) is the reference level\n\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) +  \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\\]\n\nUpdate the variables to make \\(Europe\\) the reference level:\n\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Africa}) +  \\widehat\\beta_2 \\cdot I(\\text{Americas}) + \\widehat\\beta_3 \\cdot I(\\text{Asia})\\]"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#we-can-change-the-reference-level-to-europe-22",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#we-can-change-the-reference-level-to-europe-22",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "We can change the reference level to Europe (2/2)",
    "text": "We can change the reference level to Europe (2/2)\n\nNow update the coefficients of the regression equation using the output below.\n\n\n\n\n\n\nWorld regions\nAverage life expectancy\nDifference with Europe\n\n\n\n\nAfrica\n61.32\n-16.29\n\n\nAmericas\n74.64\n-2.97\n\n\nAsia\n71.70\n-5.91\n\n\nEurope\n77.61\n0.00\n\n\n\n\n\n\\[\\widehat{\\textrm{LE}} = 77.61 -16.29 \\cdot I(\\text{Africa}) -2.97 \\cdot I(\\text{Americas}) -5.91 \\cdot I(\\text{Asia})\\]"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#r-change-reference-level-to-europe-12",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#r-change-reference-level-to-europe-12",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "R: Change reference level to Europe (1/2)",
    "text": "R: Change reference level to Europe (1/2)\n\nfour_regions data type was originally a character - check this with str()\n\n\nstr(gapm$four_regions) \n\n chr [1:195] \"asia\" \"europe\" \"africa\" \"europe\" \"africa\" \"americas\" ...\n\n\n\nIn order to change the reference level, we need to convert it to data type factor\n\nI also did this at the beginning to capitalize each region\n\n\n\ngapm_ex = gapm %&gt;% \n mutate(\n   four_regions = factor(four_regions, \n                         levels = c(\"africa\", \"americas\", \"asia\", \"europe\"), \n                         labels = c(\"Africa\", \"Americas\", \"Asia\", \"Europe\"))\n   )\nstr(gapm_ex$four_regions) \n\n Factor w/ 4 levels \"Africa\",\"Americas\",..: 3 4 1 4 1 2 2 4 3 4 ...\n\nlevels(gapm_ex$four_regions) # order of factor levels\n\n[1] \"Africa\"   \"Americas\" \"Asia\"     \"Europe\""
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#r-change-reference-level-to-europe-22",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#r-change-reference-level-to-europe-22",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "R: Change reference level to Europe (2/2)",
    "text": "R: Change reference level to Europe (2/2)\n\nNow change the order of the factor levels\nCode below uses fct_relevel() from the forcats package that gets loaded as a part of the tidyverse\nAny levels not mentioned will be left in their existing order, after the explicitly mentioned levels.\n\n\ngapm2 &lt;- gapm2 %&gt;% \n  mutate(\n    four_regions = fct_relevel(four_regions, \"Europe\")\n    )\n\n\nCheck the order:\n\n\nlevels(gapm2$four_regions)\n\n[1] \"Europe\"   \"Africa\"   \"Americas\" \"Asia\""
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#r-run-model-with-europe-as-the-reference-level",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#r-run-model-with-europe-as-the-reference-level",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "R: Run model with Europe as the reference level",
    "text": "R: Run model with Europe as the reference level\n\nlevels(gapm2$four_regions)\n\n[1] \"Europe\"   \"Africa\"   \"Americas\" \"Asia\"    \n\nmodel2 &lt;- lm(LifeExpectancyYrs ~ four_regions, data = gapm2)\ntidy(model2) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% \n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n77.61\n0.84\n92.72\n0.00\n    four_regionsAfrica\n−16.29\n1.13\n−14.37\n0.00\n    four_regionsAmericas\n−2.97\n1.28\n−2.33\n0.02\n    four_regionsAsia\n−5.91\n1.13\n−5.21\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\\widehat{\\textrm{LE}} &=  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Africa}) +  \\widehat\\beta_2 \\cdot I(\\text{Americas}) + \\widehat\\beta_3 \\cdot I(\\text{Asia}) \\\\ \\widehat{\\textrm{LE}} &= 77.61 -16.29 \\cdot I(\\text{Africa}) -2.97 \\cdot I(\\text{Americas}) -5.91 \\cdot I(\\text{Asia}) \\end{aligned}\\]"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#fitted-values-residuals",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#fitted-values-residuals",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Fitted values & residuals",
    "text": "Fitted values & residuals\n\n\nSimilar to as before:\n\nObserved values \\(Y\\) are the values in the dataset\nFitted values \\(\\widehat{Y}\\) are the values that fall on the best-fit line for a specific value of x are the means of the outcome stratified by the categorical predictor’s levels\nResiduals (\\(\\widehat{\\epsilon} = Y - \\widehat{Y}\\)) are the differences between the two"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#fitted-values-are-the-same-as-the-means",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#fitted-values-are-the-same-as-the-means",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Fitted values are the same as the means",
    "text": "Fitted values are the same as the means\n\nm1_aug &lt;- augment(model1)\n\nggplot(m1_aug, aes(x = four_regions, y = .fitted)) + geom_point() +\n  theme(axis.text = element_text(size = 22), axis.title = element_text(size = 22))"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#residual-plots-now-the-spread-within-each-region",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#residual-plots-now-the-spread-within-each-region",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Residual plots (now the spread within each region)",
    "text": "Residual plots (now the spread within each region)\n\nggplot(m1_aug, aes(x=.resid)) + geom_histogram() + \n  theme(axis.text = element_text(size = 22), title = element_text(size = 22))"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#poll-everywhere-question-3",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#poll-everywhere-question-3",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#lets-look-at-life-expectancy-vs.-four-income-levels",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#lets-look-at-life-expectancy-vs.-four-income-levels",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Let’s look at life expectancy vs. four income levels",
    "text": "Let’s look at life expectancy vs. four income levels\n\nGapminder discusses individual income levels\n\n \n\nIncome levels for a country is based on average GDP per capita, and grouped into:\n\nLow income\nLower middle income\nUpper middle income\nHigh income"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#visualizing-the-ordinal-variable-income-levels",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#visualizing-the-ordinal-variable-income-levels",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Visualizing the ordinal variable, income levels",
    "text": "Visualizing the ordinal variable, income levels\n\n\n\n\n\n\n\n\n\n\n\n\nA few changes needed:\n\nPut the income levels in order\n\n\ngapm2 = gapm2 %&gt;%\n mutate(income_levels = factor(income_levels, \n            ordered = T, \n            levels = c(\"Low income\", \n            \"Lower middle income\", \n            \"Upper middle income\", \n            \"High income\")))\n\n\nMake the income levels readable\n\nHow to Rotate Axis Labels in ggplot2?"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#much-better-visualizing-the-ordinal-variable-income-levels",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#much-better-visualizing-the-ordinal-variable-income-levels",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Much better: Visualizing the ordinal variable, income levels",
    "text": "Much better: Visualizing the ordinal variable, income levels\n\n\n\nggplot(gapm2, aes(x = income_levels, y = LifeExpectancyYrs)) +\n  geom_jitter(size = 1, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  labs(x = \"Income levels\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. income levels\",\n       caption = \"Diamonds = Income level averages\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20), \n        axis.text.x=element_text(angle = 20, vjust = 1, hjust=1))"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#how-can-we-code-this-variable",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#how-can-we-code-this-variable",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "How can we code this variable?",
    "text": "How can we code this variable?\nWe have two options:\n\n\n\n\nTreat the levels as nominal, and use reference cell coding\n\n\n\nLike we did with world regions\nThis option will not break the linearity assumption\nFor \\(g\\) categories of the variable, we will have \\(g-1\\) coefficients to estimate\n\n\n\n\n\n\nUse the ordinal values to score the levels and treat as a numerical variable\n\n\n\nEven if a variable is inherently ordered, we need to check that linearity holds if categories are represented as numbers\nThis way of coding preserves more power in the model (less coefficients to estimate means more power)\nOnly one coefficient to estimate"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#some-important-considerations-when-scoring-ordinal-variables",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#some-important-considerations-when-scoring-ordinal-variables",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Some important considerations when scoring ordinal variables",
    "text": "Some important considerations when scoring ordinal variables\n\nEven if a variable is inherently ordered, we need to check that linearity holds if categories are represented as numbers (more in next lessons)\n\nLinearity is an assumption of linear regression: that the relationship between X and Y is linear\n\n\n \n\nAssumes differences between adjacent groups are equal\n\nIncome levels are pre-set groups by Gapminder\nMight be hard to interpret “every 1-level increase in income level”\n\n\n \n\nIs the variable part of the main relationship that you are investigating? (even if linearity holds)\n\nIf yes, consider leaving as reference cell coding unless the interpretation makes sense\nIf no, and just needed as an adjustment in your model, then power benefit of scoring might be worth it!"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#check-that-linearity-holds-for-income-levels",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#check-that-linearity-holds-for-income-levels",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Check that linearity holds for income levels",
    "text": "Check that linearity holds for income levels\n\n\n\nUsing visual assessment, linearity holds for our income levels (more in next lessons)\nWe can use the ordinal encoding for income levels"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#poll-everywhere-question-4",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#poll-everywhere-question-4",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#ordinal-coding-scoring",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#ordinal-coding-scoring",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Ordinal coding / Scoring",
    "text": "Ordinal coding / Scoring\n\nMap each income level to a number\nUsually start at 1\n\n\n\n\nIncome Level\nScore\n\n\n\n\nLow income\n1\n\n\nLower middle income\n2\n\n\nUpper middle income\n3\n\n\nHigh income\n4\n\n\n\n\ngapm2 = gapm2 %&gt;%\n  mutate(income_num = as.numeric(income_levels))\nstr(gapm2$income_num)\n\n num [1:187] 1 3 3 4 2 4 3 2 4 4 ..."
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#run-the-model-with-the-scored-income",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#run-the-model-with-the-scored-income",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Run the model with the scored income",
    "text": "Run the model with the scored income\n\nmod_inc2 = lm(LifeExpectancyYrs ~ income_num, data = gapm2)\ntidy(mod_inc2) %&gt;% gt() %&gt;% tab_options(table.font.size = 37) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n54.01\n1.06\n51.03\n0.00\n    income_num\n6.25\n0.37\n16.91\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{\\textrm{LE}} &=  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot \\text{Income level} \\\\\n\\widehat{\\textrm{LE}} &=  54.01 + 6.25 \\cdot \\text{Income level}\n\\end{aligned}\\]\n\nKeep in mind: We cannot calculate the expected outcome outside of the scoring values\n\nFor example, we cannot find the mean life expectancy for an income level of 1.5"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#interpreting-the-model",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#interpreting-the-model",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Interpreting the model",
    "text": "Interpreting the model\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n54.01\n1.06\n51.03\n0.00\n51.92\n56.10\n    income_num\n6.25\n0.37\n16.91\n0.00\n5.52\n6.98\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{LE}} =  54.01 + 6.25 \\cdot \\text{Income level}\\]\n\nInterpreting the intercept: At an income level of 0, mean life expectancy is 54.01 (95% CI: 51.92, 56.10).\n\nNote: this does not make sense because there is no income level of 0!\n\nInterpreting the coefficient for income: For every 1-level increase in income level, mean life expectancy increases 6.25 years (95% CI: 5.52, 6.98)."
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#what-if-life-expectancy-vs.-income-level-looked-like-this",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#what-if-life-expectancy-vs.-income-level-looked-like-this",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "What if life expectancy vs. income level looked like this?",
    "text": "What if life expectancy vs. income level looked like this?\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo longer maintaining the linearity assumption\nNeed to use reference cell coding\n\n \n\nWe would fit the following model: \\[\\begin{aligned}\n\\textrm{LE} = & \\beta_0 + \\beta_1 \\cdot I(\\text{Lower middle income}) + \\\\ & \\beta_2 \\cdot I(\\text{Upper middle income}) + \\\\ & \\beta_3 \\cdot I(\\text{High income}) + \\epsilon\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#if-time",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#if-time",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "If time…",
    "text": "If time…\nLet’s walk through categorical variables that have multiple selections\n\nSo each group is not mutually exclusive\nWe could make an indicator for each category, but individuals could be a part of multiple categories\n\n \n\nAlso, thinking about income levels - can we combine two groups to make one??\n\n\n\nLesson 5: Categorical Covariates"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates_key_info.html#key-dates",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates\n\nHomework 1 due tomorrow 1/23 at 11pm"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf_key_info.html#key-dates",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag_key_info.html#key-dates",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates\n\nLab 2 due 2/7\nHomework 3 due 2/14\nLab 3 due 2/21"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval_key_info.html",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "If you finish an exit ticket before the start of class I will give you a 0 for that day\nExit ticket today: question on homework/lab due date\n\nThursday or Friday"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval_key_info.html#announcements",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "If you finish an exit ticket before the start of class I will give you a 0 for that day\nExit ticket today: question on homework/lab due date\n\nThursday or Friday"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval_key_info.html#key-dates",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html",
    "title": "Lesson 6: SLR: Checking model assumptions",
    "section": "",
    "text": "Describe the model assumptions made in linear regression using ordinary least squares\nDetermine if the relationship between our sampled X and Y is linear\nUse QQ plots to determine if our fitted model holds the normality assumption\nUse residual plots to determine if our fitted model holds the equality of variance assumption\n\n\n\n\nWe have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]\n\n\n\n\n\nRows: 188 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (2): life_expectancy_years_2011, female_literacy_rate_2011\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe residuals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "Let’s remind ourselves of the model that we have been working with",
    "text": "Let’s remind ourselves of the model that we have been working with\n\nWe have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    FemaleLiteracyRate\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{LE}} &= 50.9 + 0.232 \\cdot \\text{FLR}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "Our residuals will help us a lot in our diagnostics!",
    "text": "Our residuals will help us a lot in our diagnostics!\n\n\n\nThe residuals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#least-squares-model-assumptions-line",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#least-squares-model-assumptions-line",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "Least-squares model assumptions: LINE",
    "text": "Least-squares model assumptions: LINE\n \nThese are the model assumptions made in ordinary least squares:\n \n\n[L] Linearity of relationship between variables\n\n\n[I] Independence of the \\(Y\\) values\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (or residuals)\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n \nNote: These assumptions are baked into the population model. We look at the population parameters when we discuss these assumptions, but we use the estimated model with our data to check if the assumptions are held up."
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#l-linearity",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#l-linearity",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "L: Linearity",
    "text": "L: Linearity\n\n\n\nThe relationship between the variables is linear (a straight line):\n\nThe mean value of \\(Y\\) given \\(X\\), \\(\\mu_{y|x}\\) or \\(E[Y|X]\\), is a straight-line function of \\(X\\)\n\n\n\\[\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\]"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#i-independence-of-observations",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#i-independence-of-observations",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "I: Independence of observations",
    "text": "I: Independence of observations\n\nThe \\(Y\\)-values are statistically independent of one another\nExamples of when they are not independent, include\n\nrepeated measures (such as baseline, 3 months, 6 months)\ndata from clusters, such as different hospitals or families\n\nThis condition is checked by reviewing the study design and not by inspecting the data\n\n \n\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#poll-everywhere-question-1",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#poll-everywhere-question-1",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-normality",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-normality",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "N: Normality",
    "text": "N: Normality\n\n\n\nFor any fixed value of \\(X\\), \\(Y\\) has normal distribution.\n\nNote: This is not about \\(Y\\) alone, but \\(Y|X\\)\n\nEquivalently, the measurement (random) errors \\(\\epsilon_i\\) ’s normally distributed\n\nThis is more often what we check"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#e-equality-of-variance-of-the-residuals",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#e-equality-of-variance-of-the-residuals",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "E: Equality of variance of the residuals",
    "text": "E: Equality of variance of the residuals\n\nThe variance of \\(Y\\) given \\(X\\) (\\(\\sigma_{Y|X}^2\\)), is the same for any \\(X\\)\n\nWe use just \\(\\sigma^2\\) to denote the common variance\n\nThis is also called homoscedasticity"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#summary-of-line-model-assumptions",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#summary-of-line-model-assumptions",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "Summary of LINE model assumptions",
    "text": "Summary of LINE model assumptions\n\n\\(Y\\) values are independent (check study design!)\n\n\n\n\n\n\nThe distribution of \\(Y\\) given \\(X\\) is\n\nnormal\nwith mean \\(\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\)\nand common variance \\(\\sigma^2\\)\n\n\nThis means that the residuals are\n\nnormal\nwith mean = 0\nand common variance \\(\\sigma^2\\)\n\n\n\n\n\nIn mathematical form:\n\n\n\n\n\n\\(Y|X \\overset{\\text{i.i.d.}}{\\sim} N(\\beta_0 + \\beta_1X, \\sigma^2)\\)\n\n\\(\\epsilon \\overset{\\text{i.i.d.}}{\\sim} N(0, \\sigma^2)\\)"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#how-do-we-determine-if-our-model-follows-the-line-assumptions",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#how-do-we-determine-if-our-model-follows-the-line-assumptions",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "How do we determine if our model follows the LINE assumptions?",
    "text": "How do we determine if our model follows the LINE assumptions?\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nCheck if there is a linear relationship between the mean response (Y) and the explanatory variable (X)\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nCheck that the observations are independent\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\nCheck that the responses (at each level X) are normally distributed\n\nUsually measured through the residuals\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nCheck that the variance (or standard deviation) of the responses is equal for all levels of X\n\nUsually measured through the residuals"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#l-linearity-of-relationship-between-variables",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#l-linearity-of-relationship-between-variables",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "L: Linearity of relationship between variables",
    "text": "L: Linearity of relationship between variables\n\n\n\n\n\nIs the association between the variables linear?\n\n\n\n\nDiagnostic tool: Scatterplot of \\(X\\) vs. \\(Y\\)"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#poll-everywhere-question-2",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#poll-everywhere-question-2",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#i-independence-of-the-residuals-y-values",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#i-independence-of-the-residuals-y-values",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "I: Independence of the residuals (\\(Y\\) values)",
    "text": "I: Independence of the residuals (\\(Y\\) values)\n\nAre the data points independent of each other?\n\n \n\nDiagnostic tool: reviewing the study design and not by inspecting the data"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-normality-of-the-residuals",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-normality-of-the-residuals",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "N: Normality of the residuals",
    "text": "N: Normality of the residuals\n\nWe need to check if the errors/residuals (\\(\\epsilon_i\\)’s) are normally distributed\n\n \n\nDiagnostic tools:\n\nDistribution plots of residuals\nQQ plots of residuals\n\n\n \n\nExtra resource on how QQ plots are made"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-extract-models-residuals-in-r",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-extract-models-residuals-in-r",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "N: Extract model’s residuals in R",
    "text": "N: Extract model’s residuals in R\n\nFirst extract the residuals’ values from the model output using the augment() function from the broom package.\nGet a tibble with the orginal data, as well as the residuals and some other important values.\n\n\nmodel1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate, \n                data = gapm)\naug1 &lt;- augment(model1) \n\nglimpse(aug1)\n\nRows: 80\nColumns: 8\n$ LifeExpectancyYrs  &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 76.9, 58.…\n$ FemaleLiteracyRate &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 96.7, 85.…\n$ .fitted            &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.65980, 7…\n$ .resid             &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074, 2.3402…\n$ .hat               &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077123, 0.0…\n$ .sigma             &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.176043, 6…\n$ .cooksd            &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2.400993e…\n$ .std.resid         &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.47579667, 0.…"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-check-normality-with-usual-distribution-plots",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-check-normality-with-usual-distribution-plots",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "N: Check normality with “usual” distribution plots",
    "text": "N: Check normality with “usual” distribution plots\nNote that below I save each figure as an object, and then combine them together in one row of output using grid.arrange() from the gridExtra package\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_boxplot()\n\ngrid.arrange(hist1, density1, box1, nrow = 1)"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-normal-qq-plots-qq-quantile-quantile",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-normal-qq-plots-qq-quantile-quantile",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "N: Normal QQ plots (QQ = quantile-quantile)",
    "text": "N: Normal QQ plots (QQ = quantile-quantile)\n\nIt can be tricky to eyeball with a histogram or density plot whether the residuals are normal or not\nQQ plots are often used to help with this\n\n\n\n\nVertical axis: data quantiles\n\ndata points are sorted in order and\nassigned quantiles based on how many data points there are\n\nHorizontal axis: theoretical quantiles\n\nmean and standard deviation (SD) calculated from the data points\ntheoretical quantiles are calculated for each point, assuming the data are modeled by a normal distribution with the mean and SD of the data\n\n\n \n\nData are approximately normal if points fall on a line."
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-examples-of-normal-qq-plots-from-n100-observations",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-examples-of-normal-qq-plots-from-n100-observations",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "N: Examples of Normal QQ plots (from \\(n=100\\) observations)",
    "text": "N: Examples of Normal QQ plots (from \\(n=100\\) observations)\n\n\n\n\nNormal\n\nUniform\n\nT\n\nSkewed"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-examples-of-normal-qq-plots-from-n10-observations",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-examples-of-normal-qq-plots-from-n10-observations",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "N: Examples of Normal QQ plots (from \\(n=10\\) observations)",
    "text": "N: Examples of Normal QQ plots (from \\(n=10\\) observations)\n\n\n\n\nNormal\n\nUniform\n\nT\n\nSkewed"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-examples-of-normal-qq-plots-from-n1000-observations",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-examples-of-normal-qq-plots-from-n1000-observations",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "N: Examples of Normal QQ plots (from \\(n=1000\\) observations)",
    "text": "N: Examples of Normal QQ plots (from \\(n=1000\\) observations)\n\n\n\n\nNormal\n\nUniform\n\nT\n\nSkewed"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-we-can-compare-the-qq-plots-model-vs.-theoretical",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-we-can-compare-the-qq-plots-model-vs.-theoretical",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "N: We can compare the QQ plots: model vs. theoretical",
    "text": "N: We can compare the QQ plots: model vs. theoretical\n\n\n\nQQ plot from Life Expectancy vs. Female Literacy Rate Regression\n\n\nggplot(aug1, \n      aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line() \n\n\n\n\n\n\n\n\n\n\nSimulated QQ plot of Normal Residuals with \\(n = 80\\)\n\n\nggplot() +\n  stat_qq(aes(\n    sample = rnorm(80))) + \n  geom_abline(\n    intercept = 0, slope = 1, \n    color = \"blue\")"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-shapiro-wilk-test-of-normality",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-shapiro-wilk-test-of-normality",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "N: Shapiro-Wilk Test of Normality",
    "text": "N: Shapiro-Wilk Test of Normality\n\nGoodness-of-fit test for the normal distribution: Is there evidence that our residuals are from a normal distribution?\nHypothesis test:\n\n\\[\\begin{aligned}\nH_0 & : \\text{data are from a normally distributed population} \\\\\nH_1 & : \\text{data are NOT from a normally distributed population}\n\\end{aligned}\\]\n\n\n\nshapiro.test(aug1$.resid)\n\n\n    Shapiro-Wilk normality test\n\ndata:  aug1$.resid\nW = 0.90575, p-value = 2.148e-05\n\n\n\n\n\nConclusion\n\n\nReject the null. Data are not from a normal distribution."
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#e-equality-of-variance-of-the-residuals-1",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#e-equality-of-variance-of-the-residuals-1",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "E: Equality of variance of the residuals",
    "text": "E: Equality of variance of the residuals\n\nHomoscedasticity: How do we determine if the variance across X values is constant?\nDiagnostic tool: residual plot"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#e-creating-a-residual-plot",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#e-creating-a-residual-plot",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "E: Creating a residual plot",
    "text": "E: Creating a residual plot\n\n\n\n\\(x\\) = explanatory variable from regression model\n\n(or the fitted values for a multiple regression)\n\n\\(y\\) = residuals from regression model\n\n\nggplot(aug1, \n       aes(x = FemaleLiteracyRate, \n           y = .resid)) + \n  geom_point(size = 2) +\n  geom_abline( intercept = 0, slope = 0,\n    size = 2, color = \"#FF8021\") +\n  labs(title = \"Residual plot\") +\n  theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 30))"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#autoplot-can-be-a-helpful-tool",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#autoplot-can-be-a-helpful-tool",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "autoplot() can be a helpful tool",
    "text": "autoplot() can be a helpful tool\n\nlibrary(ggfortify)\nautoplot(model1) + theme(text=element_text(size=14))"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#summary-of-the-assumptions-and-their-diagnostic-tool",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#summary-of-the-assumptions-and-their-diagnostic-tool",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "Summary of the assumptions and their diagnostic tool",
    "text": "Summary of the assumptions and their diagnostic tool\n\n\n\n\n\n\n\n\nAssumption\nWhat needs to hold?\nDiagnostic tool\n\n\n\n\nLinearity\n\\(\\text{}\\)\n\nRelationship between \\(X\\) and \\(Y\\) is linear\n\n\nScatterplot of \\(Y\\) vs. \\(X\\)\n\n\\(\\text{}\\)\n\n\nIndependence\n\\(\\text{}\\)\n\nObservations are independent from each other\n\n\nStudy design\n\n\\(\\text{}\\)\n\n\nNormality\n\\(\\text{}\\)\n\nResiduals (and thus \\(Y|X\\)) are normally distributed\n\n\nQQ plot of residuals\nDistribution of residuals\n\n\n\nEquality of variance\n\\(\\text{}\\)\n\nVariance of residuals (and thus \\(Y|X\\)) is same across \\(X\\) values (homoscedasticity)\n\n\nResidual plot\n\n\\(\\text{}\\)"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates_key_info.html",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Question in exit ticket: Could you please repeat questions for the live-streamers?\n\nI will try to remember! This is one of those cases where the recording is not the exact substitute for the class. It’s just hard for me to think about the mic picking up everything while I’m speaking.\n\nGoing through labs\n\nPlease submit your labs using html files!! (I took off a point if you submitted a qmd only)\nPlease write your answers outside of the task box and below it\nI have adjusted grades this morning and gave some important feedback on your research questions\n\nPlease make sure to review this and integrate into the next lab!\n\nYour grade might have changed - please check my comments\n\nYour grade may not have changed - please check my comments\n\n\n\n\n\n\n\n\nOffice hours: Fridays 10-11am\n\nOnline Zoom Link\n\nEmail: ainsworl@ohsu.edu\n\n\n\n\n\nOffice hours: Mondays 4:30-5:30 pm\n\nOnline Webex Link\n\nEmail: handka@ohsu.edu\n\n\n\n\n\nOffice hours: Thursdays 10-11am\n\nOnline Webex Link\n\nEmail: sunm@ohsu.edu"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates_key_info.html#announcements",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Question in exit ticket: Could you please repeat questions for the live-streamers?\n\nI will try to remember! This is one of those cases where the recording is not the exact substitute for the class. It’s just hard for me to think about the mic picking up everything while I’m speaking.\n\nGoing through labs\n\nPlease submit your labs using html files!! (I took off a point if you submitted a qmd only)\nPlease write your answers outside of the task box and below it\nI have adjusted grades this morning and gave some important feedback on your research questions\n\nPlease make sure to review this and integrate into the next lab!\n\nYour grade might have changed - please check my comments\n\nYour grade may not have changed - please check my comments\n\n\n\n\n\n\n\n\nOffice hours: Fridays 10-11am\n\nOnline Zoom Link\n\nEmail: ainsworl@ohsu.edu\n\n\n\n\n\nOffice hours: Mondays 4:30-5:30 pm\n\nOnline Webex Link\n\nEmail: handka@ohsu.edu\n\n\n\n\n\nOffice hours: Thursdays 10-11am\n\nOnline Webex Link\n\nEmail: sunm@ohsu.edu"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#why-slr-ish",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#why-slr-ish",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Why “SLR-ish”?",
    "text": "Why “SLR-ish”?\n\nThe strict definition of simple linear regression: only two variables that are BOTH continuous\n\n \n\nCommon (but kinda wrong) use of simple linear regression: only two variables with outcome continuous and predictor not specified\n\n \n\nI’m including multi-level categorical covariates in SLR mostly because it’s easier to learn now!"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#linear-regression-with-a-categorical-covariate-12",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#linear-regression-with-a-categorical-covariate-12",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Linear regression with a categorical covariate (1/2)",
    "text": "Linear regression with a categorical covariate (1/2)\n\n\nBad option for visualization:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_point() +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nGood option for visualization:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_jitter(size = 1, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\",\n       caption = \"Diamonds = region averages\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nUsed geom_jitter()\n\n\nGood option for visualization:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_boxplot() +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates.html#linear-regression-with-a-categorical-covariate-22",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates.html#linear-regression-with-a-categorical-covariate-22",
    "title": "Lesson 5: SLR-ish: Categorical Covariates",
    "section": "Linear regression with a categorical covariate (2/2)",
    "text": "Linear regression with a categorical covariate (2/2)\n\n\n\nWhen using a categorical covariate/predictor (that is not ordered),\n\nWe do NOT, technically, find a best-fit line\n\nInstead we model the means of the outcome\n\nFor the different levels of the categorical variable\n\n\n \n\nIn 511, we used Kruskal-Wallis test and our ANOVA table to test if groups means were statistically different from one another\nWe can do this using linear models AND we can include other variables in the model"
  },
  {
    "objectID": "homework/HW_03.html#question-2",
    "href": "homework/HW_03.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2\nThis question and data are adapted from this textbook.\nIn an experiment designed to describe the dose–response curve for vitamin K, individual rats were depleted of their vitamin K reserves and then fed dried liver for 4 days at different dosage levels. The response of each rat was measured as the concentration of a clotting agent needed to clot a sample of its blood in 3 minutes. The results of the experiment on 12 rats are given in the following table; values are expressed in common logarithms for both dose and response.\n\nclot = read_excel(here(\"./data/CH05Q09.xls\"))\nclot %&gt;% gt() %&gt;%\n  cols_label(RAT = md(\"**Rat**\"),\n             LOGCONC = md(\"**Log10 Concentration (Y)**\"),\n             LOGDOSE = md(\"**Log10 Dose (X)**\"))\n\n\n\n\n\n  \n    \n      Rat\n      Log10 Concentration (Y)\n      Log10 Dose (X)\n    \n  \n  \n    1\n2.65\n0.18\n    2\n2.25\n0.33\n    3\n2.26\n0.42\n    4\n1.95\n0.54\n    5\n1.72\n0.65\n    6\n1.60\n0.75\n    7\n1.55\n0.83\n    8\n1.32\n0.92\n    9\n1.13\n1.01\n    10\n1.07\n1.04\n    11\n0.95\n1.09\n    12\n0.88\n1.15\n  \n  \n  \n\n\n\n\nUse the log-transformed values as given in the dataset.\nUse the following scatterplot to build your answers off of:\n\n\n\n\n\n\n\n\n\n\nPart a\nFit a linear regression model to the data and add the regression line to the plot.\n\n\nPart b\nUse R to create the ANOVA table for the regression described in the exercise.\n\n\nPart c\nUsing the F-test, determine whether there is an association between the log10 concentration and log10 dose.\n\n\n\n\n\n\nNote\n\n\n\nMake sure to include all needed steps for an F-test. Calculating the F test statistic (step 5) is not needed if you use the ANOVA table. Make sure your conclusion connects back the research context.\n\n\n\n\nPart d\nRewrite your hypothesis test in Part c to show the null and alternative models that we are testing. Did we reject the smaller (reduced) model?\n\n\n\n\n\n\nNote\n\n\n\nYou do not need to go through the hypothesis test process again. A quick statement on rejecting or not is okay.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you prefer to write out the models by hand, remember that you can take a picture of your work and insert it into this document. HW0 can be a good reference for how we’ve done this before."
  },
  {
    "objectID": "homework/HW_03.html#question-3",
    "href": "homework/HW_03.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\nWe will continue to work with the study and dataset from Question 2 above.\n\nPart a\nFind the correlation coefficient between the two variables. Is the value consistent with your description of the relationship in Question 2? Why or why not?\n\n\nPart b\nCalculate the coefficient of determination using linear regression summary output. Can we also calculate the coefficient of determination from the ANOVA in Question 2?\n\n\nPart c\nGive an interpretation of the coefficient of determination in the context of the study.\n\n\nQuestion 1\nA high respiratory rate is a potential diagnostic indicator of respiratory infection in children. To judge whether a respiratory rate is “high” however, a physician must have a clear picture of the distribution of normal rates. To this end, Italian researchers measured the respiratory rates (in breaths/minute) of 618 children between the ages of 15 days and 3 years (measured in months).\nThe data and problem framing came from the Sleuth3 package. Please make sure to run the following code to load the data. You can directly access the dataset ex0824 from the package. I have included a new assignment of the data to q1_data if you would like to use that.\n\nif(!require(Sleuth3)) { install.packages(\"Sleuth3\"); library(Sleuth3) }\nq1_data = ex0824 \n\n\nPart a\nCreate a scatterplot of the dependent and independent variables with both the best-fit line and a smoothed curve through the points. Describe the relationship between the dependent and independent variables, and also comment on whether you think it is reasonable to use a linear regression to model the relationship.\n\n\nPart b\nWrite out the population regression model for the simple linear regression model. Please leave the variables untransformed for now.\n\n\nPart c\nFit the regression model, display the regression table, and write out the fitted regression line.\n\n\nPart d\nAssess the normality of the model’s fitted residuals by creating a histogram, density plot, and boxplot of the residuals to visually inspect the distribution of the residuals, and describe any deviations from normality.\n\n\nPart e\nAssess the normality of the model’s fitted residuals by creating QQ plot of the residuals.\nBonus work, but not required: Compare the QQ plot to 4 such plots simulated from normal data, and discuss why or why not the residuals could have come from a normal distribution.\n\n\nPart f\nTest the normality of the model’s fitted residuals and comment on whether the test’s conclusion is consistent with your visual inspection or not. Make sure to include the hypotheses, needed R code, and a conclusion to the test based on the p-value (as shown in these slides).\n\n\nPart g\nCreate a residual plot using ggplot and the residuals. Discuss what this means in the context of our model assumptions.\n\n\nPart h\nDetermine whether there are any observations with high leverage. Please use the cutoff, \\(h_i &gt; 6/n\\). If there are observations with high leverage, print the observations and state how many high leverage points there are.\n\n\nPart i\nPrint the 10 observations with highest Cook’s distance. If there are observations with high Cook’s distance (\\(d_i &gt;1\\)), state how many observations have high Cook’s distance.\n\n\nPart j\nCreate a histogram for rate. Describe its distribution shapes.\n\n\nPart k\nUsing the above histogram, and Tukey’s ladder of transformations, discuss the range of transformations that will be appropriate for Rate. Explain your reasoning.\nThen use gladder() to decide on two possible transformations. Explain your reasoning.\nNote: questions below will ask about model fit with the transformations. For now, just explain why you chose the two that you did.\n\n\nPart l\nAdd the two rate transformations you chose above to the dataset. You do not need to print any output, just make sure the code is visible.\n\n\nPart m\nCreate scatterplots using two transformed rates and age. Discuss if either transformation potentially improves the model fit. Explain why or why not. Note: including lines will help!\n\n\nPart n\nUsing one of the transoformed outcomes, fit the regression model, display the regression table, and write out the fitted regression line.\n\n\nPart o\nAssess the normality of the model’s fitted residuals by creating QQ plot of the residuals. Does the transformation improve the QQ plot?\n\n\nPart p\nCreate a residual plot using ggplot and the residuals. Discuss what this means in the context of our model assumptions. Does the transformation improve our model assumptions?\n\n\nPart q\nBetween the model with the untransformed outcome and the transformed outcome, which would you recommend using for analysis? (Hint: there are pros and cons to both models)\n\n\n\nQuestion 2\nThis question uses the same dataset as HW 2, question 1.\nThis question is based on data collected as part of an observational study of patients who suffered from stroke.\nDataset: The main goal was to study various psychological factors: optimism, fatalism, depression, spirituality, and their relationship with stroke severity and other health outcomes among the study participants. Data were collected using questionnaires during a baseline interview and also medical chart review. More information about this study can be found in the article Fatalism, optimism, spirituality, depressive symptoms and stroke outcome: a population based analysis.\nThe dataset that you will work with is called completedata.sas7bdat. The two variables we are interested in are:\n\nCovariate 1: Fatalism (larger values indicate that the individual feels less control of their life)\n\nPotential scores range from 8 to 40\n\nCovariate 2: Optimism (larger values indicate that the individual feels higher levels of optimism)\n\nPotential scores range from 6 to 24\n\nCovariate 3: Spirituality (larger values indicate that the individual has more belief in a higher power)\n\nPotential scores range from 2 to 8\n\nOutcome: Depression (larger values imply increased depression)\n\nPotential scores range from 0 to 27\n\n\nFor our homework purposes we will assume each variable is continuous.\n\ndep_df = read_sas(here(\"./data/completedata.sas7bdat\"))\n\n\nPart a\nFit the regression model with all the covariates (Fatalism, Optimism, Spirituality), display the regression table, and write out the fitted regression line.\n\n\nPart b\nInterpret each coefficient (\\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\)).\nDoes the intercept make sense for the range of values that each covariate can take? Explain.\n\n\nPart c\nRecall in Homework 2, we ran a simple linear regression model for Depression vs. Fatalism with the following interpretation for the coefficient: For every 1 point higher fatalism score, there is an expected difference of 0.25 points higher depression score (95%CI: 0.17, 0.32).\nDoes the addition of Optimism and Spirituality change our coefficient estimate for Fatalism? (No need for an official hypothesis test here. I just want us to note some differences.)\n\n\nPart d\nFrom the fitted regression model, calculate the regression line when Optimism score is 10 and Spirituality score is 6.\n\n\nPart e\nDoes at least one of the covariates contribute significantly to the prediction of Depression? (Note: this is an overall test. Please follow the hypothesis test steps. To complete step 4-6, simply output your ANOVA table.)\n\n\nPart f\nDoes the addition of Spirituality add significantly to the prediction of Depression achieved by Fatalism and Optimism?\n\n\nPart g\nDoes the addition of Spirituality and Optimism add significantly to the prediction of Depression achieved by Fatalism?"
  },
  {
    "objectID": "homework/HW_02.html#questions",
    "href": "homework/HW_02.html#questions",
    "title": "Homework 2",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nThis homework assignment is based on data collected as part of an observational study of patients who suffered from stroke.\nDataset: The main goal was to study various psychological factors: optimism, fatalism, depression, spirituality, and their relationship with stroke severity and other health outcomes among the study participants. Data were collected using questionnaires during a baseline interview and also medical chart review. More information about this study can be found in the article Fatalism, optimism, spirituality, depressive symptoms and stroke outcome: a population based analysis.\nThe dataset that you will work with is called completedata.sas7bdat. It is SIMILAR but does not exactly match the data in the article. It contains information on complete cases (i.e. excludes participants who had missing data on one or more variables of interest) who suffered a stroke. The two variables we are interested in are:\n\nCovariate: Fatalism (larger values indicate that the individual feels less control of their life)\n\nScores range from 8 to 40\n\nOutcome: Depression (larger values imply increased depression)\n\nScores range from 0 to 27\n\n\nFor our homework purposes we will assume they are continuous.\n\nfatal_dep = read_sas(here(\"./data/completedata.sas7bdat\"))\n\n\nPart a\nPlot the data, with title and axis labels, for Depression (y-axis) vs. Fatalism (x-axis). Comment on what you see.\n\n\nPart b\nFit a linear regression model to estimate the association between the predictor Fatalism and the outcome Depression.\nInterpret the slope and intercept. Does the intercept make sense?\n\n\n\n\n\n\nNote\n\n\n\nMake sure to include the confidence interval. Whenever asked to interpret coefficients, you must include confidence intervals. Also, the “units” for fatalism and depression are scores.\n\n\n\n\nPart c\nIn your dataset, make a new variable FatalismC, equal to Fatalism centered at its median (C is for centered).\n\\[\n\\text{FatalismC} = \\text{Fatalism} - \\text{median of Fatalism}\n\\]\nThis is one way of centering a variable, and can be used when the intercept estimate does not make sense. (Hint: the mutate() function will work well here!)\nPlot the data, with title and axis labels, for Depression (y-axis) vs. FatalismC (x-axis).\n\n\nPart d\nRe-run the regression from Part b using this new variable for FatalismC. Interpret the new slope and intercept. Which of the following are the same as Part b: intercept, slope?\n\n\n\n\n\n\nNote\n\n\n\nMake sure to include the confidence interval. Whenever asked to interpret coefficients, you must include confidence intervals. Also, the “units” for the centered fatalism is still the score.\n\n\n\n\nPart e\nFrom the above interpretations, what would be the equivalent conclusion from a hypothesis test for the association between Depression and Fatalism?\n\n\n\n\n\n\nNote\n\n\n\nYou do not need to go through the whole process for the hypothesis test. You only need to state whether it is rejected or not and site the confidence interval as evidence.\n\n\n\n\n\nQuestion 2\nFor this problem we will be using the penguins dataset from the palmerpenguins R package. We will look at the association between flipper length of penguins (measured in mm) and specific species of penguins.\nDescription from help file:\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\n\nMore info about the data.\n\n# first install the palmerpenguins package\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\ndata(penguins)\n\n# run the command below to learn more about the variables in the penguins dataset\n# ?penguins\n\n\nPart a\nCalculate the average flipper lengths stratified by each of the penguin species.\n\n\nPart b\nMake a scatterplot (with jitter) of flipper lengths by species, and include diamond-shape points for the averages of the flipper lengths for each of the species.\n\n\nPart c\nWrite out the fitted regression equation that models the flipper length by penguin species. Use LaTeX math markup or insert an image of your equation. Do not yet insert values for the regression coefficients, i.e. use the generic coefficients \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1\\). Use Adelie as the reference level.\n\n\nPart d\nRun the linear regression of flipper lengths vs. species in R, and display the regression table output. Which species did R choose as the reference level, and how did you determine this?\n\n\nPart e\nHow do we interpret each of the regression coefficients for this model? Write out a separate interpretation for each of the coefficients.\n\n\nPart f\nCalculate the mean flipper length (and the 95% CI) of penguins in the Chinstrap and Gentoo species using the predict() function."
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#section",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#section",
    "title": "Lesson 6: SLR: More inference",
    "section": "",
    "text": "Recall, F-statistic is proportion of variation explained by the model to the variation not explained by model\nIf there is strong evidence that \\(F&gt;1\\), then we can say"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#we-can-extend-our-look-at-the-f-test",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#we-can-extend-our-look-at-the-f-test",
    "title": "Lesson 6: SLR: More inference",
    "section": "We can extend our look at the F-test",
    "text": "We can extend our look at the F-test\nWe can create a hypothesis test for more than one coefficient at a time…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=\\beta_2=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_1\\neq0\\) and/or \\(\\beta_2\\neq0\\)\n\n\n\n\nin a slightly different way…\n\n\n\n\n\n\nNull model\n\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\nSmaller (reduced) model\n\n\n\n\n\n\nAlternative* model\n\n\n\n\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\)\nLarger (full) model\n\n\n\n\n\n*This is not quite the alternative, but if we reject the null, then this is the model we move forward with"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#building-a-very-important-toolkit-three-types-of-tests",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#building-a-very-important-toolkit-three-types-of-tests",
    "title": "Lesson 6: SLR: More inference",
    "section": "Building a very important toolkit: three types of tests",
    "text": "Building a very important toolkit: three types of tests\n\n\nOverall test (in a couple classes)\n\n\nDoes at least one of the covariates/predictors contribute significantly to the prediction of Y?\n\n\n\n\nTest for addition of a single variable (covariate subset test)\n\n\nDoes the addition of one particular covariate add significantly to the prediction of Y achieved by other covariates already present in the model?\n\n\n\n\nTest for addition of group of variables (covariate subset test) (in a couple classes)\n\n\nDoes the addition of some group of covariates add significantly to the prediction of Y achieved by other covariates already present in the model?"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#when-running-a-f-test-for-linear-models",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#when-running-a-f-test-for-linear-models",
    "title": "Lesson 6: SLR: More inference",
    "section": "When running a F-test for linear models…",
    "text": "When running a F-test for linear models…\n\nWe need to define a larger, full model (more parameters)\nWe need to define a smaller, reduced model (fewer parameters)\nUse the F-statistic to decide whether or not we reject the smaller model\n\nThe F-statistic compares the SSE of each model to determine if the full model explains a significant amount of additional variance\n\n\n\n\n \n\\[\nF = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\n\\]\n\n\n\\(SSE(R) \\geq SSE(F)\\)\nNumerator measures difference in unexplained variation between the models\n\nBig difference = added parameters greatly reduce the unexplained variation (increase explained variation)\nSmaller difference = added parameters don’t reduce the unexplained variation\n\nTake ratio of difference to the unexplained variation in the full model"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#testing-association-between-continuous-outcome-and-categorical-vsariable",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#testing-association-between-continuous-outcome-and-categorical-vsariable",
    "title": "Lesson 6: SLR: More inference",
    "section": "Testing association between continuous outcome and categorical vsariable",
    "text": "Testing association between continuous outcome and categorical vsariable\n\nBefore we used the F-test (or t-test) to determine association between two continuous variables\nWe CANNOT use the t-test to determine association between a continuous outcome and a multi-level categorical variable\nWe CAN use the F-test to do this!\n\n \n\nWe can use the t-test or F-test for a categorical variable with only 2 levels"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#f-test-general-steps-for-hypothesis-test-for-multiple-coefficients",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#f-test-general-steps-for-hypothesis-test-for-multiple-coefficients",
    "title": "Lesson 6: SLR: More inference",
    "section": "F-test: general steps for hypothesis test for multiple coefficients",
    "text": "F-test: general steps for hypothesis test for multiple coefficients\n\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nOften, we are curious if the coefficient is 0 or not:\n\\[\\begin{align}\nH_0 :& \\beta_1 = ... = \\beta_j = 0\\\\\n\\text{vs. } H_A:& \\beta_1 \\neq 0 \\text{ and/or } \\\\\n&\\beta_2 \\neq 0 ... \\text{ and/or } \\beta_j \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=1\\) and denominator \\(df=n-(k+1)\\).\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic for \\(\\widehat\\beta_1\\) is\n\\[F = \\frac{MSR}{MSE}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{1, n-(k+1)} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject: \\(P(F_{1, n-(k+1)} &gt; F) &lt; \\alpha\\)\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at the \\(100\\alpha\\%\\) significiance level. There is (sufficient/insufficient) evidence that there is significant association between (\\(Y\\)) and (\\(X\\)) (p-value = \\(P(F_{1, n-(k+1)} &gt; F)\\))."
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-1",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-1",
    "title": "Lesson 6: SLR: More inference",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\)\n\nSteps 1-4 are setting up our hypothesis test: not much change from the general steps\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nWe are testing if the slope is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=1\\) and denominator \\(df=n-2 = 80-2\\).\n\n\nnobs(model1)\n\n[1] 80"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-24-1",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-24-1",
    "title": "Lesson 6: SLR: More inference",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/4)\n\n\n\nCompute the value of the test statistic\n\n\n\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1\n2052.812\n2052.81234\n54.4136\n1.501286e-10\n    Residuals\n78\n2942.635\n37.72609\nNA\nNA\n  \n  \n  \n\n\n\n\n\nOption 1: Calculate the test statistic using the values in the ANOVA table\n\n\\[F = \\frac{MSR}{MSE} = \\frac{2052.81}{37.73}=54.414\\]\n\nOption 2: Get the test statistic value (F) from the ANOVA table\n\n\nI tend to skip this step because I can do it all with step 6"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-34-1",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-34-1",
    "title": "Lesson 6: SLR: More inference",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (3/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (3/4)\n\n\n\nCalculate the p-value\n\n\n\n\nAs per Step 4, test statistic \\(F\\) can be modeled by a \\(F\\)-distribution with \\(df1 = 3\\) and \\(df2 = n-4\\).\n\nWe had 80 countries’ data, so \\(n=80\\)\n\nOption 1: Use pf() and our calculated test statistic\n\n\n# p-value is ALWAYS the right tail for F-test\npf(33.5331, df1 = 3, df2 = 76, lower.tail = FALSE)\n\n[1] 6.514508e-14\n\n\n\nOption 2: Use the ANOVA table\n\n\nanova(model2) %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    four_regions\n3\n2845.643\n948.5477\n33.53311\n6.514481e-14\n    Residuals\n76\n2149.804\n28.2869\nNA\nNA"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-44-1",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-44-1",
    "title": "Lesson 6: SLR: More inference",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (4/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (4/4)\n\n\n\nWrite conclusion for the hypothesis test\n\n\n\nWe reject the null hypothesis that all three coefficients are equal to 0 at the \\(5\\%\\) significance level. There is sufficient evidence that there is association between female life expectancy and the country’s world region (p-value &lt; 0.0001)."
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#did-you-notice-anything-about-the-p-value-1",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#did-you-notice-anything-about-the-p-value-1",
    "title": "Lesson 6: SLR: More inference",
    "section": "Did you notice anything about the p-value?",
    "text": "Did you notice anything about the p-value?\nThe p-value of the t-test and F-test are the same!!\n\nFor the t-test:\n\n\ntidy(model1) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    FemaleLiteracyRate\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\nFor the F-test:\n\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1\n2052.812\n2052.81234\n54.4136\n1.501286e-10\n    Residuals\n78\n2942.635\n37.72609\nNA\nNA\n  \n  \n  \n\n\n\n\nThis is true when we use the F-test for a single coefficient!"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#f-test-general-steps-for-hypothesis-test-for-j-level-categorical-variable",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#f-test-general-steps-for-hypothesis-test-for-j-level-categorical-variable",
    "title": "Lesson 6: SLR: More inference",
    "section": "F-test: general steps for hypothesis test for \\(j\\)-level categorical variable",
    "text": "F-test: general steps for hypothesis test for \\(j\\)-level categorical variable\n\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nOften, we are curious if the coefficient is 0 or not:\n\\[\\begin{align}\nH_0 :& \\beta_1 = ... = \\beta_j = 0\\\\\n\\text{vs. } H_A:& \\beta_1 \\neq 0 \\text{ and/or } \\\\\n&\\beta_2 \\neq 0 ... \\text{ and/or } \\beta_j \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=1\\) and denominator \\(df=n-(k+1)\\).\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic for \\(\\widehat\\beta_1\\) is\n\\[F = \\frac{MSR}{MSE}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{1, n-(j+1)} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject: \\(P(F_{1, n-(j+1)} &gt; F) &lt; \\alpha\\)\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at the \\(100\\alpha\\%\\) significiance level. There is (sufficient/insufficient) evidence that there is significant association between (\\(Y\\)) and (\\(X\\)) (p-value = \\(P(F_{1, n-(j+1)} &gt; F)\\))."
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#testing-association-between-continuous-outcome-and-categorical-variable",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#testing-association-between-continuous-outcome-and-categorical-variable",
    "title": "Lesson 6: SLR: More inference",
    "section": "Testing association between continuous outcome and categorical variable",
    "text": "Testing association between continuous outcome and categorical variable\n\nBefore we used the F-test (or t-test) to determine association between two continuous variables\nWe CANNOT use the t-test to determine association between a continuous outcome and a multi-level categorical variable\nWe CAN use the F-test to do this!\n\n \n\nWe can use the t-test or F-test for a categorical variable with only 2 levels"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#lets-say-we-want-to-test-the-association-between-life-expectancy-and-world-region",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#lets-say-we-want-to-test-the-association-between-life-expectancy-and-world-region",
    "title": "Lesson 6: SLR: More inference",
    "section": "Let’s say we want to test the association between life expectancy and world region",
    "text": "Let’s say we want to test the association between life expectancy and world region\n\n\n\\[\\begin{aligned}\n\\widehat{\\textrm{LE}} = & 61.96 + 13.64 \\cdot I(\\text{Americas}) + \\\\ &7.33 \\cdot I(\\text{Asia}) + 14.1 \\cdot I(\\text{Europe}) \\\\\n\\widehat{\\textrm{LE}} = & \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) + \\\\ & \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\n\\end{aligned}\\]\n\nWe need to figure out if the model with world region explains significantly more variation than the model without world region!"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-world-region",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-world-region",
    "title": "Lesson 6: SLR: More inference",
    "section": "Life expectancy example: hypothesis test for world region",
    "text": "Life expectancy example: hypothesis test for world region\n\nSteps 1-4 are setting up our hypothesis test: not much change from the general steps\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nWe are testing if the slope is 0 or not:\n\\[\\begin{align}\nH_0 :& \\beta_1 = \\beta_2 = \\beta_3 0\\\\\n\\text{vs. } H_A:& \\beta_1 \\neq 0 \\text{ and/or } \\beta_2 \\neq 0 \\text{ and/or } \\beta_3 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=j\\) and denominator \\(df=n-(j+1) = 80-(3+1)\\).\n\n\nnobs(model1)\n\n[1] 80"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-world-region-24",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-world-region-24",
    "title": "Lesson 6: SLR: More inference",
    "section": "Life expectancy example: hypothesis test for world region (2/4)",
    "text": "Life expectancy example: hypothesis test for world region (2/4)\n\n\n\nCompute the value of the test statistic\n\n\n\n\nmodel2 &lt;- gapm %&gt;% lm(formula = LifeExpectancyYrs ~ four_regions)\nanova(model2) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    four_regions\n3\n2845.643\n948.5477\n33.53311\n6.514481e-14\n    Residuals\n76\n2149.804\n28.2869\nNA\nNA\n  \n  \n  \n\n\n\n\n\nOption 1: Calculate the test statistic using the values in the ANOVA table\n\n\\[F = \\frac{MSR}{MSE} = \\frac{948.5476696}{28.2869012} = 33.5331065\\]\n\nOption 2: Get the test statistic value (F) from the ANOVA table\n\n\nI tend to skip this step because I can do it all with step 6"
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval.html#correlation-coefficient-r-or-r",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval.html#correlation-coefficient-r-or-r",
    "title": "Lesson 6: SLR: More inference",
    "section": "Correlation coefficient (\\(r\\) or \\(R\\))",
    "text": "Correlation coefficient (\\(r\\) or \\(R\\))\n\n\nThe (Pearson) correlation coefficient \\(r\\) of variables \\(X\\) and \\(Y\\) can be computed using the formula:\n\\[\\begin{aligned}\nr  & = \\frac{\\sum_{i=1}^n (X_i - \\overline{X})(Y_i - \\overline{Y})}{\\Big(\\sum_{i=1}^n (X_i - \\overline{X})^2 \\sum_{i=1}^n (Y_i - \\overline{Y})^2\\Big)^{1/2}} \\\\\n& = \\frac{SSXY}{\\sqrt{SSX \\cdot SSY}}\n\\end{aligned}\\]\nwe have the relationship\n\\[\\widehat{\\beta}_1 = r\\frac{SSY}{SSX},\\ \\ \\text{or},\\ \\  r = \\widehat{\\beta}_1\\frac{SSX}{SSY}\\]"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#poll-everywhere-question-3",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#poll-everywhere-question-3",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#least-squares-model-assumptions-line-1",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#least-squares-model-assumptions-line-1",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "Least-squares model assumptions: LINE",
    "text": "Least-squares model assumptions: LINE\n \nThese are the model assumptions made in ordinary least squares:\n \n\n[L] Linearity of relationship between variables\n\n\n[I] Independence of the \\(Y\\) values\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\n[E] Equality of variance of the residuals (homoscedasticity)"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#l-linearity-1",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#l-linearity-1",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "L: Linearity",
    "text": "L: Linearity\n\n\n\nThe relationship between the variables is linear (a straight line):\n\nThe mean value of \\(Y\\) given \\(X\\), \\(\\mu_{y|x}\\) or \\(E[Y|X]\\), is a straight-line function of \\(X\\)\n\n\n\\[\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\]"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#i-independence-of-observations-1",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#i-independence-of-observations-1",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "I: Independence of observations",
    "text": "I: Independence of observations\n\nThe \\(Y\\)-values are statistically independent of one another\nExamples of when they are not independent, include\n\nrepeated measures (such as baseline, 3 months, 6 months)\ndata from clusters, such as different hospitals or families\n\nThis condition is checked by reviewing the study design and not by inspecting the data\n\n \n\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-normality-1",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-normality-1",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "N: Normality",
    "text": "N: Normality\n\n\n\nFor any fixed value of \\(X\\), \\(Y\\) has normal distribution.\n\nNote: This is not about \\(Y\\) alone, but \\(Y|X\\)\n\nEquivalently, the measurement (random) errors \\(\\epsilon_i\\) ’s normally distributed\n\nThis is more often what we check"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#summary-of-line-model-assumptions-1",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#summary-of-line-model-assumptions-1",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "Summary of LINE model assumptions",
    "text": "Summary of LINE model assumptions\n\n\\(Y\\) values are independent (check study design!)\n\n\n\n\n\n\nThe distribution of \\(Y\\) given \\(X\\) is\n\nnormal\nwith mean \\(\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\)\nand common variance \\(\\sigma^2\\)\n\n\nThis means that the residuals are\n\nnormal\nwith mean = 0\nand common variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#how-do-we-determine-if-our-model-follows-the-line-assumptions-1",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#how-do-we-determine-if-our-model-follows-the-line-assumptions-1",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "How do we determine if our model follows the LINE assumptions?",
    "text": "How do we determine if our model follows the LINE assumptions?\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nCheck if there is a linear relationship between the mean response (Y) and the explanatory variable (X)\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nCheck that the observations are independent\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\nCheck that the responses (at each level X) are normally distributed\n\nUsually measured through the residuals\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nCheck that the variance (or standard deviation) of the responses is equal for all levels of X\n\nUsually measured through the residuals"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#e-equality-of-variance-of-the-residuals-2",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#e-equality-of-variance-of-the-residuals-2",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "E: Equality of variance of the residuals",
    "text": "E: Equality of variance of the residuals\n\nHomoscedasticity: How do we determine if the variance across X values is constant?\nDiagnostic tool: residual plot"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions_key_info.html",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "From exit ticket at 11:15am on 1/28, everyone either voted for Friday or “no preference”\n\nHW and labs will be due on Friday now!!\nIncluding HW 2!\n\nMake sure you are turning in your HW\n\nEven HW 0! Some people have not turned it in\nThere is no late penalty!\n\nFor muddy points, it’s helpful if you have exact questions along with the muddy point.\n\nFor example, from Monday I received a lot of “F test.”"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions_key_info.html#announcements",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "From exit ticket at 11:15am on 1/28, everyone either voted for Friday or “no preference”\n\nHW and labs will be due on Friday now!!\nIncluding HW 2!\n\nMake sure you are turning in your HW\n\nEven HW 0! Some people have not turned it in\nThere is no late penalty!\n\nFor muddy points, it’s helpful if you have exact questions along with the muddy point.\n\nFor example, from Monday I received a lot of “F test.”"
  },
  {
    "objectID": "labs/Lab_02_instructions.html",
    "href": "labs/Lab_02_instructions.html",
    "title": "Lab 2 Instructions",
    "section": "",
    "text": "Caution\n\n\n\nThis lab is ready to go! Nicky (1/28/2025)"
  },
  {
    "objectID": "labs/Lab_02_instructions.html#directions",
    "href": "labs/Lab_02_instructions.html#directions",
    "title": "Lab 2 Instructions",
    "section": "Directions",
    "text": "Directions\nPlease turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here. Please use the linked qmd file and not this one! (This is specifically the instructions.)\nThe rest of this lab’s instructions are embedded into the lab activities.\n\nPurpose\nThe main purpose of this lab is to introduce our dataset, codebook, and variables. We will continue to think about the context of our research question, but our main focus is to become familiar with the data.\n\n\nGrading\nThis lab is graded out of 12 points. Nicky will use the following rubric displayed on the Project page."
  },
  {
    "objectID": "labs/Lab_02_instructions.html#lab-activities",
    "href": "labs/Lab_02_instructions.html#lab-activities",
    "title": "Lab 2 Instructions",
    "section": "Lab activities",
    "text": "Lab activities\n\n1. Access and download the data\nThis serves as good practice for accessing data that is online or needs to be downloaded from a collaborator.\nData can be accessed here. Under “Weight IAT 2004-2024” there are several drop down menus:\n\nI opened the first “Datasets & Codebooks,” then selected “OSF Storage (United States).” Once selected, the “Download as zip” option pops up in the top right part of the Files section.\n\nWe will be working with the Weight_IAT.public.2021.csv dataset. Please locate the zip file called Weight IAT.public.2021-CSV.zip . To download, you need to click the row of the zip file, but you can’t click the name of the zip file. If a link opens, then you clicked the name. If the row is highlighted blue and clickable “Download” and “View” buttons appear on the top right, then you selected it correctly! (See below image for what it should look like.)\n\nThen click the “Download” button to download! Note that the name does not have an underscore between “Weight” and “IAT.” I like to have my datasets named without spaces, so I will replace the space with an underscore.\nFor the codebook, perform the same process for the file named: Weight_IAT_public_2021_codebook.xlsx\nYou will need to unzip the actual data.\nMove the data to a folder that you can easily access as you work from this document. I like to have a folder named data to house my data.\n\n\n\n\n\n\nTask Summary\n\n\n\nDownload the 2021 data and codebook from the archives and store in accessible folder.\n\n\n\n\n2. Load data and needed packages\nFirst, load the packages that you will need in the remainder of this lab. You can add to this as you need to. At the top of your R code chunk, you can add the following option to repress the messages from the loading packages:\n\n\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(here)\nif(!require(lubridate)) { install.packages(\"lubridate\"); library(lubridate) }\n\nUsing R, load the data (csv file) into this document. Note that this is a csv file that we can load with basic R packages. Name your dataset something that feels intuitive to you and will distinguish it from other datasets that you work with.\nLoading the csv file every time you render will take a long time. One way to speed this up is by saving the data as an rda file (R data file). Change the following R code to save the rda file. You will also need to remove the #| eval: false at the top of the code chunk once you have corrected the code. If you are confused on the syntax, don’t forget that you can use ?save for more information.\n\nsave(&lt;whatever you called the read csv file&gt;, file = \"Where you would like to save the file with its name\")\n\nCheck that you have an rda file where you saved it. Now use load() with the file path to load the rda data here.\n\nload(file = \"Where you would like to save the file with its name\")\n\nAt this point, if you think you loaded the file correctly, add #| eval: false to the code chunk where you loaded the csv file and back to the chunk where you saved the rda file.\nTake a glimpse at the data to make sure you loaded it correctly.\nHow many rows and columns are in the dataset? Do you think we will need all these variables for our analysis?\n\n\n\n\n\n\nTask Summary\n\n\n\nRead csv, save as rda, load rda, glimpse at data.\nHow many rows and columns are in the dataset? Do you think we will need all these variables for our analysis?\n\n\n\n\n3. Data wrangling\nAs you go through this process, it is important that you look at the codebook for more information on each variable.\n\n3.1 What’s our target population?\nAs many of you mentioned in Lab 1, individuals taking the IAT test are not necessarily representative of the world population. I want you to articulate the target population that you think our analysis can give information about. To what population can we generalize our analysis results? We can get very specific with this population, but try to restrict your population to 3-5 characteristics.\nAfter you articulate the population, I want to add one more restriction to our population: US residency. The sample includes individuals residing in many different countries. Since we are discussing attitudes and beliefs that is inherently connected to society and culture, I think it is important that we restrict our analysis and discussion to a country that we have some social experience in. Thus, let’s restrict our data to the US only by filtering the variable countryres to category 1 (corresponding to the US).\n\n\n\n\n\n\nTask\n\n\n\nDescribe our target population. Keep your description to 3-5 characteristics, not including our restriction on the US population.\n\n\n\n\n3.2 Restrict your analysis to 1 outcome and 9 possible covariates/predictors\nWe are going to restrict our analysis to the single outcome, IAT score, which is named D_biep.Thin_Good_all. You can rename this variable.\nWe will also restrict our analysis to the following 9 potential variables so our work is a little more manageable.\n\n\n\n\n\n\nTask\n\n\n\nFrom the following 8 attitudes and beliefs, please select 3 that you think will be the most important variables related to your research question. In 1-2 lines, briefly explain why you chose each variable. This can be informal and bulleted.\n\n\n(Make sure you chose the variable that is part of your research question!)\n\nExplicit anti-fat bias (att7)\nSelf-perception of weight (iam_001)\nFat group identity (identfat_001 )\nThin group identity (identthen_001 )\nControllability of weight of others (controlother_001)\nControllability of weight of yourself (controlyou_001)\nAwareness of societal standards (mostpref_001 )\nInternalization of societal standards (important_001)\n\nWe will start our data exploration with the following 4 demographic variables:\n\nAge (we need to construct from birthmonth, birthyear, month, and year)\nRace (raceomb_002 or raceombmulti)\nEthnicity (ethnicityomb)\nSex assigned at birth (birthSex)\n\nPlease pick 2 additional variables to include in your analysis:\n\nEducation (edu_14)\nGender (genderIdentity)\nSelf-reported BMI (through self-reported height and weight)\nPolitical identity\nReligion\n\nI have chosen these variables for a mixture of reasons. For example, I have left out variables about residence and occupation because those variables have hundreds of categories that would be overwhelming in linear regression. For the 4 required demographic variables, I chose age because I really want us to get practice with a continuous variable. I chose race and ethnicity because of the intertwined history of racism and anti-fat bias in Western countries (including the U.S. where most participants reside).\n\n\n\n\n\n\nA note of the available variables on race\n\n\n\nThe dataset has two separate race variables. One has mutually exclusive categories (raceomb_002) and the other allows participants to make multiple selections (raceombmulti). The former (raceomb_002) allows one participant to identify with only one race category.\nImportant lesson from We All Count about using a multiple selection race question. We can try out all these options!\n\n\nFinally, I chose sex assigned at birth because adults in 2021 in the US were likely raised in a society where your sex assigned at birth impacted the gender stereotypes that you were raised in, which could impact exposure to diet culture. This in addition to the many medical conditions associated with one’s sex assigned at birth that may affect weight. The reason why I am leaving gender as an optional variable is because the question on gender allows participants to chose multiple options. The binary sex assigned at birth will make our analysis a little easier from a statistics stand point. Unfortunately, we need to balance achievable learning objectives and the most appropriate variable. Since I have required race as a variable and has a multi-level option, I do not want to overload our analysis with another multi-level variable. Sex assigned at birth will not create more work for you (that is outside of the course objectives) while capturing medical conditions and some of the societal impact of diet culture. This is certainly a limitation in our analysis that we should address in our discussion. I do encourage you to look into gender if the binary sex assigned at birth does not feel right for you. I am happy to help!\n\n\n\n\n\n\nA word on self-reported BMI\n\n\n\nThis variable is rooted in racism and anti-fat bias. The American Medical Association made a few press releases on policies using BMI as a measure, with alternative measures (frankly, just other measures of fatness to use as a diagnostic tool instead of checking true indicators of health). However, I can think of a couple examples where BMI might help us understand some context in this research, so I have left it as an option. Although still self-reported, it might be interesting to see how BMI (which is the closest measurement available in this dataset to an “objective” measure of fatness) is related to individuals’ attitudes and beliefs. I am not saying there is anything to the relationship, but it might be worth checking out if you are interested.\nI will also say, in this dataset, there are MANY issues constructing the variable for BMI from height and weight. If you do not feel strongly about including it, I would suggest you avoid the variable self-reported BMI. It is not worth bringing in a racist and anti-fat variable into the dataset if you do not have a specific use for it. If you do plan to use it, please come to me for help as early as possible!\n\n\nIf you would like to investigate a variable outside the list, please let me know by emailing or chatting with me.\n\n\n\n\n\n\nTask\n\n\n\nUsing R, select your identified variables from your dataset.\n\n\n\n\n3.3 Manipulating variables that are coded as numeric variables\nMany variables in this dataset are coded as numeric values, but have specific categories linking up to the numbers. Using mutate() and cases() similar to our Data Management lesson, please create a new categorical variable with the specified categories from the codebook. Make sure that you create a variable with a new name! Since some of these variables are ordered categories, we will investigate if it’s appropriate to use the numeric or categorical version of the variable.\n\n\n\n\n\n\nExample of how I would create new variable for self-perception of weight (iam_001):\n\n\n\nBy looking at the codebook, I see that respondents answer the following question: “Currently, I am:”\n\n“Very underweight”\n“Moderately underweight”\n“Slightly underweight”\n“Neither underweight nor underweight”\n“Slightly overweight”\n“Moderately overweight”\n“Very overweight”\n\nIf I look at the data as is, I see that the variable is numeric.\n\niat_2021 %&gt;%\n  dplyr::select(iam_001) %&gt;%\n  tbl_summary()\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 465,8861\n    \n  \n  \n    iam_001\n\n        1\n2,023 (0.6%)\n        2\n7,902 (2.4%)\n        3\n24,399 (7.3%)\n        4\n148,081 (44%)\n        5\n88,566 (27%)\n        6\n43,090 (13%)\n        7\n18,978 (5.7%)\n        Unknown\n132,847\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\nAgain, I want to create a varaible with the answers instead of numbers, so I will change transform the variable to include the text:\n\niat_2021 = iat_2021 %&gt;%\n  mutate(iam_001_f = case_match(iam_001,\n                             7 ~ \"Very overweight\",\n                             6 ~ \"Moderately overweight\",\n                             5 ~ \"Slightly overweight\",\n                             4 ~ \"Neither underweight nor underweight\",\n                             3 ~ \"Slightly underweight\",\n                             2 ~ \"Moderately underweight\",\n                             1 ~ \"Very underweight\",\n                             .default = NA # to add NA if unknown\n                             ) %&gt;% factor())\niat_2021 %&gt;%\n  dplyr::select(iam_001_f) %&gt;%\n  tbl_summary()\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 465,8861\n    \n  \n  \n    iam_001_f\n\n        Moderately overweight\n43,090 (13%)\n        Moderately underweight\n7,902 (2.4%)\n        Neither underweight nor underweight\n148,081 (44%)\n        Slightly overweight\n88,566 (27%)\n        Slightly underweight\n24,399 (7.3%)\n        Very overweight\n18,978 (5.7%)\n        Very underweight\n2,023 (0.6%)\n        Unknown\n132,847\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\nggplot(data=iat_2021) +\n  geom_boxplot(aes(x = iam_001_f, y = IAT_score))\n\nWarning: Removed 129215 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nI have called the new variable iam_001_f to indicate that the variable is not in factor form. You can also call it something like iam_001_cat to indicate the categorical form.\n\n\n\n\n\n\n\n\nTask\n\n\n\nIdentify and list the variables that are coded numerically and correspond to categories. Create a new variable for the categorical/factor version of the variable. It is up to you to check that your code ran properly!! If you are using multi-choice categorical variables (might include race, gender), then do not convert the variable yet!\n\n\n\n\n3.4 Creating age from birth date and test date\nThis dataset does not have an available “age” variable. However, we have enough information to determine each individual’s age from the test date and their self-reported birth date. We can use the lubridate package to configure the age. First, we need to use make_date() to construct the birth date and test date. Below, I have implemented make_date() to make the birth date.\n\n\n\n\n\n\nTask\n\n\n\nFrom the codebook, find the variables that we can use to construct the test date. Then use make_date() to create the test date.\n\n\n\niat_2021 = iat_2021 %&gt;%\n  mutate(birthdate = make_date(month = birthmonth, year = birthyear), \n         testdate = make_date(month = month, year = year))\n\nOnce the two dates are created, we can use further use lubridate to calculate the age in years. This code is a little complicated, so here is an example of how I have created age:\n\niat_2021 = iat_2021 %&gt;%\n  mutate(age = interval(start = birthdate, end = testdate) %&gt;%\n          as.period() %&gt;% year()) %&gt;%\n  select(-birthmonth, -birthyear, -year, -month, \n         -testdate, -birthdate)\n\nNote that the name of my dataset is iat_2021 and I feed it into mutate(). Within mutate(), I assigned age to the interval between the name of my birth date (birthdate) and the name of my test date (testdate). I need to convert the interval to a period of time (as.period()), then to a measurement of years (year()).\n\n\n\n\n\n\nTask\n\n\n\nFollowing the above example, create an age variable that measures the years between individuals’ birth and test date. Then remove the variables used to make age.\n\n\n\n\n3.5 If you chose BMI, create the variable\nRaw data from weight and height are categorical. This is according to the codebook associated with this dataset. Please find your codebook file named Weight_IAT_public_2021_codebook.csv . You can find the value names for myweight_002 and myheight_002.\n\nFor example, in the weight variable,\n\nmost categories identify a lower limit to the weight in the group. One example group is weight is greater than or equal to 200 pounds and less than 205 pounds (labelled as “200 lb :: 91 kg”).\nthe first category for weight is “below 50lb:: 23kg” with 258 observations\nthe last category for weight is “above 440lb:: above 200kg” with 295 observations\n\nWhile the 5 groups of weight leading up the last category have 33, 28, 34, 20, and 89 observations, respectively.\n\n\n\nI will post an extra resource outlining some of my work on the BMI variable.\n\n\n3.6 Make a new dataset with only complete cases\nHandling missing data is outside the scope of our class. There are many techniques to handling missing data, but we will use complete case analysis. This means we will only use observations that have information for every variable we chose. The function drop_na() will give you the complete cases. You can feed your dataset into the function and assign it as a new dataframe.\nFor example:\n\nnew_df = old_df %&gt;% drop_na()\n\nYou will also need to save the new dataset so that you can load it in future labs/work.\nYou can use something like:\n\nsave(new_df, file = \"IAT_data_complete.Rda\")\n\nMake sure this dataset saves into your project folder!\n\n\n\n\n\n\nTask\n\n\n\nMake a new dataset with only complete cases. Save this dataset in your project folder.\n\n\n\n\n\n4. Some exploratory data analysis\n\n4.1 Peek at your outcome\nThis serves as a check to make sure we are all looking at the correct outcome: IAT score.\n\n\n\n\n\n\nTask\n\n\n\nPlease plot a histogram of the IAT scores. What do you notice about the outcome?\n\n\n\n\n4.2 Univariate exploratory data analysis\n\n\n\n\n\n\nTask\n\n\n\nUsing ggplot or tables, visualize your variables. Get a sense of each variable’s distribution. Do you notice anything out of the ordinary?\n\n\n\n\n4.3 Bivariate exploratory data analysis\n\n\n\n\n\n\nTask\n\n\n\nTake a look at the scatterplot, violin, or box plot of IAT score and your variable of interest. Use R and ggplot to make this plot. If your variable of interest is categorical, then make sure to use a violin or boxplot.\n\n\n\n\n\n5. Revisit your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate the research question that you proposed in Lab 1. Please make sure it is only one question, one sentence long. What are your thoughts on the research question now that we looked at the data? Feel free to change it now that we’ve looked at the data. If you change your question, make sure 4.2 reflects the new research question.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn research, we typically do NOT change our research question after looking at the data! Researchers typically form their questions from other research and their expertise. We may not have expertise in this field and we have not been studying implicit bias, so I want to be a little more flexible with our analysis."
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#lets-remind-ourselves-of-one-model-we-have-been-working-with",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#lets-remind-ourselves-of-one-model-we-have-been-working-with",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "Let’s remind ourselves of one model we have been working with",
    "text": "Let’s remind ourselves of one model we have been working with\n\nWe have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\nPopulation model: \\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\nEstimated model:\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    FemaleLiteracyRate\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{LE}} &= 50.9 + 0.232 \\cdot \\text{FLR}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#our-residuals-will-help-us-a-lot-in-our-diagnostics-and-assumptions",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#our-residuals-will-help-us-a-lot-in-our-diagnostics-and-assumptions",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "Our residuals will help us a lot in our diagnostics and assumptions!",
    "text": "Our residuals will help us a lot in our diagnostics and assumptions!\n\n\n\nThe residuals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-check-normality-with-distribution-plots-of-residuals",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-check-normality-with-distribution-plots-of-residuals",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "N: Check normality with distribution plots of residuals",
    "text": "N: Check normality with distribution plots of residuals\nNote that below I save each figure as an object, and then combine them together in one row of output using grid.arrange() from the gridExtra package\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_boxplot()\n\ngrid.arrange(hist1, density1, box1, nrow = 1)"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#we-didnt-really-go-over-our-options-when-these-assumptions-do-not-hold",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#we-didnt-really-go-over-our-options-when-these-assumptions-do-not-hold",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "We didn’t really go over our options when these assumptions do not hold",
    "text": "We didn’t really go over our options when these assumptions do not hold\n\nWe will consider this more once we get into multiple linear regression\nFor now, with SLR, when assumptions do not hold, I conclude we need to add more variables in the model\n\n \n\nAnother note: I did not make these plots very presentable\n\nAxes were left with whatever names were given to them\nThese plots are usually just for us!\nNot really something that you include in a formal report\n\n\n\n\nLesson 7: SLR 4"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-check-normality-with-distribution-plots-of-residuals-12",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-check-normality-with-distribution-plots-of-residuals-12",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "N: Check normality with distribution plots of residuals (1/2)",
    "text": "N: Check normality with distribution plots of residuals (1/2)\nNote that below I save each figure as an object, and then combine them together in one row of output using grid.arrange() from the gridExtra package\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_boxplot()\n\ngrid.arrange(hist1, density1, box1, nrow = 1)"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-check-normality-with-distribution-plots-of-residuals-22",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#n-check-normality-with-distribution-plots-of-residuals-22",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "N: Check normality with distribution plots of residuals (2/2)",
    "text": "N: Check normality with distribution plots of residuals (2/2)\n\nSo do these plots of the residuals look normal?\n\n\ngrid.arrange(hist1, density1, box1, nrow = 1)\n\n\n\nMy assessment: Looks like our residuals could be normal if we did not have those values around -20"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions_key_info.html#homework-lab-review",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions_key_info.html#homework-lab-review",
    "title": "Key Info and Announcements",
    "section": "Homework / Lab Review",
    "text": "Homework / Lab Review"
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates_muddy_points.html#muddy-points-from-2024",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates_muddy_points.html#muddy-points-from-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from 2024",
    "text": "Muddy Points from 2024\n\n1. Why do we need to create a new variable for ordinal / scoring?\nOtherwise R will treat income as non-ordinal, and use the default reference cell coding. So if we want our variables to be scored (and numeric) then we must put it in a form R can recognize.\n\n\n2. I’m a little confused on how the R code works for recoding/reordering our variables, specifically 1) why we use the mutate function but then use the same name for the variable/how that works and 2) why you need to include the list of each variable name in a vector. Basically, what each piece of that code does exactly and why it’s needed.\n\nMutate is just a function to create/change a variable. So if we are not fundamentally changing any aspect of the variable, we can call it by the same name. Helps keep our data frame neat by not tacking on additional variables.\nWhen I am including the list of levels I am giving R the exact order to read each level. So if I want to go from high income to low income, I would reset the levels to the below code. Then R would read high income as the first level.\n\ngapm2 = gapm2 %&gt;%\n mutate(income_levels = factor(income_levels, \n            ordered = T, \n            levels = c(\"High income\", \n                       \"Upper middle income\", \n                       \"Lower middle income\", \n                       \"Low income\")))\n\n\n\n\n3. Is there a rationale or strategy in choosing the most appropriate reference group?\nOften no, not if the groups are not ordered. Things that you may consider:\n\nIs there a central group that you want to make comparisons to?\nIs there any social consequences of continually centering comparisons to one group? We may be consequentially centering the narrative around that group.\nWhen we interpret the coefficients, is there one group as the reference that makes it a little easier to interpret? (this has more of an effect in 513)\n\n\n\n4. How do we build the regression indicators?\nIn R, we don’t need to build the indicators. If we have a variable that is a facotr with mutually exclusive groups, then R will automatically create the indicators within the lm() function."
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval_muddy_points.html#muddy-points-from-2024",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval_muddy_points.html#muddy-points-from-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from 2024",
    "text": "Muddy Points from 2024\n\n7. What is the relationship between the ANOVA for linear regression and ANOVA for group differences?\n\n\n8. The limitations of the different tests, like an F statistic vs. t-test\n\n\n9. unexplained vs the explained\n\n\n10. Changing the confidence level in tidy()\nHere is a good site about the input! Looks like we would use conf.level to change 95% confidence interval to some other percent."
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates_muddy_points.html",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Residuals are always tied to the expected outcome. Since each category has a different expected outcome (based on their respective estimated coefficients), the countries from same category will have the same expected outcome (in SLR). Thus, for countries in the same category, their residuals (\\(\\widehat{\\epsilon} = Y_i - \\widehat{Y_i}\\)) will all have the same \\(\\widehat{Y_i}\\)\n\n\n\nGood question! Factors is one of the coding options in R for categorical variables. Different from characters or strings, factors allow you to attach specific attributes to the variable. This includes assigning order to the categories and setting reference levels.\n\n\n\nYou can make predictions! “Predictions,” for how we used it with continuous predictors/covariates, is just the expected outcome for a given X. For categorical covariates, the expected outcome given X is the mean of each categorical group.\nWith only one variable in the model, it might feel more appropriate to use something like the ANOVA table, but we do not typically have only one variable in the model. This is just to help us set up the foundation of linear regression and understand categorical covariates in our model before we move to multiple linear regression."
  },
  {
    "objectID": "lessons/05_Cat_covariates/05_Cat_covariates_muddy_points.html#muddy-points-from-2025",
    "href": "lessons/05_Cat_covariates/05_Cat_covariates_muddy_points.html#muddy-points-from-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "Residuals are always tied to the expected outcome. Since each category has a different expected outcome (based on their respective estimated coefficients), the countries from same category will have the same expected outcome (in SLR). Thus, for countries in the same category, their residuals (\\(\\widehat{\\epsilon} = Y_i - \\widehat{Y_i}\\)) will all have the same \\(\\widehat{Y_i}\\)\n\n\n\nGood question! Factors is one of the coding options in R for categorical variables. Different from characters or strings, factors allow you to attach specific attributes to the variable. This includes assigning order to the categories and setting reference levels.\n\n\n\nYou can make predictions! “Predictions,” for how we used it with continuous predictors/covariates, is just the expected outcome for a given X. For categorical covariates, the expected outcome given X is the mean of each categorical group.\nWith only one variable in the model, it might feel more appropriate to use something like the ANOVA table, but we do not typically have only one variable in the model. This is just to help us set up the foundation of linear regression and understand categorical covariates in our model before we move to multiple linear regression."
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval_muddy_points.html",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "This is a very good question! I was using them a little loosely in today’s class. Deviation refers to a single observation’s (i’s) difference between values. So the deviation between the best fit line and an observation is \\(Y_i - \\widehat{Y_i}\\). We often call this specific deviation the residual or error.\nVariation refers to the entire sample. We calculate the variation by summing the squared deviations. It will give us a sense of how the sample, as a whole, is spread.\n\n\n\nMore evidence… that the model explains the variation more than it does not. The larger the F statistic gets (once already &gt; 1), the more and more evidence that the model explains a big portion of the total variation! Always remember that the F statistic measures the portion of explain to unexplained variation!\n\n\n\nWe do not always need to come to a conclusion on the statistical significance. The confidence interval provides a lot of good information about the spread of the data. A coefficient estimate like 10 might seem really powerful, but once you see that the confidence interval spans from 1 to 19 (even though significant!), you might be less inclined to really broadcast the 10 alone. Remember, statistically significant just means there’s evidence that it is not 0, not that we have evidence that it is 10.\nMy tangent: that’s why I get so frustrated when I read news articles or hear people reporting things like “you are 10x more likely to get blah.” Yeah, 10 is a scary big number, but I want to know if that 10 has a confidence interval of 9 to 11 or 1 to 19. It changes things and let’s me know the quality of the data they were working with!\n\n\n\nUnfortunately no. It only works for two continuous variables\n\n\n\nBe careful! It did NOT say “+” The null hypothesis is that all the coefficients are 0. That \\(\\beta_1 = \\beta_2 = \\beta_3 = 0\\). This is just quicker to write than \\(\\beta_1 = 0, \\beta_2 = 0, \\beta_3 = 0\\).\nThe null hypothesis is saying that there is no difference between the reference group and all the other groups. Basically that no matter what group, the mean outcome is the same.\n\n\n\nFor ordinal variables you can find the mean outcome for each level, then subtract them from each other. For example, mean life expectancy for low income would be \\(E(LE|X=1) = \\widehat{\\beta_0} + \\widehat{\\beta_1}(1)\\) and mean life expectancy for upper middle income would be \\(E(LE|X=3) = \\widehat{\\beta_0} + \\widehat{\\beta_1}(2)\\). And then we can just take the difference: \\(E(LE|X=3) - E(LE|X=1)\\).\nThe trick is in the confidence intervals! We need to find the standard error for the difference in expected values! We can start with the variance:\n\\[\n\\begin{aligned}\nVar \\big( E(LE|X=3) - & E(LE|X=1) \\big)\\\\\n= & Var \\big( \\widehat{\\beta_0} + \\widehat{\\beta_1}(3) - \\widehat{\\beta_0} - \\widehat{\\beta_1}(1) \\big)\\\\\n= & Var \\big( \\widehat{\\beta_1}(3) - \\widehat{\\beta_1}(1) \\big)\\\\\n= & Var \\big( \\widehat{\\beta_1}(3-1) \\big) \\\\\n= & Var \\big( 2 \\widehat{\\beta_1} \\big) \\\\\n= & 4 Var \\big( \\widehat{\\beta_1} \\big)\n\\end{aligned}\n\\]\nFrom there we know the standard error of \\(\\beta_1\\) and can calculated the confidence interval."
  },
  {
    "objectID": "lessons/06_SLR_Eval/06_SLR_Eval_muddy_points.html#muddy-points-from-2025",
    "href": "lessons/06_SLR_Eval/06_SLR_Eval_muddy_points.html#muddy-points-from-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "This is a very good question! I was using them a little loosely in today’s class. Deviation refers to a single observation’s (i’s) difference between values. So the deviation between the best fit line and an observation is \\(Y_i - \\widehat{Y_i}\\). We often call this specific deviation the residual or error.\nVariation refers to the entire sample. We calculate the variation by summing the squared deviations. It will give us a sense of how the sample, as a whole, is spread.\n\n\n\nMore evidence… that the model explains the variation more than it does not. The larger the F statistic gets (once already &gt; 1), the more and more evidence that the model explains a big portion of the total variation! Always remember that the F statistic measures the portion of explain to unexplained variation!\n\n\n\nWe do not always need to come to a conclusion on the statistical significance. The confidence interval provides a lot of good information about the spread of the data. A coefficient estimate like 10 might seem really powerful, but once you see that the confidence interval spans from 1 to 19 (even though significant!), you might be less inclined to really broadcast the 10 alone. Remember, statistically significant just means there’s evidence that it is not 0, not that we have evidence that it is 10.\nMy tangent: that’s why I get so frustrated when I read news articles or hear people reporting things like “you are 10x more likely to get blah.” Yeah, 10 is a scary big number, but I want to know if that 10 has a confidence interval of 9 to 11 or 1 to 19. It changes things and let’s me know the quality of the data they were working with!\n\n\n\nUnfortunately no. It only works for two continuous variables\n\n\n\nBe careful! It did NOT say “+” The null hypothesis is that all the coefficients are 0. That \\(\\beta_1 = \\beta_2 = \\beta_3 = 0\\). This is just quicker to write than \\(\\beta_1 = 0, \\beta_2 = 0, \\beta_3 = 0\\).\nThe null hypothesis is saying that there is no difference between the reference group and all the other groups. Basically that no matter what group, the mean outcome is the same.\n\n\n\nFor ordinal variables you can find the mean outcome for each level, then subtract them from each other. For example, mean life expectancy for low income would be \\(E(LE|X=1) = \\widehat{\\beta_0} + \\widehat{\\beta_1}(1)\\) and mean life expectancy for upper middle income would be \\(E(LE|X=3) = \\widehat{\\beta_0} + \\widehat{\\beta_1}(2)\\). And then we can just take the difference: \\(E(LE|X=3) - E(LE|X=1)\\).\nThe trick is in the confidence intervals! We need to find the standard error for the difference in expected values! We can start with the variance:\n\\[\n\\begin{aligned}\nVar \\big( E(LE|X=3) - & E(LE|X=1) \\big)\\\\\n= & Var \\big( \\widehat{\\beta_0} + \\widehat{\\beta_1}(3) - \\widehat{\\beta_0} - \\widehat{\\beta_1}(1) \\big)\\\\\n= & Var \\big( \\widehat{\\beta_1}(3) - \\widehat{\\beta_1}(1) \\big)\\\\\n= & Var \\big( \\widehat{\\beta_1}(3-1) \\big) \\\\\n= & Var \\big( 2 \\widehat{\\beta_1} \\big) \\\\\n= & 4 Var \\big( \\widehat{\\beta_1} \\big)\n\\end{aligned}\n\\]\nFrom there we know the standard error of \\(\\beta_1\\) and can calculated the confidence interval."
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions_key_info.html#homework-1-review",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions_key_info.html#homework-1-review",
    "title": "Key Info and Announcements",
    "section": "Homework 1 Review",
    "text": "Homework 1 Review\n\nCheck your feedback!!\nQuestion 6: regression with age and weight\n\nThe dry weight is our outcome! It is important to always use Y ~ X in the lm() function!\nStating the population simple linear regression model = writing it out using \\(\\beta\\)s and stuff\n\nQuestion 4: The choice of an alternative hypothesis should depend primarily on (choose all that apply). Explain your reasoning.\n\nWhen creating the hypothesis tests, you should not use the data collected to determine the test\nYour alternative should be formed from prior work/research\n\nPopulation models should either be written like \\(E(Y|X) = \\beta_0 + \\beta_1 X\\) or \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions_muddy_points.html",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "This is just tied to how the study is set up. We are basically saying that our study design does not make our observations inherently correlated to one another.\nFor example, let’s think of a study measuring 5 adults’ height every month over a year. We have 12 height measurements for each adult. Since most adults’ heights will not change over their adulthood, those measurements are probably pretty close to one another. However, the 5 sets of measurements for each adult are probably different. We cannot assume that all 60 (5x12) measurements are independent from each other. Each adult’s 12 measurements will be highly correlated with one another. Thus, we do not have independent outcomes in this study.\n\n\n\nFor the most part, we will focus on investigating homoscedasticity (equal variance) from the residual plots. We track across the X-axis, and make sure that the spread of the data looks pretty even across x-values.\n\n\n\nThis is within the geom_smooth() function in our ggplot code.\nHere is the plot without bands:\n\nggplot(gapm, aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_point() +\n1  geom_smooth(method = \"lm\", se = FALSE, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\")\n\n\n1\n\nse = FALSE tells R to omit the bands\n\n\n\n\n\n\n\nHere is the plot with bands:\n\nggplot(gapm, aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_point() +\n1  geom_smooth(method = \"lm\", se = TRUE, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\")\n\n\n1\n\nse = TRUE tells R to show the bands"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions_muddy_points.html#muddy-points-from-winter-2024",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions_muddy_points.html#muddy-points-from-winter-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Winter 2024",
    "text": "Muddy Points from Winter 2024\n\n1. Equality of the residuals - what’s the bias refer in a residual plot? Is that suggesting a non linear relationship between two variables?\nHere is the plot that this question is referring to:\n\n\n\n\n\nThe answer is already in the question! The residual plot can also be used to look at linearity! The above plots that say “biased” mean they do not follow the linearity assumption.\n\n\n2. QQ Plot: What is it? And can you explain the axes, meaning of “quantiles”, and why assuming normality would result in a straight line?\nI cannot answer this question better than this video! They go through a smaller dataset of gene expression values and how to make a QQ plot from the data. Remember, our QQ plot is of our residual values!!\n\n\n3. I’m still a little confused on how to determine if a dataset has a normal distribution. Feels like a subjective decision.\nFirst thing that I want to address: when we are talking about normality, we are not determining if the dataset follows a normal distribution. We are determining if the fitted model violates the normality assumption that we need to use in our population model. We do this by seeing if the fitted residuals follow a normal distribution. I just want to draw attention to this. There is very particular language being used here.\nSecond thing… Yes! These diagnostic tools are somewhat subjective. You are welcome to use the Shapiro-Wilk test every time you look at a QQ plot! I realize a test with a conclusion might feel more objective and comfortable as we are learning about the model diagnostics. I suggest trying to make a conclusion visually with a QQ plot, then see if it matches the Shapiro-Wilk test. Remember, even in the Shapiro-Wilk test, the null hypothesis is that the fitted residuals come from a normal distribution. So we have to work to disprove that. You can come to the QQ plot with that same prior. If the QQ plot gives blantent evidence that the fitted residuals are not normally distributed, then we violate the assumption.\nWe’ll keep practicing! As we keep going through regression, we’ll realize that model building is very much an art! There is no one answer in statistics!\n\n\n4. What are the small nuances in interpreting the normality through a QQ plot?\nThanks for this question! This helped me realize that I was not articulating very well some of my more subconscious thoughts in a QQ plot.\nBelow are the distribution samples ant their QQ plots from lecture:\n\n\n\n\n\nI drew red, blue and green lines to bracket certain areas of the plots. I basically start by looking within the red brackets. Do all the points seem to stay close to the black line? If this doesn’t hold for the red bracketed area, then I would say our fitted residuals are not normal. Then I look at the area from the red lines to the blue lines. This is less definite, but if the points don’t seem to stay close to the black line, then I’d say our fitted residuals are not normal. Then I’d look at the are between the blue and green line. If the points aren’t close to the black line, then I am likely okay with it and would NOT make the conclusion that the fitted residuals are NOT normal. Notice, that I am not saying I call them normal. They seem to not violate the normal assumption.\nExtra note: The t-distribution is similar to a normal, but it includes larger tails. This is to adjust the normal distribution when our sample size of data is small. However, our assumption aims for fitted residuals to follow a normal distribution. We can be a little more flexible with the QQ plot when we have a smaller sample size, but we should not aim for a t-distribution. Both the normal and t-distribution samples “passed” my normality assessment.\nWe can check out the example:\n\n\n\n\n\nIn this example, I would say that the fitted residuals violate the normal assumption. Notice that we have points off the black line between the red and blue lines. And even within the red lines, we have some curve. This is okay for our example! That’s because we have not yet included other (likely needed) variables in the model. And what does that mean? The other variables in the model will help explain MORE variance in our Y, which would alter the fitted residuals!!\nDraw the red, blue, and green lines on the other QQ plot slides. See what you find, especially when we have different sample sizes!"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag_key_info.html",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Moved some of the lab vs. homework dates around - please update your schedule!\n\nMostly because I wanted you to have a week between Lab 4 and the poster!\n\nHW 2 submissions\n\nI had a cross link on my HW 2 page\nSome of you did some HW 3 problems in HW 2 and missed some HW 2 problems\n\nI’ll make a note on HW 3 - you need to do those HW 2 problems in HW 3 instead\n\n\nAnything else?"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag_key_info.html#announcements",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Moved some of the lab vs. homework dates around - please update your schedule!\n\nMostly because I wanted you to have a week between Lab 4 and the poster!\n\nHW 2 submissions\n\nI had a cross link on my HW 2 page\nSome of you did some HW 3 problems in HW 2 and missed some HW 2 problems\n\nI’ll make a note on HW 3 - you need to do those HW 2 problems in HW 3 instead\n\n\nAnything else?"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#process-of-regression-data-analysis",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#process-of-regression-data-analysis",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Process of regression data analysis",
    "text": "Process of regression data analysis\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#process-of-regression-data-analysis",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions.html#process-of-regression-data-analysis",
    "title": "Lesson 7: SLR: Checking model assumptions",
    "section": "Process of regression data analysis",
    "text": "Process of regression data analysis\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#types-of-influential-points",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#types-of-influential-points",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Types of influential points",
    "text": "Types of influential points\n\n\n\n\nOutliers\n\n\n\nAn observation (\\(X_i, Y_i\\)) whose response \\(Y_i\\) does not follow the general trend of the rest of the data\n\n\n\n\n\n\n\n\n \n \n\n\n\nHigh leverage observations\n\n\n\nAn observation (\\(X_i, Y_i\\)) whose predictor \\(X_i\\) has an extreme value\n\\(X_i\\) can be an extremely high or low value compared to the rest of the observations"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#countries-that-are-outliers-r_i-2-1",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#countries-that-are-outliers-r_i-2-1",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Countries that are outliers (\\(|r_i| > 2\\))",
    "text": "Countries that are outliers (\\(|r_i| &gt; 2\\))\nLabel only countries with large leverage:\n\nggplot(aug1, aes(x = FemaleLiteracyRate, y = LifeExpectancyYrs,\n                 label = country)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  geom_text(aes(label = ifelse(abs(.std.resid) &gt; 3, as.character(country), ''))) +\n  geom_vline(xintercept = mean(aug1$FemaleLiteracyRate), color = \"grey\") +\n  geom_hline(yintercept = mean(aug1$LifeExpectancyYrs), color = \"grey\")"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#visual-countries-with-high-leverage-h_i-4n",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#visual-countries-with-high-leverage-h_i-4n",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Visual: Countries with high leverage (\\(h_i > 4/n\\))",
    "text": "Visual: Countries with high leverage (\\(h_i &gt; 4/n\\))\nLabel only countries with large leverage:\n\nggplot(aug1, aes(x = FemaleLiteracyRate, y = LifeExpectancyYrs,\n                 label = country)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  geom_text(aes(label = ifelse(.hat &gt; 4/80, as.character(country), ''))) +\n  geom_vline(xintercept = mean(aug1$FemaleLiteracyRate), color = \"grey\") +\n  geom_hline(yintercept = mean(aug1$LifeExpectancyYrs), color = \"grey\")"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#countries-that-are-outliers-r_i-3",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#countries-that-are-outliers-r_i-3",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Countries that are outliers (\\(|r_i| > 3\\))",
    "text": "Countries that are outliers (\\(|r_i| &gt; 3\\))\n\nWe can identify the countries that are outliers\n\n\naug1 %&gt;% \n  filter(abs(.std.resid) &gt; 3)\n\n# A tibble: 1 × 24\n  country  LifeExpectancyYrs FemaleLiteracyRate .std.resid .fitted .resid   .hat\n  &lt;chr&gt;                &lt;dbl&gt;              &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Swazila…              48.9               87.3      -3.65    71.2  -22.3 0.0133\n# ℹ 17 more variables: .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, CO2emissions &lt;dbl&gt;,\n#   ElectricityUsePP &lt;dbl&gt;, FoodSupplykcPPD &lt;dbl&gt;, IncomePP &lt;dbl&gt;,\n#   population &lt;dbl&gt;, WaterSourcePrct &lt;dbl&gt;, geo &lt;chr&gt;, four_regions &lt;chr&gt;,\n#   eight_regions &lt;chr&gt;, six_regions &lt;chr&gt;, members_oecd_g77 &lt;chr&gt;,\n#   Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt;, `World bank region` &lt;chr&gt;,\n#   `World bank, 4 income groups 2017` &lt;chr&gt;"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#visual-countries-that-are-outliers-r_i-3",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#visual-countries-that-are-outliers-r_i-3",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Visual: Countries that are outliers (\\(|r_i| > 3\\))",
    "text": "Visual: Countries that are outliers (\\(|r_i| &gt; 3\\))\nLabel only countries with large internally standardized residuals:\n\nggplot(aug1, aes(x = FemaleLiteracyRate, y = LifeExpectancyYrs,\n                 label = country)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  geom_text(aes(label = ifelse(abs(.std.resid) &gt; 3, as.character(country), ''))) +\n  geom_vline(xintercept = mean(aug1$FemaleLiteracyRate), color = \"grey\") +\n  geom_hline(yintercept = mean(aug1$LifeExpectancyYrs), color = \"grey\")"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#what-does-the-model-look-like-without-outliers",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#what-does-the-model-look-like-without-outliers",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "What does the model look like without outliers?",
    "text": "What does the model look like without outliers?\nSensitivity analysis removing countries that are outliers\n\naug1_no_out &lt;- aug1 %&gt;% filter(abs(.std.resid) &lt;= 3) \n\nmodel1_no_out &lt;- aug1_no_out %&gt;% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\ntidy(model1_no_out) %&gt;% gt() %&gt;% # Without outliers\n tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.937\n2.438\n20.896\n0.000\n    FemaleLiteracyRate\n0.236\n0.029\n8.164\n0.000\n  \n  \n  \n\n\n\ntidy(model1) %&gt;% gt() %&gt;% # With outliers\n tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n    FemaleLiteracyRate\n0.232\n0.031\n7.377\n0.000"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#tools-to-measure-influential-points",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#tools-to-measure-influential-points",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Tools to measure influential points",
    "text": "Tools to measure influential points\n\nInternally standardized residual (outlier)\n\n \n\nLeverage (high leverage point)\n\n \n\nCook’s distance (overall influence, both)"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#model-without-observations-with-high-cooks-distance",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#model-without-observations-with-high-cooks-distance",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Model without observations with high Cook’s distance",
    "text": "Model without observations with high Cook’s distance\n\nmodel1_lowcd &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate,\n                    data = aug1_lowcd)\ntidy(model1_lowcd) %&gt;% gt() %&gt;% # Without high-leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n52.388\n2.078\n25.208\n0.000\n    FemaleLiteracyRate\n0.226\n0.024\n9.208\n0.000\n  \n  \n  \n\n\n\ntidy(model1) %&gt;% gt() %&gt;% # With high leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n    FemaleLiteracyRate\n0.232\n0.031\n7.377\n0.000"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#what-does-the-model-look-like-without-the-high-cooks-distance-points",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#what-does-the-model-look-like-without-the-high-cooks-distance-points",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "What does the model look like without the high Cook’s distance points?",
    "text": "What does the model look like without the high Cook’s distance points?\nSensitivity analysis removing countries with high Cook’s distance\n\naug1_lowcd &lt;- aug1 %&gt;% filter(.cooksd &lt;= 0.04)\nmodel1_lowcd &lt;- aug1_lowcd %&gt;% lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\ntidy(model1_lowcd) %&gt;% gt() %&gt;% # Without high Cook's distance points\n tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n52.388\n2.078\n25.208\n0.000\n    FemaleLiteracyRate\n0.226\n0.024\n9.208\n0.000\n  \n  \n  \n\n\n\ntidy(model1) %&gt;% gt() %&gt;% # With high Cook's distance points\n tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n    FemaleLiteracyRate\n0.232\n0.031\n7.377\n0.000"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#a-note-from-nicky",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#a-note-from-nicky",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "A note from Nicky",
    "text": "A note from Nicky\n\nIt’s always weird to be using numbers to help you diagnose an issue, but the issue kinda gets unresolved\nBasically, deleting an observation should be justified outside of the numbers!\n\nIf it’s an honest data point, then it’s giving us important information!\n\nA really well thought out explanation from StackExchange"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#important-note-on-influential-observations",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#important-note-on-influential-observations",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Important note on influential observations",
    "text": "Important note on influential observations\n\nIt’s always weird to be using numbers to help you diagnose an issue, but the issue kinda gets unresolved\n\n \n\nBasically, deleting an observation should be justified outside of the numbers!\n\nIf it’s an honest data point, then it’s giving us important information!\n\n\n \n\nA really well thought out explanation from StackExchange"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#checking-our-model",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag.html#checking-our-model",
    "title": "Lesson 8: SLR: Model Diagnostics",
    "section": "Checking our model",
    "text": "Checking our model\n\nAn observation may be influential if the model is not correctly specified\n\nWe may also see issues with the LINE assumptions\n\nWhat are our options to specify the model “correctly?”\n\nSee if we need to add predictors to our model\n\nNicky’s thought for our life expectancy example\n\nTry a transformation if there is an issue with linearity or normality\nTry a transformation if there is unequal variance\nTry a weighted least squares approach if unequal variance (might be lesson at end of course)\nTry a robust estimation procedure if we have a lot of outlier issues (outside scope of class)"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag_muddy_points.html#muddy-points-from-winter-2024",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag_muddy_points.html#muddy-points-from-winter-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Winter 2024",
    "text": "Muddy Points from Winter 2024\n\nExplain formula for internally standardized residuals. 2. How do we determine WHAT the source of the issue is for an outlier… i.e. “outlier detection”. Is it just highly situational most times? 3. Leverage (h sub i) what is the significance of it? What does it “tell us” as values change?\n“cubic or square transformations must include original X, don’t do this for Y”\nThe transformations of the X and Y started to feel really abstract and kinda confusing\nthe relationship between what we’re learning about models and how we’ll actually apply that knowledge to a figure in a report, for example\nI understand the reasoning for transforming to make the data to fit the LINE assumptions, but if it makes it hard to interpret, why do we do it?\n\nHi and Hat metrics and not knowing the magnitude or reason for it was confusing.\nhttps://online.stat.psu.edu/stat501/lesson/11/11.2\n\n\nwhat are some other real life examples of influential points besides human error. Can they appear more organically\nI’m a little confused in the section about the power ladder – we’re supposed to be looking at the skew of the distribution of the residuals, but the examples shown were of different distributions (x alone, y alone, and x vs y). What should we be looking at to determine what transform, if any, to use?\nI’m not super clear on why transformations are even ok to do if they fundamentally change the shape of the data. You had a good sentence right at the end (second to last slide?) – “for every change in x, there’s a (…) change in longevity squared” or something like that? I think it’d be helpful to repeat or elaborate on that.\nI’m still not very clear on how to identify outliers vs high leveraged data points by looking at a plot alone.\n\n\nWhat is the reasoning behind why we usually want to transform the independent variable instead of the dependent?\nThis is more about interpretations in multiple linear models. We can have a model with multiple variables as covariates (so multiple X’s on the right side of the equation). If we transform Y, then all the interpretations with every covariate changes. If we can fit model assumptions better with a transformation to one X, then we only change one interpretation!\n\n\n\n\n\n\n\nIn the Tukey’s power ladder, is log(x) mean log10(x) or natural log(x)?\n\n\n\n\n\nExtra materials with examples of transformations\nhttps://www.youtube.com/watch?v=HIcqQhn3vSM&ab_channel=jbstatistics\nhttps://online.stat.psu.edu/stat501/lesson/9"
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag_muddy_points.html",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "gladder will show you what the transformation of a single variable looks like. We can use it as a visual assessment to determine which transformations we might want to try for a variable.\nI showed it for both FLR and LE. Note that I did it for each variable separately! Then I decided LE and FLR, separately, might benefit from a squared or cubed transformation.\n\n\n\nI suggest trying all the ones on the way. This is why I like gladder(). Instead of making a choice on going “up” or “down,” we can look at all the plots and see how each transformation will help us make the variable more normally distributed.\n\n\n\nYes, that is a good order of things! We will talk more about the reasoning for X first when we get to multiple linear regression. The main point is that transforming our covariates (X’s) will not impact the linear relationship between other X’s and the outcome (Y). If we transform Y first, then we need to make sure all X’s have maintained their linear relationship with the transformed Y.\n\n\n\nThere are cases where the LINE assumptions are blatantly broken. When there are obvious issues, especially with linearity, then we need to make a transformation.\nSome fields typically use transformations because of known properties of the data. For example, gene expression data are often log-transformed. In this case, there is heteroscedasticity inherent in the data that needs to be fixed (giving it homoscedasticity).\n\n\n\nThey are NOT synonymous. Only high leverage points are observations far from \\(\\overline{X}\\). Outliers are observations that do not follow the general trend of the other observations. This means an outlier can be right at \\(\\overline{X}\\), but have a Y-value falls very far from the \\(\\widehat{Y}\\) line."
  },
  {
    "objectID": "lessons/08_SLR_Diagnostics/08_SLR_Diag_muddy_points.html#muddy-points-from-winter-2025",
    "href": "lessons/08_SLR_Diagnostics/08_SLR_Diag_muddy_points.html#muddy-points-from-winter-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "gladder will show you what the transformation of a single variable looks like. We can use it as a visual assessment to determine which transformations we might want to try for a variable.\nI showed it for both FLR and LE. Note that I did it for each variable separately! Then I decided LE and FLR, separately, might benefit from a squared or cubed transformation.\n\n\n\nI suggest trying all the ones on the way. This is why I like gladder(). Instead of making a choice on going “up” or “down,” we can look at all the plots and see how each transformation will help us make the variable more normally distributed.\n\n\n\nYes, that is a good order of things! We will talk more about the reasoning for X first when we get to multiple linear regression. The main point is that transforming our covariates (X’s) will not impact the linear relationship between other X’s and the outcome (Y). If we transform Y first, then we need to make sure all X’s have maintained their linear relationship with the transformed Y.\n\n\n\nThere are cases where the LINE assumptions are blatantly broken. When there are obvious issues, especially with linearity, then we need to make a transformation.\nSome fields typically use transformations because of known properties of the data. For example, gene expression data are often log-transformed. In this case, there is heteroscedasticity inherent in the data that needs to be fixed (giving it homoscedasticity).\n\n\n\nThey are NOT synonymous. Only high leverage points are observations far from \\(\\overline{X}\\). Outliers are observations that do not follow the general trend of the other observations. This means an outlier can be right at \\(\\overline{X}\\), but have a Y-value falls very far from the \\(\\widehat{Y}\\) line."
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#loading-the-data",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#loading-the-data",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Loading the data",
    "text": "Loading the data\n\n# Load the data - update code if the file is not in the same location\n# on your computer\ngapm &lt;- read_excel(\"data/Gapminder_vars_2011.xlsx\", \n                   na = \"NA\")  # important!!!! \n\ngapm_sub &lt;- gapm %&gt;% \n  drop_na(LifeExpectancyYrs, FemaleLiteracyRate, FoodSupplykcPPD)\n  # above drops rows with NAs in any of the three variables\n\nglimpse(gapm_sub)\n\nRows: 72\nColumns: 18\n$ country                            &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\",…\n$ CO2emissions                       &lt;dbl&gt; 0.4120, 1.7900, 1.2500, 5.3600, 4.6…\n$ ElectricityUsePP                   &lt;dbl&gt; NA, 2210, 207, NA, 2900, 1810, 258,…\n$ FoodSupplykcPPD                    &lt;dbl&gt; 2110, 3130, 2410, 2370, 3160, 2790,…\n$ IncomePP                           &lt;dbl&gt; 1660, 10200, 5910, 18600, 19600, 70…\n$ LifeExpectancyYrs                  &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8,…\n$ FemaleLiteracyRate                 &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5,…\n$ population                         &lt;dbl&gt; 2.97e+07, 2.93e+06, 2.42e+07, 9.57e…\n$ WaterSourcePrct                    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8,…\n$ geo                                &lt;chr&gt; \"afg\", \"alb\", \"ago\", \"atg\", \"arg\", …\n$ four_regions                       &lt;chr&gt; \"asia\", \"europe\", \"africa\", \"americ…\n$ eight_regions                      &lt;chr&gt; \"asia_west\", \"europe_east\", \"africa…\n$ six_regions                        &lt;chr&gt; \"south_asia\", \"europe_central_asia\"…\n$ members_oecd_g77                   &lt;chr&gt; \"g77\", \"others\", \"g77\", \"g77\", \"g77…\n$ Latitude                           &lt;dbl&gt; 33.00000, 41.00000, -12.50000, 17.0…\n$ Longitude                          &lt;dbl&gt; 66.00000, 20.00000, 18.50000, -61.8…\n$ `World bank region`                &lt;chr&gt; \"South Asia\", \"Europe & Central Asi…\n$ `World bank, 4 income groups 2017` &lt;chr&gt; \"Low income\", \"Upper middle income\"…"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#so-what-do-the-coefficients-mean",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#so-what-do-the-coefficients-mean",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "So what do the coefficients mean??",
    "text": "So what do the coefficients mean??"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#another-way-to-think-of-ssy-ssr-and-sse",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#another-way-to-think-of-ssy-ssr-and-sse",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Another way to think of SSY, SSR, and SSE",
    "text": "Another way to think of SSY, SSR, and SSE\n\nLet’s create a data frame of each component within the SS’s\n\nDeviation in SSY: \\(Y_i - \\overline{Y}\\)\nDeviation in SSR: \\(\\widehat{Y}_i- \\overline{Y}\\)\nDeviation in SSE: \\(Y_i - \\widehat{Y}_i\\)\n\nUsing our simple linear regression model as an example:\n\n\nmr1 &lt;- gapm_sub %&gt;% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)\naug_mlr1 = augment(mr1)\nSS_df = gapm_sub %&gt;% select(LifeExpectancyYrs) %&gt;%\n  mutate(SSY_dev = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         y_hat = aug_mlr1$.fitted, \n         SSR_dev = y_hat - mean(LifeExpectancyYrs), \n         SSE_dev = aug_mlr1$.resid)"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#plot-the-components-of-each-sum-of-squares",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#plot-the-components-of-each-sum-of-squares",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Plot the components of each sum of squares",
    "text": "Plot the components of each sum of squares\n\n\nCode to make the below plots\nSSY_plot = ggplot(SS_df, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y)))\nSSR_plot = ggplot(SS_df, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y)))\nSSE_plot = ggplot(SS_df, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i]))\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = 64.64\\]\n \n\\[SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = 27.24\\]\n \n\\[SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = 37.39\\]"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#when-running-a-f-test-for-linear-models",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#when-running-a-f-test-for-linear-models",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "When running a F-test for linear models…",
    "text": "When running a F-test for linear models…\n\nWe need to define a larger, full model (more parameters)\nWe need to define a smaller, reduced model (fewer parameters)\nUse the F-statistic to decide whether or not we reject the smaller model\n\nThe F-statistic compares the SSE of each model to determine if the full model explains a significant amount of additional variance\n\n\n\n\n \n\\[\nF = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\n\\]\n\n\n\\(SSE(R) \\geq SSE(F)\\)\nNumerator measures difference in unexplained variation between the models\n\nBig difference = added parameters greatly reduce the unexplained variation (increase explained variation)\nSmaller difference = added parameters don’t reduce the unexplained variation\n\nTake ratio of difference to the unexplained variation in the full model"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#poll-everywhere-question-2",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#poll-everywhere-question-2",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#building-the-anova-table-.visibilityhidden",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#building-the-anova-table-.visibilityhidden",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Building the ANOVA table {.visibility=“hidden”}",
    "text": "Building the ANOVA table {.visibility=“hidden”}\nANOVA table (\\(k\\) = # of predictors, \\(n\\) = # of observations)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariation Source\ndf\nSS\nMS\ntest statistic\np-value\n\n\n\n\nRegression\n\\(k\\)\n\\(SSR\\)\n\\(MSR = \\frac{SSR}{k}\\)\n\\(F = \\frac{MSR}{MSE}\\)\n\\(P(F_{(k, n-k-1)}&gt;F)\\)\n\n\nError\n\\(n-k-1\\)\n\\(SSE\\)\n\\(MSE = \\frac{SSE}{n-k-1}\\)\n\n\n\n\nTotal\n\\(n-1\\)\n\\(SSY\\)"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#slr-another-way-to-think-of-ssy-ssr-and-sse",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#slr-another-way-to-think-of-ssy-ssr-and-sse",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "SLR: Another way to think of SSY, SSR, and SSE",
    "text": "SLR: Another way to think of SSY, SSR, and SSE\n\nLet’s create a data frame of each component within the SS’s\n\nDeviation in SSY: \\(Y_i - \\overline{Y}\\)\nDeviation in SSR: \\(\\widehat{Y}_i- \\overline{Y}\\)\nDeviation in SSE: \\(Y_i - \\widehat{Y}_i\\)\n\nUsing our simple linear regression model as an example:\n\n\nslr1 &lt;- gapm_sub %&gt;% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\naug_slr1 = augment(slr1)\nSS_dev_slr = gapm_sub %&gt;% select(LifeExpectancyYrs) %&gt;%\n  mutate(SSY_dev = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         y_hat = aug_slr1$.fitted, \n         SSR_dev = y_hat - mean(LifeExpectancyYrs), \n         SSE_dev = aug_slr1$.resid)"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#mlr-another-way-to-think-of-ssy-ssr-and-sse",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#mlr-another-way-to-think-of-ssy-ssr-and-sse",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "MLR: Another way to think of SSY, SSR, and SSE",
    "text": "MLR: Another way to think of SSY, SSR, and SSE\n\nLet’s create a data frame of each component within the SS’s\n\nDeviation in SSY: \\(Y_i - \\overline{Y}\\)\nDeviation in SSR: \\(\\widehat{Y}_i- \\overline{Y}\\)\nDeviation in SSE: \\(Y_i - \\widehat{Y}_i\\)\n\nUsing our simple linear regression model as an example:\n\n\nmr1 &lt;- gapm_sub %&gt;% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)\naug_mlr1 = augment(mr1)\nSS_df = gapm_sub %&gt;% select(LifeExpectancyYrs) %&gt;%\n  mutate(SSY_dev = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         y_hat = aug_mlr1$.fitted, \n         SSR_dev = y_hat - mean(LifeExpectancyYrs), \n         SSE_dev = aug_mlr1$.resid)"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#plot-the-components-of-each-sum-of-squares-1",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#plot-the-components-of-each-sum-of-squares-1",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Plot the components of each sum of squares",
    "text": "Plot the components of each sum of squares\n\n\nCode to make the below plots\nSSY_plot = ggplot(SS_df, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y)))\nSSR_plot = ggplot(SS_df, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y)))\nSSE_plot = ggplot(SS_df, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i]))\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = 64.64\\]\n \n\\[SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = 36.39\\]\n \n\\[SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = 28.25\\]"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#slr-plot-the-components-of-each-sum-of-squares",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#slr-plot-the-components-of-each-sum-of-squares",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "SLR: Plot the components of each sum of squares",
    "text": "SLR: Plot the components of each sum of squares\n\n\nCode to make the below plots\nSSY_plot = ggplot(SS_dev_slr, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y)))\nSSR_plot = ggplot(SS_dev_slr, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y)))\nSSE_plot = ggplot(SS_dev_slr, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i]))\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = 64.64\\]\n \n\\[SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = 27.24\\]\n \n\\[SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = 37.39\\]"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#mlr-plot-the-components-of-each-sum-of-squares",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#mlr-plot-the-components-of-each-sum-of-squares",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "MLR: Plot the components of each sum of squares",
    "text": "MLR: Plot the components of each sum of squares\n\n\nCode to make the below plots\nSSY_plot = ggplot(SS_df, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y)))\nSSR_plot = ggplot(SS_df, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y)))\nSSE_plot = ggplot(SS_df, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i]))\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = 64.64\\]\n \n\\[SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = 36.39\\]\n \n\\[SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = 28.25\\]"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#what-did-you-notice-in-the-plots",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#what-did-you-notice-in-the-plots",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "What did you notice in the plots?",
    "text": "What did you notice in the plots?\n\n\n\nSimple Linear Regression\n\n\n\nMultiple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n \n\\[SSR = 27.24\\]\n \n \n\\[SSE =37.39\\]\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n \n\\[SSR = 36.39\\]\n \n \n\\[SSE =28.25\\]\n\n\n\nNext class: we can determine if model fit is better by comparing the SSE’s of different models\n\n\n\nLesson 9: MLR Intro"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#interpreting-the-estimated-population-coefficients-1",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#interpreting-the-estimated-population-coefficients-1",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Interpreting the estimated population coefficients",
    "text": "Interpreting the estimated population coefficients\nFor an estimated model: \\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_2 + \\widehat{\\beta}_2 X_2\\)\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_0\\)\n\n\n\\[\\begin{aligned}\n\\widehat{Y} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 0 + \\widehat{\\beta}_2 0\\\\\n\\widehat{Y} = &\\widehat{\\beta}_0 \\\\\n\\end{aligned}\\]\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB)."
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#interpreting-the-estimated-population-coefficients-widehatbeta_0",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#interpreting-the-estimated-population-coefficients-widehatbeta_0",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Interpreting the estimated population coefficients: \\(\\widehat{\\beta}_0\\)",
    "text": "Interpreting the estimated population coefficients: \\(\\widehat{\\beta}_0\\)\n\nFor an estimated model: \\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_2 + \\widehat{\\beta}_2 X_2\\)\n\n\\[\\begin{aligned}\n\\widehat{Y} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 0 + \\widehat{\\beta}_2 0\\\\\n\\widehat{Y} = &\\widehat{\\beta}_0 \\\\\n\\end{aligned}\\]\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB)."
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#interpreting-the-estimated-population-coefficients-widehatbeta_1",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#interpreting-the-estimated-population-coefficients-widehatbeta_1",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Interpreting the estimated population coefficients: \\(\\widehat{\\beta}_1\\)",
    "text": "Interpreting the estimated population coefficients: \\(\\widehat{\\beta}_1\\)\n\nFor an estimated model: \\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_2 + \\widehat{\\beta}_2 X_2\\)\n\n\\[\\begin{aligned}\n\\widehat{Y} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_1 + \\widehat{\\beta}_2 X_2\\\\\n\\widehat{Y} = &\\widehat{\\beta}_0 \\\\\n\\end{aligned}\\]\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB)."
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#interpreting-the-estimated-population-coefficients-widehatbeta_2",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#interpreting-the-estimated-population-coefficients-widehatbeta_2",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Interpreting the estimated population coefficients: \\(\\widehat{\\beta}_2\\)",
    "text": "Interpreting the estimated population coefficients: \\(\\widehat{\\beta}_2\\)\n\nFor an estimated model: \\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_2 + \\widehat{\\beta}_2 X_2\\)\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB)."
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#color-slide",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#color-slide",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Interpreting the estimated population coefficients: \\(\\widehat{\\beta}_2\\)",
    "text": "Interpreting the estimated population coefficients: \\(\\widehat{\\beta}_2\\)\n\n\n\n\nFor an estimated model: \\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_2 + \\widehat{\\beta}_2 X_2\\)\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB)."
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#color-slide1",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#color-slide1",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Interpreting the estimated population coefficient: \\(\\widehat{\\beta}_0\\)",
    "text": "Interpreting the estimated population coefficient: \\(\\widehat{\\beta}_0\\)\n\n\n\n\nFor an estimated model: \\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_2 + \\widehat{\\beta}_2 X_2\\)\n\n\\[\\begin{aligned}\n\\widehat{Y} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 0 + \\widehat{\\beta}_2 0\\\\\n\\widehat{Y} = &\\widehat{\\beta}_0 \\\\\n\\end{aligned}\\]\nInterpretation: The expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB)."
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#color-slide2",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#color-slide2",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Interpreting the estimated population coefficient: \\(\\widehat{\\beta}_1\\)",
    "text": "Interpreting the estimated population coefficient: \\(\\widehat{\\beta}_1\\)\n\n\n\n\nWe will use: \\(x_{1a}\\) and \\(x_{1b} = x_{1a} + 1\\), with the implication that \\(\\Delta{x_1} = x_{1b} - x_{1a} = 1\\)\nOur goal is to get to a statement with \\(\\widehat{\\beta}_1\\) alone:\n\n\\[\\begin{aligned}\n\\widehat{Y}|x_{1a} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1a} + \\widehat{\\beta}_2 X_2\\\\\n\\widehat{Y}|x_{1b} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1b} + \\widehat{\\beta}_2 X_2\\\\\n\\widehat{Y}|x_{1b} - \\widehat{Y}|x_{1a} = & \\bigg[\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1b} + \\widehat{\\beta}_2 X_2\\bigg] - \\bigg[\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1a} + \\widehat{\\beta}_2 X_2\\bigg] \\\\\n\\widehat{Y}|x_{1b} - \\widehat{Y}|x_{1a} = &  \\widehat{\\beta}_1 x_{1b} - \\widehat{\\beta}_1 x_{1a}\\\\\n\\widehat{Y}|x_{1b} - \\widehat{Y}|x_{1a} = &  \\widehat{\\beta}_1 (x_{1b} - x_{1a})\\\\\n\\widehat{Y}|x_{1b} - \\widehat{Y}|x_{1a} = &  \\widehat{\\beta}_1\\\\\n\\end{aligned}\\]\nInterpretation: For every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB)."
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#color-slide3",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#color-slide3",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "Interpreting the estimated population coefficient: \\(\\widehat{\\beta}_2\\)",
    "text": "Interpreting the estimated population coefficient: \\(\\widehat{\\beta}_2\\)\n\n\n\n\nWe can so the same for \\(X_2\\): \\(x_{2a}\\) and \\(x_{2b} = x_{2a} + 1\\), with the implication that \\(\\Delta{x_2} = x_{2b} - x_{2a} = 1\\)\nOur goal is to get to a statement with \\(\\widehat{\\beta}_1\\) alone:\n\n\\[\\begin{aligned}\n\\widehat{Y}|x_{2a} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1a} + \\widehat{\\beta}_2 X_2\\\\\n\\widehat{Y}|x_{2b} = &\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{1b} + \\widehat{\\beta}_2 X_2\\\\\n\\widehat{Y}|x_{2b} - \\widehat{Y}|x_{2a} = & \\bigg[\\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_1 + \\widehat{\\beta}_2 x_{2b} \\bigg] - \\bigg[\\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_1 + \\widehat{\\beta}_2 x_{2a}\\bigg] \\\\\n\\widehat{Y}|x_{2b} - \\widehat{Y}|x_{2a} = &  \\widehat{\\beta}_2 x_{2b} - \\widehat{\\beta}_2 x_{2a}\\\\\n\\widehat{Y}|x_{2b} - \\widehat{Y}|x_{2a} = &  \\widehat{\\beta}_2 (x_{2b} - x_{2a})\\\\\n\\widehat{Y}|x_{2b} - \\widehat{Y}|x_{2a} = &  \\widehat{\\beta}_2\\\\\n\\end{aligned}\\]\nInterpretation: For every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB)."
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro.html#what-we-need-in-our-interpretations-of-coefficients-reference",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro.html#what-we-need-in-our-interpretations-of-coefficients-reference",
    "title": "Lesson 9: Introduction to Multiple Linear Regression (MLR)",
    "section": "What we need in our interpretations of coefficients (reference)",
    "text": "What we need in our interpretations of coefficients (reference)\n\n\n\nUnits of Y\nUnits of X\nIf discussing intercept: Mean or average or expected before Y\nIf discussing coefficient for continuous covariate: Mean or average or expected before difference, increase, or decrease\n\nOR: Mean or average or expected before Y\nNOT: predicted\nOnly need before difference or Y!!\n\nConfidence interval\n\n\n\nIf other covariates in the model\n\nDiscussing intercept: Must state that variables are equal to 0\n\nor at their centered value if centered!\n\nDiscussing coefficient for covariate: Must state “adjusting for all other variables”, “Controlling for all other variables”, or “Holding all other variables constant”\n\nIf only one other variable in the model, then replace “all other variables” with the single variable name"
  },
  {
    "objectID": "lessons/07_SLR_Assumptions/07_SLR_Assumptions_muddy_points.html#muddy-points-from-winter-2025",
    "href": "lessons/07_SLR_Assumptions/07_SLR_Assumptions_muddy_points.html#muddy-points-from-winter-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "This is just tied to how the study is set up. We are basically saying that our study design does not make our observations inherently correlated to one another.\nFor example, let’s think of a study measuring 5 adults’ height every month over a year. We have 12 height measurements for each adult. Since most adults’ heights will not change over their adulthood, those measurements are probably pretty close to one another. However, the 5 sets of measurements for each adult are probably different. We cannot assume that all 60 (5x12) measurements are independent from each other. Each adult’s 12 measurements will be highly correlated with one another. Thus, we do not have independent outcomes in this study.\n\n\n\nFor the most part, we will focus on investigating homoscedasticity (equal variance) from the residual plots. We track across the X-axis, and make sure that the spread of the data looks pretty even across x-values.\n\n\n\nThis is within the geom_smooth() function in our ggplot code.\nHere is the plot without bands:\n\nggplot(gapm, aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_point() +\n1  geom_smooth(method = \"lm\", se = FALSE, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\")\n\n\n1\n\nse = FALSE tells R to omit the bands\n\n\n\n\n\n\n\nHere is the plot with bands:\n\nggplot(gapm, aes(x = FemaleLiteracyRate,\n                 y = LifeExpectancyYrs)) +\n  geom_point() +\n1  geom_smooth(method = \"lm\", se = TRUE, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\")\n\n\n1\n\nse = TRUE tells R to show the bands"
  },
  {
    "objectID": "hw_answers/HW_03_ans.html",
    "href": "hw_answers/HW_03_ans.html",
    "title": "Homework 3 Answers",
    "section": "",
    "text": "Answers are not necessarily complete! This is just meant to serve as a check if you are stuck."
  },
  {
    "objectID": "hw_answers/HW_03_ans.html#directions",
    "href": "hw_answers/HW_03_ans.html#directions",
    "title": "Homework 3 Answers",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the HW3 datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as your HW3 .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\nFor each question, make sure to include all code and resulting output in the html file to support your answers\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nWrite all answers in complete sentences as if communicating the results to a collaborator.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your .qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "hw_answers/HW_03_ans.html#questions",
    "href": "hw_answers/HW_03_ans.html#questions",
    "title": "Homework 3 Answers",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nThis question and data are adapted from this textbook.\nIn an experiment designed to describe the dose–response curve for vitamin K, individual rats were depleted of their vitamin K reserves and then fed dried liver for 4 days at different dosage levels. The response of each rat was measured as the concentration of a clotting agent needed to clot a sample of its blood in 3 minutes. The results of the experiment on 12 rats are given in the following table; values are expressed in common logarithms for both dose and response.\n\nclot = read_excel(here(\"data/CH05Q09.xls\"))\nclot %&gt;% gt() %&gt;%\n  cols_label(RAT = md(\"**Rat**\"),\n             LOGCONC = md(\"**Log10 Concentration (Y)**\"),\n             LOGDOSE = md(\"**Log10 Dose (X)**\"))\n\n\n\n\n\n  \n    \n      Rat\n      Log10 Concentration (Y)\n      Log10 Dose (X)\n    \n  \n  \n    1\n2.65\n0.18\n    2\n2.25\n0.33\n    3\n2.26\n0.42\n    4\n1.95\n0.54\n    5\n1.72\n0.65\n    6\n1.60\n0.75\n    7\n1.55\n0.83\n    8\n1.32\n0.92\n    9\n1.13\n1.01\n    10\n1.07\n1.04\n    11\n0.95\n1.09\n    12\n0.88\n1.15\n  \n  \n  \n\n\n\n\nUse the log-transformed values as given in the dataset.\nUse the following scatterplot to build your answers off of:\n\n\n\n\n\n\n\n\n\n\nPart a\n\nHere is the code for fitting the model:\n\nsummary(clot_mod)\n\n\nCall:\nlm(formula = LOGCONC ~ LOGDOSE, data = clot)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.097151 -0.026859 -0.003392  0.028279  0.095355 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.93620    0.04230   69.41 9.39e-15 ***\nLOGDOSE     -1.78501    0.05267  -33.89 1.18e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05589 on 10 degrees of freedom\nMultiple R-squared:  0.9914,    Adjusted R-squared:  0.9905 \nF-statistic:  1149 on 1 and 10 DF,  p-value: 1.182e-11\n\n\n\n\n\nPart b\n\n\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    LOGDOSE\n1\n3.58845400\n3.588454000\n1148.759\n1.182233e-11\n    Residuals\n10\n0.03123767\n0.003123767\nNA\nNA\n  \n  \n  \n\n\n\n\n\n\n\nPart c\n\n\\[ F = 1148.759 \\]\n\n\n\nPart d\n\nNot shown bc answer is complete solution\n\n\n\n\nQuestion 2\n\nPart a\n\n\n\n[1] -0.9956757\n\n\n\n\n\nPart b\n\n\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    LOGDOSE\n1\n3.58845400\n3.588454000\n1148.759\n1.182233e-11\n    Residuals\n10\n0.03123767\n0.003123767\nNA\nNA\n  \n  \n  \n\n\n\n\n[1] 0.9913701\n\n\n\n\n\nPart c\n\nNot given\n\n\n\n\nQuestion 3\n\nif(!require(Sleuth3)) { install.packages(\"Sleuth3\"); library(Sleuth3) }\nq1_data = ex0824 \n\n\nPart a\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart b\n\nNot given\n\n\n\nPart c\nFit the regression model, display the regression table, and write out the fitted regression line.\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n47.0521633\n0.50421678\n93.31733\n0.000000e+00\n    Age\n-0.6957134\n0.02937509\n-23.68378\n1.168799e-88\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{\\text{RR}} &= 47.05 -0.7 \\cdot \\text{Age}\n\\end{aligned}\\]\n\n\n\nPart d\n\n\n\n[1] \"Rate\"       \"Age\"        \".fitted\"    \".resid\"     \".hat\"      \n[6] \".sigma\"     \".cooksd\"    \".std.resid\"\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nPart e\n\n\n\n\n\n\n\n\n\nPart f\n\n\n\n\n\n\n\n  \n    \n      statistic\n      p.value\n      method\n    \n  \n  \n    0.9782974\n6.177141e-08\nShapiro-Wilk normality test\n  \n  \n  \n\n\n\n\n\n\n\nPart g\n\n\n\n\n\n\n\n\n\nPart h\n\n\n\n# A tibble: 0 × 8\n# ℹ 8 variables: Rate &lt;int&gt;, Age &lt;dbl&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#   .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;\n\n\n\n\n\nPart i\n\n\n\n# A tibble: 618 × 8\n    Rate   Age .fitted .resid    .hat .sigma .cooksd .std.resid\n   &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1    78   1.9    45.7   32.3 0.00347   7.74  0.0296       4.12\n 2    75   0.5    46.7   28.3 0.00395   7.76  0.0259       3.62\n 3    73   3.1    44.9   28.1 0.00310   7.77  0.0201       3.59\n 4    72   1.8    45.8   26.2 0.00350   7.78  0.0197       3.35\n 5    70   2.5    45.3   24.7 0.00328   7.78  0.0164       3.15\n 6    69   1.9    45.7   23.3 0.00347   7.79  0.0154       2.97\n 7    69   4.5    43.9   25.1 0.00273   7.78  0.0140       3.20\n 8    66   4.9    43.6   22.4 0.00263   7.80  0.0107       2.85\n 9    27   1.5    46.0  -19.0 0.00360   7.81  0.0107      -2.43\n10    48  25.3    29.5   18.5 0.00361   7.81  0.0102       2.37\n# ℹ 608 more rows\n\n\n\n\n\nPart j\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nPart k\n\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\nℹ The deprecated feature was likely used in the describedata package.\n  Please report the issue to the authors.\n\n\n\n\n\n\n\n\nPart l\n\nNot given\n\n\n\nPart m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart n\n\nFor the log-transformed Rate:\n\nq1_model2 = lm (Rate_log ~ Age, data = q1_data2)\nq1_mod_t2 = tidy(q1_model2)\nq1_mod_t2 %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n3.84511854\n0.0126276539\n304.49984\n0.000000e+00\n    Age\n-0.01900896\n0.0007356727\n-25.83888\n2.740318e-100\n  \n  \n  \n\n\n\n\n\n\n\nPart o\n\nFor log-transformed Rate:\n\n\n\n\n\n\n\n\nPart p\n\nFor log-transformed Rate:\n\n\n\n\n\n\n\n\nPart q\n\nNot given\n\n\n\n\nQuestion 4\n\ndep_df = read_sas(here(\"data/completedata.sas7bdat\"))\n\n\nPart a\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n6.4144\n2.0501\n3.1288\n0.0018\n2.3882\n10.4406\n    Fatalism\n0.1527\n0.0452\n3.3784\n0.0008\n0.0639\n0.2414\n    Optimism\n−0.3179\n0.0722\n−4.4058\n0.0000\n−0.4596\n−0.1762\n    Spirituality\n0.3587\n0.1291\n2.7781\n0.0056\n0.1051\n0.6122\n  \n  \n  \n\n\n\n\nAnother fun way to display:\n\ntbl_regression(q2_mod_f1, intercept = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n6.4\n2.4, 10\n0.002\n    Fatalism\n0.15\n0.06, 0.24\n&lt;0.001\n    Optimism\n-0.32\n-0.46, -0.18\n&lt;0.001\n    Spirituality\n0.36\n0.11, 0.61\n0.006\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n\nPart b\n\n\n\\(\\beta_0\\): The expected depression score is 6.4 when fatalism, depression, and spirituality scores are 0 (95% CI: 2.4, 10.4).\n\nSame as homework 2: The intercept does not make sense. A score of 0 is outside the range of possible scores for fatalism, optimism, and spirituality.\n\n\\(\\beta_1\\): For every 1 point higher fatalism score, there is an expected difference of 0.15 points higher depression score, adjusting for optimism and spirituality score (95% CI: 0.06, 0.24).\n\n\n\n\nPart c\n\nNot given\n\n\n\nPart d\n\n\\[\\begin{aligned}\n\\widehat{\\text{Depression}} &= 5.39 + 0.15 \\cdot \\text{Fatalism}\n\\end{aligned}\\]"
  },
  {
    "objectID": "hw_answers/HW_03_ans.html#questions-1",
    "href": "hw_answers/HW_03_ans.html#questions-1",
    "title": "Homework 3 Answers",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nThis question and data are adapted from this textbook.\nIn an experiment designed to describe the dose–response curve for vitamin K, individual rats were depleted of their vitamin K reserves and then fed dried liver for 4 days at different dosage levels. The response of each rat was measured as the concentration of a clotting agent needed to clot a sample of its blood in 3 minutes. The results of the experiment on 12 rats are given in the following table; values are expressed in common logarithms for both dose and response.\n\nclot = read_excel(here(\"data/CH05Q09.xls\"))\nclot %&gt;% gt() %&gt;%\n  cols_label(RAT = md(\"**Rat**\"),\n             LOGCONC = md(\"**Log10 Concentration (Y)**\"),\n             LOGDOSE = md(\"**Log10 Dose (X)**\"))\n\n\n\n\n\n  \n    \n      Rat\n      Log10 Concentration (Y)\n      Log10 Dose (X)\n    \n  \n  \n    1\n2.65\n0.18\n    2\n2.25\n0.33\n    3\n2.26\n0.42\n    4\n1.95\n0.54\n    5\n1.72\n0.65\n    6\n1.60\n0.75\n    7\n1.55\n0.83\n    8\n1.32\n0.92\n    9\n1.13\n1.01\n    10\n1.07\n1.04\n    11\n0.95\n1.09\n    12\n0.88\n1.15\n  \n  \n  \n\n\n\n\nUse the log-transformed values as given in the dataset.\nUse the following scatterplot to build your answers off of:\n\n\n\n\n\n\n\n\n\n\nPart a\nFit a linear regression model to the data and add the regression line to the plot.\nSolution:\n\nHere is the code for fitting the model:\n\nclot_mod &lt;- lm(LOGCONC ~ LOGDOSE, data = clot)\n\nIt is not necessary, but typically good practice to look at the summary or regression table of the model.\n\nsummary(clot_mod)\n\n\nCall:\nlm(formula = LOGCONC ~ LOGDOSE, data = clot)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.097151 -0.026859 -0.003392  0.028279  0.095355 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.93620    0.04230   69.41 9.39e-15 ***\nLOGDOSE     -1.78501    0.05267  -33.89 1.18e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05589 on 10 degrees of freedom\nMultiple R-squared:  0.9914,    Adjusted R-squared:  0.9905 \nF-statistic:  1149 on 1 and 10 DF,  p-value: 1.182e-11\n\ntbl_regression(clot_mod, intercept = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n2.9\n2.8, 3.0\n&lt;0.001\n    LOGDOSE\n-1.8\n-1.9, -1.7\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart b\nUse R to create the ANOVA table for the regression described in the exercise.\nSolution:\n\n\nanova(clot_mod) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    LOGDOSE\n1\n3.58845400\n3.588454000\n1148.759\n1.182233e-11\n    Residuals\n10\n0.03123767\n0.003123767\nNA\nNA\n  \n  \n  \n\n\n\n\n\n\n\nPart c\nUsing the F-test, determine whether there is an association between the log10 concentration and log10 dose.\n\n\n\n\n\n\nNote\n\n\n\nMake sure to include all needed steps for an F-test. Calculating the F test statistic (step 5) is not needed if you use the ANOVA table. Make sure your conclusion connects back the research context.\n\n\nSolution:\n\n\nWe will assume that we have done the needed model evaluation\nNull hypothesis:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\nSignificance level:\n\\[\\alpha = 0.05\\]\nTest statistic distribution:\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=1\\) and denominator \\(df=n-2=12-2=10\\).\nCompute the value of the test statistic:\nThis step is optional because the ANOVA table shows us the test statistic and the p-value at the same time.\nThe F-test statistic can be found under the statistic column.\n\\[ F = 1148.759 \\]\nCalculate the p-value:\nThe p-value can be found on the ANOVA table. \\[ p-value = 1.18 \\cdot 10^{-11}\\]\nConclusion:\nWe reject the null hypothesis that the slope is 0 at the \\(5\\%\\) significance level. There is sufficient evidence that there is significant association between the concentration of clotting agent and dose of dried liver (p-value &lt; 0.0001).\n\n\n\n\n\n\n\nNote on grading\n\n\n\nStep 5 is not necessary when we use the ANOVA table from Part b.\n\n\n\n\n\nPart d\nRewrite your hypothesis test in Part e to show the null and alternative models that we are testing. Did we reject the smaller (reduced) model?\n\n\n\n\n\n\nNote\n\n\n\nYou do not need to go through the hypothesis test process again. A quick statement on rejecting or not is okay.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you prefer to write out the models by hand, remember that you can take a picture of your work and insert it into this document. HW0 can be a good reference for how we’ve done this before.\n\n\nSolution:\n\n\nNull model: \\[Y = \\beta_0 + \\epsilon\\]\nAlternative model: \\[Y = \\beta_0 + \\beta_1X + \\epsilon\\]\n\nWe rejected the reduced (null, smaller) model. We had sufficient evidence for the alternative model that captures the association between concentration and dose.\nWe can also write the models as such:\n\nNull model: \\[\\text{Log}_{10}\\text{conc} = \\beta_0 + \\epsilon\\]\nAlternative model: \\[\\text{Log}_{10}\\text{conc} = \\beta_0 + \\beta_1\\text{Log}_{10}\\text{dose} + \\epsilon\\]\n\nOR\n\nNull model: \\[E[Y] = \\beta_0\\]\nAlternative model: \\[E[Y|X] = \\beta_0 + \\beta_1X\\]\n\nOR\n\nNull model: \\[E[\\text{Log}_{10}\\text{conc}] = \\beta_0\\]\nAlternative model: \\[E[\\text{Log}_{10}\\text{conc}|\\text{Log}_{10}\\text{dose}] = \\beta_0 + \\beta_1\\text{Log}_{10}\\text{dose}\\]\n\n\n\n\n\nQuestion 2\nWe will continue to work with the study and dataset from Question 2 above.\n\nPart a\nFind the correlation coefficient between the two variables. Is the value consistent with your description of the relationship in Question 2? Why or why not?\nSolution:\n\n\n(r &lt;- cor(clot$LOGCONC, clot$LOGDOSE))\n\n[1] -0.9956757\n\n\nThe correlation coefficient between the log-transformed clotting agent concentrations and the log-transformed vitamin K doses is -0.9956757, indicating a strong negative linear relationship, which is consistent with what we observed from the scatterplot.\n\n\n\nPart b\nCalculate the coefficient of determination using linear regression summary output. Can we also calculate the coefficient of determination from the ANOVA in Question 2?\nSolution:\n\nThe coefficient of determination is labeled as Multiple R-squared in the summary() output and as r.squared in the glance() output.\n\nsummary(clot_mod)\n\n\nCall:\nlm(formula = LOGCONC ~ LOGDOSE, data = clot)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.097151 -0.026859 -0.003392  0.028279  0.095355 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.93620    0.04230   69.41 9.39e-15 ***\nLOGDOSE     -1.78501    0.05267  -33.89 1.18e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05589 on 10 degrees of freedom\nMultiple R-squared:  0.9914,    Adjusted R-squared:  0.9905 \nF-statistic:  1149 on 1 and 10 DF,  p-value: 1.182e-11\n\nglance(clot_mod) %&gt;% gt()\n\n\n\n\n\n  \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.9913701\n0.9905071\n0.05589067\n1148.759\n1.182233e-11\n1\n18.67896\n-31.35792\n-29.9032\n0.03123767\n10\n12\n  \n  \n  \n\n\n\n\nWe can calculate the coefficient of determination from the ANOVA table that we showed in class.\n\n\n\n\n\n\nNote on the answer\n\n\n\nThis statement is a sufficient answer, but we can continue below.\nAlso, this answer can be “no” if they are referring to the R output for anova() since we only have SSE and SSR, even though SSY = SSE + SSR. We can calculate SSY from the output.\n\n\nWe would use the formala for \\(R^2\\):\n\\[R^2 = \\frac{SSR}{SSY}\\]\nWe can find SSR and SSe in our ANOVA table from Question 2 and then calculate SSY.\n\nanova(clot_mod) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    LOGDOSE\n1\n3.58845400\n3.588454000\n1148.759\n1.182233e-11\n    Residuals\n10\n0.03123767\n0.003123767\nNA\nNA\n  \n  \n  \n\n\n\nclot_anova = anova(clot_mod) %&gt;% tidy()\n\nSSY = sum(clot_anova$sumsq)\nSSR = clot_anova$sumsq[1]\n\n(R2 = SSR/SSY)\n\n[1] 0.9913701\n\n\n\n\n\nPart c\nGive an interpretation of the coefficient of determination in the context of the study.\nSolution:\n\n99.1% of the variation in the log-transformed clotting agent concentration values is explained by regressing on the log-transformed doses.\n\n\n\n\nQuestion 3\nA high respiratory rate is a potential diagnostic indicator of respiratory infection in children. To judge whether a respiratory rate is “high” however, a physician must have a clear picture of the distribution of normal rates. To this end, Italian researchers measured the respiratory rates (in breaths/minute) of 618 children between the ages of 15 days and 3 years (measured in months).\nThe data and problem framing came from the Sleuth3 package. Please make sure to run the following code to load the data. You can directly access the dataset ex0824 from the package. I have included a new assignment of the data to q1_data if you would like to use that.\n\nif(!require(Sleuth3)) { install.packages(\"Sleuth3\"); library(Sleuth3) }\nq1_data = ex0824 \n\n\nPart a\nCreate a scatterplot of the dependent and independent variables with both the best-fit line and a smoothed curve through the points. Describe the relationship between the dependent and independent variables, and also comment on whether you think it is reasonable to use a linear regression to model the relationship.\nSolution:\n\n\nggplot(data = q1_data, aes(x = Age, y = Rate)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_smooth(se = F, color = \"#F14124\") +\n  labs(title = \"Scatterplot of Respiratory Rate vs. Age\", \n       x = \"Age (months)\", \n       y = \"Respiratory Rate (breaths/minute)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe relationship between respiratory rate and age is negative, meaning that as age increases the respiratory rate decreases. The relationship looks fairly strong, although I would say it is not linear since there is some curvature to the trend. It looks like the respiratory rate decreases at a larger magnitude from age 0 to 10 months compared to age 10 to 36 months.\n\n\n\nPart b\nWrite out the population regression model for the simple linear regression model. Please leave the variables untransformed for now.\n\nFor \\(RR\\) = Respiratory rate:\n\\[RR = \\beta_0 + \\beta_1 Age + \\epsilon\\]\nOR\n\\[E[RR|Age] = \\beta_0 + \\beta_1 Age\\]\n\n\n\nPart c\nFit the regression model, display the regression table, and write out the fitted regression line.\n\n\nq1_model = lm (Rate ~ Age, data = q1_data)\nq1_mod_t = tidy(q1_model)\nq1_mod_t %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n47.0521633\n0.50421678\n93.31733\n0.000000e+00\n    Age\n-0.6957134\n0.02937509\n-23.68378\n1.168799e-88\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{\\text{RR}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot \\text{Age} \\\\\n\\widehat{\\text{RR}} &= 47.05 -0.7 \\cdot \\text{Age}\n\\end{aligned}\\]\nNote: See my LaTeX code in my .qmd file if you want to see how I pulled the population coefficient estimates directly from my R code.\n\n\n\nPart d\nAssess the normality of the model’s fitted residuals by creating a histogram, density plot, and boxplot of the residuals to visually inspect the distribution of the residuals, and describe any deviations from normality.\n\n\n# augment is from the broom package\nq1_aug &lt;- augment(q1_model)\nnames(q1_aug) # Just me looking at the names to call in each plot\n\n[1] \"Rate\"       \"Age\"        \".fitted\"    \".resid\"     \".hat\"      \n[6] \".sigma\"     \".cooksd\"    \".std.resid\"\n\n\nNow we use .resid to make each plot:\n\nhist1 &lt;- ggplot(q1_aug, aes(x = .resid)) +\n  geom_histogram()\n\ndensity1 &lt;- ggplot(q1_aug, aes(x = .resid)) +\n  geom_density()\n\nbox1 &lt;- ggplot(q1_aug, aes(x = .resid)) +\n  geom_boxplot()\n\ngrid.arrange(hist1, density1, box1, nrow = 1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFrom the figures above, we see that the model’s fitted residuals have a mostly unimodal, slightly skewed right distribution. Based on the boxplot, there are several outliers among the residuals. It looks pretty close to normal except for the right skew.\n\n\n\nPart e\nAssess the normality of the model’s fitted residuals by creating QQ plot of the residuals.\nBonus work, but not required: Compare the QQ plot to 4 such plots simulated from normal data, and discuss why or why not the residuals could have come from a normal distribution.\n\nNormal QQ plot of fitted residuals:\nThe normal QQ plot of the fitted residuals shows deviations from normality primarily for the larger residual values (around 2 and above on the theoretical X axis). The larger residuals fall above the line indicating that they are further from the mean than would be expected for a normal distribution, while the lower residual values fall fairly close to the line. Thus the right tail that we observed in the distribution above (in Part d) is larger as one would expect from a normal distribution, and the figures look skewed right since the larger residuals are further from the mean rather than tapering off to the right to create a right tail. We might be okay with this QQ plot, but because the scatterplot of respiratory rate and age looked curved, we might consider a transformation.\n\nggplot(q1_aug, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line() +   # line\n  labs(y = \"Fitted residuals\", \n       x = \"Theoretical quantiles\")\n\n\n\n\n \nExtra: Simulated QQ plots with n = 618\nThe randomly generated normal QQ plots below vary in how normal they look. Compared to the normal QQ plot of the model’s residuals, at a theoretical quantile of 2 and above, the simulated sample stays closer to the line. A few points taper off, but they do not deviate from the line as much as the model’s fitted residuals.\n\nset.seed(2323)\n\n(samplesize &lt;- nobs(q1_model))\n\n[1] 618\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, color = \"blue\") # line y=x\n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, color = \"blue\")\n\ngrid.arrange(rand_qq1, rand_qq2, rand_qq3, rand_qq4, ncol =2)\n\n\n\n\n\n\n\nPart f\nTest the normality of the model’s fitted residuals and comment on whether the test’s conclusion is consistent with your visual inspection or not. Make sure to include the hypotheses, needed R code, and a conclusion to the test based on the p-value (as shown in these slides).\n\nShapiro-Wilk test of normality:\n\n\\(H_0\\): data are from a normal population\n\\(H_A\\): data are NOT from a normal population\n\n\nshapiro.test(q1_aug$.resid) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      statistic\n      p.value\n      method\n    \n  \n  \n    0.9782974\n6.177141e-08\nShapiro-Wilk normality test\n  \n  \n  \n\n\n\n\nConclusion: Reject the null. Data are not from a normal distribution.\n\n\n\nPart g\nCreate a residual plot using ggplot and the residuals. Discuss what this means in the context of our model assumptions.\n\n\nggplot(q1_aug, aes(x = Age, y = .resid)) + # may also use y = .std.resid\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"#FF8021\") +\n  labs(title = \"Residual plot\")\n\n\n\n\nAs age increases, the variance of the residuals decrease. The spread of the residuals becomes smaller. This means the variance is not constant. We do not achieve homoscedasticity, instead we have heteroscedasticity. Thus, we do not meet the “E” assumption for equality of variance.\n\n\n\nPart h\nDetermine whether there are any observations with high leverage. Please use the cutoff, \\(h_i &gt; 6/n\\). If there are observations with high leverage, print the observations and state how many high leverage points there are.\n\nFirst, we find the number of observations to use in the cutoff. I will use the cutoff, \\(h_i &gt; 6/n\\).\n\nn = nobs(q1_model)\n\nThen I will filter the values to determine if there are high leverage points.\n\nq1_aug %&gt;% filter(.hat &gt; 6/n) %&gt;%\n  arrange(desc(.hat))\n\n# A tibble: 0 × 8\n# ℹ 8 variables: Rate &lt;int&gt;, Age &lt;dbl&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#   .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;\n\n\nThere are 0 high leverage points. Note that if we used the cutoff \\(4/n\\) there would be 43 high leverage points.\nThis is extra work only if curious about using the \\(4/n\\) cutoff and their position on the scatterplot.\n\nggplot(q1_aug, aes(x = Age, y = Rate)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  geom_text(aes(label = ifelse(.hat &gt; 4/n, as.character(rownames(q1_aug)), '')), \n                nudge_x = 1.5, nudge_y = 0) +\n  geom_vline(xintercept = mean(q1_aug$Age), color = \"grey\") +\n  geom_hline(yintercept = mean(q1_aug$Rate), color = \"grey\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nBecause the “high leverage” points according the to 4/n cutoff are still within the data, and more importantly, within the defined, desired age range of the study, then we don’t need to worry about them. They are also not high leverage points with a less conservative, but still viable, cutoff.\n\n\n\nPart i\nPrint the 10 observations with highest Cook’s distance. If there are observations with high Cook’s distance (\\(d_i &gt;1\\)), state how many observations have high Cook’s distance.\n\nI will arrange the augmented data frame by highest to lowest Cook’s distance. It automatically prints the top 10.\n\nq1_aug %&gt;% arrange(desc(.cooksd))\n\n# A tibble: 618 × 8\n    Rate   Age .fitted .resid    .hat .sigma .cooksd .std.resid\n   &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1    78   1.9    45.7   32.3 0.00347   7.74  0.0296       4.12\n 2    75   0.5    46.7   28.3 0.00395   7.76  0.0259       3.62\n 3    73   3.1    44.9   28.1 0.00310   7.77  0.0201       3.59\n 4    72   1.8    45.8   26.2 0.00350   7.78  0.0197       3.35\n 5    70   2.5    45.3   24.7 0.00328   7.78  0.0164       3.15\n 6    69   1.9    45.7   23.3 0.00347   7.79  0.0154       2.97\n 7    69   4.5    43.9   25.1 0.00273   7.78  0.0140       3.20\n 8    66   4.9    43.6   22.4 0.00263   7.80  0.0107       2.85\n 9    27   1.5    46.0  -19.0 0.00360   7.81  0.0107      -2.43\n10    48  25.3    29.5   18.5 0.00361   7.81  0.0102       2.37\n# ℹ 608 more rows\n\n\nThere are 0 observations with high Cook’s distance (\\(d_i &gt;1\\)).\n\n\n\nPart j\nCreate a histogram for rate. Describe its distribution shapes.\n\n\nggplot(q1_data, aes(x = Rate)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nRate looks a little closer to a normal distribution with the right skew.\n\n\n\nPart k\nUsing the above histogram, and Tukey’s ladder of transformations, discuss the range of transformations that will be appropriate for Rate. Explain your reasoning.\nThen use gladder() to decide on two possible transformations. Explain your reasoning.\nNote: questions below will ask about model fit with the transformations. For now, just explain why you chose the two that you did.\n\nSince rate is skewed right, we want to compress the larger values towards the rest of the data, so selecting a power &lt; 1 is a good idea.\nLet’s look at the gladder() output to confirm.\n\ngladder(q1_data$Rate)\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\nℹ The deprecated feature was likely used in the describedata package.\n  Please report the issue to the authors.\n\n\n\n\n\nSome of the lower powers (from -3 to -1) keep the right skew. It looks like the log transformation (power 0) will result in the best spread of our rate data. The density plot looks fairly normal, and we do not have much skew to the data. Thus, I will move forward with the log transformation of rate. I think this is the best option.\nEither the square root (power -1/2) or inverse square root (power -1) are viable second options. Both have improved the spread and have reduced the right skew. I would have a slight preference for the inverse square root only because the histogram for the square root have a lot of alternating high and low counts next to each other.\n\n\n\nPart l\nAdd the two rate transformations you chose above to the dataset. You do not need to print any output, just make sure the code is visible.\n\nI will add all 3 that are acceptable.\n\nq1_data2 = q1_data %&gt;%\n  mutate(Rate_log = log(Rate), \n         Rate_sr = sqrt(Rate), \n         Rate_inv_sr = 1/sqrt(Rate))\n\n\n\n\nPart m\nCreate scatterplots using two transformed rates and age. Discuss if either transformation potentially improves the model fit. Explain why or why not. Note: including lines will help!\n\n\nggplot(data = q1_data2, aes(x = Age, y = Rate_log)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Scatterplot of Log Respiratory Rate vs. Age\", \n       x = \"Age (months)\", \n       y = \"Log Respiratory Rate (log breaths/minute)\")\n\n\n\n\n\n\n\nggplot(data = q1_data2, aes(x = Age, y = Rate_sr)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Scatterplot of Square root Respiratory Rate vs. Age\", \n       x = \"Age (months)\", \n       y = \"SqRoot Respiratory Rate \\n (square root breaths/minute)\")\n\n\n\n\n\n\n\nggplot(data = q1_data2, aes(x = Age, y = Rate_inv_sr)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Scatterplot of Inverse square root Respiratory Rate vs. Age\", \n       x = \"Age (months)\", \n       y = \"Inverse SqRoot Respiratory Rate \\n (inverse square root breaths/minute)\")\n\n\n\n\n\n\n\n\nAll three (only need two) improve the linearity of the relationship between Rate and Age. For all three, there is less curvature in the plot at the lower ages. And it looks like the spread of the outcome is more constant across ages.\nNicky’s note: my rank order of best to worst transformation to keep investigating is: log-transformation, inverse square root, then square root. Log-transformation and square root look similar, but log-transformation is doing a better job of decreasing the spread at the lower ages, so I would definitely investigate log-transformation over square root. I would investigate inverse square root over square root because I can’t quite tell what the residual plot will look like, while I have a sense of the square root’s residual plot (because it will be very similar to the original data). I think the inverse square root results in more curvature, so I would investigate the log-transformation over the inverse square root. And while both transformations are hard to wrap my head around, the log-transformation at least continues to follow the increasing trend of the original data. Inverse sqaure root might easily be mis-interpreted.\n\n\n\nPart n\nUsing one of the transformed outcomes, fit the regression model, display the regression table, and write out the fitted regression line.\nSolution:\n\nFor the log-transformed Rate:\n\nq1_model2 = lm (Rate_log ~ Age, data = q1_data2)\nq1_mod_t2 = tidy(q1_model2)\nq1_mod_t2 %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n3.84511854\n0.0126276539\n304.49984\n0.000000e+00\n    Age\n-0.01900896\n0.0007356727\n-25.83888\n2.740318e-100\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{\\text{log-RR}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot \\text{Age} \\\\\n\\widehat{\\text{log-RR}} &= 3.85 -0.02 \\cdot \\text{Age}\n\\end{aligned}\\]\nFor the square-root Rate:\n\nq1_model3 = lm (Rate_sr ~ Age, data = q1_data2)\nq1_mod_t3 = tidy(q1_model3)\nq1_mod_t3 %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n6.84609789\n0.03925737\n174.39012\n0.000000e+00\n    Age\n-0.05707352\n0.00228709\n-24.95465\n1.618702e-95\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{\\sqrt{\\text{RR}}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot \\text{Age} \\\\\n\\widehat{\\sqrt{\\text{RR}}} &= 6.85 -0.06 \\cdot \\text{Age}\n\\end{aligned}\\]\nFor the inverse square-root Rate:\n\nq1_model4 = lm (Rate_inv_sr ~ Age, data = q1_data2)\nq1_mod_t4 = tidy(q1_model4)\nq1_mod_t4 %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n0.146236446\n1.051613e-03\n139.05918\n0.000000e+00\n    Age\n0.001606453\n6.126577e-05\n26.22105\n2.384887e-102\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{\\text{RR}^{-1/2}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot \\text{Age} \\\\\n\\widehat{\\text{RR}^{-1/2}} &= 0.15 + 0.002 \\cdot \\text{Age}\n\\end{aligned}\\]\nNote: See my LaTeX code in my .qmd file if you want to see how I pulled the population coefficient estimates directly from my R code.\n\n\n\nPart o\nAssess the normality of the model’s fitted residuals by creating QQ plot of the residuals. Does the transformation improve the QQ plot?\nSolution:\n\nFor log-transformed Rate:\n\nq1_aug2 &lt;- augment(q1_model2)\nggplot(q1_aug2, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line() +   # line\n  labs(y = \"Fitted residuals\", \n       x = \"Theoretical quantiles\")\n\n\n\n\nThe QQ plot for the model with log-transformed rate looks pretty good. Some of the higher theoretical values correspond to lower fitted residuals, but for the most part, the points stay on the line. The transformation has definitely improved the QQ plot!\nFor square-root Rate:\n\nq1_aug3 &lt;- augment(q1_model3)\nggplot(q1_aug3, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line() +   # line\n  labs(y = \"Fitted residuals\", \n       x = \"Theoretical quantiles\")\n\n\n\n\nFor inverse square-root Rate:\n\nq1_aug4 &lt;- augment(q1_model4)\nggplot(q1_aug4, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line() +   # line\n  labs(y = \"Fitted residuals\", \n       x = \"Theoretical quantiles\")\n\n\n\n\n\n\n\nPart p\nCreate a residual plot using ggplot and the residuals. Discuss what this means in the context of our model assumptions. Does the transformation improve our model assumptions?\n\nAll three residual plots below show that each transformation improved the constant variance assumption. The spread of the residuals do not have such a defined pattern as age increases. I would say for the log-transformation in the range of 0-5 months, the spread is a little more than the rest of the ages, but it’s admissible. For the square root and inverse square root, it looks a little like the residuals follow a curve. That would be okay, but both have a slight curve in their scatterplot, some curve in the QQ plot, and are harder to interpret. For the square root, the spread of residuals definitely still decreases as age increases.\nFor log-transformed Rate:\n\nggplot(q1_aug2, aes(x = Age, y = .resid)) + # may also use y = .std.resid\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"#FF8021\") +\n  labs(title = \"Residual plot\")\n\n\n\n\nFor square-root Rate:\n\nggplot(q1_aug3, aes(x = Age, y = .resid)) + # may also use y = .std.resid\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"#FF8021\") +\n  labs(title = \"Residual plot\")\n\n\n\n\nFor inverse square-root Rate:\n\nggplot(q1_aug4, aes(x = Age, y = .resid)) + # may also use y = .std.resid\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"#FF8021\") +\n  labs(title = \"Residual plot\")\n\n\n\n\n\n\n\nPart q\nBetween the model with the untransformed outcome and the transformed outcome, which would you recommend using for analysis? (Hint: there are pros and cons to both models)\nSolution:\n\nI would recommend the log-transformed rate model over the untransformed rate model. The transformation helped with linearity, constant variance, and normality. While the log will be a little hard to interpret, it is definitely worth using in the model!\nI would not use the square root nor inverse square root over the log-transformed rate.\n\n\n\n\nQuestion 4\nThis question uses the same dataset as HW 2, question 1.\nThis question is based on data collected as part of an observational study of patients who suffered from stroke.\nDataset: The main goal was to study various psychological factors: optimism, fatalism, depression, spirituality, and their relationship with stroke severity and other health outcomes among the study participants. Data were collected using questionnaires during a baseline interview and also medical chart review. More information about this study can be found in the article Fatalism, optimism, spirituality, depressive symptoms and stroke outcome: a population based analysis.\nThe dataset that you will work with is called completedata.sas7bdat. The two variables we are interested in are:\n\nCovariate 1: Fatalism (larger values indicate that the individual feels less control of their life)\n\nPotential scores range from 8 to 40\n\nCovariate 2: Optimism (larger values indicate that the individual feels higher levels of optimism)\n\nPotential scores range from 6 to 24\n\nCovariate 3: Spirituality (larger values indicate that the individual has more belief in a higher power)\n\nPotential scores range from 2 to 8\n\nOutcome: Depression (larger values imply increased depression)\n\nPotential scores range from 0 to 27\n\n\nFor our homework purposes we will assume each variable is continuous.\n\ndep_df = read_sas(here(\"data/completedata.sas7bdat\"))\n\n\nPart a\nFit the regression model with all the covariates (Fatalism, Optimism, Spirituality), display the regression table, and write out the fitted regression line.\nSolution:\n\nOur population model is: (not needed in work) \\[ \\text{Depression} = \\beta_0 + \\beta_1 \\text{Fatalism} + \\beta_2 \\text{Optimism} + \\beta_3 \\text{Spirituality}\\]\nWe can fit the model in R and report the regression table:\n\nq2_mod_f1 = lm(Depression ~ Fatalism + Optimism + Spirituality, \n                data = dep_df)\n\nq2_mod_ft = tidy(q2_mod_f1, conf.int = T)\n\nq2_mod_ft %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 15) %&gt;% fmt_number(decimals = 4)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n6.4144\n2.0501\n3.1288\n0.0018\n2.3882\n10.4406\n    Fatalism\n0.1527\n0.0452\n3.3784\n0.0008\n0.0639\n0.2414\n    Optimism\n−0.3179\n0.0722\n−4.4058\n0.0000\n−0.4596\n−0.1762\n    Spirituality\n0.3587\n0.1291\n2.7781\n0.0056\n0.1051\n0.6122\n  \n  \n  \n\n\n\ntbl_regression(q2_mod_f1, intercept = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n6.4\n2.4, 10\n0.002\n    Fatalism\n0.15\n0.06, 0.24\n&lt;0.001\n    Optimism\n-0.32\n-0.46, -0.18\n&lt;0.001\n    Spirituality\n0.36\n0.11, 0.61\n0.006\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nOur population model is: (not needed in work) \\[\\begin{aligned}\n\\widehat{\\text{Depression}} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\text{Fatalism} + \\widehat\\beta_2 \\text{Optimism} + \\widehat\\beta_3 \\text{Spirituality} \\\\\n\\widehat{\\text{Depression}} &= 6.4 + 0.15 \\cdot \\text{Fatalism} -0.32 \\cdot \\text{Optimism} + 0.36 \\cdot \\text{Spirituality}\n\\end{aligned}\\]\n\n\n\nPart b\nInterpret each coefficient (\\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\)).\nDoes the intercept make sense for the range of values that each covariate can take? Explain.\nSolution:\n\n\n\\(\\beta_0\\): The expected depression score is 6.4 when fatalism, depression, and spirituality scores are 0 (95% CI: 2.4, 10.4).\n\nSame as homework 2: The intercept does not make sense. A score of 0 is outside the range of possible scores for fatalism, optimism, and spirituality.\n\n\\(\\beta_1\\): For every 1 point higher fatalism score, there is an expected difference of 0.15 points higher depression score, adjusting for optimism and spirituality score (95% CI: 0.06, 0.24).\n\\(\\beta_2\\): For every 1 point higher optimism score, there is an expected difference of 0.32 points lower depression score, adjusting for fatalism and spirituality score (95% CI: -0.46, -0.18).\n\\(\\beta_4\\): For every 1 point higher spirituality score, there is an expected difference of 0.36 points higher depression score, adjusting for fatalism and optimism score (95% CI: 0.11, 0.61).\n\n\n\n\nPart c\nRecall in Homework 2, we ran a simple linear regression model for Depression vs. Fatalism with the following interpretation for the coefficient: For every 1 point higher fatalism score, there is an expected difference of 0.25 points higher depression score (95%CI: 0.17, 0.32).\nDoes the addition of Optimism and Spirituality change our coefficient estimate for Fatalism? (No need for an official hypothesis test here. I just want us to note some differences.)\nSolution:\n\nYes, the addition of optimism and spirituality changes our coefficient estimate for fatalism. We go from 0.25 to 0.15. The sign stays positive, meaning an increase in fatalism score is associated with an increase in depression score. However, the magnitude of this effect is reduced with the addition of the other variables.\n\n\n\nPart d\nFrom the fitted regression model, calculate the regression line when Optimism score is 10 and Spirituality score is 6.\nSolution:\n\n\\[\\begin{aligned}\n\\widehat{\\text{Depression}} &= 6.4 + 0.15 \\cdot \\text{Fatalism} -0.32 \\cdot \\text{Optimism} + 0.36 \\cdot \\text{Spirituality} \\\\\n\\widehat{\\text{Depression}} &= 6.4 + 0.15 \\cdot \\text{Fatalism} -0.32 \\cdot 10 + 0.36 \\cdot 6 \\\\\n\\widehat{\\text{Depression}} &= 6.4 + 0.15 \\cdot \\text{Fatalism} -3.18 + 2.15\\\\\n\\widehat{\\text{Depression}} &= 5.39 + 0.15 \\cdot \\text{Fatalism}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#slr-another-way-to-think-of-ssy-ssr-and-sse",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#slr-another-way-to-think-of-ssy-ssr-and-sse",
    "title": "Lesson 10: MLR: Inference + F-test",
    "section": "SLR: Another way to think of SSY, SSR, and SSE",
    "text": "SLR: Another way to think of SSY, SSR, and SSE\n\nLet’s create a data frame of each component within the SS’s\n\nDeviation in SSY: \\(Y_i - \\overline{Y}\\)\nDeviation in SSR: \\(\\widehat{Y}_i- \\overline{Y}\\)\nDeviation in SSE: \\(Y_i - \\widehat{Y}_i\\)\n\nUsing our simple linear regression model as an example:\n\n\nslr1 &lt;- gapm_sub %&gt;% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate)\naug_slr1 = augment(slr1)\nSS_dev_slr = gapm_sub %&gt;% select(LifeExpectancyYrs) %&gt;%\n  mutate(SSY_dev = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         y_hat = aug_slr1$.fitted, \n         SSR_dev = y_hat - mean(LifeExpectancyYrs), \n         SSE_dev = aug_slr1$.resid)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#slr-plot-the-components-of-each-sum-of-squares",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#slr-plot-the-components-of-each-sum-of-squares",
    "title": "Lesson 10: MLR: Inference + F-test",
    "section": "SLR: Plot the components of each sum of squares",
    "text": "SLR: Plot the components of each sum of squares\n\n\nCode to make the below plots\nSSY_plot = ggplot(SS_dev_slr, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y)))\nSSR_plot = ggplot(SS_dev_slr, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y)))\nSSE_plot = ggplot(SS_dev_slr, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i]))\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = 64.64\\]\n \n\\[SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = 27.24\\]\n \n\\[SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = 37.39\\]"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#mlr-another-way-to-think-of-ssy-ssr-and-sse",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#mlr-another-way-to-think-of-ssy-ssr-and-sse",
    "title": "Lesson 10: MLR: Inference + F-test",
    "section": "MLR: Another way to think of SSY, SSR, and SSE",
    "text": "MLR: Another way to think of SSY, SSR, and SSE\n\nLet’s create a data frame of each component within the SS’s\n\nDeviation in SSY: \\(Y_i - \\overline{Y}\\)\nDeviation in SSR: \\(\\widehat{Y}_i- \\overline{Y}\\)\nDeviation in SSE: \\(Y_i - \\widehat{Y}_i\\)\n\nUsing our simple linear regression model as an example:\n\n\nmr1 &lt;- gapm_sub %&gt;% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)\naug_mlr1 = augment(mr1)\nSS_df = gapm_sub %&gt;% select(LifeExpectancyYrs) %&gt;%\n  mutate(SSY_dev = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         y_hat = aug_mlr1$.fitted, \n         SSR_dev = y_hat - mean(LifeExpectancyYrs), \n         SSE_dev = aug_mlr1$.resid)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#mlr-plot-the-components-of-each-sum-of-squares",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#mlr-plot-the-components-of-each-sum-of-squares",
    "title": "Lesson 10: MLR: Inference + F-test",
    "section": "MLR: Plot the components of each sum of squares",
    "text": "MLR: Plot the components of each sum of squares\n\n\nCode to make the below plots\nSSY_plot = ggplot(SS_df, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y)))\nSSR_plot = ggplot(SS_df, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y)))\nSSE_plot = ggplot(SS_df, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i]))\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = 64.64\\]\n \n\\[SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = 36.39\\]\n \n\\[SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = 28.25\\]"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#what-did-you-notice-in-the-plots",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#what-did-you-notice-in-the-plots",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "What did you notice in the plots?",
    "text": "What did you notice in the plots?\n\n\n\nSimple Linear Regression\n\n\n\nMultiple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n \n\\[SSR = 27.24\\]\n \n \n\\[SSE =37.39\\]\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n \n\\[SSR = 36.39\\]\n \n \n\\[SSE =28.25\\]"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#variation-explained-vs.-unexplained-1",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#variation-explained-vs.-unexplained-1",
    "title": "Lesson 10: MLR: Inference + F-test",
    "section": "Variation: Explained vs. Unexplained",
    "text": "Variation: Explained vs. Unexplained\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 &= \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY &= SSR + SSE\n\\end{aligned}\\]\n\n\\(Y_i - \\overline{Y}\\) = the deviation of \\(Y_i\\) around the mean \\(\\overline{Y}\\)\n\n(the total amount deviation unexplained at \\(X_{i1},\\ldots,X_{ik}\\) ).\n\n\\(\\widehat{Y}_i- \\overline{Y}\\) = the deviation of the fitted value \\(\\widehat{Y}_i\\) around the mean \\(\\overline{Y}\\)\n\n(the amount deviation explained by the regression at \\(X_{i1},\\ldots,X_{ik}\\) ).\n\n\\(Y_i - \\widehat{Y}_i\\) = the deviation of the observation \\(Y\\) around the fitted regression line\n\n(the amount deviation unexplained by the regression at \\(X_{i1},\\ldots,X_{ik}\\) )"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#poll-everywhere-question-4-1",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#poll-everywhere-question-4-1",
    "title": "Lesson 10: MLR: Inference + F-test",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#covariate-subset-test-group-of-coefficients",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#covariate-subset-test-group-of-coefficients",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Covariate subset test: group of coefficients",
    "text": "Covariate subset test: group of coefficients\nDoes the addition of some group of covariates of interest (or a multi-level categorical variable) add significantly to the prediction of Y obtained through other independent variables already present in the model?\n\nFor a general population MLR model, \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\nWe can create a hypothesis test for a group of covariate coefficients (subset of many)… For example…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=\\beta_3 =0\\) (this can be any coefficients)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\nAt least one \\(\\beta_j\\neq0\\) (for \\(j=2,3\\))\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(Y = \\beta_0 + \\beta_2 X_2 + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X + \\beta_3 X_3+\\epsilon\\)"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#slr-plot-histogram-of-deviations-for-le-beta_0-beta_1-flr-epsilon",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#slr-plot-histogram-of-deviations-for-le-beta_0-beta_1-flr-epsilon",
    "title": "Lesson 10: MLR: Inference + F-test",
    "section": "SLR: Plot histogram of deviations for \\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)",
    "text": "SLR: Plot histogram of deviations for \\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\n\nCode to make the below plots\nSSY_plot = ggplot(SS_dev_slr, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y)))\nSSR_plot = ggplot(SS_dev_slr, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y)))\nSSE_plot = ggplot(SS_dev_slr, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i]))\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = 64.64\\]\n \n\\[SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = 27.24\\]\n \n\\[SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = 37.39\\]"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#plot-histogram-of-deviations-for-le-beta_0-beta_1-flr-epsilon",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#plot-histogram-of-deviations-for-le-beta_0-beta_1-flr-epsilon",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Plot histogram of deviations for \\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)",
    "text": "Plot histogram of deviations for \\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\n\nCode to make the below plots\nSSY_plot = ggplot(SS_dev_slr, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y)))\nSSR_plot = ggplot(SS_dev_slr, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y)))\nSSE_plot = ggplot(SS_dev_slr, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i]))\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = 64.64\\]\n \n\\[SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = 27.24\\]\n \n\\[SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = 37.39\\]"
  },
  {
    "objectID": "lessons/10_MLR_Inf/10_MLR_Inf.html#plot-histogram-of-deviations-for-le-beta_0-beta_1-flr-beta_2-fs-epsilon",
    "href": "lessons/10_MLR_Inf/10_MLR_Inf.html#plot-histogram-of-deviations-for-le-beta_0-beta_1-flr-beta_2-fs-epsilon",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Plot histogram of deviations for \\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\)",
    "text": "Plot histogram of deviations for \\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\)\n\n\nCode to make the below plots\nSSY_plot = ggplot(SS_df, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y)))\nSSR_plot = ggplot(SS_df, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y)))\nSSE_plot = ggplot(SS_df, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i]))\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = 64.64\\]\n \n\\[SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = 36.39\\]\n \n\\[SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = 28.25\\]"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#lets-map-that-to-our-regression-analysis-process",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#lets-map-that-to-our-regression-analysis-process",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Let’s map that to our regression analysis process",
    "text": "Let’s map that to our regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#we-must-revisit-our-dear-friend-the-f-test",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#we-must-revisit-our-dear-friend-the-f-test",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "We must revisit our dear friend, the F-test!",
    "text": "We must revisit our dear friend, the F-test!\n\nhttps://www.writerswrite.co.za/foreshadowing/"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#remember-from-lesson-5-f-test-vs.-t-test-for-the-population-slope",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#remember-from-lesson-5-f-test-vs.-t-test-for-the-population-slope",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Remember from Lesson 5: F-test vs. t-test for the population slope",
    "text": "Remember from Lesson 5: F-test vs. t-test for the population slope\nThe square of a \\(t\\)-distribution with \\(df = \\nu\\) is an \\(F\\)-distribution with \\(df = 1, \\nu\\)\n\\[T_{\\nu}^2 \\sim F_{1,\\nu}\\]\n\nWe can use either F-test or t-test to run the following hypothesis test:\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote that the F-test does not support one-sided alternative tests, but the t-test does!"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#remember-from-lesson-5-planting-a-seed-about-the-f-test",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#remember-from-lesson-5-planting-a-seed-about-the-f-test",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Remember from Lesson 5: Planting a seed about the F-test",
    "text": "Remember from Lesson 5: Planting a seed about the F-test\nWe can think about the hypothesis test for the slope…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_1\\neq0\\)\n\n\n\n\nin a slightly different way…\n\n\n\n\n\n\nNull model (\\(\\beta_1=0\\))\n\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\nSmaller (reduced) model\n\n\n\n\n\n\nAlternative model (\\(\\beta_1\\neq0\\))\n\n\n\n\\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)\nLarger (full) model\n\n\n\n\n\n\nIn multiple linear regression, we can start using this framework to test multiple coefficient parameters at once\n\nDecide whether or not to reject the smaller reduced model in favor of the larger full model\nCannot do this with the t-test!"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#we-can-extend-this",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#we-can-extend-this",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "We can extend this!!",
    "text": "We can extend this!!\nWe can create a hypothesis test for more than one coefficient at a time…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=\\beta_2=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_1\\neq0\\) and/or \\(\\beta_2\\neq0\\)\n\n\n\n\nin a slightly different way…\n\n\n\n\n\n\nNull model\n\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\nSmaller (reduced) model\n\n\n\n\n\n\nAlternative* model\n\n\n\n\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\)\nLarger (full) model\n\n\n\n\n\n*This is not quite the alternative, but if we reject the null, then this is the model we move forward with"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#poll-everywhere-question-1",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#poll-everywhere-question-1",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#building-a-very-important-toolkit-three-types-of-tests",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#building-a-very-important-toolkit-three-types-of-tests",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Building a very important toolkit: three types of tests",
    "text": "Building a very important toolkit: three types of tests\n\n\nOverall test\n\n\nDoes at least one of the covariates/predictors contribute significantly to the prediction of Y?\n\n\n\n\nTest for addition of a single variable’s coefficient (covariate subset test)\n\n\nDoes the addition of one particular covariate (with a single coefficient) add significantly to the prediction of Y achieved by other covariates already present in the model?\n\n\n\n\nTest for addition of group of variables’ coefficient (covariate subset test)\n\n\nDoes the addition of some group of covariates (or one covariate with multiple coefficients) add significantly to the prediction of Y achieved by other covariates already present in the model?"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#variation-explained-vs.-unexplained",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#variation-explained-vs.-unexplained",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Variation: Explained vs. Unexplained",
    "text": "Variation: Explained vs. Unexplained\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 &= \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY &= SSR + SSE\n\\end{aligned}\\]\n\n\\(Y_i - \\overline{Y}\\) = the deviation of \\(Y_i\\) around the mean \\(\\overline{Y}\\)\n\nthe total amount deviation\n\n\\(\\widehat{Y}_i- \\overline{Y}\\) = the deviation of the fitted value \\(\\widehat{Y}_i\\) around the mean \\(\\overline{Y}\\)\n\nthe amount deviation explained by the regression at \\(X_{i1},\\ldots,X_{ik}\\)\n\n\\(Y_i - \\widehat{Y}_i\\) = the deviation of the observation \\(Y\\) around the fitted regression line\n\nthe amount deviation unexplained by the regression at \\(X_{i1},\\ldots,X_{ik}\\)"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#plot-histogram-of-deviations-for-le-beta_0-beta_1-flr-epsilon",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#plot-histogram-of-deviations-for-le-beta_0-beta_1-flr-epsilon",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Plot histogram of deviations for \\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)",
    "text": "Plot histogram of deviations for \\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\n\nCode to make the below plots\nSSY_plot = ggplot(SS_dev_slr, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y)))\nSSR_plot = ggplot(SS_dev_slr, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y)))\nSSE_plot = ggplot(SS_dev_slr, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i]))\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = 64.64\\]\n \n\\[SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = 27.24\\]\n \n\\[SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = 37.39\\]"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#plot-histogram-of-deviations-for-le-beta_0-beta_1-flr-beta_2-fs-epsilon",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#plot-histogram-of-deviations-for-le-beta_0-beta_1-flr-beta_2-fs-epsilon",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Plot histogram of deviations for \\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\)",
    "text": "Plot histogram of deviations for \\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\)\n\n\nCode to make the below plots\nSSY_plot = ggplot(SS_df, aes(SSY_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - bar(Y)))\nSSR_plot = ggplot(SS_df, aes(SSR_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) +xlab(expression(hat(Y)[i] - bar(Y)))\nSSE_plot = ggplot(SS_df, aes(SSE_dev)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) + xlab(expression(Y[i] - hat(Y)[i]))\ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = 64.64\\]\n \n\\[SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = 36.39\\]\n \n\\[SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = 28.25\\]"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#what-did-you-notice-in-the-plots",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#what-did-you-notice-in-the-plots",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "What did you notice in the plots?",
    "text": "What did you notice in the plots?\n\n\n\nSimple Linear Regression\n\n\n\nMultiple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n \n\\[SSR = 27.24\\]\n \n \n\\[SSE =37.39\\]\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n \n\\[SSR = 36.39\\]\n \n \n\\[SSE =28.25\\]"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#when-running-a-f-test-for-linear-models",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#when-running-a-f-test-for-linear-models",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "When running a F-test for linear models…",
    "text": "When running a F-test for linear models…\n\nWe need to define a larger, full model (more parameters)\nWe need to define a smaller, reduced model (fewer parameters)\nUse the F-statistic to decide whether or not we reject the smaller model\n\nThe F-statistic compares the SSE of each model to determine if the full model explains a significant amount of additional variance\n\n\n\n\n \n\\[\nF = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\n\\]\n\n\n\\(SSE(R) \\geq SSE(F)\\)\nNumerator measures difference in unexplained variation between the models\n\nBig difference = added parameters greatly reduce the unexplained variation (increase explained variation)\nSmaller difference = added parameters don’t reduce the unexplained variation\n\nTake ratio of difference to the unexplained variation in the full model"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#poll-everywhere-question-2",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#poll-everywhere-question-2",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#we-will-keep-working-with-the-mlr-model-from-last-class",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#we-will-keep-working-with-the-mlr-model-from-last-class",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "We will keep working with the MLR model from last class",
    "text": "We will keep working with the MLR model from last class\nNew population model for example:\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]\n\n# Fit regression model:\nmr1 &lt;- gapm_sub %&gt;% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)\ntidy(mr1, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157 \\ \\text{FLR}\n+ 0.008\\ \\text{FS}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#overall-f-test",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#overall-f-test",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Overall F-test",
    "text": "Overall F-test\nDoes at least one of the covariates/predictors contribute significantly to the prediction of Y?\n\nFor a general population MLR model, \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\nWe can create a hypothesis test for all the covariate coefficients…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=\\beta_2= \\ldots=\\beta_k=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\nAt least one \\(\\beta_j\\neq0\\) (for \\(j=1, 2, \\ldots, k\\))\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k + \\epsilon\\)"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#overall-f-test-general-steps-for-hypothesis-test",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#overall-f-test-general-steps-for-hypothesis-test",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Overall F-test: general steps for hypothesis test",
    "text": "Overall F-test: general steps for hypothesis test\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1=\\beta_2= \\ldots=\\beta_k=0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=1, 2, \\ldots, k\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}} = \\frac{MSR_{full}}{MSE_{full}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject if: \\(P(F_{k, n-k-1} &gt; F) &lt; \\alpha\\)\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that at least one predictor’s coefficient is not 0 (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#overall-f-test-a-word-on-the-conclusion",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#overall-f-test-a-word-on-the-conclusion",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Overall F-test: a word on the conclusion",
    "text": "Overall F-test: a word on the conclusion\n\nIf \\(H_0\\) is rejected, we conclude there is sufficient evidence that at least one predictor’s coefficient is different from zero.\nSame as: at least one independent variable contributes significantly to the prediction of \\(Y\\)\n\n \n\nIf \\(H_0\\) is not rejected, we conclude there is insufficient evidence that at least one predictor’s coefficient is different from zero.\nSame as: Not enough evidence that at least one independent variable contributes significantly to the prediction of \\(Y\\)"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#lets-think-about-our-mlr-example-for-life-expectancy",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#lets-think-about-our-mlr-example-for-life-expectancy",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Let’s think about our MLR example for life expectancy",
    "text": "Let’s think about our MLR example for life expectancy\nOur proposed population model\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\ \\text{FS}\n\\end{aligned}\\]\nOur main question for the Overall F-test: Is the regression model containing female literacy rate and food supply useful in estimating countries’ life expectancy?\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(LE = \\beta_0 + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\)"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\nFit and get augmented values for reduced model:\n\n\nmod_red1 = gapm_sub %&gt;% \n  lm(formula = LifeExpectancyYrs ~ 1)\naug_red1  = augment(mod_red1)\n\n\nFit and get augmented values for full model:\n\n\nmod_full1 = gapm_sub %&gt;% \n  lm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD)\naug_full1  = augment(mod_full1)\n\n\nCalculate the deviances for each model:\n\n\nSS_df2 = gapm_sub %&gt;% select(LifeExpectancyYrs) %&gt;%\n  mutate(SSY_diff_r1 = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         SSR_diff_r1 = aug_red1$.fitted - mean(LifeExpectancyYrs), \n         SSE_diff_r1 = aug_red1$.resid, \n         SSY_diff_f1 = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         SSR_diff_f1 = aug_full1$.fitted - mean(LifeExpectancyYrs), \n         SSE_diff_f1 = aug_full1$.resid)"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-1",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-1",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\n\n\nReduced / null model\n\n\\[LE = \\beta_0 + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 0\\]\n \n\\[SSE = 64.64\\]\n\n\n\n\nFull / Alternative model\n\n\\[LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 36.39\\]\n \n\\[SSE = 28.25\\]"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#poll-everywhere-question-3",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#poll-everywhere-question-3",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-13",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-13",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (1/3)",
    "text": "So let’s step through our hypothesis test (1/3)\n\n\n\nMet underlying LINE assumptions\n\n\n\n \n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1=\\beta_2=0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_1\\neq0 \\text{ or } \\beta_2\\neq0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k =2\\) and denominator \\(df=n-k-1 = 72 - 2-1=69\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-23",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-23",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (2/3)",
    "text": "So let’s step through our hypothesis test (2/3)\n\n\n\nCompute the value of the test statistic / 6. Calculate the p-value\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}=44.443\\] OR use ANOVA table:\n\nanova(mod_red1, mod_full1) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ 1\n71.000\n4,589.119\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD\n69.000\n2,005.556\n2.000\n2,583.563\n44.443\n0.000"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-33",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-33",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (3/3)",
    "text": "So let’s step through our hypothesis test (3/3)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n \nWe reject the null hypothesis at the 5% significance level. There is sufficient evidence that either countries’ female literacy rate or the food supply (or both) contributes significantly to the prediction of life expectancy (p-value &lt; 0.001)."
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#covariate-subset-test-single-variable",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#covariate-subset-test-single-variable",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Covariate subset test: Single variable",
    "text": "Covariate subset test: Single variable\nDoes the addition of one particular covariate of interest (a numeric covariate with only one coefficient) add significantly to the prediction of Y achieved by other covariates already present in the model?\n\nFor a general population MLR model, \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\beta_j X_j +\\ldots + \\beta_k X_k + \\epsilon\\]\n\nWe can create a hypothesis test for a single \\(j\\) covariate coefficient (where \\(j\\) can be any value \\(1, 2, \\ldots, k\\))…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_j=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_j\\neq0\\)\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(\\begin{aligned}Y = &\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_j X_j +\\\\ &\\ldots + \\beta_k X_k + \\epsilon \\end{aligned}\\)"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#single-covariate-f-test-general-steps-for-hypothesis-test-reference",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#single-covariate-f-test-general-steps-for-hypothesis-test-reference",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Single covariate F-test: general steps for hypothesis test (reference)",
    "text": "Single covariate F-test: general steps for hypothesis test (reference)\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_j=0\\\\\n\\text{vs. } H_A&: \\beta_j\\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that predictor/covariate \\(j\\) significantly improves the prediction of Y, given all the other covariates are in the model (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#lets-think-about-our-mlr-example-for-life-expectancy-1",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#lets-think-about-our-mlr-example-for-life-expectancy-1",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Let’s think about our MLR example for life expectancy",
    "text": "Let’s think about our MLR example for life expectancy\nOur proposed population model\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\ \\text{FS}\n\\end{aligned}\\]\nOur main question for the single covariate subset F-test: Is the regression model containing food supply improve the estimation of countries’ life expectancy, given female literacy rate is already in the model?\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\)"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-2",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-2",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\n\nReduced / null model \\[LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 27.24\\]\n \n\\[SSE = 37.39\\]\n\n\n\nFull / Alternative model \\[LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 36.39\\]\n \n\\[SSE = 28.25\\]"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#poll-everywhere-question-4",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#poll-everywhere-question-4",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-13-1",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-13-1",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (1/3)",
    "text": "So let’s step through our hypothesis test (1/3)\n\n\n\nMet underlying LINE assumptions\n\n\n\n \n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_2=0\\\\\n\\text{vs. } H_A&: \\beta_2\\neq0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k =2\\) and denominator \\(df=n-k-1 = 72 - 2-1=69\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-23-1",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-23-1",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (2/3)",
    "text": "So let’s step through our hypothesis test (2/3)\n\n\n\nCompute the value of the test statistic / 6. Calculate the p-value\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\] ANOVA table:\n\nanova(mod_red2, mod_full2) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate\n70.000\n2,654.875\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD\n69.000\n2,005.556\n1.000\n649.319\n22.339\n0.000"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-33-1",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-33-1",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (3/3)",
    "text": "So let’s step through our hypothesis test (3/3)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n \nWe reject the null hypothesis at the 5% significance level. There is sufficient evidence that countries’ food supply contributes significantly to the prediction of life expectancy, given that female literacy rate is already in the model (p-value &lt; 0.001)."
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#covariate-subset-test-group-of-coefficients",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#covariate-subset-test-group-of-coefficients",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Covariate subset test: group of coefficients",
    "text": "Covariate subset test: group of coefficients\nDoes the addition of some group of covariates of interest (or a multi-level categorical variable) add significantly to the prediction of Y obtained through other independent variables already present in the model?\n\nFor a general population MLR model, \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\nWe can create a hypothesis test for a group of covariate coefficients (subset of many)… For example…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=\\beta_3 =0\\) (this can be any coefficients)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\nAt least one \\(\\beta_j\\neq0\\) (for \\(j=2,3\\))\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(Y = \\beta_0 + \\beta_2 X_2 + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X + \\beta_3 X_3+\\epsilon\\)"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#covariate-subset-f-test-general-steps-for-hypothesis-test-reference",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#covariate-subset-f-test-general-steps-for-hypothesis-test-reference",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Covariate subset F-test: general steps for hypothesis test (reference)",
    "text": "Covariate subset F-test: general steps for hypothesis test (reference)\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\nFor example:\n\\[\\begin{align}\nH_0 &: \\beta_1 = \\beta_3 = 0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=1,3\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that predictors/covariates \\(2,3\\) significantly improve the prediction of Y, given all the other covariates are in the model (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#we-need-to-slightly-alter-our-mlr-example-for-life-expectancy",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#we-need-to-slightly-alter-our-mlr-example-for-life-expectancy",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "We need to slightly alter our MLR example for life expectancy",
    "text": "We need to slightly alter our MLR example for life expectancy\nOur proposed population model to include water source percent (WS):\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\beta_3 WS + \\epsilon\\]\n\nWe don’t have a fitted multiple regression model for this yet!\n\nOur main question for the group covariate subset F-test: Is the regression model containing food supply and water source percent improve the estimation of countries’ life expectancy, given percent female literacy rate is already in the model?\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\beta_3 WS + \\epsilon\\)"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-3",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-3",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\n\nReduced / null model \\[LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 27.24\\]\n \n\\[SSE = 37.39\\]\n\n\n\nFull / Alternative model \\[LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\beta_3 WS + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 43.26\\]\n \n\\[SSE = 21.38\\]"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-13-2",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-13-2",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (1/3)",
    "text": "So let’s step through our hypothesis test (1/3)\n\n\n\nMet underlying LINE assumptions\n\n\n\n \n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_2=\\beta_3=0\\\\\n\\text{vs. } H_A&: \\beta_2\\neq0 \\text{ and/or } \\beta_3\\neq0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k =2\\) and denominator \\(df=n-k-1 = 72 - 2-1=69\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-23-2",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-23-2",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (2/3)",
    "text": "So let’s step through our hypothesis test (2/3)\n\n\n\nCompute the value of the test statistic / 6. Calculate the p-value\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\] ANOVA table:\n\nanova(mod_red3, mod_full3) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate\n70.000\n2,654.875\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD + WaterSourcePrct\n68.000\n1,517.916\n2.000\n1,136.959\n25.467\n0.000"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-33-2",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#so-lets-step-through-our-hypothesis-test-33-2",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "So let’s step through our hypothesis test (3/3)",
    "text": "So let’s step through our hypothesis test (3/3)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n \nWe reject the null hypothesis at the 5% significance level. There is sufficient evidence that countries’ food supply or water source (or both) contribute significantly to the prediction of life expectancy, given that female literacy rate is already in the model (p-value &lt; 0.001)."
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test.html#other-ways-to-word-the-hypothesis-tests-reference",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test.html#other-ways-to-word-the-hypothesis-tests-reference",
    "title": "Lesson 10: MLR: Using the F-test",
    "section": "Other ways to word the hypothesis tests (reference)",
    "text": "Other ways to word the hypothesis tests (reference)\n\nSingle covariate subset F-test\n\n\\(H_0:\\) \\(X^*\\) does not significantly improve the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_p\\) are already in the model\n\\(H_A:\\) \\(X^*\\) significantly improves the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_p\\) are already in the model\n\nGroup covariate subset F-test\n\n\\(H_0:\\) The addition of the \\(s\\) variables \\(X_1^*, X_2^*, \\ldots, X_s^*\\) does not significantly improve the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_q\\) are already in the model\n\\(H_A:\\) The addition of the \\(s\\) variables \\(X_1^*, X_2^*, \\ldots, X_s^*\\) significantly improves the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_q\\) are already in the model\n\n\n\n\nLesson 10: MLR 2"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test_key_info.html#key-dates",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates\n\nHW 3 due this Friday\nMidterm feedback due 2/14 with HW 3\nLab 3 due 2/21"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test_key_info.html",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Will go over HW 2 next class\nBREATHE reps, Vincenzo and Abe, will be coming to the last 15-20 minutes of class\nI added midterm feedback to HW 3: PLEASE DO THIS BY 2/14 (turn in with HW 3)\n\nPlease complete the midterm feedback\n\nPlease do your exit tickets! If you were sick or had extenuating circumstance, let me know!\n\nThat way I can give you more than a week to fill them out\n\nPlease turn in your HW 0, 1, and 2!!!\n\nNO LATE PENALTY! But you need to turn them in to get credit\nThis is a major portion of your grade!"
  },
  {
    "objectID": "lessons/10_MLR_F-test/10_MLR_F-test_key_info.html#announcements",
    "href": "lessons/10_MLR_F-test/10_MLR_F-test_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Will go over HW 2 next class\nBREATHE reps, Vincenzo and Abe, will be coming to the last 15-20 minutes of class\nI added midterm feedback to HW 3: PLEASE DO THIS BY 2/14 (turn in with HW 3)\n\nPlease complete the midterm feedback\n\nPlease do your exit tickets! If you were sick or had extenuating circumstance, let me know!\n\nThat way I can give you more than a week to fill them out\n\nPlease turn in your HW 0, 1, and 2!!!\n\nNO LATE PENALTY! But you need to turn them in to get credit\nThis is a major portion of your grade!"
  },
  {
    "objectID": "lessons/09_MLR_Intro/09_MLR_Intro_muddy_points.html#muddy-points-from-winter-2024",
    "href": "lessons/09_MLR_Intro/09_MLR_Intro_muddy_points.html#muddy-points-from-winter-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Winter 2024",
    "text": "Muddy Points from Winter 2024\n\n1. Why is it not called multivariate?\nMultivariate models refer to models that have multiple outcomes. In multiple/multivariable models, we still only have one outcome, \\(Y\\), but multiple predictors.\n\n\n2. When adjusting for another variable, how do we calculate the new slope and intercept?\nFirst, I want to clarify one thing: in the case of two covariates, we only have the best fit plane. The intercept is really just a placeholder for when the covariates are 0. And the coefficients for each covariate are no longer stand alone slopes, unless we examine a specific instance when one covariate takes on a realized value. (Okay, that was kinda a lot of vague language. Let’s go to our example.)\nFor \\[\\widehat{\\text{LE}} = 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\ \\text{FS}\\], we have a regression plane.\nWhen we derive the regression lines for either variable, FLR or FS, we can think of the other variable as part of the intercept for the line:\nFor FLR: \\[\\widehat{\\text{LE}} = [33.595 + 0.008\\ \\text{FS}] + 0.157\\ \\text{FLR} \\] For FS: \\[\\widehat{\\text{LE}} = [33.595 + 0.157\\ \\text{FLR}]\n+ 0.008\\ \\text{FS}\\]\nFor FLR, any given FS value will result in the same slope, but the intercept will change. That’s why we say “holding FS constant,” “adjusting for FS,” or “controlling for FS” when we discuss the \\(\\widehat\\beta_1=0.157\\) estimate. Depending on the FS value, the intercept will change. So we can write: \\[(\\widehat{\\text{LE}}|FS=3000) = [33.595 + 0.008\\cdot 3000] + 0.157\\ \\text{FLR}= 57.595 + 0.157\\ \\text{FLR}\\]\nTry going through the same process for FS when FLR is 30%.\n\n\n3. I know we can’t really do visualizations past 3 coefficients but I still can’t really wrap my head around how this will work once we add a third covariate to the model.\nYeah… this is a tough one! You can try to think of the fitted Y \\(\\widehat{Y}|X\\) as being built from all the covariate values, and really just the equation for the best-fit line that we estimate. At three covariates, we need to let go of some of the visualizations.\n\n\n4. Still a little unsure about OLS\nI was going to write out an explanation to this, but then I wrote my explanation to the below question. I honestly think it helps bring context to OLS and talk about it in a new way that might help if it’s been confusing so far.\n\n\n5. We keep getting back to \\(\\widehat{Y}\\), \\(Y_i\\), and \\(\\overline{Y}\\) and their relationship to the population parameter estimates. Can you clarify this?\nI think it’ll be helpful to use the dataset I created from our quiz. I still think this relationship is best communicated with simple linear regression. What you didn’t see on the quiz was that I simulated the data:\n\nset.seed(444) # Set the seed so that every time I run this I get the same results\nx = runif(n=200, min = 40, max = 85) # I am sampling 200 points from a uniform distribution with minimum value 40 and maximum value 85\ny = rnorm(n=200, 215 - 0.85*x, 13) # Then I can construct my y-observations based on x. Notice that 215 is the true, underlying intercept and -0.85 is the true underlying slope\ndf = data.frame(Age = x, HR = y) # Then I combine these into a dataframe\n\nThen we can look at the scatterplot:\n\nlibrary(ggplot2)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_point(size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1, colour=\"#F14124\") +\n  labs(x = \"Age (years)\", \n       y = \"Peak exercise heart rate (bpm)\",\n       title = \"Peak exercise heart rate vs. Age\") +\n    theme(axis.title = element_text(size = 11), \n        axis.text = element_text(size = 11), \n        title = element_text(size = 11))\n\n\n\n\n\n\n\n\n\nEach point represents an observation \\((X_i, Y_i)\\). That is where we get \\(Y_i\\) from\nThe red line represents \\(\\widehat{Y}\\). We can look at each \\(\\widehat{Y}|X\\), so we look at the expected \\(Y\\) at a specific age like 70 years old.\nNow we need to find \\(\\overline{Y}\\). This does not take \\(X\\) into account. So we can look at the observed \\(Y\\)’s and find the mean\n\nggplot(df, aes(HR)) + geom_histogram()\n\n\n\n\n\n\n\nmean(df$HR)\n\n[1] 162.8725\n\n\n\nThen we can draw a line on the scatterplot for \\(\\overline{Y}\\):\n\nggplot(df, aes(x = x, y = y)) +\n  geom_point(size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1, colour=\"#F14124\") +\n  labs(x = \"Age (years)\", \n       y = \"Peak exercise heart rate (bpm)\",\n       title = \"Peak exercise heart rate vs. Age\") +\n    theme(axis.title = element_text(size = 11), \n        axis.text = element_text(size = 11), \n        title = element_text(size = 11)) +\n  geom_hline(yintercept = mean(df$HR), linewidth = 1, colour=\"green\")\n\n\n\n\n\n\n\n\nWhen we talk about SSY (total variation), we can think of the histogram of the Y’s\n\nggplot(df, aes(HR)) + geom_histogram() + xlim(100, 225)\n\n\n\n\n\n\n\n\nThen the total variation of these observed values is related to the \\(\\sum_{i=1}^n (Y_i - \\overline{Y})^2\\). Let’s plot \\(Y_i - \\overline{Y}\\):\n\ndf = df %&gt;% mutate(y_center = HR - mean(HR))\nggplot(df, aes(y_center)) + geom_histogram() + xlim(-60, 50)+ylim(0, 35)\n\n\n\n\n\n\n\n\nHowever, we can fit a regression line to show the relationship between Y and X. For every observation \\(X_i\\) there is a specific \\(\\widehat{Y}\\) from the regression line. So if we take the difference between the mean Y and the fitted Y, then we get the variation that is explained by the regression.\n\nmod1 = lm(HR ~ Age, data = df)\naug1 = augment(mod1)\ndf = df %&gt;% mutate(fitted_y = aug1$.fitted, \n                   diff_mean_fit = fitted_y - mean(HR))\nggplot(df, aes(diff_mean_fit)) + geom_histogram() + xlim(-60, 50)+ylim(0, 35)\n\n\n\n\n\n\n\n\nIn the plot above, there is variation! And it means that some of the variation in the plot of Y alone is actually coming from this variation explained by the regression model!!\nBut there is left over variation that is not explained by the model… What is that? It’s related to our residuals: \\(\\widehat\\epsilon_i = Y_i - \\widehat{Y}_i\\)\nSo we’ll calculate the residuals (or more appropriately, use the calculation of the residuals that R gave us)\n\nmod1 = lm(HR ~ Age, data = df)\naug1 = augment(mod1)\ndf = df %&gt;% mutate(diff_y_fitted = aug1$.resid)\nggplot(df, aes(diff_y_fitted)) + geom_histogram() + xlim(-60, 50) +ylim(0, 35)\n\n\n\n\n\n\n\n\nOur aim in regression (through ordinary least squares) is to minimize the variance in the above plot. The more variance our model can explain, the less variance in the residuals. In SLR, we can only explain so much variance with a single predictor. As we include more predictors in our model, the model has the opportunity to explain even MORE variance."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#do-we-think-income-level-can-be-an-effect-modifier-for-world-region",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#do-we-think-income-level-can-be-an-effect-modifier-for-world-region",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Do we think income level can be an effect modifier for world region?",
    "text": "Do we think income level can be an effect modifier for world region?\n\n\n\nTaking a break from female literacy rate to demonstrate interactions for two categorical variables\nWe can start by visualizing the relationship between life expectancy and world region by income level\nQuestions of interest: Does the effect of world region on life expectancy differ depending on income level?\n\nThis is the same as: Is income level an effect modifier for world region?\n\nLet’s run an interaction model to see!"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#model-with-interaction-between-a-multi-level-categorical-and-continuous-variables",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#model-with-interaction-between-a-multi-level-categorical-and-continuous-variables",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Model with interaction between a multi-level categorical and continuous variables",
    "text": "Model with interaction between a multi-level categorical and continuous variables\nModel we are fitting:\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\\\ & \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]\n\n\\(LE\\) as life expectancy\n\\(I(\\text{high income})\\) as indicator of high income\n\\(I(\\text{Americas})\\), \\(I(\\text{Asia})\\), \\(I(\\text{Europe})\\) as the indicator for each world region\n\nIn R:\n\n# gapm_sub = gapm_sub %&gt;% mutate(income_levels2 = relevel(income_levels2, ref = \"Higher income\")) # for poll everywhere\n\nm_int_wr_inc = lm(LifeExpectancyYrs ~ income_levels2 + four_regions +\n                  income_levels2*four_regions, data = gapm_sub)\nm_int_wr_inc = lm(LifeExpectancyYrs ~ income_levels2*four_regions, \n                data = gapm_sub)"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#poll-everywhere-question-4",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#poll-everywhere-question-4",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#comparing-fitted-regression-means-for-each-world-region",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#comparing-fitted-regression-means-for-each-world-region",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Comparing fitted regression means for each world region",
    "text": "Comparing fitted regression means for each world region\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\widehat\\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 60.85 + 2.10 \\cdot I(\\text{high income}) + 10.8 \\cdot I(\\text{Americas}) + 7.47\\cdot  I(\\text{Asia}) + 11.50 \\cdot I(\\text{Europe}) + \\\\ & 2.64 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + 1.54 \\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & 2.38 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\nAfrica\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 + \\widehat\\beta_4 \\cdot 0 + \\\\ & \\widehat\\beta_5 I(\\text{high income}) \\cdot 0 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 0+ \\\\& \\widehat\\beta_7 I(\\text{high income}) \\cdot 0 \\\\\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income})\\\\\n\\end{aligned}\\]\n\n\n\n\n\nThe Americas\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 \\cdot 0 + \\widehat\\beta_4 \\cdot 0 + \\\\ &  \\widehat\\beta_5 I(\\text{high income}) \\cdot 1 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 0+ \\\\ & \\widehat\\beta_7 I(\\text{high income}) \\cdot 0 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_2\\big) + \\\\ &\\big(\\widehat\\beta_1 + \\widehat\\beta_5\\big)I(\\text{high income}) \\\\\n\\end{aligned}\\]\n\n\n\n\n\nAsia\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 1 + \\widehat\\beta_4 \\cdot 0 + \\\\ & \\widehat\\beta_5 I(\\text{high income}) \\cdot 0 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 1+ \\\\ & \\widehat\\beta_7 I(\\text{high income}) \\cdot 0 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_3\\big) + \\\\ &\\big(\\widehat\\beta_1 + \\widehat\\beta_6\\big)I(\\text{high income}) \\\\\n\\end{aligned}\\]\n\n\n\n\n\nEurope\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 + \\widehat\\beta_4 \\cdot 1 + \\\\ & \\widehat\\beta_5 I(\\text{high income}) \\cdot 0 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 0+ \\\\ & \\widehat\\beta_7 I(\\text{high income}) \\cdot 1 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_4\\big) + \\\\ & \\big(\\widehat\\beta_1 + \\widehat\\beta_7\\big)I(\\text{high income}) \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#comparing-fitted-regression-means-for-each-income-level",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#comparing-fitted-regression-means-for-each-income-level",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Comparing fitted regression means for each income level",
    "text": "Comparing fitted regression means for each income level\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\widehat\\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 60.85 + 2.10 \\cdot I(\\text{high income}) + 10.8 \\cdot I(\\text{Americas}) + 7.47\\cdot  I(\\text{Asia}) + 11.50 \\cdot I(\\text{Europe}) + \\\\ & 2.64 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + 1.54 \\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & 2.38 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 0 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot 0\\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot 0 \\cdot I(\\text{Asia})+ \\widehat\\beta_7 \\cdot 0\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 1 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot 1\\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot 1 \\cdot I(\\text{Asia})+ \\widehat\\beta_7 \\cdot 1\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & (\\widehat\\beta_0 + \\widehat\\beta_1)  + (\\widehat\\beta_2 + \\widehat\\beta_5) I(\\text{Americas}) + (\\widehat\\beta_3 + \\widehat\\beta_6)  I(\\text{Asia}) + \\\\ & (\\widehat\\beta_4 + \\widehat\\beta_7)  I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\n\nExample interpretation: The America’s effect on mean life expectancy increases \\(\\widehat{\\beta}_5\\) comparing high income to low income countries."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#lets-take-a-look-back-at-the-plot",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#lets-take-a-look-back-at-the-plot",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Let’s take a look back at the plot",
    "text": "Let’s take a look back at the plot\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\\\ & \\widehat\\beta_4 I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & (\\widehat\\beta_0 + \\widehat\\beta_1)  + (\\widehat\\beta_2 + \\widehat\\beta_5) I(\\text{Americas}) + \\\\& (\\widehat\\beta_3 + \\widehat\\beta_6)  I(\\text{Asia}) +  (\\widehat\\beta_4 + \\widehat\\beta_7)  I(\\text{Europe}) \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#interpretation-for-interaction-between-two-categorical-variables",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#interpretation-for-interaction-between-two-categorical-variables",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Interpretation for interaction between two categorical variables",
    "text": "Interpretation for interaction between two categorical variables\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot I(\\text{high income})\\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\widehat\\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{high income})\\bigg]  + \\bigg[\\widehat\\beta_2 + \\widehat\\beta_5 \\cdot I(\\text{high income})\\bigg] I(\\text{Americas}) + \\\\ & \\bigg[\\widehat\\beta_3 + \\widehat\\beta_6 \\cdot I(\\text{high income})\\bigg]  I(\\text{Asia}) +  \\bigg[\\widehat\\beta_4 + \\widehat\\beta_7 \\cdot I(\\text{high income})\\bigg]  I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_1\\) = mean change in the Africa’s life expectancy, comparing high income to low income countries\n\\(\\beta_5\\) = mean change in the Americas’ effect, comparing high income to low income countries\n\\(\\beta_6\\) = mean change in Asia’s effect, comparing high income to low income countries\n\\(\\beta_7\\) = mean change in Europe’s effect, comparing high income to low income countries"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#test-interaction-between-two-categorical-variables",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#test-interaction-between-two-categorical-variables",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Test interaction between two categorical variables",
    "text": "Test interaction between two categorical variables\n\nWe run an F-test for a group of coefficients (\\(\\beta_5\\), \\(\\beta_6\\), \\(\\beta_7\\)) in the below model (see lesson 9)\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\\\ & \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_5= \\beta_6 = \\beta_7 =0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_5\\neq0\\) and/or \\(\\beta_6\\neq0\\) and/or \\(\\beta_7\\neq0\\)\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\\\& \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\epsilon \\end{aligned}\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\\\ & \\beta_4 I(\\text{Europe}) + \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\\\ & \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#test-interaction-between-multi-level-categorical-continuous-variables",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#test-interaction-between-multi-level-categorical-continuous-variables",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Test interaction between multi-level categorical & continuous variables",
    "text": "Test interaction between multi-level categorical & continuous variables\n\nFit the reduced and full model\n\n\nm_int_wr_inc_red = lm(LifeExpectancyYrs ~ income_levels2 + four_regions, \n                   data = gapm_sub)\nm_int_wr_inc_full = lm(LifeExpectancyYrs ~ income_levels2 + four_regions +\n                          income_levels2*four_regions, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ income_levels2 + four_regions\n67.000\n1,693.242\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ income_levels2 + four_regions + income_levels2 * four_regions\n64.000\n1,681.304\n3.000\n11.938\n0.151\n0.928\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and income level (p = 0.928)."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#displaying-the-regression-table-and-writing-fitted-regression-equation-1",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#displaying-the-regression-table-and-writing-fitted-regression-equation-1",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Displaying the regression table and writing fitted regression equation",
    "text": "Displaying the regression table and writing fitted regression equation\n\ntidy_m_fs = tidy(m_int_fs, conf.int=T) \ntidy_m_fs %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 5)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n70.32060\n0.72393\n97.13721\n0.00000\n68.87601\n71.76518\n    FLR_c\n0.15532\n0.03808\n4.07905\n0.00012\n0.07934\n0.23130\n    FS_c\n0.00849\n0.00182\n4.67908\n0.00001\n0.00487\n0.01212\n    FLR_c:FS_c\n−0.00001\n0.00008\n−0.06908\n0.94513\n−0.00016\n0.00015\n  \n  \n  \n\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & 70.32 + 0.16 \\cdot FLR^c + 0.01 \\cdot FS^c - 0.00001 \\cdot FLR^c \\cdot FS^c\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#examples-of-confounders",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#examples-of-confounders",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Examples of confounders",
    "text": "Examples of confounders"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#exploratory-approach-to-identifying-confounders",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#exploratory-approach-to-identifying-confounders",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Exploratory approach to identifying confounders",
    "text": "Exploratory approach to identifying confounders\n\ngapm2 %&gt;% ggpairs()"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#proxies-and-confounders-the-good-and-the-harmful",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#proxies-and-confounders-the-good-and-the-harmful",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Proxies and confounders: the good and the harmful",
    "text": "Proxies and confounders: the good and the harmful\n\nThis is totally my own tangent\nA proxy variable is used to stand-in or represent another variable that is harder to measure\nSometimes a confounder can be used as a proxy if it is hard to measure you explanatory variable/variable of interest\nProxies can be helpful statistically while harmful socially OR helpful for both!\n\n \n\nExamples\n\nBad: BMI serving as a measurement for physical health or diet\n\nMany studies show how harmful, mentally and physically, it is to equate BMI to health\n\nInteresting: Using occurrence of online search queries as a proxy for public health risk perception\nHelpful contextualization: Using race as a proxy for systemic racism, and thus a way to identify how to and who needs resources\n\nIn our lab, I discuss using sex assigned at birth in our model"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#steps-in-decision-tree-form",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#steps-in-decision-tree-form",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Steps in decision tree form",
    "text": "Steps in decision tree form"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#pause-itll-be-helpful-to-center-female-literacy-rate",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#pause-itll-be-helpful-to-center-female-literacy-rate",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "PAUSE: It’ll be helpful to center female literacy rate",
    "text": "PAUSE: It’ll be helpful to center female literacy rate\n\n\nCentering female literacy rate: \\[ FLR^c = FLR - \\overline{FLR}\\]\nCentering in R:\n\n\ngapm_sub = gapm_sub %&gt;% \n  mutate(FLR_c = FemaleLiteracyRate - mean(FemaleLiteracyRate))\n\n\nI’m going to print the mean so I can use it for my interpretations\n\n\n(mean_FLR = mean(gapm_sub$FemaleLiteracyRate))\n\n[1] 82.03056\n\n\n\nNow all intercept values (in each respective world region) will be the mean life expectancy when female literacy rate is 82.03%\nWe will used center FLR for the rest of the lecture"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#centering-continuous-variables-when-we-are-including-interactions-1",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#centering-continuous-variables-when-we-are-including-interactions-1",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Centering continuous variables when we are including interactions",
    "text": "Centering continuous variables when we are including interactions\n\n\nFor Europe, the mean life expectancy had a regression line with a large intercept\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_4\\big) + \\big(\\widehat\\beta_1 + \\widehat\\beta_7\\big)FLR \\\\\n\\widehat{LE} = & (58.23 + 63.63) + (0.051 - 0.519)FLR \\\\\n\\widehat{LE} = & 121.86 -0.468FLR \\\\\n\\end{aligned}\\]\n\nCentering the continuous variables in a model (when they are involved in interactions) helps with:\n\nInterpretations of the coefficient estimates\nCorrelation between the main effect for the variable and the interaction that it is involved with\n\nTo be discussed in future lecture: leads to multicollinearity issues\n\n\nOther online sources about when and when not to center:\n\nThe why and when of centering continuous predictors in regression modeling\nWhen not to center a predictor variable in regression"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#itll-be-helpful-to-center-female-literacy-rate-1",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#itll-be-helpful-to-center-female-literacy-rate-1",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "It’ll be helpful to center female literacy rate",
    "text": "It’ll be helpful to center female literacy rate\n\n\nCentering female literacy rate: \\[ FLR^c = FLR - \\overline{FLR}\\]\nCentering in R:\n\n\ngapm_sub = gapm_sub %&gt;% \n  mutate(FLR_c = FemaleLiteracyRate - mean(FemaleLiteracyRate))\n\n\nI’m going to print the mean so I can use it for my interpretations\n\n\n(mean_FLR = mean(gapm_sub$FemaleLiteracyRate))\n\n[1] 82.03056\n\n\n\nNow all intercept values (in each respective world region) will be the mean life expectancy when female literacy rate is 82.03%\nWe will used center FLR for the rest of the lecture"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#pause-centering-continuous-variables-when-including-interactions",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#pause-centering-continuous-variables-when-including-interactions",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "PAUSE: Centering continuous variables when including interactions",
    "text": "PAUSE: Centering continuous variables when including interactions\n\n\nFor the high income group, the mean life expectancy had a regression line with a small intercept\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & (\\widehat\\beta_0 +\\widehat\\beta_2) + (\\widehat\\beta_1 +\\widehat\\beta_3) FLR \\\\\n\\widehat{LE} = & (54.85 - 16.65) + (0.156 + 0.228) \\cdot FLR\\\\\n\\widehat{LE} = & 38.2 + 0.384 \\cdot FLR\\\\\n\\end{aligned}\\]\n\nIntercept of 38.2 is misleading because\n\nMakes you think some of the life expectancies for high income countries are lower than that of low income countries (depending on the FLR)\nThere are no high income countries with FLR less than ~70%\n\nOther online sources about when and when not to center:\n\nThe why and when of centering continuous predictors in regression modeling\nWhen not to center a predictor variable in regression"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#centering-a-variable",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#centering-a-variable",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Centering a variable",
    "text": "Centering a variable\n\n\nCentering a variable means that we will subtract the mean or median (or other measurement of center) from the measured value\nMean centered: \\[X_i^c = X_i - \\overline{X}\\]\nMedian centered: \\[X_i^c = X_i - \\text{median } X\\]\nCentering the continuous variables in a model (when they are involved in interactions) helps with:\n\nInterpretations of the coefficient estimates\nCorrelation between the main effect for the variable and the interaction that it is involved with\n\nTo be discussed in future lecture: leads to multicollinearity issues"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#displaying-the-regression-table-and-writing-fitted-regression-equation-again",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#displaying-the-regression-table-and-writing-fitted-regression-equation-again",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Displaying the regression table and writing fitted regression equation AGAIN",
    "text": "Displaying the regression table and writing fitted regression equation AGAIN\n\nm_int_inc2 = gapm_sub %&gt;% \n  lm(formula = LifeExpectancyYrs ~ FLR_c*income_levels2)\n\n\ntidy(m_int_inc2, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n69.281\n1.387\n49.964\n0.000\n66.514\n72.047\n    FLR_c\n0.156\n0.039\n3.990\n0.000\n0.078\n0.235\n    income_levels2Higher income\n4.405\n1.725\n2.554\n0.013\n0.963\n7.848\n    FLR_c:income_levels2Higher income\n0.228\n0.164\n1.392\n0.168\n−0.099\n0.555\n  \n  \n  \n\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 I(\\text{high income}) + \\widehat\\beta_3 FLR^c \\cdot I(\\text{high income}) \\\\\n\\widehat{LE} = & 69.281 + 0.156 \\cdot FLR^c + 4.405 \\cdot I(\\text{high income}) + 0.228 \\cdot FLR^c \\cdot I(\\text{high income})\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-3-1",
    "href": "lessons/11_Interactions_01/11_Interactions_01.html#poll-everywhere-question-3-1",
    "title": "Lesson 11: Interactions, Part 1",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#a-small-word-on-the-homework",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#a-small-word-on-the-homework",
    "title": "HW 2 and Lab 2",
    "section": "A small word on the homework",
    "text": "A small word on the homework\n\nMostly good work!\nMain note: Please look at the solutions to make sure you have the correct beta’s and interpretations when we work with categorical variables!!"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#a-note-from-me",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#a-note-from-me",
    "title": "HW 2 and Lab 2",
    "section": "A note from me",
    "text": "A note from me\n\nI know the lab instructions are wordy\nThis class is really about the technical (“objective”) skills of regression\n\nBut in order to responsibly practice statistics, you need to critically think about the subjective choices you make\n\nAnd I’m really trying to lay out my thought process in the labs so that you have some idea of the subjective choices that I’m kinda restricting us to\n\nAnd that’s really just bc you’re learning A LOT in this class\nSo taking on extra learning objectives would be overwhelming"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#other-overall-issues",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#other-overall-issues",
    "title": "HW 2 and Lab 2",
    "section": "Other overall issues",
    "text": "Other overall issues\n\nNo need to load the codebook into R!!!\n\nCodebooks are typically opened in excel and will give you extra information on the variables\n\nYou gotta show all your code!\n\nIf you got points off for not showing any code, resubmit with the code showing and I’ll give you credit back\n\nBe careful when making assumptions about the data\n\nExample: someone created a cisgender variable by seeing if SAB was the same as gender identity\n\nI would be wary of that - definitions of cis and trans are highly personal - only use and refer to participants as they self-identify\n\n\nDo not immediately make age categories!! It is important to look at age (numeric) vs. IAT\n\nWhy pixelate your data?? We only do it if we need to (aka age as numeric is NOT linear with IAT score)"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#what-is-our-target-population",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#what-is-our-target-population",
    "title": "HW 2 and Lab 2",
    "section": "3.1: What is our target population?",
    "text": "3.1: What is our target population?\n\nThis is an important thing to flag as you analyze your results and interpret them for an audience\n\n \n\nWe restricted our population to the US\nHarvard says the test is only for individuals 18+ years old\nTest takers need access to the internet and a computer (or phone?)\n\n \n\nAnother thought\n\nSometimes your target population defines your sample\nOther times your sample defines your target population\n\nHere we have a convenience sample, with specific restrictions and accessibility\n\nThat means the population that we can generalize to is limited to those restrictions and accessibility!!\n\nWe need to discuss these limitations when we present these results to the world!"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#restrict-your-analysis-to-1-outcome-and-9-possible-covariatespredictors",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#restrict-your-analysis-to-1-outcome-and-9-possible-covariatespredictors",
    "title": "HW 2 and Lab 2",
    "section": "3.2 Restrict your analysis to 1 outcome and 9 possible covariates/predictors",
    "text": "3.2 Restrict your analysis to 1 outcome and 9 possible covariates/predictors\n\n\nNeeded to pick the variable from your research question + 2 others (or 3 if you chose a different variable in your research question)\n\nExplicit anti-fat bias (att7)\nSelf-perception of weight (iam_001)\nFat group identity (identfat_001 )\nThin group identity (identthen_001 )\nControllability of weight of others (controlother_001)\nControllability of weight of yourself (controlyou_001)\nAwareness of societal standards (mostpref_001 )\nInternalization of societal standards (important_001)\n\n\nNeeded to include all 4 demographic variables:\n\nAge (we need to construct from birthmonth, birthyear, testmonth, and testyear)\nRace (raceomb_002 or raceombmulti)\nEthnicity (ethnicityomb)\nSex assigned at birth (birthSex)\n\nPlease pick only 2 additional variables:\n\nEducation (edu_14)\nGender (genderIdentity)\nSelf-reported BMI (through self-reported height and weight)\nPolitical identity\nReligion"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#restrict-your-analysis-to-1-outcome-and-9-possible-covariatespredictors-1",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#restrict-your-analysis-to-1-outcome-and-9-possible-covariatespredictors-1",
    "title": "HW 2 and Lab 2",
    "section": "3.2 Restrict your analysis to 1 outcome and 9 possible covariates/predictors",
    "text": "3.2 Restrict your analysis to 1 outcome and 9 possible covariates/predictors\n\nStart by loading the data\n\n\nload(file = here(\"../Project/data/iat_data.rda\"))\niat_2021 = iat_2021 %&gt;% \n  select(IAT_score = D_biep.Thin_Good_all, \n         att7, iam_001, identfat_001, \n         myweight_002, myheight_002,\n         identthin_001, controlother_001, \n         controlyou_001, mostpref_001,\n         important_001, \n         birthmonth, birthyear, month, year, \n         raceomb_002, raceombmulti, ethnicityomb, \n         edu, edu_14, \n         genderIdentity, \n         birthSex) %&gt;%\n  drop_na()"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#manipulating-variables-that-are-coded-as-numeric-variables",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#manipulating-variables-that-are-coded-as-numeric-variables",
    "title": "HW 2 and Lab 2",
    "section": "3.3: Manipulating variables that are coded as numeric variables",
    "text": "3.3: Manipulating variables that are coded as numeric variables\n\nNo need to make plots here (that was just part of my example)\n\nPlots and tables are a good way to check that you accomplished the correct translation\n\nGiving the levels order:\n\n\niat_2021 = iat_2021 %&gt;% mutate(iam_001_f = case_match(iam_001,\n                                  7 ~ \"Very overweight\",\n                                  6 ~ \"Moderately overweight\",\n                                  5 ~ \"Slightly overweight\",\n                                  4 ~ \"Neither underweight nor underweight\",\n                                  3 ~ \"Slightly underweight\",\n                                  2 ~ \"Moderately underweight\",\n                                  1 ~ \"Very underweight\",\n                                  .default = NA) %&gt;% \n             factor(levels = c(\"Very underweight\", # Assigns the level order!\n                               \"Moderately underweight\", \n                               \"Slightly underweight\", \n                               \"Neither underweight nor underweight\", \n                               \"Slightly overweight\", \n                               \"Moderately overweight\", \n                               \"Very overweight\")))"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#manipulating-variables-that-are-coded-as-numeric-variables-1",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#manipulating-variables-that-are-coded-as-numeric-variables-1",
    "title": "HW 2 and Lab 2",
    "section": "3.3: Manipulating variables that are coded as numeric variables",
    "text": "3.3: Manipulating variables that are coded as numeric variables\n\nNow when we print a table, we can see them in order\n\n\niat_2021 %&gt;%\n  dplyr::select(iam_001_f) %&gt;%\n  tbl_summary()\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 242,7621\n    \n  \n  \n    iam_001_f\n\n        Very underweight\n1,341 (0.6%)\n        Moderately underweight\n5,436 (2.2%)\n        Slightly underweight\n17,224 (7.1%)\n        Neither underweight nor underweight\n106,836 (44%)\n        Slightly overweight\n65,418 (27%)\n        Moderately overweight\n32,259 (13%)\n        Very overweight\n14,248 (5.9%)\n  \n  \n  \n    \n      1 n (%)"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#if-you-chose-bmi-create-the-variable",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#if-you-chose-bmi-create-the-variable",
    "title": "HW 2 and Lab 2",
    "section": "3.5 If you chose BMI, create the variable",
    "text": "3.5 If you chose BMI, create the variable\n\nIf you worked with BMI, please make sure you followed the help page!\nPlease come double check with me that you are creating it correctly!"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#if-time-from-lesson-5",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#if-time-from-lesson-5",
    "title": "HW 2 and Lab 2",
    "section": "If time… (from Lesson 5)",
    "text": "If time… (from Lesson 5)\nLet’s walk through categorical variables that have multiple selections\n\nSo each group is not mutually exclusive\nWe could make an indicator for each category, but individuals could be a part of multiple categories"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#approaches-to-multiple-race-questions-from-we-all-count",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#approaches-to-multiple-race-questions-from-we-all-count",
    "title": "HW 2 and Lab 2",
    "section": "4 Approaches to Multiple-Race Questions from We All Count",
    "text": "4 Approaches to Multiple-Race Questions from We All Count\n\nThis method works for any multi-level variable"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#multi-responsemulti-selection-variables-raceombmulti-and-genderidentity",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#multi-responsemulti-selection-variables-raceombmulti-and-genderidentity",
    "title": "HW 2 and Lab 2",
    "section": "Multi-response/multi-selection variables: raceombmulti and genderIdentity",
    "text": "Multi-response/multi-selection variables: raceombmulti and genderIdentity\n\nThere is extra data management skills that we need to address these\nLet’s walk through categorical variables that have multiple selections\nAnother note: I find that the race variable is still lacking (where is my MENA representation??)\n\nMENA = Middle Eastern and North African"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#section-10",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#section-10",
    "title": "HW 2 and Lab 2",
    "section": "",
    "text": "HW 2 and Lab 2"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#final-notes",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#final-notes",
    "title": "HW 2 and Lab 2",
    "section": "Final notes",
    "text": "Final notes\n\nFor now, I suggest the binary approach!\n\nThis is the perfect level of pushing ourselves coding wise and thinking critically about these multi-response variables\n\nTake a look at this article: https://doi.org/10.1016/j.socscimed.2017.12.026\n\nIt gets into some of the considerations and uses of intersectionality in analyses\n\n\n\n\nHW 2 and Lab 2"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#bivariate-exploratory-data-analysis",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#bivariate-exploratory-data-analysis",
    "title": "HW 2 and Lab 2",
    "section": "4.3 Bivariate exploratory data analysis",
    "text": "4.3 Bivariate exploratory data analysis\n\nYou only needed to create one plot!!\nMy research question: Is self-perception of weight associated with IAT score?\n\n\n\nHow I made the plot\nggplot(iat_2021, aes(x = iam_001_f, y = IAT_score))+\n  geom_boxplot()+\n   labs(x = \"Self-perception of weight\", \n       y = \"IAT Score\", \n       title = \"IAT Score by self-perception of weight\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5), \n        axis.text.x = element_text(angle = 30, vjust = 1, hjust=1))"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#bivariate-exploratory-data-analysis-1",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#bivariate-exploratory-data-analysis-1",
    "title": "HW 2 and Lab 2",
    "section": "4.3 Bivariate exploratory data analysis",
    "text": "4.3 Bivariate exploratory data analysis\n\nYou only needed to create one plot!!\nMy research question: Is self-perception of weight associated with IAT score?\n\n\n\nHow I made the plot\nggplot(iat_2021, aes(x = IAT_score, y = iam_001_f))+\n  geom_boxplot()+\n   labs(y = \"Self-perception of weight\", \n       x = \"IAT Score\", \n       title = \"IAT Score by self-perception of weight\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#bivariate-exploratory-data-analysis-2",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#bivariate-exploratory-data-analysis-2",
    "title": "HW 2 and Lab 2",
    "section": "4.3 Bivariate exploratory data analysis",
    "text": "4.3 Bivariate exploratory data analysis\n\nYou only needed to create one plot!!\nMy research question: Is self-perception of weight associated with IAT score?\n\n\n\nHow I made the plot\nggplot(iat_2021, aes(x = IAT_score, color = iam_001_f))+\n  geom_density() +\n   labs(x = \"IAT Score\", \n       title = \"IAT Score by self-perception of weight\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#bivariate-exploratory-data-analysis-3",
    "href": "lessons/Review-slides_HW-Labs/Review_HW_02_Lab_02.html#bivariate-exploratory-data-analysis-3",
    "title": "HW 2 and Lab 2",
    "section": "4.3 Bivariate exploratory data analysis",
    "text": "4.3 Bivariate exploratory data analysis\n\nYou only needed to create one plot!!\nMy research question: Is self-perception of weight associated with IAT score?\n\n\n\nHow I made the plot\nlibrary(ggridges)\nggplot(iat_2021, aes(x = IAT_score, y = iam_001_f))+\n  geom_density_ridges(alpha = 0.3, \n          show.legend = FALSE) +  \n   labs(y = \"Self-perception of weight\", \n       x = \"IAT Score\", \n       title = \"IAT Score by self-perception of weight\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01_key_info.html",
    "href": "lessons/11_Interactions_01/11_Interactions_01_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "From a previous student My colleagues and I are conducting a focus group with masters of public health students to understand how students are currently engaging and utilizing resources at the OHSU-PSU School of Public Health. The focus group will happen on Saturday, February 15th at 11am-12:30 pm. We would love to get more students involved from your program department to participate this upcoming Saturday and we would appreciate it if you could pass this opportunity along to your students.\n\nInterested students can sign up here and we will follow up students with next steps: https://forms.gle/dpGj2XaqpBAEYihb9\n\nNo class on Monday!\nI added midterm feedback to HW 3: PLEASE DO THIS BY 2/14 (turn in with HW 3)\n\nPlease complete the midterm feedback"
  },
  {
    "objectID": "lessons/11_Interactions_01/11_Interactions_01_key_info.html#announcements",
    "href": "lessons/11_Interactions_01/11_Interactions_01_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "From a previous student My colleagues and I are conducting a focus group with masters of public health students to understand how students are currently engaging and utilizing resources at the OHSU-PSU School of Public Health. The focus group will happen on Saturday, February 15th at 11am-12:30 pm. We would love to get more students involved from your program department to participate this upcoming Saturday and we would appreciate it if you could pass this opportunity along to your students.\n\nInterested students can sign up here and we will follow up students with next steps: https://forms.gle/dpGj2XaqpBAEYihb9\n\nNo class on Monday!\nI added midterm feedback to HW 3: PLEASE DO THIS BY 2/14 (turn in with HW 3)\n\nPlease complete the midterm feedback"
  },
  {
    "objectID": "labs/Lab_03_instructions.html",
    "href": "labs/Lab_03_instructions.html",
    "title": "Lab 3 Instructions",
    "section": "",
    "text": "Caution\n\n\n\nThis lab is ready to go! Nicky (2/14/2025)"
  },
  {
    "objectID": "labs/Lab_03_instructions.html#directions",
    "href": "labs/Lab_03_instructions.html#directions",
    "title": "Lab 3 Instructions",
    "section": "1 Directions",
    "text": "1 Directions\nPlease turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here. Please use the linked qmd file and not this one! (This is specifically the instructions.)\nThe rest of this lab’s instructions are embedded into the lab activities.\n\n1.1 Purpose\nThe main purpose of this lab is to perform some quality control on our data, recode some of the multi-selection categorical variables, continue data exploration, and start analyzing the main relationship of our research question.\n\n\n1.2 Grading\nThis lab is graded out of 12 points. Nicky will use the following rubric displayed on the Project page."
  },
  {
    "objectID": "labs/Lab_03_instructions.html#lab-activities",
    "href": "labs/Lab_03_instructions.html#lab-activities",
    "title": "Lab 3 Instructions",
    "section": "2 Lab activities",
    "text": "2 Lab activities\nBefore starting this lab, you should go back to Lab 2, save a new .rda file that contains all the new variables from that Lab. Then you can load it here!\n\n2.1 Restate your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\nHow is implicit anti-fat bias, as measured by the IAT score, associated with “insert main independent variable here”?\n\n\n2.2 Quality Control\nThere are a few more issues with the data that we need to look into. First, there is another coding for NA values in the race variable: -999. We will need to filter out these observations.\nWe will also need to look at individuals who have potentially answered the survey questions untruthfully. We cannot catch everything, but a good place to start is by looking at individuals who have done more than one of the following:\n\nselected the earliest or latest possible birth year\nselected the lowest or highest possible education\nselected all gender identities (for those using gender identity)\nselected all races (for those using multiple selection race)\nselected the lowest or highest weight (for those looking at BMI)\nselected the lowest or highest height (for those looking at BMI)\n\nI want to take a second to mention that any of the above selections, and combinations of the above selections, are valid. However, we should start to flag the possibility that someone has not gone through the survey properly if we notice that most or all of the respondent’s answers are the first answer choice, last answer choice, or selected all options. Additionally, not all of these carry the same importance in discerning validity. For example, a recorded age of 111 years old is the most striking to me. When paired with other selections that are the maximum or minimum (or first or last) option, then I will record it for future investigation. If this observation looks to be an outlier or high leverage point in our analysis, that is when I’ll decide to remove it.\n\n\n\n\n\n\nTasks\n\n\n\n\nFilter out observations with a value of -999 in the race variable.\nGlimpse at the observations that may indicate a respondent who has not properly completed the survey portion. This will require filtering for specific answer choices. Please see examples of filter() on it’s documentation page.\n\n\n\n\n\n2.3 Working with multi-selection variables\nIn the list of variables that we may choose to work with (in Lab 2), there are two that allowed respondents to select multiple categories. The two variables are genderIdentitiy and raceombmulti. If you did not choose these variables to work with, you may skip this section.\nIf you chose one or both of these variables, then we need to make new variables that correspond to indicators for each possible selection in the respective variable.\nLet’s start with the grepl function. For this function, we can input one of our column names and a value, then it will output, for each row, if the value is in the column. For example, in genderIdentity an individual may identify as a “Trans female/Trans woman” and “Gender queer/Gender nonconforming.” In our dataset in R, this would show as [4,5] in genderIdentity. If we want to create two separate indicators for anyone who identifies as “Trans female/Trans woman” then I need to look for the value 4 in the column genderIdentity. I will run a separate indicator to find individuals who identify as “Gender queer/Gender nonconforming.” Here is an example code of how I would use grepl to do this:\n\niat_prep_new = iat_prep_old %&gt;%\n  mutate(ind_tf_tw = grepl(4, genderIdentity), \n         ind_gq_gnc = grepl(5, genderIdentity))\n\nYou will need to extend this to all other gender identities.\nFor race, raceombmulti is also the follow up question to raceomb_002. So our indicators need to reflect both variables. In this case, we need to use grepl on both columns at once. For example, if I want to create an indicator for individuals who identify as American Indian/Alaskan Native then I need to find individuals who identify as American Indian/Alaskan Native only and individuals who identify as American Indian/Alaskan Native in addition to another race. For example, my code might look like:\n\niat_prep_new = iat_prep_old %&gt;%\n  mutate(ind_AIAN = grepl(1, raceomb_002) | grepl(1, raceombmulti))\n\nI suggest only searching for 1-7 in both raceomb_002 and raceombmulti. Note that if raceomb_002 = 8 , then individuals identified as “multiracial” and will select values in raceombmulti.\n\n\n\n\n\n\nTask\n\n\n\nIf you are using genderIdentity or raceombmulti, create indicator variables for each possible selection.\n\n\n\n\n2.4 Thinking about potential confounders and effect modifiers\nBefore we explore more of the data, I want us to take a second to think through potential confounders and effect modifiers from the covariates that we selected in Lab 2. For some of the covariates, we were asked to explain why we chose them. Now I want you to consider how each could alter the relationship between IAT score and your variable of interest (from your research question). For each covariate, explain how it might or might not change the relationship. For example, if our variable of interest is fat group identity, then we may consider that self-perception of size is a confounder since it could be linked with fat group identity and potentially be associated with IAT score.\nFor multi-level, unordered categorical covariates, you might consider if a specific category has an impact. For example, we might consider creating an indicator for white, non-Hispanic/Lantinx respondents since the history of fatphobia is tied with white-centered colonization and white supremacy (Redpath, 2023). Thus IAT scores might look different for White respondents vs. minority respondents (those who answered American Indian/Alaskan Natives; East Asian; South Asian; Native Hawaiian or other Pacific Islander; Black or African American; or Hispanic or Latino). Alternatively, we may not want to center our analysis on whiteness. The same fatphobic history involving white supremacy was particularly targetting Black people. So perhaps we want make an indicator for Black or African American respondents. Another option is leaving race as is - we may have enough data to handle the inclusion of all groups!\nThe purpose of this section is to make sure we are thinking about the relationships between variables in our analysis. I do not want us to make any decisions based solely on the data. I want any changes or manipulations in our variables to be motivated by research-backed evidence.\nFinally, for this project, we are most interested in the relationship we identified in our research question! Other variables are supporting this question, and improving that model fit so that we get as close to the true relationship in our research question as possible!\n\n\n\n\n\n\nTask\n\n\n\nFor each variable, consider how each could alter the relationship between IAT score and your variable of interest (from your research question). For each covariate, explain how it might or might not change the relationship.\n\n\n\n\n2.5 Continuing data exploration\nIn this section, we are going to further explore the variables that we might be adjusting for in our model (potential covariates outside the variable or interest in our research question).\n\n2.5.1 Bivariate exploration\nWe want to look at all other relationships between IAT score and each covariate (outside of the research question variable). Some of you have already made these plots in Lab 2, so you can simply refine them and display them here. There are a few questions that I want you to consider:\n\nFor categorical variables, is there an inherent order? Does the ordered values follow an approximately linear relationship? Are the categories “evenly spaced”? For example, education categories are not necessarily evenly spaced.\nAgain for categorical variables, is there a natural place to divide the categories up? For example, in education, it might be helpful to control for the fact that students in college might be asked to complete this test as an assignment. Thus, we might make an indicator for individuals in college vs. not. This decision can be informed by our plot, but it should not be driven by our plot!!\n\n\n\n\n\n\n\nTask\n\n\n\nFor each variable outside of your research question, create the appropriate plot to visualize the relationship between IAT score and the variable. Comment if there is an obvious trend or not.\n\n\n\n\n2.5.2 Multivariate exploration\nNow we want to extend our plots for Lab 2 where we looked at the outcome (IAT score) and our main variable of interest (as identified by our research question). Here, we will run the same plot, but include another variable. This will help us visualize potential confounders or effect modifiers. Note that if you made indicator variables (for race, gender identity or any other variable), then you should have a plot for each indicator variable.\nYou will need to really think about what kind of plot will best display these relationships! IAT score is continuous, and many of your variable of interest is categorical. You may consider side-by-side boxplots where the color is the additional variable. You might also consider a jitter plot or only plotting the means. Remember you’re goal for plotting is to get a sense of the relationship only from the plot! Your audience should not have to work hard to understand what the plot is communicating. For example, I wanted to look at IAT score, internalization of societal standards, and race. I might make my plot like such:\n\n\nCode to contruct multivariate plot\nggplot(iat_prep2, aes(x = important_001, y=IAT_score, color = as.factor(raceomb_002_f))) +\n  # geom_jitter(size = 2, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 3, shape = 18) +\n  stat_summary(fun = mean, geom=\"line\") +\n  scale_x_discrete(limits = levels(iat_prep2$important_001_f)) +\n  labs(x = \"Importance of weight to sense of self \\n (Internalization of societal standards)\", \n       y = \"IAT score\",\n       title = \"Mean IAT scores for importance of weight to sense of self by race\", \n       color = \"Race\") +\n  theme(axis.text.x = element_text(angle = 45, size = 8, hjust = 1))\n\n\n\n\n\n\n\nCode to contruct multivariate plot\nggplot(iat_prep2, aes(x = important_001, y=IAT_score, color = as.factor(raceomb_002_f))) +\n  # geom_jitter(size = 2, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 3, shape = 18) +\n  stat_summary(fun = mean, geom=\"line\") +\n  scale_x_discrete(limits = levels(iat_prep2$important_001_f), labels = function(x) str_wrap(x, width=10)) +\n  labs(x = \"Importance of weight to sense of self \\n (Internalization of societal standards)\", \n       y = \"IAT score\",\n       title = \"Mean IAT scores for importance of weight to sense of self by race\", \n       color = \"Race\")\n\n\n\n\n\nNote that the above plot is specific for these variables!! Other variables may require a different type of visua\nlization!! Also note that I originally had geom_jitter() in my plot, which would make the plot really hard to understand!! Try uncommenting it to see what I mean by “hard to understand.” Also, think about why I connected the mean IAT scores across the different levels of internalization. I had a hard time connecting specific race’s points to identify a trend. Again, try commenting out stat_summary(fun = mean, geom=\"line\") to see what I mean.\n\n\n\n\n\n\nNote\n\n\n\nAn aside: You may see that collapsing groups might wash out differences. If we make an indicator for Black of African American respondents, as we mentioned above, then including White respondents with other minority groups may wash out their association with IAT score and wrongly lead us to a model that says identifying as Black or African American has no association with IAT, where we clearly see that Black or African American respondents have a unique trajectory for IAT scores.\n\n\nPlease make sure that you have made the needed changes to your plot in Lab 2. I noticed many unordered groups in plots where there should be an inherent order and unreadable axes because the text was not tilted. Please see discussion on Slack for what some students did to achieve these plots.\nHere are a few sources that might help you get started with the visualizations:\n\nIntro to R\nModern Data Visualization with R\n\n\n\n\n\n\n\nTask\n\n\n\nFor at least 3 variables outside of your research question, create the appropriate plot to visualize the relationship between IAT score, your main variable (in research question), and the variable outside your research question. Comment whether you can determine anything from the plot or not. If you can, is there any indication that the variable is a confounder or effect modifier?\n\n\n\n\n\n2.6 Fit a simple linear regression\nAs a starting point, it is good to fit a simple linear regression for our primary research question. This is often called the “crude” association. It just means that we are not adjusting for any other variables, and establishing the “starting point” for our analysis. It is likely that the results of the regression will change as we add other variables in the model.\n\n\n\n\n\n\nTask\n\n\n\nRun a simple linear regression model for the relationship in your primary research question. Print the regression table. Interpret the results and comment on the initial trend you see.\n\n\n\n\n\n\n\n\nBonus Task\n\n\n\nThis is not required in Lab 3. However, if you want to run a multiple linear regression model with one other variable that you plotted in Multivariate Exploration, then you should try it! Do the results align with your ideas in Section 2.4 and/or the visualization you saw in Section 2.5?"
  },
  {
    "objectID": "labs/Lab_03_instructions.html#bibliography",
    "href": "labs/Lab_03_instructions.html#bibliography",
    "title": "Lab 3 Instructions",
    "section": "3 Bibliography",
    "text": "3 Bibliography\nRedpath, F. (2023). Abolish the Body Mass Index: A Historical and Current Analysis of the Traumatizing Nature of the BMI. Tapestries:  Interwoven Voices of Local and Global Identities, 12(1). https://digitalcommons.macalester.edu/tapestries/vol12/iss1/12"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02_key_info.html",
    "href": "lessons/12_Interactions_02/12_Interactions_02_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Please make sure you are looking at the lab instructions!\n\nThe instructions are on the site\nThe qmd you download does NOT have the instructions on them\n\n\n\n\nThursday, Feb. 20th Noon-1pm via Zoom. Please register for Zoom Link: https://portlandstate.joinhandshake.com/events/1668028/share_preview\nJoin us for a virtual panel featuring professionals from public service, healthcare, nonprofit organizations, and education! What You’ll Experience:\n\nCareer Journeys: Hear firsthand how each panelist entered and advanced in their field, and learn what inspired their paths in public health.\nInsider Tips: Discover the strategies panelists recommend for job searching and landing roles in their organizations and industries.\nJob Search Guidance: Get expert advice on how to stand out in applications, interviews, and networking to secure the job you want."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02_key_info.html#announcements",
    "href": "lessons/12_Interactions_02/12_Interactions_02_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Please make sure you are looking at the lab instructions!\n\nThe instructions are on the site\nThe qmd you download does NOT have the instructions on them\n\n\n\n\nThursday, Feb. 20th Noon-1pm via Zoom. Please register for Zoom Link: https://portlandstate.joinhandshake.com/events/1668028/share_preview\nJoin us for a virtual panel featuring professionals from public service, healthcare, nonprofit organizations, and education! What You’ll Experience:\n\nCareer Journeys: Hear firsthand how each panelist entered and advanced in their field, and learn what inspired their paths in public health.\nInsider Tips: Discover the strategies panelists recommend for job searching and landing roles in their organizations and industries.\nJob Search Guidance: Get expert advice on how to stand out in applications, interviews, and networking to secure the job you want."
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-questions-2-4",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-questions-2-4",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Poll Everywhere Questions 2-4",
    "text": "Poll Everywhere Questions 2-4"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-5",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#step-5-check-for-interactions-5",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Step 5: Check for interactions",
    "text": "Step 5: Check for interactions\n\nI went through all the ANOVA tables, and found the following significant interactions:\n\nNone!\n\n\n\nanova_res\n\n[[1]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * CO2_q\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq     F Pr(&gt;F)\n1     54 919.29                          \n2     57 946.46 -3   -27.171 0.532 0.6623\n\n[[2]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * income_levels1\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     54 933.66                           \n2     57 946.46 -3   -12.802 0.2468 0.8633\n\n[[3]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * four_regions\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     54 850.47                           \n2     57 946.46 -3   -95.987 2.0315 0.1203\n\n[[4]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * WaterSourcePrct\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     56 943.42                           \n2     57 946.46 -1   -3.0399 0.1804 0.6726\n\n[[5]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * FoodSupplykcPPD\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     56 915.40                           \n2     57 946.46 -1   -31.063 1.9003 0.1735\n\n[[6]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * members_oecd_g77\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     55 934.95                           \n2     57 946.46 -2   -11.513 0.3386 0.7142\n\n\n \n\nThink about it: does that track with what we saw in our interactions lecture?"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-questions-6-8",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#poll-everywhere-questions-6-8",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Poll Everywhere Questions 6-8",
    "text": "Poll Everywhere Questions 6-8"
  },
  {
    "objectID": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#testing-for-percent-change-delta-in-a-coefficient",
    "href": "lessons/14_Purposeful_selection/14_Purposeful_selection.html#testing-for-percent-change-delta-in-a-coefficient",
    "title": "Lesson 14: Purposeful model selection",
    "section": "Testing for percent change ( \\(\\Delta\\%\\)) in a coefficient",
    "text": "Testing for percent change ( \\(\\Delta\\%\\)) in a coefficient\n\nLet’s say we have \\(X_1\\) and \\(X_2\\), and we specifically want to see if \\(X_2\\) is a confounder for \\(X_1\\) (the explanatory variable or variable of interest)\nIf we are only considering \\(X_1\\) and \\(X_2\\), then we need to run the following two models:\n\nFitted model 1 / reduced model (mod1): \\(\\widehat{Y} = \\widehat\\beta_0 + \\widehat\\beta_1X_1\\)\n\nWe call the above \\(\\widehat\\beta_1\\) the reduced model coefficient: \\(\\widehat\\beta_{1, \\text{mod1}}\\) or \\(\\widehat\\beta_{1, \\text{red}}\\)\n\nFitted model 2 / Full model (mod2): \\(\\widehat{Y} = \\widehat\\beta_0 + \\widehat\\beta_1X_1 +\\widehat\\beta_2X_2\\)\n\nWe call this \\(\\widehat\\beta_1\\) the full model coefficient: \\(\\widehat\\beta_{1, \\text{mod2}}\\) or \\(\\widehat\\beta_{1, \\text{full}}\\)\n\n\n\n\n\n\n\n\n\nCalculation for % change in coefficient\n\n\n\\[\n\\Delta\\% = 100\\% \\cdot\\frac{\\widehat\\beta_{1, \\text{mod1}} - \\widehat\\beta_{1, \\text{mod2}}}{\\widehat\\beta_{1, \\text{mod2}}} = 100\\% \\cdot \\frac{\\widehat\\beta_{1, \\text{red}} - \\widehat\\beta_{1, \\text{full}}}{\\widehat\\beta_{1, \\text{full}}}\n\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#model-with-interaction-between-a-multi-level-categorical-and-binary-variables",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#model-with-interaction-between-a-multi-level-categorical-and-binary-variables",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Model with interaction between a multi-level categorical and binary variables",
    "text": "Model with interaction between a multi-level categorical and binary variables\nModel we are fitting:\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\\\ & \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]\n\n\\(LE\\) as life expectancy\n\\(I(\\text{high income})\\) as indicator of high income\n\\(I(\\text{Americas})\\), \\(I(\\text{Asia})\\), \\(I(\\text{Europe})\\) as the indicator for each world region\n\nIn R:\n\n# gapm_sub = gapm_sub %&gt;% mutate(income_levels2 = relevel(income_levels2, ref = \"Higher income\")) # for poll everywhere\n\nm_int_wr_inc = lm(LifeExpectancyYrs ~ income_levels2 + four_regions +\n                  income_levels2*four_regions, data = gapm_sub)\nm_int_wr_inc = lm(LifeExpectancyYrs ~ income_levels2*four_regions, \n                data = gapm_sub)"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#test-interaction-between-two-categorical-variables-12",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#test-interaction-between-two-categorical-variables-12",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Test interaction between two categorical variables (1/2)",
    "text": "Test interaction between two categorical variables (1/2)\n\nWe run an F-test for a group of coefficients (\\(\\beta_5\\), \\(\\beta_6\\), \\(\\beta_7\\)) in the below model (see lesson 9)\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\\\ & \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_5= \\beta_6 = \\beta_7 =0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_5\\neq0\\) and/or \\(\\beta_6\\neq0\\) and/or \\(\\beta_7\\neq0\\)\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\\\& \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\epsilon \\end{aligned}\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\\\ & \\beta_4 I(\\text{Europe}) + \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\\\ & \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#test-interaction-between-two-categorical-variables-22",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#test-interaction-between-two-categorical-variables-22",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Test interaction between two categorical variables (2/2)",
    "text": "Test interaction between two categorical variables (2/2)\n\nFit the reduced and full model\n\n\nm_int_wr_inc_red = lm(LifeExpectancyYrs ~ income_levels2 + four_regions, \n                   data = gapm_sub)\nm_int_wr_inc_full = lm(LifeExpectancyYrs ~ income_levels2 + four_regions +\n                          income_levels2*four_regions, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\n\n\n\n\n\n  \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ income_levels2 + four_regions\n67.000\n1,693.242\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ income_levels2 + four_regions + income_levels2 * four_regions\n64.000\n1,681.304\n3.000\n11.938\n0.151\n0.928\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between world region and income level (p = 0.928)."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#how-to-find-the-confidence-interval-for-each-slope",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#how-to-find-the-confidence-interval-for-each-slope",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "How to find the confidence interval for each slope?",
    "text": "How to find the confidence interval for each slope?\n\nIn the example with FS and FLR, we showed:\n\n\n\nBest-fit line for Food Supply of 3812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\big(\\widehat\\beta_0 + 1000 \\widehat\\beta_2 \\big)+  \\big(\\widehat\\beta_1 + 1000 \\widehat\\beta_3  \\big) FLR^c\n\\end{aligned}\\]\n\n\n\nOften, we want to report the estimate of the combined coefficients: \\(\\widehat\\beta_1 + 1000 \\widehat\\beta_3\\)\n\nThis allows us to make a statement like: “At a food supply of 3812 kcal PPD, mean life expectancy increases (\\(\\widehat\\beta_1 + 1000 \\widehat\\beta_3\\)) years for every one percent increase in female literacy rate (95% CI: __, __).”\n\n\n \n\nWe can calculate \\(\\widehat\\beta_1 + 1000 \\widehat\\beta_3\\) by using the values of the estimated coefficients\nBUT we always want to have a 95% confidence interval when we report this combined estimate!!"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#math-behind-it",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#math-behind-it",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Math behind it",
    "text": "Math behind it\n\nIf we want a confidence interval for \\(\\widehat\\beta_1 + 1000 \\widehat\\beta_3\\), then we would use the formula:\n\n\\[\\bigg(\\widehat\\beta_1 + 1000 \\widehat\\beta_3 \\bigg) \\pm t^* \\times SE_{(\\beta_1 + 1000 \\beta_3)}\\]\n\nThe hard part is figuring out what \\(SE_{(\\beta_1 + 1000 \\beta_3)}\\) (or \\(\\text{Var}(\\beta_1 + 1000 \\beta_3)\\)) equals\nWe need to go back to variance of linear combinations (BSTA 512/612, EPI 525): \\[\\text{Var}(aX + bY) = a^2\\text{Var}(X) + b^2\\text{Var}(Y) + 2ab\\text{Cov}(X, Y)\\] or \\[\\text{Var}(aX - bY) = a^2\\text{Var}(X) + b^2\\text{Var}(Y) - 2ab\\text{Cov}(X, Y)\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#using-an-r-function",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#using-an-r-function",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Using an R function",
    "text": "Using an R function\n\\(\\widehat\\beta_1 + 1000 \\widehat\\beta_3\\)\n\nlibrary(gmodels)\nm_int_fs %&gt;% gmodels::estimable(\n                   c(\"(Intercept)\" = 0,      # beta0\n                     \"FLR_c\"       = 1,      # beta1\n                     \"FS_c\"        = 0,      # beta2\n                     \"FLR_c:FS_c\"  = 1000),  # beta3\n                   conf.int = 0.95)\n\n              Estimate Std. Error  t value DF  Pr(&gt;|t|)    Lower.CI  Upper.CI\n(0 1 0 1000) 0.1499879  0.1024019 1.464698 68 0.1476115 -0.05435192 0.3543277"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#another-example",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#another-example",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Another example",
    "text": "Another example\n\nm_int_inc2 = gapm_sub %&gt;% \n  lm(formula = LifeExpectancyYrs ~ FLR_c*income_levels2)\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{high income}) + \\widehat\\beta_3 FLR \\cdot I(\\text{high income}) \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot I(\\text{high income}) + 0.228 \\cdot FLR \\cdot I(\\text{high income})\n\\end{aligned}\\]\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 FLR \\cdot 0 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot 0 + \\\\\n& 0.228 \\cdot FLR \\cdot 0 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR\\\\\n\\end{aligned}\\]\n\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 FLR \\cdot 1 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot 1 + \\\\ & 0.228 \\cdot FLR \\cdot 1 \\\\\n\\widehat{LE} = & 38.2 + 0.384 \\cdot FLR\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#another-example-1",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#another-example-1",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Another example",
    "text": "Another example\n\nlibrary(gmodels)\nm_int_inc2$coefficients\n\n                      (Intercept)                             FLR_c \n                       67.6818102                         0.1564398 \n      income_levels2Higher income FLR_c:income_levels2Higher income \n                        2.0729925                         0.2282290 \n\nm_int_inc2 %&gt;% gmodels::estimable(\n                   c(\"(Intercept)\" = 0,      # beta0\n                     \"FLR_c\"       = 1,      # beta1\n                     \"income_levels2Higher income\" = 0,      # beta2\n                     \"FLR_c:income_levels2Higher income\"  = 1),  # beta3\n                   conf.int = 0.95)\n\n           Estimate Std. Error  t value DF   Pr(&gt;|t|)   Lower.CI  Upper.CI\n(0 1 0 1) 0.3846688  0.1591843 2.416499 68 0.01836001 0.06702138 0.7023161"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#lets-calculate-it",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#lets-calculate-it",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Let’s calculate it!",
    "text": "Let’s calculate it!\n\nA helpful function that returns the variance-covariance matric of all the coefficients in model m_int_fs:\n\n\n\n\nvcov(m_int_fs)\n\n              (Intercept)         FLR_c          FS_c    FLR_c:FS_c\n(Intercept)  5.240754e-01 -6.771205e-03  5.586960e-05 -2.609611e-05\nFLR_c       -6.771205e-03  1.449828e-03 -3.150719e-05  1.543619e-06\nFS_c         5.586960e-05 -3.150719e-05  3.294981e-06 -1.273649e-08\nFLR_c:FS_c  -2.609611e-05  1.543619e-06 -1.273649e-08  5.949082e-09\n\n\n\n\\[ \\begin{aligned}\n\\text{Var}(\\beta_1) & = 0.0014498 \\\\\n\\text{Var}(\\beta_3) & = 6\\times 10^{-9} \\\\\n\\text{Cov}(\\beta_1, \\beta_3) & = 1.544\\times 10^{-6} \\\\\n\\end{aligned}\\]\n\n\n\\[ \\begin{aligned}\n\\text{Var}(\\beta_1 + 1000 \\beta_3) & = \\text{Var}(\\beta_1) + 1000^2\\text{Var}(\\beta_3) + 2000\\text{Cov}(\\beta_1, \\beta_3) \\\\\n\\text{Var}(\\beta_1 + 1000 \\beta_3) & = 0.0014498 + 1000^2 \\times 6\\times 10^{-9} + 2000 \\times 1.544\\times 10^{-6} \\\\\n\\text{Var}(\\beta_1 + 1000 \\beta_3) & = 0.0104861 \\\\\nSE_{(\\beta_1 + 1000 \\beta_3)} & = \\sqrt{0.0104861} \\\\\nSE_{(\\beta_1 + 1000 \\beta_3)} & = 0.1024019\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#reference-calculating-se_beta_1-1000-beta_3-by-hand",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#reference-calculating-se_beta_1-1000-beta_3-by-hand",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Reference: calculating \\(SE_{(\\beta_1 + 1000 \\beta_3)}\\) by hand",
    "text": "Reference: calculating \\(SE_{(\\beta_1 + 1000 \\beta_3)}\\) by hand\n\nA helpful function that returns the variance-covariance matric of all the coefficients in model m_int_fs:\n\n\n\n\nvcov(m_int_fs)\n\n              (Intercept)         FLR_c          FS_c    FLR_c:FS_c\n(Intercept)  5.240754e-01 -6.771205e-03  5.586960e-05 -2.609611e-05\nFLR_c       -6.771205e-03  1.449828e-03 -3.150719e-05  1.543619e-06\nFS_c         5.586960e-05 -3.150719e-05  3.294981e-06 -1.273649e-08\nFLR_c:FS_c  -2.609611e-05  1.543619e-06 -1.273649e-08  5.949082e-09\n\n\n\n\\[ \\begin{aligned}\n\\text{Var}(\\beta_1) & = 0.0014498 \\\\\n\\text{Var}(\\beta_3) & = 6\\times 10^{-9} \\\\\n\\text{Cov}(\\beta_1, \\beta_3) & = 1.544\\times 10^{-6} \\\\\n\\end{aligned}\\]\n\n\n\\[ \\begin{aligned}\n\\text{Var}(\\beta_1 + 1000 \\beta_3) & = \\text{Var}(\\beta_1) + 1000^2\\text{Var}(\\beta_3) + 2000\\text{Cov}(\\beta_1, \\beta_3) \\\\\n\\text{Var}(\\beta_1 + 1000 \\beta_3) & = 0.0014498 + 1000^2 \\times 6\\times 10^{-9} + 2000 \\times 1.544\\times 10^{-6} \\\\\n\\text{Var}(\\beta_1 + 1000 \\beta_3) & = 0.0104861 \\\\\nSE_{(\\beta_1 + 1000 \\beta_3)} & = \\sqrt{0.0104861} \\\\\nSE_{(\\beta_1 + 1000 \\beta_3)} & = 0.1024019\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#getting-a-95-confidence-interval-requires-linear-combinations",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#getting-a-95-confidence-interval-requires-linear-combinations",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Getting a 95% confidence interval requires linear combinations!",
    "text": "Getting a 95% confidence interval requires linear combinations!\n\nIf we want a confidence interval for \\(\\widehat\\beta_1 + 1000 \\widehat\\beta_3\\), then we would use the formula:\n\n\\[\\bigg(\\widehat\\beta_1 + 1000 \\widehat\\beta_3 \\bigg) \\pm t^* \\times SE_{(\\beta_1 + 1000 \\beta_3)}\\]\n\nThe hard part is figuring out what \\(SE_{(\\beta_1 + 1000 \\beta_3)}\\) (or \\(\\text{Var}(\\beta_1 + 1000 \\beta_3)\\)) equals\nWe need to go back to variance of linear combinations (BSTA 512/612, EPI 525): \\[\\text{Var}(aX + bY) = a^2\\text{Var}(X) + b^2\\text{Var}(Y) + 2ab\\text{Cov}(X, Y)\\] or \\[\\text{Var}(aX - bY) = a^2\\text{Var}(X) + b^2\\text{Var}(Y) - 2ab\\text{Cov}(X, Y)\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#we-can-use-r-and-estimable-to-find-the-estimate-and-ci",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#we-can-use-r-and-estimable-to-find-the-estimate-and-ci",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "We can use R and estimable() to find the estimate and CI",
    "text": "We can use R and estimable() to find the estimate and CI\nFor \\(\\widehat\\beta_1 + 1000 \\widehat\\beta_3\\):\n\nlibrary(gmodels)\nm_int_fs %&gt;% estimable(\n                   c(\"(Intercept)\" = 0,      # beta0\n                     \"FLR_c\"       = 1,      # beta1\n                     \"FS_c\"        = 0,      # beta2\n                     \"FLR_c:FS_c\"  = 1000),  # beta3\n                   conf.int = 0.95)\n\n              Estimate Std. Error  t value DF  Pr(&gt;|t|)    Lower.CI  Upper.CI\n(0 1 0 1000) 0.1499879  0.1024019 1.464698 68 0.1476115 -0.05435192 0.3543277\n\n\n \nOur conclusion: At a food supply of 3812 kcal PPD, mean life expectancy increases 0.14999 years for every one percent increase in female literacy rate (95% CI: -0.05435, 0.35433)."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#another-example-income-binary-and-flr-12",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#another-example-income-binary-and-flr-12",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Another example: income (binary) and FLR (1/2)",
    "text": "Another example: income (binary) and FLR (1/2)\n\nm_int_inc2 = gapm_sub %&gt;% \n  lm(formula = LifeExpectancyYrs ~ FLR_c*income_levels2)\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{high income}) + \\widehat\\beta_3 FLR \\cdot I(\\text{high income}) \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot I(\\text{high income}) + 0.228 \\cdot FLR \\cdot I(\\text{high income})\n\\end{aligned}\\]\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 FLR \\cdot 0 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot 0 + \\\\\n& 0.228 \\cdot FLR \\cdot 0 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR\\\\\n\\end{aligned}\\]\n\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 FLR \\cdot 1 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot 1 + \\\\ & 0.228 \\cdot FLR \\cdot 1 \\\\\n\\widehat{LE} = & 38.2 + 0.384 \\cdot FLR\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#another-example-income-binary-and-flr-22",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#another-example-income-binary-and-flr-22",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "Another example: income (binary) and FLR (2/2)",
    "text": "Another example: income (binary) and FLR (2/2)\n\nm_int_inc2$coefficients # I just need to see the exact names\n\n                      (Intercept)                             FLR_c \n                       67.6818102                         0.1564398 \n      income_levels2Higher income FLR_c:income_levels2Higher income \n                        2.0729925                         0.2282290 \n\nm_int_inc2 %&gt;% estimable(\n                   c(\"(Intercept)\" = 0,      # beta0\n                     \"FLR_c\"       = 1,      # beta1\n                     \"income_levels2Higher income\" = 0,      # beta2\n                     \"FLR_c:income_levels2Higher income\"  = 1),  # beta3\n                   conf.int = 0.95)\n\n           Estimate Std. Error  t value DF   Pr(&gt;|t|)   Lower.CI  Upper.CI\n(0 1 0 1) 0.3846688  0.1591843 2.416499 68 0.01836001 0.06702138 0.7023161\n\n\n \nOur conclusion: For countries with high income, mean life expectancy increases 0.385 years for every one percent increase in female literacy rate (95% CI: 0.067, 0.702)."
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#if-our-example-had-an-effect-mearue-modifier",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#if-our-example-had-an-effect-mearue-modifier",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "If our example had an effect mearue modifier",
    "text": "If our example had an effect mearue modifier\n\nNone of our examples had a significant interaction, so it’s hard to demonstrate exactly how we would report this\nLet’s say, just for example, that income had a significant interaction with FLR\n\nHow would we report this to an audience??\n\nHere’s how to report on an interaction/EMM:\n\nWe found that a country’s income status (high or low) is a significant effect measure modifier on female literacy rate (include p-value for interaction test here). For countries with high income, mean life expectancy increases 0.385 years for every one percent increase in female literacy rate (95% CI: 0.067, 0.702). For countries with low income, mean life expectancy increases 2.073 years for every one percent increase in female literacy rate (95% CI: -2.922, 7.068).”"
  },
  {
    "objectID": "lessons/12_Interactions_02/12_Interactions_02.html#if-our-example-had-an-effect-measure-modifier",
    "href": "lessons/12_Interactions_02/12_Interactions_02.html#if-our-example-had-an-effect-measure-modifier",
    "title": "Lesson 12: Interactions, Part 2",
    "section": "If our example had an effect measure modifier",
    "text": "If our example had an effect measure modifier\n\nNone of our examples had a significant interaction, so it’s hard to demonstrate exactly how we would report this\nLet’s say, just for example, that income had a significant interaction with FLR\n\nHow would we report this to an audience??\n\nHere’s how to report on an interaction/EMM:\n\nWe found that a country’s income status (high or low) is a significant effect measure modifier on female literacy rate (include p-value for interaction test here). For countries with high income, mean life expectancy increases 0.385 years for every one percent increase in female literacy rate (95% CI: 0.067, 0.702). For countries with low income, mean life expectancy increases 2.073 years for every one percent increase in female literacy rate (95% CI: -2.922, 7.068).”"
  }
]