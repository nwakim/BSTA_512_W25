---
title: "Lesson 1: Review"
author: "Nicky Wakim"
title-slide-attributes:
    data-background-color: "#213c96"
date: "01/6/2025"
categories: ["Week 1"]
format: 
  revealjs:
    theme: "../simple_NW.scss"
    chalkboard: true
    slide-number: true
    show-slide-number: all
    width: 1955
    height: 1100
    footer: "Lesson 1: Review"
    html-math-method: mathjax
    highlight-style: ayu
---

```{r}
#| label: "setup" 
#| include: false
#| message: false
#| warning: false

library(tidyverse)    
library(oibiostat)
library(janitor)
library(rstatix)
library(knitr)
library(gtsummary)
library(moderndive)
library(gt)
library(broom) # NEW!!

set.seed(456)

data("dds.discr")
```

## What did we learn in 511/611? (1/2)

-   In 511, we talked about *categorical* and *continuous* outcomes (dependent variables)

     

-   We also talked about their relationship with 1-2 *continuous* or *categorical* exposure (independent variables or predictor)

     

-   We had many good ways to assess the relationship between an outcome and exposure:

     

|                      |                                                  |                                                                                                                  |
|----------------------|--------------------------------------------------|------------------------------------------------------------------------------------------------------------------|
|                      | Continuous Outcome                               | Categorical Outcome                                                                                              |
| Continuous Exposure  | Correlation, simple linear regression            | ??                                                                                                               |
| Categorical Exposure | t-tests, paired t-tests, 2 sample t-tests, ANOVA | proportion t-test, Chi-squared goodness of fit test, Fisher's Exact test, Chi-squared test of independence, etc. |

: {tbl-colwidths="\[15, 35,35\]"}

## What did we learn in 511/611? (2/2)

-   You set up a really **important foundation**

    -   Including distributions, mathematical definitions, hypothesis testing, and more!

     

-   Tests and statistical approaches learned are incredibly helpful!

     

-   While you had to learn a lot of different tests and approaches for each combination of categorical/continuous exposure with categorical/continuous outcome

    -   **Those tests cannot handle more complicated data**

     

-   **What happens when other variables influence the relationship between your exposure and outcome?**

    -   Do we just ignore them?

```{css, echo=FALSE}
.reveal code {
  max-height: 100% !important;
}
```

## What will we learn in this class?

-   We will be building towards models that can handle many variables!

     

    -   **Regression** is the building block for modeling multivariable relationships

     

-   In Linear Models we will *build, interpret, and evaluate* **linear regression models**

## Process of regression data analysis

::: box
![](../img_slides/arrow2.png){.absolute top="13.5%" right="62.1%" width="155"} ![](../img_slides/arrow2.png){.absolute top="13.5%" right="28.4%" width="155"}![](../img_slides/arrow_back4.png){.absolute top="7.5%" right="30.5%" width="820"} ![](../img_slides/arrow_down.png){.absolute top="60.5%" right="48%" width="85"}

::: columns
::: {.column width="30%"}
::: RAP1
::: RAP1-title
Model Selection
:::

::: RAP1-cont
-   Building a model

-   Selecting variables

-   Prediction vs interpretation

-   Comparing potential models
:::
:::
:::

::: {.column width="4%"}
:::

::: {.column width="30%"}
::: RAP2
::: RAP2-title
Model Fitting
:::

::: RAP2-cont
-   Find best fit line

-   Using OLS in this class

-   Parameter estimation

-   Categorical covariates

-   Interactions
:::
:::
:::

::: {.column width="4%"}
:::

::: {.column width="30%"}
::: RAP3
::: RAP3-title
Model Evaluation
:::

::: RAP3-cont
-   Evaluation of model fit
-   Testing model assumptions
-   Residuals
-   Transformations
-   Influential points
-   Multicollinearity
:::
:::
:::
:::
:::

::: RAP4
::: RAP4-title
Model Use (Inference)
:::

::: RAP4-cont
::: columns
::: {.column width="50%"}
-   Inference for coefficients
-   Hypothesis testing for coefficients
:::

::: {.column width="50%"}
-   Inference for expected $Y$ given $X$
-   Prediction of new $Y$ given $X$
:::
:::
:::
:::

## Main sections of the course

1.  Review

2.  Simple Linear Regression

    -   [Model evaluation]{style="color:#A7EA52;"} and [Model use]{style="color:#4FADF3;"}

3.  Intro to MLR: estimation and testing

    -   [Model use]{style="color:#4FADF3;"}

4.  Diving into our predictors: categorical variables, interactions between variable

    -   [Model fitting]{style="color:#34AC8B;"}

5.  Key ingredients: model evaluation, diagnostics, selection, and building

    -   [Model evaluation]{style="color:#A7EA52;"} and [Model selection]{style="color:#FF8021;"}

```{r}
library(ggplot2)
```

## Main sections of the course

::: lob
1.  Review
:::

2.  Intro to SLR: estimation and testing

    -   [Model fitting]{style="color:#34AC8B;"}

3.  Intro to MLR: estimation and testing

    -   [Model fitting]{style="color:#34AC8B;"}

4.  Diving into our predictors: categorical variables, interactions between variable

    -   [Model fitting]{style="color:#34AC8B;"}

5.  Key ingredients: model evaluation, diagnostics, selection, and building

    -   [Model evaluation]{style="color:#A7EA52;"} and [Model selection]{style="color:#FF8021;"}

## Before we begin

- Feel free to visit my or Meike's Introducation to Biostatistics

-   [Meike's BSTA 511 page](https://niederhausen.github.io/BSTA_511_F24/)

-   [Nicky's BSTA 525 page](https://nwakim.github.io/F24_EPI_525/)

## Learning Objectives

1.  Identify important descriptive statistics and visualize data from a continuous variable

2.  Identify important distributions that will be used in 512/612

3.  Use our previous tools in 511 to estimate a parameter and construct a confidence interval

4.  Use our previous tools in 511 to conduct a hypothesis test

5.  Define error rates and power

## Learning Objectives

::: lob
1.  Identify important descriptive statistics and visualize data from a continuous variable
:::

2.  Identify important distributions that will be used in 512/612

3.  Use our previous tools in 511 to estimate a parameter and construct a confidence interval

4.  Use our previous tools in 511 to conduct a hypothesis test

5.  Define error rates and power


## Some Basic Statistics "Talk"

::: columns
::: column
-   Random variable $Y$

    -   Sample $Y_i, i=1,\dots, n$

-   Summation:

    $\sum_{i=1}^n Y_i =Y_1 + Y_2 + \ldots + Y_n$

-   Product:

    $\prod_{i=1}^n Y_i = Y_1 \times Y_2 \times \ldots \times Y_n$
:::

::: column
:::
:::

## Descriptive Statistics: continuous variables

::: columns
::: {.column width="40%"}
**Measures of central tendency**

-   Sample mean

    $$
    \overline{x} = \dfrac{x_1+x_2+...+x_n}{n}=\dfrac{\sum_{i=1}^nx_i}{n}
    $$

-   Median
:::

::: column
**Measures of variability (or dispersion)**

-   Sample variance

    -   Average of the squared deviations from the sample mean

-   Sample standard deviation

    $$
    \begin{aligned}
    s = & \sqrt{\dfrac{(x_1-\overline{x})^2+(x_2-\overline{x})^2+...+(x_n-\overline{x})^2}{n-1}} \\
    = & \sqrt{\dfrac{\sum_{i=1}^n(x_i-\overline{x})^2}{n-1}}
    \end{aligned}
    $$

-   IQR

    -   Range from 1st to 3rd quartile
:::
:::

## Descriptive Statistics: continuous variables (R code)

::: columns
::: {.column width="40%"}
**Measures of central tendency**

-   Sample mean

    ```{r}
    #| eval: false
    #| echo: true

    mean( sample )
    ```

-   Median

    ```{r}
    #| eval: false
    #| echo: true

    median( sample )
    ```
:::

::: column
**Measures of variability (or dispersion)**

-   Sample variance

    ```{r}
    #| eval: false
    #| echo: true

    var( sample )
    ```

-   Sample standard deviation

    ```{r}
    #| eval: false
    #| echo: true

    sd( sample )
    ```

-   IQR

    ```{r}
    #| eval: false
    #| echo: true

    IQR( sample )
    ```
:::
:::

- Or all together!!

```{r}
#| echo: true
dds.discr %>% get_summary_stats(age)
```

## Data visualization

-   Using the library `ggplot2` to visualize data

-   We will load the package:

```{r}
#| echo: true

library(ggplot2)
```

## Histogram using `ggplot2`

We can make a basic graph for a continuous variable:

::: columns
```{r}
data("dds.discr")
```

::: {.column width="50%"}
```{r}
#| echo: true
#| fig-align: center

ggplot(data = dds.discr, 
       aes(x = age)) +
  geom_histogram()
```
:::

::: {.column width="50%"}
```{r}
#| echo: true
#| fig-align: center

ggplot() +
  geom_histogram(data = dds.discr, 
       aes(x = age))
```
:::
:::

[Some more information](https://www.sharpsightlabs.com/blog/histogram-r-ggplot2/) on histograms using `ggplot2`

## Spruced up histogram using `ggplot2`

We can make a more formal, presentable graph:

```{r}
#| echo: true
#| fig-align: center

ggplot(data = dds.discr, 
       aes(x = age)) +
  geom_histogram() +
  theme(text = element_text(size=20)) +
  labs(x = "Age", 
       y = "Count", 
       title = "Distribution of Age in Sample")
```

I would like you to turn in homework, labs, and project reports with graphs like these.

## Other basic plots from `ggplot2`

We can also make a density and boxplot for the continuous variable with `ggplot2`

::: columns
::: {.column width="50%"}
```{r}
#| echo: true

ggplot(data = dds.discr, 
       aes(x = age)) +
  geom_density()
```
:::

::: {.column width="50%"}
```{r}
#| echo: true

ggplot(data = dds.discr, 
       aes(x = age)) +
  geom_boxplot()
```
:::
:::

## Learning Objectives

1.  Identify important descriptive statistics and visualize data from a continuous variable

::: lob
2.  Identify important distributions that will be used in 512/612
:::

3.  Use our previous tools in 511 to estimate a parameter and construct a confidence interval

4.  Use our previous tools in 511 to conduct a hypothesis test

5.  Define error rates and power


## Distributions that will be used in this class

-   Normal distribution

-   Chi-square distribution

-   Student's t distribution

-   F distribution

## Normal Distribution

::: columns
::: column

- Where did we see this? 

    - Basically everywhere! Think Central Limit Theorem
    
 
    
-   Notation: $Y\sim N(\mu,\sigma^2)$

-   Arguably the most important distribution in statistics

-   If we know $E(Y)=\mu$, $Var(Y)=\sigma^2$ then

    -   2/3 of $Y$'s distribution lies within 1 $\sigma$ of $\mu$

    -   95% is within $\mu\pm 2\sigma$

    -   $>99$% lies within $\mu\pm 3\sigma$

 

-   Linear combinations of Normal's are Normal\
    e.g., $(aY+b)\sim \mbox{N}(a\mu+b,\;a^2\sigma^2)$

-   Standard normal: $Z=\frac{Y-\mu}{\sigma} \sim \mbox{N}(0,1)$
:::

::: column
![](../img_slides/normal.png)
:::
:::

## Chi-squared distribution

::: columns
::: column
- Where did we see this? 

    - Hypothesis test if two categorical variables were independent

 

-   Notation: $X \sim \chi^2_{df}$ OR $X \sim \chi^2_{\nu}$

    -   Degrees of freedom (df): $df=n-1$

    -   $X$ takes on only positive values
    
 

-   If $Z_i\sim \mbox{N}(0,1)$, then $Z_i^2\sim \chi^2_1$

    -   A standard normal distribution squared is the Chi squared distribution with df of 1.

:::

::: column
![](../img_slides/chi_squared.png)
:::
:::

## Student's t Distribution

::: columns
::: column

- Where did we see this? 

    - Inference of means: single sample, paired, two independent samples

 
    
-   Notation: $T \sim t_{df}$ OR $T \sim t_{n-1}$

    -   Degrees of freedom (df): $df=n-1$

    -   $T = \dfrac{\overline{x} - \mu_x}{\dfrac{s}{\sqrt{n}}}\sim t_{n-1}$

 

-   In linear modeling, used for inference on individual regression parameters

    -   Think: our estimated coefficients ( $\hat{\beta_{}}$ )
:::

::: column
![](../img_slides/student_t.png)
:::
:::

## F-Distribution

::: columns
::: column

- Where did we see this? 

    - Inference for 2+ means: ANOVA test
    
-   Model ratio of sample variances (and is a ratio of Chi-squared RVs)

-   If $X_1^2\sim \chi^2_{df1}$ and $X_2^2\sim \chi^2_{df2}$, where $X_1^2\perp X_2^2$, then:

$$\dfrac{X_1^2/df1}{X_2^2/df2} \sim F_{df1,df2}$$ 

-   Important relationship with $t$ distribution: $T^2 \sim F_{1,\nu}$

    -   The square of a t-distribution with $df=\nu$

    -   is an F-distribution with numerator df ($df_1 = 1$) and denominator df ($df_2 = \nu$)
:::

::: column
![](../img_slides/f_dist.png)
:::
:::


## R code for probability distributions

::: columns
::: column
[Here is a site](https://www.stat.umn.edu/geyer/5101/examp/rlook.html) with the various probability distributions and their R code.

-   It also includes practice with R code to see what each function outputs
:::

::: column
![](../img_slides/r_code_prob_dist.png){fig-align="center" width="785"}
:::
:::

## Learning Objectives

1.  Identify important descriptive statistics and visualize data from a continuous variable

2.  Identify important distributions that will be used in 512/612

::: lob
3.  Use our previous tools in 511 to estimate a parameter and construct a confidence interval
:::

4.  Use our previous tools in 511 to conduct a hypothesis test

5.  Define error rates and power


## Confidence interval for one mean

::: columns
::: column
The confidence interval for population mean $\mu$:

$$
\overline{x} \pm t^{*}\dfrac{s}{\sqrt{n}}
$$

-   where $t^*$ is the critical value for the 95% (or other percent) corresponding to the t-distribution and dependent on $df=n-1$

::: proposition
::: prop-title
We can use `R` to find the critical t-value, $t^*$
:::

::: prop-cont
For example the critical value for the 95% CI with $n=10$ subjects is...

```{r}
#| echo: true

qt(0.975, df=9)
```

-   Recall, that as the $df$ increases, the t-distribution converges towards the Normal distribution
:::
:::
:::

::: column
:::
:::

## Confidence interval for one mean

::: columns
::: column
The confidence interval for population mean $\mu$:

$$
\overline{x} \pm t^{*}\dfrac{s}{\sqrt{n}}
$$

-   where $t^*$ is the critical value for the 95% (or other percent) corresponding to the t-distribution and dependent on $df=n-1$

::: proposition
::: prop-title
We can use `R` to find the critical t-value, $t^*$
:::

::: prop-cont
For example the critical value for the 95% CI with $n=10$ subjects is...

```{r}
#| echo: true

qt(0.975, df=9)
```

-   Recall, that as the $df$ increases, the t-distribution converges towards the Normal distribution
:::
:::
:::

::: column
We can also use `t.test` in R to calculate the confidence interval if we have a dataset.

```{r}
#| echo: true 

t.test(dds.discr$age)
```
:::
:::

## Confidence interval for two independent means

::: columns
::: column
The confidence interval for difference in independent population means, $\mu_1$ and $\mu_2$:

$$
\overline{x}_1 - \overline{x}_2 \pm t^{*}\sqrt{\dfrac{s_1^2}{n_1} + \dfrac{s_2^2}{n_2}}
$$

-   where $t^*$ is the critical value for the 95% (or other percent) corresponding to the t-distribution and dependent on $df=n_1 + n_2 -2$
:::

::: column
:::
:::

 

- Please check out my notes on this if you'd like: <https://nwakim.github.io/F24_EPI_525/schedule.html>
  - It's under Lesson 13

## Here's a decent source for other R code for tests in 511

[Website from UCLA](https://stats.oarc.ucla.edu/r/whatstat/what-statistical-analysis-should-i-usestatistical-analyses-using-r/)

## Learning Objectives

1.  Identify important descriptive statistics and visualize data from a continuous variable

2.  Identify important distributions that will be used in 512/612

3.  Use our previous tools in 511 to estimate a parameter and construct a confidence interval

::: lob
4.  Use our previous tools in 511 to conduct a hypothesis test
:::

5.  Define error rates and power


## Reference: Steps in a Hypothesis Test

1.  Check the [**assumptions**]{style="color:#4FADF3"}

    - What sampling distribution are you using? What assumptions are required for it?

2.  Set the [**level of significance**]{style="color:#4FADF3"} $\alpha$

3.  Specify the [**null**]{style="color:#4FADF3"} ( $H_0$ ) and [**alternative**]{style="color:#4FADF3"} ( $H_A$ ) [**hypotheses**]{style="color:#4FADF3"}

    -  In symbols and/or in words
    -  Alternative: one- or two-sided?

4.  Calculate the [**test statistic**]{style="color:#4FADF3"}.

5.  Calculate the [**p-value**]{style="color:#4FADF3"} based on the observed test statistic and its sampling distribution

6.  Write a [**conclusion**]{style="color:#4FADF3"} to the hypothesis test

    -  Do we reject or fail to reject $H_0$?
    -  Write a conclusion in the context of the problem
    
## Another view: Steps in a Hypothesis Test

![](../img_slides/hypothesis_test_steps.png){fig-align="center"}

## Example: one sample t-test

```{r}
#| echo: true
#| fig-align: center

BodyTemps = read.csv("data/BodyTemperatures.csv")

ggplot(data = BodyTemps, 
       aes(x = Temperature)) +
  geom_histogram() +
  theme(text = element_text(size=20)) +
  labs(x = "Temperature", y = "Count", 
       title = "Distribution of Body Temperature in Sample") +
  geom_vline(aes(xintercept = mean(BodyTemps$Temperature, na.rm = T)), 
             color = "red", linewidth = 2)
```

## Reference: what does it all look like together?

::::: example
::: ex-title
Example of hypothesis test based on the 1992 JAMA data
:::

::: ex-cont
Is there evidence to support that the population mean body temperature is different from 98.6°F?
:::
:::::

:::::: columns
::: {.column width="50%"}
1.  **Assumptions:** The individual observations are independent and the number of individuals in our sample is 130. Thus, we can use CLT to approximate the sampling distribution.

4-5. Test statistic and p-value
:::

::: {.column width="25%"}
2.  Set $\alpha = 0.05$
:::

::: {.column width="25%"}
3.  **Hypothesis:**

    \begin{aligned}
    H_0 &: \mu = 98.6\\
    \text{vs. } H_A&: \mu \neq 98.6
    \end{aligned}
:::
::::::

```{r}
#| echo: true
#| code-fold: true


temps_ttest <- t.test(x = BodyTemps$Temperature, mu = 98.6)
tidy(temps_ttest) %>% gt() %>% tab_options(table.font.size = 36)
```


6.  **Conclusion:** We reject the null hypothesis. The average body temperature in the sample was 98.25°F (95% CI 98.12, 98.38°F), which is discernibly different from 98.6°F ( $p$-value \< 0.001).

## How did we get the 95% CI?

-   The `t.test` function can help us answer this, and give us the needed information for both approaches.

```{r}
#| echo: true

BodyTemps = read.csv("data/BodyTemperatures.csv")

t.test(x = BodyTemps$Temperature, 
       # alternative = "two-sided", 
       mu = 98.6)
```

## Learning Objectives

1.  Identify important descriptive statistics and visualize data from a continuous variable

2.  Identify important distributions that will be used in 512/612

3.  Use our previous tools in 511 to estimate a parameter and construct a confidence interval

4.  Use our previous tools in 511 to conduct a hypothesis test

::: lob
5.  Define error rates and power
:::

## Outcomes of our hypothesis test

![](../img_slides/Type_1_2_error.png){fig-align="center"}

## Prabilities of outcomes

-   Type 1 error is $\alpha$

    -   The probability that we falsely reject the null hypothesis (but the null is true!!)

-   Power is $1-\beta$

    -   The probability of correctly rejecting the null hypothesis

![](../img_slides/Power.png){fig-align="center"}

## What I think is the most intuitive way to look at it

![](../img_slides/type-i-and-type-ii-error.png){fig-align="center"}

## Do your exit ticket!!

- Don't forget to go online and fill it out!

  - This will count as your attendance
  
 
  
- I look forward to the quarter with you!
